SLURM WORKLOAD START: Fri Feb 21 14:19:23 CET 2025
Fri Feb 21 14:19:23 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100 80GB PCIe          Off |   00000000:0B:00.0 Off |                    0 |
| N/A   27C    P0             43W /  300W |       0MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: mode: prune
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: model: resnet56
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: verbose: False
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: dataset: cifar100
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: dataroot: data
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: batch_size: 128
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: total_epochs: 100
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: lr_decay_milestones: 60,80
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: lr_decay_gamma: 0.1
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: lr: 0.01
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: output_dir: run/cifar100/prune/cifar100-global-random-2.0-resnet56
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: finetune: True
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: last_epochs: 100
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: reps: 1
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: method: random
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: speed_up: 2.0
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: max_pruning_ratio: 1.0
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: reg: 1e-05
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: delta_reg: 0.0001
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: weight_decay: 0.0005
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: seed: 1
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: global_pruning: True
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: sl_total_epochs: 100
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: sl_lr: 0.01
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: sl_reg_warmup: 0
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: sl_restore: None
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: iterative_steps: 400
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: logger: <Logger cifar100-global-random-2.0-resnet56 (DEBUG)>
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: device: cuda
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: num_classes: 100
[02/21 14:19:29 cifar100-global-random-2.0-resnet56]: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 14:19:33 cifar100-global-random-2.0-resnet56]: Pruning...
[02/21 14:19:39 cifar100-global-random-2.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(7, 23, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(7, 23, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(23, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(23, 55, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=55, out_features=100, bias=True)
)
[02/21 14:19:42 cifar100-global-random-2.0-resnet56]: Params: 0.86 M => 0.57 M (66.73%)
[02/21 14:19:42 cifar100-global-random-2.0-resnet56]: FLOPs: 127.12 M => 60.68 M (47.73%, 2.09X )
[02/21 14:19:42 cifar100-global-random-2.0-resnet56]: Acc: 0.7269 => 0.0172
[02/21 14:19:42 cifar100-global-random-2.0-resnet56]: Val Loss: 1.1578 => 12.2143
[02/21 14:19:42 cifar100-global-random-2.0-resnet56]: Finetuning...
[02/21 14:20:09 cifar100-global-random-2.0-resnet56]: Epoch 0/100, Acc=0.5412, Val Loss=1.7476, lr=0.0100
[02/21 14:20:36 cifar100-global-random-2.0-resnet56]: Epoch 1/100, Acc=0.5396, Val Loss=1.7866, lr=0.0100
[02/21 14:21:03 cifar100-global-random-2.0-resnet56]: Epoch 2/100, Acc=0.5666, Val Loss=1.6627, lr=0.0100
[02/21 14:21:30 cifar100-global-random-2.0-resnet56]: Epoch 3/100, Acc=0.6200, Val Loss=1.4005, lr=0.0100
[02/21 14:21:57 cifar100-global-random-2.0-resnet56]: Epoch 4/100, Acc=0.6119, Val Loss=1.4466, lr=0.0100
[02/21 14:22:24 cifar100-global-random-2.0-resnet56]: Epoch 5/100, Acc=0.6174, Val Loss=1.4436, lr=0.0100
[02/21 14:22:51 cifar100-global-random-2.0-resnet56]: Epoch 6/100, Acc=0.6196, Val Loss=1.4390, lr=0.0100
[02/21 14:23:18 cifar100-global-random-2.0-resnet56]: Epoch 7/100, Acc=0.6053, Val Loss=1.4777, lr=0.0100
[02/21 14:23:45 cifar100-global-random-2.0-resnet56]: Epoch 8/100, Acc=0.6083, Val Loss=1.4667, lr=0.0100
[02/21 14:24:12 cifar100-global-random-2.0-resnet56]: Epoch 9/100, Acc=0.6120, Val Loss=1.5159, lr=0.0100
[02/21 14:24:39 cifar100-global-random-2.0-resnet56]: Epoch 10/100, Acc=0.5985, Val Loss=1.5736, lr=0.0100
[02/21 14:25:06 cifar100-global-random-2.0-resnet56]: Epoch 11/100, Acc=0.5744, Val Loss=1.6748, lr=0.0100
[02/21 14:25:33 cifar100-global-random-2.0-resnet56]: Epoch 12/100, Acc=0.6193, Val Loss=1.4326, lr=0.0100
[02/21 14:26:00 cifar100-global-random-2.0-resnet56]: Epoch 13/100, Acc=0.6405, Val Loss=1.3329, lr=0.0100
[02/21 14:26:27 cifar100-global-random-2.0-resnet56]: Epoch 14/100, Acc=0.6426, Val Loss=1.3258, lr=0.0100
[02/21 14:26:54 cifar100-global-random-2.0-resnet56]: Epoch 15/100, Acc=0.6373, Val Loss=1.3749, lr=0.0100
[02/21 14:27:21 cifar100-global-random-2.0-resnet56]: Epoch 16/100, Acc=0.6218, Val Loss=1.4681, lr=0.0100
[02/21 14:27:48 cifar100-global-random-2.0-resnet56]: Epoch 17/100, Acc=0.6352, Val Loss=1.4088, lr=0.0100
[02/21 14:28:15 cifar100-global-random-2.0-resnet56]: Epoch 18/100, Acc=0.6397, Val Loss=1.3995, lr=0.0100
[02/21 14:28:42 cifar100-global-random-2.0-resnet56]: Epoch 19/100, Acc=0.6360, Val Loss=1.3875, lr=0.0100
[02/21 14:29:09 cifar100-global-random-2.0-resnet56]: Epoch 20/100, Acc=0.6473, Val Loss=1.3713, lr=0.0100
[02/21 14:29:37 cifar100-global-random-2.0-resnet56]: Epoch 21/100, Acc=0.6246, Val Loss=1.4439, lr=0.0100
[02/21 14:30:04 cifar100-global-random-2.0-resnet56]: Epoch 22/100, Acc=0.6375, Val Loss=1.4175, lr=0.0100
[02/21 14:30:31 cifar100-global-random-2.0-resnet56]: Epoch 23/100, Acc=0.6039, Val Loss=1.5618, lr=0.0100
[02/21 14:30:59 cifar100-global-random-2.0-resnet56]: Epoch 24/100, Acc=0.6339, Val Loss=1.4161, lr=0.0100
[02/21 14:31:26 cifar100-global-random-2.0-resnet56]: Epoch 25/100, Acc=0.6161, Val Loss=1.4991, lr=0.0100
[02/21 14:31:53 cifar100-global-random-2.0-resnet56]: Epoch 26/100, Acc=0.6308, Val Loss=1.4585, lr=0.0100
[02/21 14:32:21 cifar100-global-random-2.0-resnet56]: Epoch 27/100, Acc=0.6395, Val Loss=1.4260, lr=0.0100
[02/21 14:32:48 cifar100-global-random-2.0-resnet56]: Epoch 28/100, Acc=0.6286, Val Loss=1.4465, lr=0.0100
[02/21 14:33:16 cifar100-global-random-2.0-resnet56]: Epoch 29/100, Acc=0.6390, Val Loss=1.4062, lr=0.0100
[02/21 14:33:43 cifar100-global-random-2.0-resnet56]: Epoch 30/100, Acc=0.6215, Val Loss=1.5258, lr=0.0100
[02/21 14:34:10 cifar100-global-random-2.0-resnet56]: Epoch 31/100, Acc=0.6436, Val Loss=1.3695, lr=0.0100
[02/21 14:34:38 cifar100-global-random-2.0-resnet56]: Epoch 32/100, Acc=0.6461, Val Loss=1.3966, lr=0.0100
[02/21 14:35:05 cifar100-global-random-2.0-resnet56]: Epoch 33/100, Acc=0.6191, Val Loss=1.5130, lr=0.0100
[02/21 14:35:32 cifar100-global-random-2.0-resnet56]: Epoch 34/100, Acc=0.6356, Val Loss=1.4561, lr=0.0100
[02/21 14:35:59 cifar100-global-random-2.0-resnet56]: Epoch 35/100, Acc=0.6348, Val Loss=1.4171, lr=0.0100
[02/21 14:36:26 cifar100-global-random-2.0-resnet56]: Epoch 36/100, Acc=0.6412, Val Loss=1.4578, lr=0.0100
[02/21 14:36:53 cifar100-global-random-2.0-resnet56]: Epoch 37/100, Acc=0.6371, Val Loss=1.4146, lr=0.0100
[02/21 14:37:20 cifar100-global-random-2.0-resnet56]: Epoch 38/100, Acc=0.6468, Val Loss=1.3784, lr=0.0100
[02/21 14:37:47 cifar100-global-random-2.0-resnet56]: Epoch 39/100, Acc=0.6532, Val Loss=1.3484, lr=0.0100
[02/21 14:38:14 cifar100-global-random-2.0-resnet56]: Epoch 40/100, Acc=0.6368, Val Loss=1.4320, lr=0.0100
[02/21 14:38:41 cifar100-global-random-2.0-resnet56]: Epoch 41/100, Acc=0.6245, Val Loss=1.4998, lr=0.0100
[02/21 14:39:08 cifar100-global-random-2.0-resnet56]: Epoch 42/100, Acc=0.6398, Val Loss=1.3989, lr=0.0100
[02/21 14:39:35 cifar100-global-random-2.0-resnet56]: Epoch 43/100, Acc=0.6507, Val Loss=1.3323, lr=0.0100
[02/21 14:40:02 cifar100-global-random-2.0-resnet56]: Epoch 44/100, Acc=0.6116, Val Loss=1.5953, lr=0.0100
[02/21 14:40:30 cifar100-global-random-2.0-resnet56]: Epoch 45/100, Acc=0.6443, Val Loss=1.3970, lr=0.0100
[02/21 14:40:57 cifar100-global-random-2.0-resnet56]: Epoch 46/100, Acc=0.6452, Val Loss=1.3994, lr=0.0100
[02/21 14:41:24 cifar100-global-random-2.0-resnet56]: Epoch 47/100, Acc=0.6385, Val Loss=1.4416, lr=0.0100
[02/21 14:41:51 cifar100-global-random-2.0-resnet56]: Epoch 48/100, Acc=0.6367, Val Loss=1.4479, lr=0.0100
[02/21 14:42:19 cifar100-global-random-2.0-resnet56]: Epoch 49/100, Acc=0.6280, Val Loss=1.4882, lr=0.0100
[02/21 14:42:46 cifar100-global-random-2.0-resnet56]: Epoch 50/100, Acc=0.6208, Val Loss=1.5264, lr=0.0100
[02/21 14:43:13 cifar100-global-random-2.0-resnet56]: Epoch 51/100, Acc=0.6253, Val Loss=1.4575, lr=0.0100
[02/21 14:43:41 cifar100-global-random-2.0-resnet56]: Epoch 52/100, Acc=0.6380, Val Loss=1.4558, lr=0.0100
[02/21 14:44:08 cifar100-global-random-2.0-resnet56]: Epoch 53/100, Acc=0.6293, Val Loss=1.5075, lr=0.0100
[02/21 14:44:35 cifar100-global-random-2.0-resnet56]: Epoch 54/100, Acc=0.6402, Val Loss=1.4724, lr=0.0100
[02/21 14:45:02 cifar100-global-random-2.0-resnet56]: Epoch 55/100, Acc=0.6406, Val Loss=1.4080, lr=0.0100
[02/21 14:45:29 cifar100-global-random-2.0-resnet56]: Epoch 56/100, Acc=0.6529, Val Loss=1.3838, lr=0.0100
[02/21 14:45:56 cifar100-global-random-2.0-resnet56]: Epoch 57/100, Acc=0.6228, Val Loss=1.5368, lr=0.0100
[02/21 14:46:23 cifar100-global-random-2.0-resnet56]: Epoch 58/100, Acc=0.6234, Val Loss=1.5308, lr=0.0100
[02/21 14:46:50 cifar100-global-random-2.0-resnet56]: Epoch 59/100, Acc=0.6398, Val Loss=1.4554, lr=0.0100
[02/21 14:47:17 cifar100-global-random-2.0-resnet56]: Epoch 60/100, Acc=0.7038, Val Loss=1.1333, lr=0.0010
[02/21 14:47:44 cifar100-global-random-2.0-resnet56]: Epoch 61/100, Acc=0.7045, Val Loss=1.1406, lr=0.0010
[02/21 14:48:11 cifar100-global-random-2.0-resnet56]: Epoch 62/100, Acc=0.7050, Val Loss=1.1449, lr=0.0010
[02/21 14:48:39 cifar100-global-random-2.0-resnet56]: Epoch 63/100, Acc=0.7084, Val Loss=1.1507, lr=0.0010
[02/21 14:49:06 cifar100-global-random-2.0-resnet56]: Epoch 64/100, Acc=0.7059, Val Loss=1.1587, lr=0.0010
[02/21 14:49:34 cifar100-global-random-2.0-resnet56]: Epoch 65/100, Acc=0.7088, Val Loss=1.1544, lr=0.0010
[02/21 14:50:01 cifar100-global-random-2.0-resnet56]: Epoch 66/100, Acc=0.7091, Val Loss=1.1726, lr=0.0010
[02/21 14:50:29 cifar100-global-random-2.0-resnet56]: Epoch 67/100, Acc=0.7060, Val Loss=1.1739, lr=0.0010
[02/21 14:50:56 cifar100-global-random-2.0-resnet56]: Epoch 68/100, Acc=0.7075, Val Loss=1.1733, lr=0.0010
[02/21 14:51:24 cifar100-global-random-2.0-resnet56]: Epoch 69/100, Acc=0.7072, Val Loss=1.1889, lr=0.0010
[02/21 14:51:52 cifar100-global-random-2.0-resnet56]: Epoch 70/100, Acc=0.7099, Val Loss=1.1892, lr=0.0010
[02/21 14:52:19 cifar100-global-random-2.0-resnet56]: Epoch 71/100, Acc=0.7070, Val Loss=1.1924, lr=0.0010
[02/21 14:52:46 cifar100-global-random-2.0-resnet56]: Epoch 72/100, Acc=0.7069, Val Loss=1.1890, lr=0.0010
[02/21 14:53:13 cifar100-global-random-2.0-resnet56]: Epoch 73/100, Acc=0.7073, Val Loss=1.2031, lr=0.0010
[02/21 14:53:41 cifar100-global-random-2.0-resnet56]: Epoch 74/100, Acc=0.7048, Val Loss=1.2046, lr=0.0010
[02/21 14:54:08 cifar100-global-random-2.0-resnet56]: Epoch 75/100, Acc=0.7020, Val Loss=1.2127, lr=0.0010
[02/21 14:54:35 cifar100-global-random-2.0-resnet56]: Epoch 76/100, Acc=0.7055, Val Loss=1.2126, lr=0.0010
[02/21 14:55:02 cifar100-global-random-2.0-resnet56]: Epoch 77/100, Acc=0.7038, Val Loss=1.2241, lr=0.0010
[02/21 14:55:29 cifar100-global-random-2.0-resnet56]: Epoch 78/100, Acc=0.7035, Val Loss=1.2221, lr=0.0010
[02/21 14:55:56 cifar100-global-random-2.0-resnet56]: Epoch 79/100, Acc=0.7025, Val Loss=1.2272, lr=0.0010
[02/21 14:56:23 cifar100-global-random-2.0-resnet56]: Epoch 80/100, Acc=0.7080, Val Loss=1.2158, lr=0.0001
[02/21 14:56:50 cifar100-global-random-2.0-resnet56]: Epoch 81/100, Acc=0.7073, Val Loss=1.2184, lr=0.0001
[02/21 14:57:17 cifar100-global-random-2.0-resnet56]: Epoch 82/100, Acc=0.7070, Val Loss=1.2134, lr=0.0001
[02/21 14:57:44 cifar100-global-random-2.0-resnet56]: Epoch 83/100, Acc=0.7061, Val Loss=1.2149, lr=0.0001
[02/21 14:58:12 cifar100-global-random-2.0-resnet56]: Epoch 84/100, Acc=0.7069, Val Loss=1.2181, lr=0.0001
[02/21 14:58:39 cifar100-global-random-2.0-resnet56]: Epoch 85/100, Acc=0.7065, Val Loss=1.2162, lr=0.0001
[02/21 14:59:06 cifar100-global-random-2.0-resnet56]: Epoch 86/100, Acc=0.7061, Val Loss=1.2218, lr=0.0001
[02/21 14:59:33 cifar100-global-random-2.0-resnet56]: Epoch 87/100, Acc=0.7075, Val Loss=1.2158, lr=0.0001
[02/21 15:00:00 cifar100-global-random-2.0-resnet56]: Epoch 88/100, Acc=0.7064, Val Loss=1.2140, lr=0.0001
[02/21 15:00:27 cifar100-global-random-2.0-resnet56]: Epoch 89/100, Acc=0.7079, Val Loss=1.2163, lr=0.0001
[02/21 15:00:54 cifar100-global-random-2.0-resnet56]: Epoch 90/100, Acc=0.7073, Val Loss=1.2153, lr=0.0001
[02/21 15:01:21 cifar100-global-random-2.0-resnet56]: Epoch 91/100, Acc=0.7048, Val Loss=1.2256, lr=0.0001
[02/21 15:01:48 cifar100-global-random-2.0-resnet56]: Epoch 92/100, Acc=0.7069, Val Loss=1.2144, lr=0.0001
[02/21 15:02:15 cifar100-global-random-2.0-resnet56]: Epoch 93/100, Acc=0.7075, Val Loss=1.2199, lr=0.0001
[02/21 15:02:42 cifar100-global-random-2.0-resnet56]: Epoch 94/100, Acc=0.7079, Val Loss=1.2179, lr=0.0001
[02/21 15:03:09 cifar100-global-random-2.0-resnet56]: Epoch 95/100, Acc=0.7077, Val Loss=1.2217, lr=0.0001
[02/21 15:03:36 cifar100-global-random-2.0-resnet56]: Epoch 96/100, Acc=0.7060, Val Loss=1.2229, lr=0.0001
[02/21 15:04:03 cifar100-global-random-2.0-resnet56]: Epoch 97/100, Acc=0.7061, Val Loss=1.2213, lr=0.0001
[02/21 15:04:30 cifar100-global-random-2.0-resnet56]: Epoch 98/100, Acc=0.7069, Val Loss=1.2189, lr=0.0001
[02/21 15:04:57 cifar100-global-random-2.0-resnet56]: Epoch 99/100, Acc=0.7066, Val Loss=1.2197, lr=0.0001
[02/21 15:04:57 cifar100-global-random-2.0-resnet56]: Best Acc=0.7099
[02/21 15:04:57 cifar100-global-random-2.0-resnet56]: Params: 0.57 M
[02/21 15:04:57 cifar100-global-random-2.0-resnet56]: ops: 60.68 M
[02/21 15:05:00 cifar100-global-random-2.0-resnet56]: Acc: 0.7066 Val Loss: 1.2197

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: mode: prune
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: model: resnet56
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: verbose: False
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: dataset: cifar100
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: dataroot: data
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: batch_size: 128
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: total_epochs: 100
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: lr_decay_milestones: 60,80
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: lr_decay_gamma: 0.1
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: lr: 0.01
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: output_dir: run/cifar100/prune/cifar100-global-l2-2.0-resnet56
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: finetune: True
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: last_epochs: 100
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: reps: 1
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: method: l2
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: speed_up: 2.0
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: max_pruning_ratio: 1.0
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: reg: 1e-05
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: delta_reg: 0.0001
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: weight_decay: 0.0005
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: seed: 1
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: global_pruning: True
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: sl_total_epochs: 100
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: sl_lr: 0.01
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: sl_reg_warmup: 0
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: sl_restore: None
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: iterative_steps: 400
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: logger: <Logger cifar100-global-l2-2.0-resnet56 (DEBUG)>
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: device: cuda
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: num_classes: 100
[02/21 15:05:08 cifar100-global-l2-2.0-resnet56]: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 15:05:11 cifar100-global-l2-2.0-resnet56]: Pruning...
[02/21 15:05:21 cifar100-global-l2-2.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(11, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(11, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(11, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(11, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(11, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(11, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(11, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(11, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(11, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(11, 25, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(11, 22, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(22, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(22, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(22, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(22, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(22, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(22, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(22, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(22, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(51, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(64, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=100, bias=True)
)
[02/21 15:05:24 cifar100-global-l2-2.0-resnet56]: Params: 0.86 M => 0.68 M (79.43%)
[02/21 15:05:24 cifar100-global-l2-2.0-resnet56]: FLOPs: 127.12 M => 63.42 M (49.89%, 2.00X )
[02/21 15:05:24 cifar100-global-l2-2.0-resnet56]: Acc: 0.7269 => 0.0168
[02/21 15:05:24 cifar100-global-l2-2.0-resnet56]: Val Loss: 1.1578 => 14.3494
[02/21 15:05:24 cifar100-global-l2-2.0-resnet56]: Finetuning...
[02/21 15:05:52 cifar100-global-l2-2.0-resnet56]: Epoch 0/100, Acc=0.5982, Val Loss=1.6272, lr=0.0100
[02/21 15:06:20 cifar100-global-l2-2.0-resnet56]: Epoch 1/100, Acc=0.6372, Val Loss=1.4221, lr=0.0100
[02/21 15:06:48 cifar100-global-l2-2.0-resnet56]: Epoch 2/100, Acc=0.6444, Val Loss=1.3491, lr=0.0100
[02/21 15:07:16 cifar100-global-l2-2.0-resnet56]: Epoch 3/100, Acc=0.6434, Val Loss=1.3872, lr=0.0100
[02/21 15:07:43 cifar100-global-l2-2.0-resnet56]: Epoch 4/100, Acc=0.6442, Val Loss=1.3610, lr=0.0100
[02/21 15:08:11 cifar100-global-l2-2.0-resnet56]: Epoch 5/100, Acc=0.6505, Val Loss=1.3419, lr=0.0100
[02/21 15:08:39 cifar100-global-l2-2.0-resnet56]: Epoch 6/100, Acc=0.6518, Val Loss=1.3818, lr=0.0100
[02/21 15:09:07 cifar100-global-l2-2.0-resnet56]: Epoch 7/100, Acc=0.6399, Val Loss=1.3979, lr=0.0100
[02/21 15:09:35 cifar100-global-l2-2.0-resnet56]: Epoch 8/100, Acc=0.6401, Val Loss=1.4349, lr=0.0100
[02/21 15:10:03 cifar100-global-l2-2.0-resnet56]: Epoch 9/100, Acc=0.6385, Val Loss=1.4463, lr=0.0100
[02/21 15:10:31 cifar100-global-l2-2.0-resnet56]: Epoch 10/100, Acc=0.6453, Val Loss=1.4176, lr=0.0100
[02/21 15:10:59 cifar100-global-l2-2.0-resnet56]: Epoch 11/100, Acc=0.6603, Val Loss=1.3529, lr=0.0100
[02/21 15:11:27 cifar100-global-l2-2.0-resnet56]: Epoch 12/100, Acc=0.6454, Val Loss=1.3645, lr=0.0100
[02/21 15:11:54 cifar100-global-l2-2.0-resnet56]: Epoch 13/100, Acc=0.6685, Val Loss=1.2940, lr=0.0100
[02/21 15:12:22 cifar100-global-l2-2.0-resnet56]: Epoch 14/100, Acc=0.6541, Val Loss=1.3383, lr=0.0100
[02/21 15:12:49 cifar100-global-l2-2.0-resnet56]: Epoch 15/100, Acc=0.6589, Val Loss=1.3137, lr=0.0100
[02/21 15:13:17 cifar100-global-l2-2.0-resnet56]: Epoch 16/100, Acc=0.6522, Val Loss=1.3770, lr=0.0100
[02/21 15:13:44 cifar100-global-l2-2.0-resnet56]: Epoch 17/100, Acc=0.6499, Val Loss=1.4049, lr=0.0100
[02/21 15:14:12 cifar100-global-l2-2.0-resnet56]: Epoch 18/100, Acc=0.6572, Val Loss=1.3429, lr=0.0100
[02/21 15:14:39 cifar100-global-l2-2.0-resnet56]: Epoch 19/100, Acc=0.6575, Val Loss=1.3525, lr=0.0100
[02/21 15:15:07 cifar100-global-l2-2.0-resnet56]: Epoch 20/100, Acc=0.6501, Val Loss=1.4187, lr=0.0100
[02/21 15:15:35 cifar100-global-l2-2.0-resnet56]: Epoch 21/100, Acc=0.6624, Val Loss=1.3619, lr=0.0100
[02/21 15:16:02 cifar100-global-l2-2.0-resnet56]: Epoch 22/100, Acc=0.6477, Val Loss=1.4178, lr=0.0100
[02/21 15:16:30 cifar100-global-l2-2.0-resnet56]: Epoch 23/100, Acc=0.6469, Val Loss=1.4225, lr=0.0100
[02/21 15:16:57 cifar100-global-l2-2.0-resnet56]: Epoch 24/100, Acc=0.6583, Val Loss=1.3751, lr=0.0100
[02/21 15:17:25 cifar100-global-l2-2.0-resnet56]: Epoch 25/100, Acc=0.6555, Val Loss=1.3779, lr=0.0100
[02/21 15:17:52 cifar100-global-l2-2.0-resnet56]: Epoch 26/100, Acc=0.6644, Val Loss=1.3203, lr=0.0100
[02/21 15:18:20 cifar100-global-l2-2.0-resnet56]: Epoch 27/100, Acc=0.6497, Val Loss=1.4455, lr=0.0100
[02/21 15:18:48 cifar100-global-l2-2.0-resnet56]: Epoch 28/100, Acc=0.6557, Val Loss=1.4034, lr=0.0100
[02/21 15:19:15 cifar100-global-l2-2.0-resnet56]: Epoch 29/100, Acc=0.6585, Val Loss=1.3909, lr=0.0100
[02/21 15:19:43 cifar100-global-l2-2.0-resnet56]: Epoch 30/100, Acc=0.6476, Val Loss=1.4350, lr=0.0100
[02/21 15:20:11 cifar100-global-l2-2.0-resnet56]: Epoch 31/100, Acc=0.6465, Val Loss=1.4493, lr=0.0100
[02/21 15:20:39 cifar100-global-l2-2.0-resnet56]: Epoch 32/100, Acc=0.6400, Val Loss=1.4926, lr=0.0100
[02/21 15:21:06 cifar100-global-l2-2.0-resnet56]: Epoch 33/100, Acc=0.6489, Val Loss=1.4466, lr=0.0100
[02/21 15:21:34 cifar100-global-l2-2.0-resnet56]: Epoch 34/100, Acc=0.6568, Val Loss=1.3642, lr=0.0100
[02/21 15:22:01 cifar100-global-l2-2.0-resnet56]: Epoch 35/100, Acc=0.6549, Val Loss=1.4088, lr=0.0100
[02/21 15:22:29 cifar100-global-l2-2.0-resnet56]: Epoch 36/100, Acc=0.6526, Val Loss=1.4264, lr=0.0100
[02/21 15:22:56 cifar100-global-l2-2.0-resnet56]: Epoch 37/100, Acc=0.6534, Val Loss=1.4127, lr=0.0100
[02/21 15:23:24 cifar100-global-l2-2.0-resnet56]: Epoch 38/100, Acc=0.6407, Val Loss=1.5247, lr=0.0100
[02/21 15:23:51 cifar100-global-l2-2.0-resnet56]: Epoch 39/100, Acc=0.6576, Val Loss=1.3666, lr=0.0100
[02/21 15:24:19 cifar100-global-l2-2.0-resnet56]: Epoch 40/100, Acc=0.6564, Val Loss=1.3890, lr=0.0100
[02/21 15:24:47 cifar100-global-l2-2.0-resnet56]: Epoch 41/100, Acc=0.6653, Val Loss=1.3819, lr=0.0100
[02/21 15:25:14 cifar100-global-l2-2.0-resnet56]: Epoch 42/100, Acc=0.6521, Val Loss=1.4542, lr=0.0100
[02/21 15:25:42 cifar100-global-l2-2.0-resnet56]: Epoch 43/100, Acc=0.6459, Val Loss=1.4401, lr=0.0100
[02/21 15:26:10 cifar100-global-l2-2.0-resnet56]: Epoch 44/100, Acc=0.6562, Val Loss=1.3969, lr=0.0100
[02/21 15:26:38 cifar100-global-l2-2.0-resnet56]: Epoch 45/100, Acc=0.6554, Val Loss=1.4042, lr=0.0100
[02/21 15:27:05 cifar100-global-l2-2.0-resnet56]: Epoch 46/100, Acc=0.6553, Val Loss=1.3923, lr=0.0100
[02/21 15:27:33 cifar100-global-l2-2.0-resnet56]: Epoch 47/100, Acc=0.6509, Val Loss=1.4204, lr=0.0100
[02/21 15:28:01 cifar100-global-l2-2.0-resnet56]: Epoch 48/100, Acc=0.6538, Val Loss=1.4237, lr=0.0100
[02/21 15:28:28 cifar100-global-l2-2.0-resnet56]: Epoch 49/100, Acc=0.6360, Val Loss=1.5108, lr=0.0100
[02/21 15:28:56 cifar100-global-l2-2.0-resnet56]: Epoch 50/100, Acc=0.6587, Val Loss=1.4021, lr=0.0100
[02/21 15:29:23 cifar100-global-l2-2.0-resnet56]: Epoch 51/100, Acc=0.6520, Val Loss=1.4317, lr=0.0100
[02/21 15:29:51 cifar100-global-l2-2.0-resnet56]: Epoch 52/100, Acc=0.6470, Val Loss=1.4806, lr=0.0100
[02/21 15:30:19 cifar100-global-l2-2.0-resnet56]: Epoch 53/100, Acc=0.6505, Val Loss=1.4494, lr=0.0100
[02/21 15:30:46 cifar100-global-l2-2.0-resnet56]: Epoch 54/100, Acc=0.6446, Val Loss=1.4972, lr=0.0100
[02/21 15:31:14 cifar100-global-l2-2.0-resnet56]: Epoch 55/100, Acc=0.6566, Val Loss=1.4094, lr=0.0100
[02/21 15:31:41 cifar100-global-l2-2.0-resnet56]: Epoch 56/100, Acc=0.6433, Val Loss=1.4739, lr=0.0100
[02/21 15:32:09 cifar100-global-l2-2.0-resnet56]: Epoch 57/100, Acc=0.6427, Val Loss=1.5209, lr=0.0100
[02/21 15:32:36 cifar100-global-l2-2.0-resnet56]: Epoch 58/100, Acc=0.6541, Val Loss=1.4109, lr=0.0100
[02/21 15:33:04 cifar100-global-l2-2.0-resnet56]: Epoch 59/100, Acc=0.6500, Val Loss=1.4689, lr=0.0100
[02/21 15:33:31 cifar100-global-l2-2.0-resnet56]: Epoch 60/100, Acc=0.7111, Val Loss=1.1621, lr=0.0010
[02/21 15:33:59 cifar100-global-l2-2.0-resnet56]: Epoch 61/100, Acc=0.7143, Val Loss=1.1721, lr=0.0010
[02/21 15:34:27 cifar100-global-l2-2.0-resnet56]: Epoch 62/100, Acc=0.7114, Val Loss=1.1738, lr=0.0010
[02/21 15:34:54 cifar100-global-l2-2.0-resnet56]: Epoch 63/100, Acc=0.7118, Val Loss=1.1840, lr=0.0010
[02/21 15:35:22 cifar100-global-l2-2.0-resnet56]: Epoch 64/100, Acc=0.7125, Val Loss=1.1893, lr=0.0010
[02/21 15:35:49 cifar100-global-l2-2.0-resnet56]: Epoch 65/100, Acc=0.7113, Val Loss=1.1897, lr=0.0010
[02/21 15:36:17 cifar100-global-l2-2.0-resnet56]: Epoch 66/100, Acc=0.7108, Val Loss=1.2034, lr=0.0010
[02/21 15:36:45 cifar100-global-l2-2.0-resnet56]: Epoch 67/100, Acc=0.7101, Val Loss=1.2024, lr=0.0010
[02/21 15:37:12 cifar100-global-l2-2.0-resnet56]: Epoch 68/100, Acc=0.7094, Val Loss=1.2094, lr=0.0010
[02/21 15:37:40 cifar100-global-l2-2.0-resnet56]: Epoch 69/100, Acc=0.7095, Val Loss=1.2142, lr=0.0010
[02/21 15:38:07 cifar100-global-l2-2.0-resnet56]: Epoch 70/100, Acc=0.7117, Val Loss=1.2134, lr=0.0010
[02/21 15:38:35 cifar100-global-l2-2.0-resnet56]: Epoch 71/100, Acc=0.7118, Val Loss=1.2181, lr=0.0010
[02/21 15:39:03 cifar100-global-l2-2.0-resnet56]: Epoch 72/100, Acc=0.7095, Val Loss=1.2272, lr=0.0010
[02/21 15:39:30 cifar100-global-l2-2.0-resnet56]: Epoch 73/100, Acc=0.7083, Val Loss=1.2405, lr=0.0010
[02/21 15:39:58 cifar100-global-l2-2.0-resnet56]: Epoch 74/100, Acc=0.7089, Val Loss=1.2303, lr=0.0010
[02/21 15:40:25 cifar100-global-l2-2.0-resnet56]: Epoch 75/100, Acc=0.7123, Val Loss=1.2335, lr=0.0010
[02/21 15:40:53 cifar100-global-l2-2.0-resnet56]: Epoch 76/100, Acc=0.7110, Val Loss=1.2417, lr=0.0010
[02/21 15:41:20 cifar100-global-l2-2.0-resnet56]: Epoch 77/100, Acc=0.7085, Val Loss=1.2452, lr=0.0010
[02/21 15:41:48 cifar100-global-l2-2.0-resnet56]: Epoch 78/100, Acc=0.7073, Val Loss=1.2525, lr=0.0010
[02/21 15:42:15 cifar100-global-l2-2.0-resnet56]: Epoch 79/100, Acc=0.7123, Val Loss=1.2543, lr=0.0010
[02/21 15:42:43 cifar100-global-l2-2.0-resnet56]: Epoch 80/100, Acc=0.7106, Val Loss=1.2446, lr=0.0001
[02/21 15:43:10 cifar100-global-l2-2.0-resnet56]: Epoch 81/100, Acc=0.7113, Val Loss=1.2447, lr=0.0001
[02/21 15:43:38 cifar100-global-l2-2.0-resnet56]: Epoch 82/100, Acc=0.7106, Val Loss=1.2429, lr=0.0001
[02/21 15:44:06 cifar100-global-l2-2.0-resnet56]: Epoch 83/100, Acc=0.7100, Val Loss=1.2452, lr=0.0001
[02/21 15:44:33 cifar100-global-l2-2.0-resnet56]: Epoch 84/100, Acc=0.7100, Val Loss=1.2492, lr=0.0001
[02/21 15:45:01 cifar100-global-l2-2.0-resnet56]: Epoch 85/100, Acc=0.7091, Val Loss=1.2445, lr=0.0001
[02/21 15:45:29 cifar100-global-l2-2.0-resnet56]: Epoch 86/100, Acc=0.7102, Val Loss=1.2494, lr=0.0001
[02/21 15:45:56 cifar100-global-l2-2.0-resnet56]: Epoch 87/100, Acc=0.7098, Val Loss=1.2508, lr=0.0001
[02/21 15:46:24 cifar100-global-l2-2.0-resnet56]: Epoch 88/100, Acc=0.7087, Val Loss=1.2434, lr=0.0001
[02/21 15:46:51 cifar100-global-l2-2.0-resnet56]: Epoch 89/100, Acc=0.7105, Val Loss=1.2468, lr=0.0001
[02/21 15:47:19 cifar100-global-l2-2.0-resnet56]: Epoch 90/100, Acc=0.7078, Val Loss=1.2442, lr=0.0001
[02/21 15:47:46 cifar100-global-l2-2.0-resnet56]: Epoch 91/100, Acc=0.7080, Val Loss=1.2556, lr=0.0001
[02/21 15:48:14 cifar100-global-l2-2.0-resnet56]: Epoch 92/100, Acc=0.7097, Val Loss=1.2467, lr=0.0001
[02/21 15:48:41 cifar100-global-l2-2.0-resnet56]: Epoch 93/100, Acc=0.7100, Val Loss=1.2496, lr=0.0001
[02/21 15:49:09 cifar100-global-l2-2.0-resnet56]: Epoch 94/100, Acc=0.7104, Val Loss=1.2455, lr=0.0001
[02/21 15:49:37 cifar100-global-l2-2.0-resnet56]: Epoch 95/100, Acc=0.7096, Val Loss=1.2512, lr=0.0001
[02/21 15:50:04 cifar100-global-l2-2.0-resnet56]: Epoch 96/100, Acc=0.7114, Val Loss=1.2543, lr=0.0001
[02/21 15:50:32 cifar100-global-l2-2.0-resnet56]: Epoch 97/100, Acc=0.7093, Val Loss=1.2486, lr=0.0001
[02/21 15:51:00 cifar100-global-l2-2.0-resnet56]: Epoch 98/100, Acc=0.7108, Val Loss=1.2494, lr=0.0001
[02/21 15:51:28 cifar100-global-l2-2.0-resnet56]: Epoch 99/100, Acc=0.7089, Val Loss=1.2498, lr=0.0001
[02/21 15:51:28 cifar100-global-l2-2.0-resnet56]: Best Acc=0.7143
[02/21 15:51:28 cifar100-global-l2-2.0-resnet56]: Params: 0.68 M
[02/21 15:51:28 cifar100-global-l2-2.0-resnet56]: ops: 63.42 M
[02/21 15:51:31 cifar100-global-l2-2.0-resnet56]: Acc: 0.7089 Val Loss: 1.2498

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: mode: prune
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: model: resnet56
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: verbose: False
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: dataset: cifar100
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: dataroot: data
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: batch_size: 128
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: total_epochs: 100
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: lr_decay_milestones: 60,80
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: lr_decay_gamma: 0.1
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: lr: 0.01
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: output_dir: run/cifar100/prune/cifar100-global-fpgm-2.0-resnet56
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: finetune: True
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: last_epochs: 100
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: reps: 1
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: method: fpgm
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: speed_up: 2.0
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: max_pruning_ratio: 1.0
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: reg: 1e-05
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: delta_reg: 0.0001
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: weight_decay: 0.0005
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: seed: 1
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: global_pruning: True
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: sl_total_epochs: 100
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: sl_lr: 0.01
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: sl_reg_warmup: 0
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: sl_restore: None
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: iterative_steps: 400
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: logger: <Logger cifar100-global-fpgm-2.0-resnet56 (DEBUG)>
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: device: cuda
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: num_classes: 100
[02/21 15:51:38 cifar100-global-fpgm-2.0-resnet56]: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 15:51:41 cifar100-global-fpgm-2.0-resnet56]: Pruning...
[02/21 15:51:54 cifar100-global-fpgm-2.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(13, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(13, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(13, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(13, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(13, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(13, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(13, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(13, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(13, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(13, 27, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(13, 17, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(17, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(17, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(17, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(17, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(17, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(17, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(17, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(17, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(17, 37, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(37, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(17, 62, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(62, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(62, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(62, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(62, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(62, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(42, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=62, out_features=100, bias=True)
)
[02/21 15:51:57 cifar100-global-fpgm-2.0-resnet56]: Params: 0.86 M => 0.62 M (71.98%)
[02/21 15:51:57 cifar100-global-fpgm-2.0-resnet56]: FLOPs: 127.12 M => 63.21 M (49.72%, 2.01X )
[02/21 15:51:57 cifar100-global-fpgm-2.0-resnet56]: Acc: 0.7269 => 0.0346
[02/21 15:51:57 cifar100-global-fpgm-2.0-resnet56]: Val Loss: 1.1578 => 8.9722
[02/21 15:51:57 cifar100-global-fpgm-2.0-resnet56]: Finetuning...
[02/21 15:52:24 cifar100-global-fpgm-2.0-resnet56]: Epoch 0/100, Acc=0.5929, Val Loss=1.5502, lr=0.0100
[02/21 15:52:52 cifar100-global-fpgm-2.0-resnet56]: Epoch 1/100, Acc=0.6030, Val Loss=1.5524, lr=0.0100
[02/21 15:53:19 cifar100-global-fpgm-2.0-resnet56]: Epoch 2/100, Acc=0.6281, Val Loss=1.4052, lr=0.0100
[02/21 15:53:46 cifar100-global-fpgm-2.0-resnet56]: Epoch 3/100, Acc=0.6479, Val Loss=1.3154, lr=0.0100
[02/21 15:54:14 cifar100-global-fpgm-2.0-resnet56]: Epoch 4/100, Acc=0.6256, Val Loss=1.4585, lr=0.0100
[02/21 15:54:42 cifar100-global-fpgm-2.0-resnet56]: Epoch 5/100, Acc=0.6406, Val Loss=1.3703, lr=0.0100
[02/21 15:55:10 cifar100-global-fpgm-2.0-resnet56]: Epoch 6/100, Acc=0.6361, Val Loss=1.4408, lr=0.0100
[02/21 15:55:37 cifar100-global-fpgm-2.0-resnet56]: Epoch 7/100, Acc=0.6286, Val Loss=1.4230, lr=0.0100
[02/21 15:56:04 cifar100-global-fpgm-2.0-resnet56]: Epoch 8/100, Acc=0.6349, Val Loss=1.4313, lr=0.0100
[02/21 15:56:32 cifar100-global-fpgm-2.0-resnet56]: Epoch 9/100, Acc=0.6245, Val Loss=1.4811, lr=0.0100
[02/21 15:56:59 cifar100-global-fpgm-2.0-resnet56]: Epoch 10/100, Acc=0.5966, Val Loss=1.6281, lr=0.0100
[02/21 15:57:26 cifar100-global-fpgm-2.0-resnet56]: Epoch 11/100, Acc=0.6184, Val Loss=1.4765, lr=0.0100
[02/21 15:57:54 cifar100-global-fpgm-2.0-resnet56]: Epoch 12/100, Acc=0.6305, Val Loss=1.4359, lr=0.0100
[02/21 15:58:21 cifar100-global-fpgm-2.0-resnet56]: Epoch 13/100, Acc=0.6557, Val Loss=1.3329, lr=0.0100
[02/21 15:58:49 cifar100-global-fpgm-2.0-resnet56]: Epoch 14/100, Acc=0.6388, Val Loss=1.3926, lr=0.0100
[02/21 15:59:16 cifar100-global-fpgm-2.0-resnet56]: Epoch 15/100, Acc=0.6461, Val Loss=1.3800, lr=0.0100
[02/21 15:59:44 cifar100-global-fpgm-2.0-resnet56]: Epoch 16/100, Acc=0.6307, Val Loss=1.4372, lr=0.0100
[02/21 16:00:11 cifar100-global-fpgm-2.0-resnet56]: Epoch 17/100, Acc=0.6488, Val Loss=1.3658, lr=0.0100
[02/21 16:00:38 cifar100-global-fpgm-2.0-resnet56]: Epoch 18/100, Acc=0.6587, Val Loss=1.3288, lr=0.0100
[02/21 16:01:05 cifar100-global-fpgm-2.0-resnet56]: Epoch 19/100, Acc=0.6362, Val Loss=1.4724, lr=0.0100
[02/21 16:01:33 cifar100-global-fpgm-2.0-resnet56]: Epoch 20/100, Acc=0.6365, Val Loss=1.4121, lr=0.0100
[02/21 16:02:00 cifar100-global-fpgm-2.0-resnet56]: Epoch 21/100, Acc=0.6575, Val Loss=1.3451, lr=0.0100
[02/21 16:02:27 cifar100-global-fpgm-2.0-resnet56]: Epoch 22/100, Acc=0.6455, Val Loss=1.4177, lr=0.0100
[02/21 16:02:54 cifar100-global-fpgm-2.0-resnet56]: Epoch 23/100, Acc=0.6587, Val Loss=1.3210, lr=0.0100
[02/21 16:03:22 cifar100-global-fpgm-2.0-resnet56]: Epoch 24/100, Acc=0.6482, Val Loss=1.3813, lr=0.0100
[02/21 16:03:49 cifar100-global-fpgm-2.0-resnet56]: Epoch 25/100, Acc=0.6419, Val Loss=1.4407, lr=0.0100
[02/21 16:04:17 cifar100-global-fpgm-2.0-resnet56]: Epoch 26/100, Acc=0.6686, Val Loss=1.2873, lr=0.0100
[02/21 16:04:44 cifar100-global-fpgm-2.0-resnet56]: Epoch 27/100, Acc=0.6435, Val Loss=1.4552, lr=0.0100
[02/21 16:05:11 cifar100-global-fpgm-2.0-resnet56]: Epoch 28/100, Acc=0.6500, Val Loss=1.3777, lr=0.0100
[02/21 16:05:39 cifar100-global-fpgm-2.0-resnet56]: Epoch 29/100, Acc=0.6316, Val Loss=1.4832, lr=0.0100
[02/21 16:06:06 cifar100-global-fpgm-2.0-resnet56]: Epoch 30/100, Acc=0.6435, Val Loss=1.4478, lr=0.0100
[02/21 16:06:33 cifar100-global-fpgm-2.0-resnet56]: Epoch 31/100, Acc=0.6379, Val Loss=1.4332, lr=0.0100
[02/21 16:07:01 cifar100-global-fpgm-2.0-resnet56]: Epoch 32/100, Acc=0.6430, Val Loss=1.4213, lr=0.0100
[02/21 16:07:28 cifar100-global-fpgm-2.0-resnet56]: Epoch 33/100, Acc=0.6505, Val Loss=1.3890, lr=0.0100
[02/21 16:07:56 cifar100-global-fpgm-2.0-resnet56]: Epoch 34/100, Acc=0.6498, Val Loss=1.3769, lr=0.0100
[02/21 16:08:24 cifar100-global-fpgm-2.0-resnet56]: Epoch 35/100, Acc=0.6561, Val Loss=1.3576, lr=0.0100
[02/21 16:08:51 cifar100-global-fpgm-2.0-resnet56]: Epoch 36/100, Acc=0.6344, Val Loss=1.5060, lr=0.0100
[02/21 16:09:19 cifar100-global-fpgm-2.0-resnet56]: Epoch 37/100, Acc=0.6532, Val Loss=1.3571, lr=0.0100
[02/21 16:09:47 cifar100-global-fpgm-2.0-resnet56]: Epoch 38/100, Acc=0.6323, Val Loss=1.5630, lr=0.0100
[02/21 16:10:14 cifar100-global-fpgm-2.0-resnet56]: Epoch 39/100, Acc=0.6467, Val Loss=1.4236, lr=0.0100
[02/21 16:10:42 cifar100-global-fpgm-2.0-resnet56]: Epoch 40/100, Acc=0.6452, Val Loss=1.4228, lr=0.0100
[02/21 16:11:09 cifar100-global-fpgm-2.0-resnet56]: Epoch 41/100, Acc=0.6586, Val Loss=1.3737, lr=0.0100
[02/21 16:11:37 cifar100-global-fpgm-2.0-resnet56]: Epoch 42/100, Acc=0.6295, Val Loss=1.5089, lr=0.0100
[02/21 16:12:05 cifar100-global-fpgm-2.0-resnet56]: Epoch 43/100, Acc=0.6465, Val Loss=1.4233, lr=0.0100
[02/21 16:12:33 cifar100-global-fpgm-2.0-resnet56]: Epoch 44/100, Acc=0.6485, Val Loss=1.3968, lr=0.0100
[02/21 16:13:01 cifar100-global-fpgm-2.0-resnet56]: Epoch 45/100, Acc=0.6566, Val Loss=1.3808, lr=0.0100
[02/21 16:13:29 cifar100-global-fpgm-2.0-resnet56]: Epoch 46/100, Acc=0.6551, Val Loss=1.3695, lr=0.0100
[02/21 16:13:57 cifar100-global-fpgm-2.0-resnet56]: Epoch 47/100, Acc=0.6442, Val Loss=1.4586, lr=0.0100
[02/21 16:14:25 cifar100-global-fpgm-2.0-resnet56]: Epoch 48/100, Acc=0.6455, Val Loss=1.4450, lr=0.0100
[02/21 16:14:53 cifar100-global-fpgm-2.0-resnet56]: Epoch 49/100, Acc=0.6411, Val Loss=1.4719, lr=0.0100
[02/21 16:15:21 cifar100-global-fpgm-2.0-resnet56]: Epoch 50/100, Acc=0.6447, Val Loss=1.4513, lr=0.0100
[02/21 16:15:49 cifar100-global-fpgm-2.0-resnet56]: Epoch 51/100, Acc=0.6507, Val Loss=1.4115, lr=0.0100
[02/21 16:16:17 cifar100-global-fpgm-2.0-resnet56]: Epoch 52/100, Acc=0.6420, Val Loss=1.4773, lr=0.0100
[02/21 16:16:44 cifar100-global-fpgm-2.0-resnet56]: Epoch 53/100, Acc=0.6269, Val Loss=1.5619, lr=0.0100
[02/21 16:17:12 cifar100-global-fpgm-2.0-resnet56]: Epoch 54/100, Acc=0.6359, Val Loss=1.4898, lr=0.0100
[02/21 16:17:40 cifar100-global-fpgm-2.0-resnet56]: Epoch 55/100, Acc=0.6317, Val Loss=1.5227, lr=0.0100
[02/21 16:18:08 cifar100-global-fpgm-2.0-resnet56]: Epoch 56/100, Acc=0.6435, Val Loss=1.4473, lr=0.0100
[02/21 16:18:36 cifar100-global-fpgm-2.0-resnet56]: Epoch 57/100, Acc=0.6508, Val Loss=1.4299, lr=0.0100
[02/21 16:19:04 cifar100-global-fpgm-2.0-resnet56]: Epoch 58/100, Acc=0.6428, Val Loss=1.4340, lr=0.0100
[02/21 16:19:31 cifar100-global-fpgm-2.0-resnet56]: Epoch 59/100, Acc=0.6473, Val Loss=1.3950, lr=0.0100
[02/21 16:19:58 cifar100-global-fpgm-2.0-resnet56]: Epoch 60/100, Acc=0.7041, Val Loss=1.1585, lr=0.0010
[02/21 16:20:26 cifar100-global-fpgm-2.0-resnet56]: Epoch 61/100, Acc=0.7052, Val Loss=1.1634, lr=0.0010
[02/21 16:20:53 cifar100-global-fpgm-2.0-resnet56]: Epoch 62/100, Acc=0.7058, Val Loss=1.1583, lr=0.0010
[02/21 16:21:21 cifar100-global-fpgm-2.0-resnet56]: Epoch 63/100, Acc=0.7048, Val Loss=1.1660, lr=0.0010
[02/21 16:21:48 cifar100-global-fpgm-2.0-resnet56]: Epoch 64/100, Acc=0.7030, Val Loss=1.1737, lr=0.0010
[02/21 16:22:16 cifar100-global-fpgm-2.0-resnet56]: Epoch 65/100, Acc=0.7035, Val Loss=1.1750, lr=0.0010
[02/21 16:22:44 cifar100-global-fpgm-2.0-resnet56]: Epoch 66/100, Acc=0.7037, Val Loss=1.1972, lr=0.0010
[02/21 16:23:12 cifar100-global-fpgm-2.0-resnet56]: Epoch 67/100, Acc=0.7067, Val Loss=1.1940, lr=0.0010
[02/21 16:23:39 cifar100-global-fpgm-2.0-resnet56]: Epoch 68/100, Acc=0.7046, Val Loss=1.2016, lr=0.0010
[02/21 16:24:06 cifar100-global-fpgm-2.0-resnet56]: Epoch 69/100, Acc=0.7025, Val Loss=1.1998, lr=0.0010
[02/21 16:24:34 cifar100-global-fpgm-2.0-resnet56]: Epoch 70/100, Acc=0.7038, Val Loss=1.2073, lr=0.0010
[02/21 16:25:01 cifar100-global-fpgm-2.0-resnet56]: Epoch 71/100, Acc=0.7058, Val Loss=1.2109, lr=0.0010
[02/21 16:25:28 cifar100-global-fpgm-2.0-resnet56]: Epoch 72/100, Acc=0.7041, Val Loss=1.2163, lr=0.0010
[02/21 16:25:56 cifar100-global-fpgm-2.0-resnet56]: Epoch 73/100, Acc=0.7038, Val Loss=1.2221, lr=0.0010
[02/21 16:26:23 cifar100-global-fpgm-2.0-resnet56]: Epoch 74/100, Acc=0.7040, Val Loss=1.2194, lr=0.0010
[02/21 16:26:50 cifar100-global-fpgm-2.0-resnet56]: Epoch 75/100, Acc=0.7055, Val Loss=1.2228, lr=0.0010
[02/21 16:27:18 cifar100-global-fpgm-2.0-resnet56]: Epoch 76/100, Acc=0.7046, Val Loss=1.2183, lr=0.0010
[02/21 16:27:45 cifar100-global-fpgm-2.0-resnet56]: Epoch 77/100, Acc=0.7076, Val Loss=1.2271, lr=0.0010
[02/21 16:28:13 cifar100-global-fpgm-2.0-resnet56]: Epoch 78/100, Acc=0.7028, Val Loss=1.2354, lr=0.0010
[02/21 16:28:41 cifar100-global-fpgm-2.0-resnet56]: Epoch 79/100, Acc=0.7053, Val Loss=1.2470, lr=0.0010
[02/21 16:29:08 cifar100-global-fpgm-2.0-resnet56]: Epoch 80/100, Acc=0.7038, Val Loss=1.2353, lr=0.0001
[02/21 16:29:35 cifar100-global-fpgm-2.0-resnet56]: Epoch 81/100, Acc=0.7047, Val Loss=1.2319, lr=0.0001
[02/21 16:30:03 cifar100-global-fpgm-2.0-resnet56]: Epoch 82/100, Acc=0.7046, Val Loss=1.2279, lr=0.0001
[02/21 16:30:30 cifar100-global-fpgm-2.0-resnet56]: Epoch 83/100, Acc=0.7040, Val Loss=1.2312, lr=0.0001
[02/21 16:30:57 cifar100-global-fpgm-2.0-resnet56]: Epoch 84/100, Acc=0.7024, Val Loss=1.2374, lr=0.0001
[02/21 16:31:25 cifar100-global-fpgm-2.0-resnet56]: Epoch 85/100, Acc=0.7041, Val Loss=1.2318, lr=0.0001
[02/21 16:31:52 cifar100-global-fpgm-2.0-resnet56]: Epoch 86/100, Acc=0.7033, Val Loss=1.2360, lr=0.0001
[02/21 16:32:20 cifar100-global-fpgm-2.0-resnet56]: Epoch 87/100, Acc=0.7045, Val Loss=1.2368, lr=0.0001
[02/21 16:32:47 cifar100-global-fpgm-2.0-resnet56]: Epoch 88/100, Acc=0.7045, Val Loss=1.2311, lr=0.0001
[02/21 16:33:14 cifar100-global-fpgm-2.0-resnet56]: Epoch 89/100, Acc=0.7018, Val Loss=1.2355, lr=0.0001
[02/21 16:33:41 cifar100-global-fpgm-2.0-resnet56]: Epoch 90/100, Acc=0.7049, Val Loss=1.2344, lr=0.0001
[02/21 16:34:08 cifar100-global-fpgm-2.0-resnet56]: Epoch 91/100, Acc=0.7045, Val Loss=1.2422, lr=0.0001
[02/21 16:34:36 cifar100-global-fpgm-2.0-resnet56]: Epoch 92/100, Acc=0.7050, Val Loss=1.2304, lr=0.0001
[02/21 16:35:03 cifar100-global-fpgm-2.0-resnet56]: Epoch 93/100, Acc=0.7021, Val Loss=1.2397, lr=0.0001
[02/21 16:35:30 cifar100-global-fpgm-2.0-resnet56]: Epoch 94/100, Acc=0.7055, Val Loss=1.2312, lr=0.0001
[02/21 16:35:58 cifar100-global-fpgm-2.0-resnet56]: Epoch 95/100, Acc=0.7029, Val Loss=1.2392, lr=0.0001
[02/21 16:36:25 cifar100-global-fpgm-2.0-resnet56]: Epoch 96/100, Acc=0.7029, Val Loss=1.2402, lr=0.0001
[02/21 16:36:53 cifar100-global-fpgm-2.0-resnet56]: Epoch 97/100, Acc=0.7049, Val Loss=1.2371, lr=0.0001
[02/21 16:37:21 cifar100-global-fpgm-2.0-resnet56]: Epoch 98/100, Acc=0.7031, Val Loss=1.2335, lr=0.0001
[02/21 16:37:49 cifar100-global-fpgm-2.0-resnet56]: Epoch 99/100, Acc=0.7034, Val Loss=1.2392, lr=0.0001
[02/21 16:37:49 cifar100-global-fpgm-2.0-resnet56]: Best Acc=0.7076
[02/21 16:37:49 cifar100-global-fpgm-2.0-resnet56]: Params: 0.62 M
[02/21 16:37:49 cifar100-global-fpgm-2.0-resnet56]: ops: 63.21 M
[02/21 16:37:52 cifar100-global-fpgm-2.0-resnet56]: Acc: 0.7034 Val Loss: 1.2392

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: mode: prune
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: model: resnet56
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: verbose: False
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: dataset: cifar100
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: dataroot: data
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: batch_size: 128
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: total_epochs: 100
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: lr_decay_milestones: 60,80
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: lr_decay_gamma: 0.1
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: lr: 0.01
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: output_dir: run/cifar100/prune/cifar100-global-obdc-2.0-resnet56
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: finetune: True
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: last_epochs: 100
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: reps: 1
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: method: obdc
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: speed_up: 2.0
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: max_pruning_ratio: 1.0
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: reg: 1e-05
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: delta_reg: 0.0001
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: weight_decay: 0.0005
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: seed: 1
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: global_pruning: True
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: sl_total_epochs: 100
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: sl_lr: 0.01
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: sl_reg_warmup: 0
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: sl_restore: None
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: iterative_steps: 400
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: logger: <Logger cifar100-global-obdc-2.0-resnet56 (DEBUG)>
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: device: cuda
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: num_classes: 100
[02/21 16:37:59 cifar100-global-obdc-2.0-resnet56]: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 16:38:02 cifar100-global-obdc-2.0-resnet56]: Pruning...
Traceback (most recent call last):
  File "/home/REDACTED/paper1/main.py", line 429, in <module>
    main()
  File "/home/REDACTED/paper1/main.py", line 369, in main
    progressive_pruning(
  File "/home/REDACTED/paper1/main.py", line 65, in progressive_pruning
    imp._prepare_model(model, pruner)
  File "/home/REDACTED/paper1/torch_pruning/pruner/importance.py", line 872, in _prepare_model
    for i, (dep, idxs) in enumerate(group):
TypeError: 'NoneType' object is not iterable
/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: mode: prune
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: model: resnet56
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: verbose: False
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: dataset: cifar100
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: dataroot: data
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: batch_size: 128
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: total_epochs: 100
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: lr_decay_milestones: 60,80
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: lr_decay_gamma: 0.1
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: lr: 0.01
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: output_dir: run/cifar100/prune/cifar100-global-lamp-2.0-resnet56
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: finetune: True
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: last_epochs: 100
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: reps: 1
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: method: lamp
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: speed_up: 2.0
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: max_pruning_ratio: 1.0
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: reg: 1e-05
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: delta_reg: 0.0001
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: weight_decay: 0.0005
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: seed: 1
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: global_pruning: True
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: sl_total_epochs: 100
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: sl_lr: 0.01
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: sl_reg_warmup: 0
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: sl_restore: None
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: iterative_steps: 400
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: logger: <Logger cifar100-global-lamp-2.0-resnet56 (DEBUG)>
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: device: cuda
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: num_classes: 100
[02/21 16:38:08 cifar100-global-lamp-2.0-resnet56]: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 16:38:11 cifar100-global-lamp-2.0-resnet56]: Pruning...
[02/21 16:38:30 cifar100-global-lamp-2.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 25, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 25, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(25, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(25, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(25, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(25, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(25, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(25, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(25, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(25, 28, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(25, 29, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(29, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(29, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(29, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(29, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(29, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(29, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(29, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(29, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(29, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(29, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=29, out_features=100, bias=True)
)
[02/21 16:38:33 cifar100-global-lamp-2.0-resnet56]: Params: 0.86 M => 0.26 M (30.06%)
[02/21 16:38:33 cifar100-global-lamp-2.0-resnet56]: FLOPs: 127.12 M => 63.04 M (49.59%, 2.02X )
[02/21 16:38:33 cifar100-global-lamp-2.0-resnet56]: Acc: 0.7269 => 0.0106
[02/21 16:38:33 cifar100-global-lamp-2.0-resnet56]: Val Loss: 1.1578 => 24.4241
[02/21 16:38:33 cifar100-global-lamp-2.0-resnet56]: Finetuning...
[02/21 16:39:00 cifar100-global-lamp-2.0-resnet56]: Epoch 0/100, Acc=0.4207, Val Loss=2.1230, lr=0.0100
[02/21 16:39:27 cifar100-global-lamp-2.0-resnet56]: Epoch 1/100, Acc=0.4753, Val Loss=1.9222, lr=0.0100
[02/21 16:39:55 cifar100-global-lamp-2.0-resnet56]: Epoch 2/100, Acc=0.5055, Val Loss=1.7657, lr=0.0100
[02/21 16:40:22 cifar100-global-lamp-2.0-resnet56]: Epoch 3/100, Acc=0.5223, Val Loss=1.7619, lr=0.0100
[02/21 16:40:49 cifar100-global-lamp-2.0-resnet56]: Epoch 4/100, Acc=0.5026, Val Loss=1.9050, lr=0.0100
[02/21 16:41:17 cifar100-global-lamp-2.0-resnet56]: Epoch 5/100, Acc=0.5384, Val Loss=1.6762, lr=0.0100
[02/21 16:41:44 cifar100-global-lamp-2.0-resnet56]: Epoch 6/100, Acc=0.5486, Val Loss=1.6400, lr=0.0100
[02/21 16:42:11 cifar100-global-lamp-2.0-resnet56]: Epoch 7/100, Acc=0.5468, Val Loss=1.7013, lr=0.0100
[02/21 16:42:38 cifar100-global-lamp-2.0-resnet56]: Epoch 8/100, Acc=0.5491, Val Loss=1.6716, lr=0.0100
[02/21 16:43:06 cifar100-global-lamp-2.0-resnet56]: Epoch 9/100, Acc=0.5384, Val Loss=1.6887, lr=0.0100
[02/21 16:43:33 cifar100-global-lamp-2.0-resnet56]: Epoch 10/100, Acc=0.5156, Val Loss=1.8660, lr=0.0100
[02/21 16:44:00 cifar100-global-lamp-2.0-resnet56]: Epoch 11/100, Acc=0.5600, Val Loss=1.5957, lr=0.0100
[02/21 16:44:27 cifar100-global-lamp-2.0-resnet56]: Epoch 12/100, Acc=0.5736, Val Loss=1.5363, lr=0.0100
[02/21 16:44:55 cifar100-global-lamp-2.0-resnet56]: Epoch 13/100, Acc=0.5832, Val Loss=1.4807, lr=0.0100
[02/21 16:45:22 cifar100-global-lamp-2.0-resnet56]: Epoch 14/100, Acc=0.5713, Val Loss=1.5841, lr=0.0100
[02/21 16:45:50 cifar100-global-lamp-2.0-resnet56]: Epoch 15/100, Acc=0.5831, Val Loss=1.5056, lr=0.0100
[02/21 16:46:17 cifar100-global-lamp-2.0-resnet56]: Epoch 16/100, Acc=0.5877, Val Loss=1.4773, lr=0.0100
[02/21 16:46:44 cifar100-global-lamp-2.0-resnet56]: Epoch 17/100, Acc=0.5811, Val Loss=1.5608, lr=0.0100
[02/21 16:47:12 cifar100-global-lamp-2.0-resnet56]: Epoch 18/100, Acc=0.5622, Val Loss=1.6802, lr=0.0100
[02/21 16:47:39 cifar100-global-lamp-2.0-resnet56]: Epoch 19/100, Acc=0.5744, Val Loss=1.5888, lr=0.0100
[02/21 16:48:06 cifar100-global-lamp-2.0-resnet56]: Epoch 20/100, Acc=0.5934, Val Loss=1.4703, lr=0.0100
[02/21 16:48:34 cifar100-global-lamp-2.0-resnet56]: Epoch 21/100, Acc=0.5922, Val Loss=1.4809, lr=0.0100
[02/21 16:49:01 cifar100-global-lamp-2.0-resnet56]: Epoch 22/100, Acc=0.5735, Val Loss=1.5635, lr=0.0100
[02/21 16:49:28 cifar100-global-lamp-2.0-resnet56]: Epoch 23/100, Acc=0.5936, Val Loss=1.4551, lr=0.0100
[02/21 16:49:55 cifar100-global-lamp-2.0-resnet56]: Epoch 24/100, Acc=0.6039, Val Loss=1.4105, lr=0.0100
[02/21 16:50:23 cifar100-global-lamp-2.0-resnet56]: Epoch 25/100, Acc=0.5809, Val Loss=1.5355, lr=0.0100
[02/21 16:50:50 cifar100-global-lamp-2.0-resnet56]: Epoch 26/100, Acc=0.5830, Val Loss=1.4861, lr=0.0100
[02/21 16:51:17 cifar100-global-lamp-2.0-resnet56]: Epoch 27/100, Acc=0.5769, Val Loss=1.5709, lr=0.0100
[02/21 16:51:45 cifar100-global-lamp-2.0-resnet56]: Epoch 28/100, Acc=0.5750, Val Loss=1.5644, lr=0.0100
[02/21 16:52:12 cifar100-global-lamp-2.0-resnet56]: Epoch 29/100, Acc=0.5883, Val Loss=1.4844, lr=0.0100
[02/21 16:52:39 cifar100-global-lamp-2.0-resnet56]: Epoch 30/100, Acc=0.5651, Val Loss=1.6800, lr=0.0100
[02/21 16:53:07 cifar100-global-lamp-2.0-resnet56]: Epoch 31/100, Acc=0.5919, Val Loss=1.4723, lr=0.0100
[02/21 16:53:34 cifar100-global-lamp-2.0-resnet56]: Epoch 32/100, Acc=0.6024, Val Loss=1.4654, lr=0.0100
[02/21 16:54:01 cifar100-global-lamp-2.0-resnet56]: Epoch 33/100, Acc=0.6008, Val Loss=1.4622, lr=0.0100
[02/21 16:54:29 cifar100-global-lamp-2.0-resnet56]: Epoch 34/100, Acc=0.5911, Val Loss=1.5214, lr=0.0100
[02/21 16:54:56 cifar100-global-lamp-2.0-resnet56]: Epoch 35/100, Acc=0.6019, Val Loss=1.4332, lr=0.0100
[02/21 16:55:24 cifar100-global-lamp-2.0-resnet56]: Epoch 36/100, Acc=0.5770, Val Loss=1.6070, lr=0.0100
[02/21 16:55:51 cifar100-global-lamp-2.0-resnet56]: Epoch 37/100, Acc=0.6046, Val Loss=1.4518, lr=0.0100
[02/21 16:56:18 cifar100-global-lamp-2.0-resnet56]: Epoch 38/100, Acc=0.5709, Val Loss=1.6455, lr=0.0100
[02/21 16:56:46 cifar100-global-lamp-2.0-resnet56]: Epoch 39/100, Acc=0.5958, Val Loss=1.4802, lr=0.0100
[02/21 16:57:13 cifar100-global-lamp-2.0-resnet56]: Epoch 40/100, Acc=0.6035, Val Loss=1.4408, lr=0.0100
[02/21 16:57:41 cifar100-global-lamp-2.0-resnet56]: Epoch 41/100, Acc=0.6092, Val Loss=1.4508, lr=0.0100
[02/21 16:58:08 cifar100-global-lamp-2.0-resnet56]: Epoch 42/100, Acc=0.5956, Val Loss=1.5036, lr=0.0100
[02/21 16:58:36 cifar100-global-lamp-2.0-resnet56]: Epoch 43/100, Acc=0.5876, Val Loss=1.5655, lr=0.0100
[02/21 16:59:03 cifar100-global-lamp-2.0-resnet56]: Epoch 44/100, Acc=0.5982, Val Loss=1.5001, lr=0.0100
[02/21 16:59:31 cifar100-global-lamp-2.0-resnet56]: Epoch 45/100, Acc=0.5935, Val Loss=1.5501, lr=0.0100
[02/21 16:59:58 cifar100-global-lamp-2.0-resnet56]: Epoch 46/100, Acc=0.5981, Val Loss=1.4722, lr=0.0100
[02/21 17:00:26 cifar100-global-lamp-2.0-resnet56]: Epoch 47/100, Acc=0.6168, Val Loss=1.4132, lr=0.0100
[02/21 17:00:53 cifar100-global-lamp-2.0-resnet56]: Epoch 48/100, Acc=0.5994, Val Loss=1.4767, lr=0.0100
[02/21 17:01:21 cifar100-global-lamp-2.0-resnet56]: Epoch 49/100, Acc=0.5935, Val Loss=1.5173, lr=0.0100
[02/21 17:01:48 cifar100-global-lamp-2.0-resnet56]: Epoch 50/100, Acc=0.5932, Val Loss=1.5140, lr=0.0100
[02/21 17:02:15 cifar100-global-lamp-2.0-resnet56]: Epoch 51/100, Acc=0.6053, Val Loss=1.4808, lr=0.0100
[02/21 17:02:43 cifar100-global-lamp-2.0-resnet56]: Epoch 52/100, Acc=0.5989, Val Loss=1.5030, lr=0.0100
[02/21 17:03:11 cifar100-global-lamp-2.0-resnet56]: Epoch 53/100, Acc=0.5973, Val Loss=1.4710, lr=0.0100
[02/21 17:03:38 cifar100-global-lamp-2.0-resnet56]: Epoch 54/100, Acc=0.5933, Val Loss=1.5164, lr=0.0100
[02/21 17:04:05 cifar100-global-lamp-2.0-resnet56]: Epoch 55/100, Acc=0.5916, Val Loss=1.5147, lr=0.0100
[02/21 17:04:33 cifar100-global-lamp-2.0-resnet56]: Epoch 56/100, Acc=0.5981, Val Loss=1.4757, lr=0.0100
[02/21 17:05:00 cifar100-global-lamp-2.0-resnet56]: Epoch 57/100, Acc=0.6062, Val Loss=1.4774, lr=0.0100
[02/21 17:05:28 cifar100-global-lamp-2.0-resnet56]: Epoch 58/100, Acc=0.6089, Val Loss=1.4624, lr=0.0100
[02/21 17:05:55 cifar100-global-lamp-2.0-resnet56]: Epoch 59/100, Acc=0.6140, Val Loss=1.4591, lr=0.0100
[02/21 17:06:22 cifar100-global-lamp-2.0-resnet56]: Epoch 60/100, Acc=0.6730, Val Loss=1.1637, lr=0.0010
[02/21 17:06:50 cifar100-global-lamp-2.0-resnet56]: Epoch 61/100, Acc=0.6719, Val Loss=1.1685, lr=0.0010
[02/21 17:07:17 cifar100-global-lamp-2.0-resnet56]: Epoch 62/100, Acc=0.6750, Val Loss=1.1626, lr=0.0010
[02/21 17:07:45 cifar100-global-lamp-2.0-resnet56]: Epoch 63/100, Acc=0.6706, Val Loss=1.1648, lr=0.0010
[02/21 17:08:13 cifar100-global-lamp-2.0-resnet56]: Epoch 64/100, Acc=0.6713, Val Loss=1.1732, lr=0.0010
[02/21 17:08:40 cifar100-global-lamp-2.0-resnet56]: Epoch 65/100, Acc=0.6713, Val Loss=1.1679, lr=0.0010
[02/21 17:09:08 cifar100-global-lamp-2.0-resnet56]: Epoch 66/100, Acc=0.6681, Val Loss=1.1845, lr=0.0010
[02/21 17:09:35 cifar100-global-lamp-2.0-resnet56]: Epoch 67/100, Acc=0.6679, Val Loss=1.1748, lr=0.0010
[02/21 17:10:03 cifar100-global-lamp-2.0-resnet56]: Epoch 68/100, Acc=0.6730, Val Loss=1.1794, lr=0.0010
[02/21 17:10:30 cifar100-global-lamp-2.0-resnet56]: Epoch 69/100, Acc=0.6707, Val Loss=1.1880, lr=0.0010
[02/21 17:10:58 cifar100-global-lamp-2.0-resnet56]: Epoch 70/100, Acc=0.6702, Val Loss=1.1937, lr=0.0010
[02/21 17:11:26 cifar100-global-lamp-2.0-resnet56]: Epoch 71/100, Acc=0.6709, Val Loss=1.1961, lr=0.0010
[02/21 17:11:53 cifar100-global-lamp-2.0-resnet56]: Epoch 72/100, Acc=0.6701, Val Loss=1.1900, lr=0.0010
[02/21 17:12:20 cifar100-global-lamp-2.0-resnet56]: Epoch 73/100, Acc=0.6675, Val Loss=1.1990, lr=0.0010
[02/21 17:12:47 cifar100-global-lamp-2.0-resnet56]: Epoch 74/100, Acc=0.6685, Val Loss=1.2033, lr=0.0010
[02/21 17:13:15 cifar100-global-lamp-2.0-resnet56]: Epoch 75/100, Acc=0.6689, Val Loss=1.2074, lr=0.0010
[02/21 17:13:42 cifar100-global-lamp-2.0-resnet56]: Epoch 76/100, Acc=0.6707, Val Loss=1.1992, lr=0.0010
[02/21 17:14:09 cifar100-global-lamp-2.0-resnet56]: Epoch 77/100, Acc=0.6702, Val Loss=1.2031, lr=0.0010
[02/21 17:14:37 cifar100-global-lamp-2.0-resnet56]: Epoch 78/100, Acc=0.6696, Val Loss=1.2240, lr=0.0010
[02/21 17:15:04 cifar100-global-lamp-2.0-resnet56]: Epoch 79/100, Acc=0.6680, Val Loss=1.2213, lr=0.0010
[02/21 17:15:31 cifar100-global-lamp-2.0-resnet56]: Epoch 80/100, Acc=0.6715, Val Loss=1.2019, lr=0.0001
[02/21 17:15:58 cifar100-global-lamp-2.0-resnet56]: Epoch 81/100, Acc=0.6729, Val Loss=1.1971, lr=0.0001
[02/21 17:16:26 cifar100-global-lamp-2.0-resnet56]: Epoch 82/100, Acc=0.6713, Val Loss=1.1989, lr=0.0001
[02/21 17:16:53 cifar100-global-lamp-2.0-resnet56]: Epoch 83/100, Acc=0.6732, Val Loss=1.2016, lr=0.0001
[02/21 17:17:20 cifar100-global-lamp-2.0-resnet56]: Epoch 84/100, Acc=0.6724, Val Loss=1.2052, lr=0.0001
[02/21 17:17:47 cifar100-global-lamp-2.0-resnet56]: Epoch 85/100, Acc=0.6705, Val Loss=1.2038, lr=0.0001
[02/21 17:18:13 cifar100-global-lamp-2.0-resnet56]: Epoch 86/100, Acc=0.6710, Val Loss=1.2031, lr=0.0001
[02/21 17:18:40 cifar100-global-lamp-2.0-resnet56]: Epoch 87/100, Acc=0.6690, Val Loss=1.2053, lr=0.0001
[02/21 17:19:07 cifar100-global-lamp-2.0-resnet56]: Epoch 88/100, Acc=0.6707, Val Loss=1.2074, lr=0.0001
[02/21 17:19:34 cifar100-global-lamp-2.0-resnet56]: Epoch 89/100, Acc=0.6705, Val Loss=1.2038, lr=0.0001
[02/21 17:20:01 cifar100-global-lamp-2.0-resnet56]: Epoch 90/100, Acc=0.6723, Val Loss=1.2019, lr=0.0001
[02/21 17:20:28 cifar100-global-lamp-2.0-resnet56]: Epoch 91/100, Acc=0.6710, Val Loss=1.2092, lr=0.0001
[02/21 17:20:55 cifar100-global-lamp-2.0-resnet56]: Epoch 92/100, Acc=0.6719, Val Loss=1.1980, lr=0.0001
[02/21 17:21:22 cifar100-global-lamp-2.0-resnet56]: Epoch 93/100, Acc=0.6696, Val Loss=1.2062, lr=0.0001
[02/21 17:21:49 cifar100-global-lamp-2.0-resnet56]: Epoch 94/100, Acc=0.6700, Val Loss=1.2042, lr=0.0001
[02/21 17:22:16 cifar100-global-lamp-2.0-resnet56]: Epoch 95/100, Acc=0.6694, Val Loss=1.2118, lr=0.0001
[02/21 17:22:43 cifar100-global-lamp-2.0-resnet56]: Epoch 96/100, Acc=0.6698, Val Loss=1.2041, lr=0.0001
[02/21 17:23:10 cifar100-global-lamp-2.0-resnet56]: Epoch 97/100, Acc=0.6702, Val Loss=1.1998, lr=0.0001
[02/21 17:23:37 cifar100-global-lamp-2.0-resnet56]: Epoch 98/100, Acc=0.6708, Val Loss=1.2026, lr=0.0001
[02/21 17:24:04 cifar100-global-lamp-2.0-resnet56]: Epoch 99/100, Acc=0.6692, Val Loss=1.2047, lr=0.0001
[02/21 17:24:04 cifar100-global-lamp-2.0-resnet56]: Best Acc=0.6750
[02/21 17:24:04 cifar100-global-lamp-2.0-resnet56]: Params: 0.26 M
[02/21 17:24:04 cifar100-global-lamp-2.0-resnet56]: ops: 63.04 M
[02/21 17:24:06 cifar100-global-lamp-2.0-resnet56]: Acc: 0.6692 Val Loss: 1.2047

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: mode: prune
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: model: resnet56
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: verbose: False
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: dataset: cifar100
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: dataroot: data
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: batch_size: 128
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: total_epochs: 100
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: lr_decay_milestones: 60,80
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: lr_decay_gamma: 0.1
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: lr: 0.01
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: output_dir: run/cifar100/prune/cifar100-global-slim-2.0-resnet56
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: finetune: True
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: last_epochs: 100
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: reps: 1
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: method: slim
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: speed_up: 2.0
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: max_pruning_ratio: 1.0
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: reg: 1e-05
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: delta_reg: 0.0001
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: weight_decay: 0.0005
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: seed: 1
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: global_pruning: True
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: sl_total_epochs: 100
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: sl_lr: 0.01
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: sl_reg_warmup: 0
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: sl_restore: None
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: iterative_steps: 400
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: logger: <Logger cifar100-global-slim-2.0-resnet56 (DEBUG)>
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: device: cuda
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: num_classes: 100
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 17:24:14 cifar100-global-slim-2.0-resnet56]: Regularizing...
[02/21 17:24:41 cifar100-global-slim-2.0-resnet56]: Epoch 0/100, Acc=0.6549, Val Loss=1.4473, lr=0.0100
[02/21 17:25:09 cifar100-global-slim-2.0-resnet56]: Epoch 1/100, Acc=0.6675, Val Loss=1.3655, lr=0.0100
[02/21 17:25:36 cifar100-global-slim-2.0-resnet56]: Epoch 2/100, Acc=0.6622, Val Loss=1.4648, lr=0.0100
[02/21 17:26:03 cifar100-global-slim-2.0-resnet56]: Epoch 3/100, Acc=0.6626, Val Loss=1.4551, lr=0.0100
[02/21 17:26:30 cifar100-global-slim-2.0-resnet56]: Epoch 4/100, Acc=0.6700, Val Loss=1.4253, lr=0.0100
[02/21 17:26:58 cifar100-global-slim-2.0-resnet56]: Epoch 5/100, Acc=0.6649, Val Loss=1.4261, lr=0.0100
[02/21 17:27:25 cifar100-global-slim-2.0-resnet56]: Epoch 6/100, Acc=0.6708, Val Loss=1.4388, lr=0.0100
[02/21 17:27:53 cifar100-global-slim-2.0-resnet56]: Epoch 7/100, Acc=0.6709, Val Loss=1.4467, lr=0.0100
[02/21 17:28:20 cifar100-global-slim-2.0-resnet56]: Epoch 8/100, Acc=0.6766, Val Loss=1.4220, lr=0.0100
[02/21 17:28:48 cifar100-global-slim-2.0-resnet56]: Epoch 9/100, Acc=0.6839, Val Loss=1.3775, lr=0.0100
[02/21 17:29:15 cifar100-global-slim-2.0-resnet56]: Epoch 10/100, Acc=0.6801, Val Loss=1.4043, lr=0.0100
[02/21 17:29:43 cifar100-global-slim-2.0-resnet56]: Epoch 11/100, Acc=0.6774, Val Loss=1.4281, lr=0.0100
[02/21 17:30:10 cifar100-global-slim-2.0-resnet56]: Epoch 12/100, Acc=0.6799, Val Loss=1.4414, lr=0.0100
[02/21 17:30:37 cifar100-global-slim-2.0-resnet56]: Epoch 13/100, Acc=0.6751, Val Loss=1.5013, lr=0.0100
[02/21 17:31:05 cifar100-global-slim-2.0-resnet56]: Epoch 14/100, Acc=0.6698, Val Loss=1.4954, lr=0.0100
[02/21 17:31:32 cifar100-global-slim-2.0-resnet56]: Epoch 15/100, Acc=0.6830, Val Loss=1.4309, lr=0.0100
[02/21 17:31:59 cifar100-global-slim-2.0-resnet56]: Epoch 16/100, Acc=0.6877, Val Loss=1.4284, lr=0.0100
[02/21 17:32:27 cifar100-global-slim-2.0-resnet56]: Epoch 17/100, Acc=0.6891, Val Loss=1.4358, lr=0.0100
[02/21 17:32:54 cifar100-global-slim-2.0-resnet56]: Epoch 18/100, Acc=0.6818, Val Loss=1.4853, lr=0.0100
[02/21 17:33:21 cifar100-global-slim-2.0-resnet56]: Epoch 19/100, Acc=0.6823, Val Loss=1.4801, lr=0.0100
[02/21 17:33:48 cifar100-global-slim-2.0-resnet56]: Epoch 20/100, Acc=0.6780, Val Loss=1.5568, lr=0.0100
[02/21 17:34:15 cifar100-global-slim-2.0-resnet56]: Epoch 21/100, Acc=0.6877, Val Loss=1.4668, lr=0.0100
[02/21 17:34:42 cifar100-global-slim-2.0-resnet56]: Epoch 22/100, Acc=0.6781, Val Loss=1.5616, lr=0.0100
[02/21 17:35:09 cifar100-global-slim-2.0-resnet56]: Epoch 23/100, Acc=0.6813, Val Loss=1.5505, lr=0.0100
[02/21 17:35:37 cifar100-global-slim-2.0-resnet56]: Epoch 24/100, Acc=0.6863, Val Loss=1.5491, lr=0.0100
[02/21 17:36:04 cifar100-global-slim-2.0-resnet56]: Epoch 25/100, Acc=0.6889, Val Loss=1.4988, lr=0.0100
[02/21 17:36:31 cifar100-global-slim-2.0-resnet56]: Epoch 26/100, Acc=0.6843, Val Loss=1.5719, lr=0.0100
[02/21 17:36:59 cifar100-global-slim-2.0-resnet56]: Epoch 27/100, Acc=0.6811, Val Loss=1.5773, lr=0.0100
[02/21 17:37:26 cifar100-global-slim-2.0-resnet56]: Epoch 28/100, Acc=0.6926, Val Loss=1.5781, lr=0.0100
[02/21 17:37:55 cifar100-global-slim-2.0-resnet56]: Epoch 29/100, Acc=0.6912, Val Loss=1.5624, lr=0.0100
[02/21 17:38:23 cifar100-global-slim-2.0-resnet56]: Epoch 30/100, Acc=0.6822, Val Loss=1.6769, lr=0.0100
[02/21 17:38:52 cifar100-global-slim-2.0-resnet56]: Epoch 31/100, Acc=0.6898, Val Loss=1.5667, lr=0.0100
[02/21 17:39:21 cifar100-global-slim-2.0-resnet56]: Epoch 32/100, Acc=0.6937, Val Loss=1.5571, lr=0.0100
[02/21 17:39:50 cifar100-global-slim-2.0-resnet56]: Epoch 33/100, Acc=0.6930, Val Loss=1.5198, lr=0.0100
[02/21 17:40:19 cifar100-global-slim-2.0-resnet56]: Epoch 34/100, Acc=0.6897, Val Loss=1.6135, lr=0.0100
[02/21 17:40:48 cifar100-global-slim-2.0-resnet56]: Epoch 35/100, Acc=0.6864, Val Loss=1.6578, lr=0.0100
[02/21 17:41:17 cifar100-global-slim-2.0-resnet56]: Epoch 36/100, Acc=0.6876, Val Loss=1.6491, lr=0.0100
[02/21 17:41:46 cifar100-global-slim-2.0-resnet56]: Epoch 37/100, Acc=0.6903, Val Loss=1.6184, lr=0.0100
[02/21 17:42:16 cifar100-global-slim-2.0-resnet56]: Epoch 38/100, Acc=0.6887, Val Loss=1.6259, lr=0.0100
[02/21 17:42:45 cifar100-global-slim-2.0-resnet56]: Epoch 39/100, Acc=0.6916, Val Loss=1.6550, lr=0.0100
[02/21 17:43:14 cifar100-global-slim-2.0-resnet56]: Epoch 40/100, Acc=0.6909, Val Loss=1.6674, lr=0.0100
[02/21 17:43:44 cifar100-global-slim-2.0-resnet56]: Epoch 41/100, Acc=0.6909, Val Loss=1.6838, lr=0.0100
[02/21 17:44:13 cifar100-global-slim-2.0-resnet56]: Epoch 42/100, Acc=0.6941, Val Loss=1.6407, lr=0.0100
[02/21 17:44:41 cifar100-global-slim-2.0-resnet56]: Epoch 43/100, Acc=0.6891, Val Loss=1.7042, lr=0.0100
[02/21 17:45:09 cifar100-global-slim-2.0-resnet56]: Epoch 44/100, Acc=0.6916, Val Loss=1.6693, lr=0.0100
[02/21 17:45:37 cifar100-global-slim-2.0-resnet56]: Epoch 45/100, Acc=0.6913, Val Loss=1.6870, lr=0.0100
[02/21 17:46:05 cifar100-global-slim-2.0-resnet56]: Epoch 46/100, Acc=0.6887, Val Loss=1.6770, lr=0.0100
[02/21 17:46:33 cifar100-global-slim-2.0-resnet56]: Epoch 47/100, Acc=0.6978, Val Loss=1.6800, lr=0.0100
[02/21 17:47:01 cifar100-global-slim-2.0-resnet56]: Epoch 48/100, Acc=0.6997, Val Loss=1.6438, lr=0.0100
[02/21 17:47:30 cifar100-global-slim-2.0-resnet56]: Epoch 49/100, Acc=0.6922, Val Loss=1.7026, lr=0.0100
[02/21 17:47:58 cifar100-global-slim-2.0-resnet56]: Epoch 50/100, Acc=0.6898, Val Loss=1.7419, lr=0.0100
[02/21 17:48:26 cifar100-global-slim-2.0-resnet56]: Epoch 51/100, Acc=0.6888, Val Loss=1.7402, lr=0.0100
[02/21 17:48:54 cifar100-global-slim-2.0-resnet56]: Epoch 52/100, Acc=0.6996, Val Loss=1.6730, lr=0.0100
[02/21 17:49:22 cifar100-global-slim-2.0-resnet56]: Epoch 53/100, Acc=0.6933, Val Loss=1.6782, lr=0.0100
[02/21 17:49:50 cifar100-global-slim-2.0-resnet56]: Epoch 54/100, Acc=0.6959, Val Loss=1.6886, lr=0.0100
[02/21 17:50:19 cifar100-global-slim-2.0-resnet56]: Epoch 55/100, Acc=0.6955, Val Loss=1.7166, lr=0.0100
[02/21 17:50:47 cifar100-global-slim-2.0-resnet56]: Epoch 56/100, Acc=0.6881, Val Loss=1.7943, lr=0.0100
[02/21 17:51:15 cifar100-global-slim-2.0-resnet56]: Epoch 57/100, Acc=0.6950, Val Loss=1.7341, lr=0.0100
[02/21 17:51:43 cifar100-global-slim-2.0-resnet56]: Epoch 58/100, Acc=0.6944, Val Loss=1.7639, lr=0.0100
[02/21 17:52:10 cifar100-global-slim-2.0-resnet56]: Epoch 59/100, Acc=0.6968, Val Loss=1.7648, lr=0.0100
[02/21 17:52:38 cifar100-global-slim-2.0-resnet56]: Epoch 60/100, Acc=0.7136, Val Loss=1.6151, lr=0.0010
[02/21 17:53:06 cifar100-global-slim-2.0-resnet56]: Epoch 61/100, Acc=0.7137, Val Loss=1.6143, lr=0.0010
[02/21 17:53:33 cifar100-global-slim-2.0-resnet56]: Epoch 62/100, Acc=0.7152, Val Loss=1.6100, lr=0.0010
[02/21 17:54:01 cifar100-global-slim-2.0-resnet56]: Epoch 63/100, Acc=0.7159, Val Loss=1.6182, lr=0.0010
[02/21 17:54:29 cifar100-global-slim-2.0-resnet56]: Epoch 64/100, Acc=0.7146, Val Loss=1.6046, lr=0.0010
[02/21 17:54:57 cifar100-global-slim-2.0-resnet56]: Epoch 65/100, Acc=0.7163, Val Loss=1.6259, lr=0.0010
[02/21 17:55:24 cifar100-global-slim-2.0-resnet56]: Epoch 66/100, Acc=0.7169, Val Loss=1.6123, lr=0.0010
[02/21 17:55:52 cifar100-global-slim-2.0-resnet56]: Epoch 67/100, Acc=0.7188, Val Loss=1.6041, lr=0.0010
[02/21 17:56:20 cifar100-global-slim-2.0-resnet56]: Epoch 68/100, Acc=0.7189, Val Loss=1.6150, lr=0.0010
[02/21 17:56:48 cifar100-global-slim-2.0-resnet56]: Epoch 69/100, Acc=0.7179, Val Loss=1.6165, lr=0.0010
[02/21 17:57:16 cifar100-global-slim-2.0-resnet56]: Epoch 70/100, Acc=0.7171, Val Loss=1.6243, lr=0.0010
[02/21 17:57:44 cifar100-global-slim-2.0-resnet56]: Epoch 71/100, Acc=0.7184, Val Loss=1.6189, lr=0.0010
[02/21 17:58:11 cifar100-global-slim-2.0-resnet56]: Epoch 72/100, Acc=0.7193, Val Loss=1.6223, lr=0.0010
[02/21 17:58:39 cifar100-global-slim-2.0-resnet56]: Epoch 73/100, Acc=0.7164, Val Loss=1.6417, lr=0.0010
[02/21 17:59:07 cifar100-global-slim-2.0-resnet56]: Epoch 74/100, Acc=0.7183, Val Loss=1.6310, lr=0.0010
[02/21 17:59:35 cifar100-global-slim-2.0-resnet56]: Epoch 75/100, Acc=0.7195, Val Loss=1.6356, lr=0.0010
[02/21 18:00:02 cifar100-global-slim-2.0-resnet56]: Epoch 76/100, Acc=0.7206, Val Loss=1.6390, lr=0.0010
[02/21 18:00:30 cifar100-global-slim-2.0-resnet56]: Epoch 77/100, Acc=0.7197, Val Loss=1.6323, lr=0.0010
[02/21 18:00:58 cifar100-global-slim-2.0-resnet56]: Epoch 78/100, Acc=0.7183, Val Loss=1.6461, lr=0.0010
[02/21 18:01:25 cifar100-global-slim-2.0-resnet56]: Epoch 79/100, Acc=0.7198, Val Loss=1.6348, lr=0.0010
[02/21 18:01:53 cifar100-global-slim-2.0-resnet56]: Epoch 80/100, Acc=0.7177, Val Loss=1.6404, lr=0.0001
[02/21 18:02:21 cifar100-global-slim-2.0-resnet56]: Epoch 81/100, Acc=0.7203, Val Loss=1.6351, lr=0.0001
[02/21 18:02:48 cifar100-global-slim-2.0-resnet56]: Epoch 82/100, Acc=0.7189, Val Loss=1.6436, lr=0.0001
[02/21 18:03:16 cifar100-global-slim-2.0-resnet56]: Epoch 83/100, Acc=0.7200, Val Loss=1.6423, lr=0.0001
[02/21 18:03:44 cifar100-global-slim-2.0-resnet56]: Epoch 84/100, Acc=0.7187, Val Loss=1.6417, lr=0.0001
[02/21 18:04:11 cifar100-global-slim-2.0-resnet56]: Epoch 85/100, Acc=0.7206, Val Loss=1.6379, lr=0.0001
[02/21 18:04:39 cifar100-global-slim-2.0-resnet56]: Epoch 86/100, Acc=0.7195, Val Loss=1.6435, lr=0.0001
[02/21 18:05:07 cifar100-global-slim-2.0-resnet56]: Epoch 87/100, Acc=0.7192, Val Loss=1.6324, lr=0.0001
[02/21 18:05:35 cifar100-global-slim-2.0-resnet56]: Epoch 88/100, Acc=0.7183, Val Loss=1.6359, lr=0.0001
[02/21 18:06:02 cifar100-global-slim-2.0-resnet56]: Epoch 89/100, Acc=0.7209, Val Loss=1.6382, lr=0.0001
[02/21 18:06:30 cifar100-global-slim-2.0-resnet56]: Epoch 90/100, Acc=0.7213, Val Loss=1.6320, lr=0.0001
[02/21 18:06:58 cifar100-global-slim-2.0-resnet56]: Epoch 91/100, Acc=0.7195, Val Loss=1.6364, lr=0.0001
[02/21 18:07:25 cifar100-global-slim-2.0-resnet56]: Epoch 92/100, Acc=0.7212, Val Loss=1.6354, lr=0.0001
[02/21 18:07:53 cifar100-global-slim-2.0-resnet56]: Epoch 93/100, Acc=0.7203, Val Loss=1.6344, lr=0.0001
[02/21 18:08:21 cifar100-global-slim-2.0-resnet56]: Epoch 94/100, Acc=0.7197, Val Loss=1.6336, lr=0.0001
[02/21 18:08:49 cifar100-global-slim-2.0-resnet56]: Epoch 95/100, Acc=0.7206, Val Loss=1.6409, lr=0.0001
[02/21 18:09:17 cifar100-global-slim-2.0-resnet56]: Epoch 96/100, Acc=0.7190, Val Loss=1.6402, lr=0.0001
[02/21 18:09:44 cifar100-global-slim-2.0-resnet56]: Epoch 97/100, Acc=0.7195, Val Loss=1.6496, lr=0.0001
[02/21 18:10:12 cifar100-global-slim-2.0-resnet56]: Epoch 98/100, Acc=0.7203, Val Loss=1.6383, lr=0.0001
[02/21 18:10:40 cifar100-global-slim-2.0-resnet56]: Epoch 99/100, Acc=0.7188, Val Loss=1.6389, lr=0.0001
[02/21 18:10:40 cifar100-global-slim-2.0-resnet56]: Best Acc=0.7213
[02/21 18:10:40 cifar100-global-slim-2.0-resnet56]: Loading the sparse model from run/cifar100/prune/cifar100-global-slim-2.0-resnet56/reg_cifar100_resnet56_slim_1e-05.pth...
[02/21 18:10:43 cifar100-global-slim-2.0-resnet56]: Pruning...
[02/21 18:10:50 cifar100-global-slim-2.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(8, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(8, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(8, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(8, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(8, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(8, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(8, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(8, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(8, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(8, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(8, 18, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(18, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(18, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(31, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(18, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(30, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(18, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(18, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(18, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(18, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(18, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(18, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(57, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(18, 62, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(62, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(62, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(62, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(58, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(62, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(63, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(62, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(62, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=62, out_features=100, bias=True)
)
[02/21 18:10:53 cifar100-global-slim-2.0-resnet56]: Params: 0.86 M => 0.67 M (77.38%)
[02/21 18:10:53 cifar100-global-slim-2.0-resnet56]: FLOPs: 127.12 M => 62.52 M (49.18%, 2.03X )
[02/21 18:10:53 cifar100-global-slim-2.0-resnet56]: Acc: 0.7213 => 0.0100
[02/21 18:10:53 cifar100-global-slim-2.0-resnet56]: Val Loss: 1.6320 => 6.6512
[02/21 18:10:53 cifar100-global-slim-2.0-resnet56]: Finetuning...
[02/21 18:11:21 cifar100-global-slim-2.0-resnet56]: Epoch 0/100, Acc=0.5086, Val Loss=1.9193, lr=0.0100
[02/21 18:11:49 cifar100-global-slim-2.0-resnet56]: Epoch 1/100, Acc=0.5483, Val Loss=1.7860, lr=0.0100
[02/21 18:12:17 cifar100-global-slim-2.0-resnet56]: Epoch 2/100, Acc=0.5699, Val Loss=1.7813, lr=0.0100
[02/21 18:12:45 cifar100-global-slim-2.0-resnet56]: Epoch 3/100, Acc=0.5996, Val Loss=1.6106, lr=0.0100
[02/21 18:13:13 cifar100-global-slim-2.0-resnet56]: Epoch 4/100, Acc=0.6086, Val Loss=1.6035, lr=0.0100
[02/21 18:13:41 cifar100-global-slim-2.0-resnet56]: Epoch 5/100, Acc=0.5998, Val Loss=1.6380, lr=0.0100
[02/21 18:14:09 cifar100-global-slim-2.0-resnet56]: Epoch 6/100, Acc=0.6295, Val Loss=1.5374, lr=0.0100
[02/21 18:14:37 cifar100-global-slim-2.0-resnet56]: Epoch 7/100, Acc=0.6214, Val Loss=1.5793, lr=0.0100
[02/21 18:15:05 cifar100-global-slim-2.0-resnet56]: Epoch 8/100, Acc=0.6318, Val Loss=1.5074, lr=0.0100
[02/21 18:15:33 cifar100-global-slim-2.0-resnet56]: Epoch 9/100, Acc=0.6112, Val Loss=1.6894, lr=0.0100
[02/21 18:16:02 cifar100-global-slim-2.0-resnet56]: Epoch 10/100, Acc=0.6345, Val Loss=1.5040, lr=0.0100
[02/21 18:16:30 cifar100-global-slim-2.0-resnet56]: Epoch 11/100, Acc=0.6380, Val Loss=1.4920, lr=0.0100
[02/21 18:16:57 cifar100-global-slim-2.0-resnet56]: Epoch 12/100, Acc=0.6424, Val Loss=1.4646, lr=0.0100
[02/21 18:17:26 cifar100-global-slim-2.0-resnet56]: Epoch 13/100, Acc=0.6330, Val Loss=1.5496, lr=0.0100
[02/21 18:17:54 cifar100-global-slim-2.0-resnet56]: Epoch 14/100, Acc=0.6428, Val Loss=1.4767, lr=0.0100
[02/21 18:18:22 cifar100-global-slim-2.0-resnet56]: Epoch 15/100, Acc=0.6299, Val Loss=1.5808, lr=0.0100
[02/21 18:18:50 cifar100-global-slim-2.0-resnet56]: Epoch 16/100, Acc=0.6403, Val Loss=1.4618, lr=0.0100
[02/21 18:19:18 cifar100-global-slim-2.0-resnet56]: Epoch 17/100, Acc=0.6198, Val Loss=1.6493, lr=0.0100
[02/21 18:19:46 cifar100-global-slim-2.0-resnet56]: Epoch 18/100, Acc=0.6382, Val Loss=1.5246, lr=0.0100
[02/21 18:20:15 cifar100-global-slim-2.0-resnet56]: Epoch 19/100, Acc=0.6385, Val Loss=1.5186, lr=0.0100
[02/21 18:20:43 cifar100-global-slim-2.0-resnet56]: Epoch 20/100, Acc=0.6183, Val Loss=1.6021, lr=0.0100
[02/21 18:21:11 cifar100-global-slim-2.0-resnet56]: Epoch 21/100, Acc=0.6306, Val Loss=1.5676, lr=0.0100
[02/21 18:21:39 cifar100-global-slim-2.0-resnet56]: Epoch 22/100, Acc=0.6326, Val Loss=1.5446, lr=0.0100
[02/21 18:22:07 cifar100-global-slim-2.0-resnet56]: Epoch 23/100, Acc=0.6423, Val Loss=1.4600, lr=0.0100
[02/21 18:22:35 cifar100-global-slim-2.0-resnet56]: Epoch 24/100, Acc=0.6279, Val Loss=1.5835, lr=0.0100
[02/21 18:23:03 cifar100-global-slim-2.0-resnet56]: Epoch 25/100, Acc=0.6221, Val Loss=1.6382, lr=0.0100
[02/21 18:23:31 cifar100-global-slim-2.0-resnet56]: Epoch 26/100, Acc=0.6498, Val Loss=1.4537, lr=0.0100
[02/21 18:24:00 cifar100-global-slim-2.0-resnet56]: Epoch 27/100, Acc=0.6420, Val Loss=1.5205, lr=0.0100
[02/21 18:24:28 cifar100-global-slim-2.0-resnet56]: Epoch 28/100, Acc=0.6319, Val Loss=1.5439, lr=0.0100
[02/21 18:24:56 cifar100-global-slim-2.0-resnet56]: Epoch 29/100, Acc=0.6374, Val Loss=1.5373, lr=0.0100
[02/21 18:25:24 cifar100-global-slim-2.0-resnet56]: Epoch 30/100, Acc=0.6441, Val Loss=1.5253, lr=0.0100
[02/21 18:25:52 cifar100-global-slim-2.0-resnet56]: Epoch 31/100, Acc=0.6396, Val Loss=1.5679, lr=0.0100
[02/21 18:26:20 cifar100-global-slim-2.0-resnet56]: Epoch 32/100, Acc=0.6457, Val Loss=1.4960, lr=0.0100
[02/21 18:26:48 cifar100-global-slim-2.0-resnet56]: Epoch 33/100, Acc=0.6345, Val Loss=1.5523, lr=0.0100
[02/21 18:27:16 cifar100-global-slim-2.0-resnet56]: Epoch 34/100, Acc=0.6499, Val Loss=1.4277, lr=0.0100
[02/21 18:27:44 cifar100-global-slim-2.0-resnet56]: Epoch 35/100, Acc=0.6188, Val Loss=1.5904, lr=0.0100
[02/21 18:28:12 cifar100-global-slim-2.0-resnet56]: Epoch 36/100, Acc=0.6297, Val Loss=1.5601, lr=0.0100
[02/21 18:28:40 cifar100-global-slim-2.0-resnet56]: Epoch 37/100, Acc=0.6374, Val Loss=1.5145, lr=0.0100
[02/21 18:29:07 cifar100-global-slim-2.0-resnet56]: Epoch 38/100, Acc=0.6365, Val Loss=1.5254, lr=0.0100
[02/21 18:29:35 cifar100-global-slim-2.0-resnet56]: Epoch 39/100, Acc=0.6341, Val Loss=1.5362, lr=0.0100
[02/21 18:30:03 cifar100-global-slim-2.0-resnet56]: Epoch 40/100, Acc=0.6351, Val Loss=1.4923, lr=0.0100
[02/21 18:30:30 cifar100-global-slim-2.0-resnet56]: Epoch 41/100, Acc=0.6122, Val Loss=1.6652, lr=0.0100
[02/21 18:30:58 cifar100-global-slim-2.0-resnet56]: Epoch 42/100, Acc=0.6464, Val Loss=1.4683, lr=0.0100
[02/21 18:31:26 cifar100-global-slim-2.0-resnet56]: Epoch 43/100, Acc=0.6239, Val Loss=1.6223, lr=0.0100
[02/21 18:31:54 cifar100-global-slim-2.0-resnet56]: Epoch 44/100, Acc=0.6247, Val Loss=1.5877, lr=0.0100
[02/21 18:32:22 cifar100-global-slim-2.0-resnet56]: Epoch 45/100, Acc=0.6351, Val Loss=1.5174, lr=0.0100
[02/21 18:32:50 cifar100-global-slim-2.0-resnet56]: Epoch 46/100, Acc=0.6424, Val Loss=1.5272, lr=0.0100
[02/21 18:33:18 cifar100-global-slim-2.0-resnet56]: Epoch 47/100, Acc=0.6196, Val Loss=1.5975, lr=0.0100
[02/21 18:33:46 cifar100-global-slim-2.0-resnet56]: Epoch 48/100, Acc=0.6348, Val Loss=1.5547, lr=0.0100
[02/21 18:34:14 cifar100-global-slim-2.0-resnet56]: Epoch 49/100, Acc=0.6369, Val Loss=1.5323, lr=0.0100
[02/21 18:34:42 cifar100-global-slim-2.0-resnet56]: Epoch 50/100, Acc=0.6431, Val Loss=1.4732, lr=0.0100
[02/21 18:35:10 cifar100-global-slim-2.0-resnet56]: Epoch 51/100, Acc=0.6468, Val Loss=1.4880, lr=0.0100
[02/21 18:35:38 cifar100-global-slim-2.0-resnet56]: Epoch 52/100, Acc=0.6498, Val Loss=1.4636, lr=0.0100
[02/21 18:36:06 cifar100-global-slim-2.0-resnet56]: Epoch 53/100, Acc=0.6408, Val Loss=1.4832, lr=0.0100
[02/21 18:36:34 cifar100-global-slim-2.0-resnet56]: Epoch 54/100, Acc=0.6411, Val Loss=1.4957, lr=0.0100
[02/21 18:37:02 cifar100-global-slim-2.0-resnet56]: Epoch 55/100, Acc=0.6381, Val Loss=1.5399, lr=0.0100
[02/21 18:37:30 cifar100-global-slim-2.0-resnet56]: Epoch 56/100, Acc=0.6332, Val Loss=1.5207, lr=0.0100
[02/21 18:37:58 cifar100-global-slim-2.0-resnet56]: Epoch 57/100, Acc=0.6341, Val Loss=1.5639, lr=0.0100
[02/21 18:38:26 cifar100-global-slim-2.0-resnet56]: Epoch 58/100, Acc=0.6344, Val Loss=1.5606, lr=0.0100
[02/21 18:38:54 cifar100-global-slim-2.0-resnet56]: Epoch 59/100, Acc=0.6259, Val Loss=1.6180, lr=0.0100
[02/21 18:39:22 cifar100-global-slim-2.0-resnet56]: Epoch 60/100, Acc=0.7026, Val Loss=1.2133, lr=0.0010
[02/21 18:39:50 cifar100-global-slim-2.0-resnet56]: Epoch 61/100, Acc=0.7054, Val Loss=1.2169, lr=0.0010
[02/21 18:40:19 cifar100-global-slim-2.0-resnet56]: Epoch 62/100, Acc=0.7046, Val Loss=1.2148, lr=0.0010
[02/21 18:40:47 cifar100-global-slim-2.0-resnet56]: Epoch 63/100, Acc=0.7061, Val Loss=1.2185, lr=0.0010
[02/21 18:41:15 cifar100-global-slim-2.0-resnet56]: Epoch 64/100, Acc=0.7047, Val Loss=1.2269, lr=0.0010
[02/21 18:41:44 cifar100-global-slim-2.0-resnet56]: Epoch 65/100, Acc=0.7042, Val Loss=1.2278, lr=0.0010
[02/21 18:42:12 cifar100-global-slim-2.0-resnet56]: Epoch 66/100, Acc=0.7050, Val Loss=1.2421, lr=0.0010
[02/21 18:42:40 cifar100-global-slim-2.0-resnet56]: Epoch 67/100, Acc=0.7071, Val Loss=1.2419, lr=0.0010
[02/21 18:43:08 cifar100-global-slim-2.0-resnet56]: Epoch 68/100, Acc=0.7036, Val Loss=1.2488, lr=0.0010
[02/21 18:43:36 cifar100-global-slim-2.0-resnet56]: Epoch 69/100, Acc=0.7015, Val Loss=1.2528, lr=0.0010
[02/21 18:44:04 cifar100-global-slim-2.0-resnet56]: Epoch 70/100, Acc=0.7053, Val Loss=1.2529, lr=0.0010
[02/21 18:44:32 cifar100-global-slim-2.0-resnet56]: Epoch 71/100, Acc=0.7049, Val Loss=1.2577, lr=0.0010
[02/21 18:45:00 cifar100-global-slim-2.0-resnet56]: Epoch 72/100, Acc=0.7076, Val Loss=1.2615, lr=0.0010
[02/21 18:45:28 cifar100-global-slim-2.0-resnet56]: Epoch 73/100, Acc=0.7038, Val Loss=1.2669, lr=0.0010
[02/21 18:45:56 cifar100-global-slim-2.0-resnet56]: Epoch 74/100, Acc=0.7025, Val Loss=1.2705, lr=0.0010
[02/21 18:46:24 cifar100-global-slim-2.0-resnet56]: Epoch 75/100, Acc=0.7059, Val Loss=1.2725, lr=0.0010
[02/21 18:46:52 cifar100-global-slim-2.0-resnet56]: Epoch 76/100, Acc=0.7036, Val Loss=1.2684, lr=0.0010
[02/21 18:47:20 cifar100-global-slim-2.0-resnet56]: Epoch 77/100, Acc=0.7051, Val Loss=1.2736, lr=0.0010
[02/21 18:47:47 cifar100-global-slim-2.0-resnet56]: Epoch 78/100, Acc=0.7040, Val Loss=1.2884, lr=0.0010
[02/21 18:48:15 cifar100-global-slim-2.0-resnet56]: Epoch 79/100, Acc=0.7025, Val Loss=1.2939, lr=0.0010
[02/21 18:48:43 cifar100-global-slim-2.0-resnet56]: Epoch 80/100, Acc=0.7028, Val Loss=1.2888, lr=0.0001
[02/21 18:49:11 cifar100-global-slim-2.0-resnet56]: Epoch 81/100, Acc=0.7024, Val Loss=1.2836, lr=0.0001
[02/21 18:49:38 cifar100-global-slim-2.0-resnet56]: Epoch 82/100, Acc=0.7046, Val Loss=1.2842, lr=0.0001
[02/21 18:50:06 cifar100-global-slim-2.0-resnet56]: Epoch 83/100, Acc=0.7043, Val Loss=1.2821, lr=0.0001
[02/21 18:50:34 cifar100-global-slim-2.0-resnet56]: Epoch 84/100, Acc=0.7053, Val Loss=1.2797, lr=0.0001
[02/21 18:51:02 cifar100-global-slim-2.0-resnet56]: Epoch 85/100, Acc=0.7027, Val Loss=1.2803, lr=0.0001
[02/21 18:51:30 cifar100-global-slim-2.0-resnet56]: Epoch 86/100, Acc=0.7026, Val Loss=1.2902, lr=0.0001
[02/21 18:51:58 cifar100-global-slim-2.0-resnet56]: Epoch 87/100, Acc=0.7049, Val Loss=1.2844, lr=0.0001
[02/21 18:52:26 cifar100-global-slim-2.0-resnet56]: Epoch 88/100, Acc=0.7032, Val Loss=1.2822, lr=0.0001
[02/21 18:52:53 cifar100-global-slim-2.0-resnet56]: Epoch 89/100, Acc=0.7039, Val Loss=1.2846, lr=0.0001
[02/21 18:53:21 cifar100-global-slim-2.0-resnet56]: Epoch 90/100, Acc=0.7040, Val Loss=1.2870, lr=0.0001
[02/21 18:53:50 cifar100-global-slim-2.0-resnet56]: Epoch 91/100, Acc=0.7042, Val Loss=1.2854, lr=0.0001
[02/21 18:54:18 cifar100-global-slim-2.0-resnet56]: Epoch 92/100, Acc=0.7058, Val Loss=1.2822, lr=0.0001
[02/21 18:54:46 cifar100-global-slim-2.0-resnet56]: Epoch 93/100, Acc=0.7033, Val Loss=1.2890, lr=0.0001
[02/21 18:55:14 cifar100-global-slim-2.0-resnet56]: Epoch 94/100, Acc=0.7030, Val Loss=1.2842, lr=0.0001
[02/21 18:55:42 cifar100-global-slim-2.0-resnet56]: Epoch 95/100, Acc=0.7055, Val Loss=1.2853, lr=0.0001
[02/21 18:56:10 cifar100-global-slim-2.0-resnet56]: Epoch 96/100, Acc=0.7017, Val Loss=1.2841, lr=0.0001
[02/21 18:56:38 cifar100-global-slim-2.0-resnet56]: Epoch 97/100, Acc=0.7029, Val Loss=1.2929, lr=0.0001
[02/21 18:57:06 cifar100-global-slim-2.0-resnet56]: Epoch 98/100, Acc=0.7049, Val Loss=1.2822, lr=0.0001
[02/21 18:57:34 cifar100-global-slim-2.0-resnet56]: Epoch 99/100, Acc=0.7042, Val Loss=1.2901, lr=0.0001
[02/21 18:57:34 cifar100-global-slim-2.0-resnet56]: Best Acc=0.7076
[02/21 18:57:34 cifar100-global-slim-2.0-resnet56]: Params: 0.67 M
[02/21 18:57:34 cifar100-global-slim-2.0-resnet56]: ops: 62.52 M
[02/21 18:57:37 cifar100-global-slim-2.0-resnet56]: Acc: 0.7042 Val Loss: 1.2901

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: mode: prune
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: model: resnet56
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: verbose: False
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: dataset: cifar100
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: dataroot: data
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: batch_size: 128
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: total_epochs: 100
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: lr_decay_milestones: 60,80
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: lr_decay_gamma: 0.1
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: lr: 0.01
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: output_dir: run/cifar100/prune/cifar100-global-group_norm-2.0-resnet56
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: finetune: True
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: last_epochs: 100
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: reps: 1
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: method: group_norm
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: speed_up: 2.0
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: max_pruning_ratio: 1.0
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: reg: 1e-05
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: delta_reg: 0.0001
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: weight_decay: 0.0005
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: seed: 1
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: global_pruning: True
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: sl_total_epochs: 100
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: sl_lr: 0.01
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: sl_reg_warmup: 0
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: sl_restore: None
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: iterative_steps: 400
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: logger: <Logger cifar100-global-group_norm-2.0-resnet56 (DEBUG)>
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: device: cuda
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: num_classes: 100
[02/21 18:57:44 cifar100-global-group_norm-2.0-resnet56]: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 18:57:48 cifar100-global-group_norm-2.0-resnet56]: Pruning...
[02/21 18:57:58 cifar100-global-group_norm-2.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(11, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(11, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(11, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(11, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(11, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(11, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(11, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(11, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(11, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(11, 25, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(11, 22, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(22, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(22, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(22, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(22, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(22, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(22, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(22, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(22, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(51, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(64, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=100, bias=True)
)
[02/21 18:58:01 cifar100-global-group_norm-2.0-resnet56]: Params: 0.86 M => 0.68 M (79.43%)
[02/21 18:58:01 cifar100-global-group_norm-2.0-resnet56]: FLOPs: 127.12 M => 63.42 M (49.89%, 2.00X )
[02/21 18:58:01 cifar100-global-group_norm-2.0-resnet56]: Acc: 0.7269 => 0.0168
[02/21 18:58:01 cifar100-global-group_norm-2.0-resnet56]: Val Loss: 1.1578 => 14.3494
[02/21 18:58:01 cifar100-global-group_norm-2.0-resnet56]: Finetuning...
[02/21 18:58:29 cifar100-global-group_norm-2.0-resnet56]: Epoch 0/100, Acc=0.5970, Val Loss=1.6188, lr=0.0100
[02/21 18:58:57 cifar100-global-group_norm-2.0-resnet56]: Epoch 1/100, Acc=0.6340, Val Loss=1.3881, lr=0.0100
[02/21 18:59:25 cifar100-global-group_norm-2.0-resnet56]: Epoch 2/100, Acc=0.6383, Val Loss=1.3882, lr=0.0100
[02/21 18:59:53 cifar100-global-group_norm-2.0-resnet56]: Epoch 3/100, Acc=0.6559, Val Loss=1.2951, lr=0.0100
[02/21 19:00:20 cifar100-global-group_norm-2.0-resnet56]: Epoch 4/100, Acc=0.6516, Val Loss=1.3710, lr=0.0100
[02/21 19:00:48 cifar100-global-group_norm-2.0-resnet56]: Epoch 5/100, Acc=0.6439, Val Loss=1.3531, lr=0.0100
[02/21 19:01:16 cifar100-global-group_norm-2.0-resnet56]: Epoch 6/100, Acc=0.6554, Val Loss=1.3360, lr=0.0100
[02/21 19:01:44 cifar100-global-group_norm-2.0-resnet56]: Epoch 7/100, Acc=0.6427, Val Loss=1.3941, lr=0.0100
[02/21 19:02:12 cifar100-global-group_norm-2.0-resnet56]: Epoch 8/100, Acc=0.6525, Val Loss=1.3519, lr=0.0100
[02/21 19:02:39 cifar100-global-group_norm-2.0-resnet56]: Epoch 9/100, Acc=0.6438, Val Loss=1.3890, lr=0.0100
[02/21 19:03:07 cifar100-global-group_norm-2.0-resnet56]: Epoch 10/100, Acc=0.6296, Val Loss=1.4441, lr=0.0100
[02/21 19:03:35 cifar100-global-group_norm-2.0-resnet56]: Epoch 11/100, Acc=0.6487, Val Loss=1.3812, lr=0.0100
[02/21 19:04:03 cifar100-global-group_norm-2.0-resnet56]: Epoch 12/100, Acc=0.6556, Val Loss=1.3419, lr=0.0100
[02/21 19:04:31 cifar100-global-group_norm-2.0-resnet56]: Epoch 13/100, Acc=0.6409, Val Loss=1.3950, lr=0.0100
[02/21 19:04:59 cifar100-global-group_norm-2.0-resnet56]: Epoch 14/100, Acc=0.6628, Val Loss=1.3218, lr=0.0100
[02/21 19:05:27 cifar100-global-group_norm-2.0-resnet56]: Epoch 15/100, Acc=0.6525, Val Loss=1.3630, lr=0.0100
[02/21 19:05:54 cifar100-global-group_norm-2.0-resnet56]: Epoch 16/100, Acc=0.6444, Val Loss=1.3867, lr=0.0100
[02/21 19:06:22 cifar100-global-group_norm-2.0-resnet56]: Epoch 17/100, Acc=0.6480, Val Loss=1.4242, lr=0.0100
[02/21 19:06:50 cifar100-global-group_norm-2.0-resnet56]: Epoch 18/100, Acc=0.6456, Val Loss=1.4312, lr=0.0100
[02/21 19:07:18 cifar100-global-group_norm-2.0-resnet56]: Epoch 19/100, Acc=0.6462, Val Loss=1.4478, lr=0.0100
[02/21 19:07:45 cifar100-global-group_norm-2.0-resnet56]: Epoch 20/100, Acc=0.6526, Val Loss=1.3585, lr=0.0100
[02/21 19:08:13 cifar100-global-group_norm-2.0-resnet56]: Epoch 21/100, Acc=0.6601, Val Loss=1.3419, lr=0.0100
[02/21 19:08:41 cifar100-global-group_norm-2.0-resnet56]: Epoch 22/100, Acc=0.6589, Val Loss=1.3677, lr=0.0100
[02/21 19:09:09 cifar100-global-group_norm-2.0-resnet56]: Epoch 23/100, Acc=0.6552, Val Loss=1.3554, lr=0.0100
[02/21 19:09:36 cifar100-global-group_norm-2.0-resnet56]: Epoch 24/100, Acc=0.6425, Val Loss=1.4458, lr=0.0100
[02/21 19:10:04 cifar100-global-group_norm-2.0-resnet56]: Epoch 25/100, Acc=0.6518, Val Loss=1.4206, lr=0.0100
[02/21 19:10:32 cifar100-global-group_norm-2.0-resnet56]: Epoch 26/100, Acc=0.6611, Val Loss=1.3289, lr=0.0100
[02/21 19:11:00 cifar100-global-group_norm-2.0-resnet56]: Epoch 27/100, Acc=0.6339, Val Loss=1.4999, lr=0.0100
[02/21 19:11:28 cifar100-global-group_norm-2.0-resnet56]: Epoch 28/100, Acc=0.6535, Val Loss=1.3988, lr=0.0100
[02/21 19:11:56 cifar100-global-group_norm-2.0-resnet56]: Epoch 29/100, Acc=0.6641, Val Loss=1.3780, lr=0.0100
[02/21 19:12:24 cifar100-global-group_norm-2.0-resnet56]: Epoch 30/100, Acc=0.6412, Val Loss=1.4859, lr=0.0100
[02/21 19:12:51 cifar100-global-group_norm-2.0-resnet56]: Epoch 31/100, Acc=0.6413, Val Loss=1.4526, lr=0.0100
[02/21 19:13:19 cifar100-global-group_norm-2.0-resnet56]: Epoch 32/100, Acc=0.6540, Val Loss=1.4152, lr=0.0100
[02/21 19:13:47 cifar100-global-group_norm-2.0-resnet56]: Epoch 33/100, Acc=0.6517, Val Loss=1.4306, lr=0.0100
[02/21 19:14:15 cifar100-global-group_norm-2.0-resnet56]: Epoch 34/100, Acc=0.6456, Val Loss=1.4415, lr=0.0100
[02/21 19:14:43 cifar100-global-group_norm-2.0-resnet56]: Epoch 35/100, Acc=0.6550, Val Loss=1.3707, lr=0.0100
[02/21 19:15:11 cifar100-global-group_norm-2.0-resnet56]: Epoch 36/100, Acc=0.6452, Val Loss=1.4570, lr=0.0100
[02/21 19:15:39 cifar100-global-group_norm-2.0-resnet56]: Epoch 37/100, Acc=0.6586, Val Loss=1.3741, lr=0.0100
[02/21 19:16:07 cifar100-global-group_norm-2.0-resnet56]: Epoch 38/100, Acc=0.6560, Val Loss=1.3849, lr=0.0100
[02/21 19:16:35 cifar100-global-group_norm-2.0-resnet56]: Epoch 39/100, Acc=0.6603, Val Loss=1.3828, lr=0.0100
[02/21 19:17:02 cifar100-global-group_norm-2.0-resnet56]: Epoch 40/100, Acc=0.6597, Val Loss=1.3898, lr=0.0100
[02/21 19:17:30 cifar100-global-group_norm-2.0-resnet56]: Epoch 41/100, Acc=0.6626, Val Loss=1.4092, lr=0.0100
[02/21 19:17:58 cifar100-global-group_norm-2.0-resnet56]: Epoch 42/100, Acc=0.6423, Val Loss=1.4961, lr=0.0100
[02/21 19:18:25 cifar100-global-group_norm-2.0-resnet56]: Epoch 43/100, Acc=0.6477, Val Loss=1.4444, lr=0.0100
[02/21 19:18:53 cifar100-global-group_norm-2.0-resnet56]: Epoch 44/100, Acc=0.6652, Val Loss=1.3662, lr=0.0100
[02/21 19:19:21 cifar100-global-group_norm-2.0-resnet56]: Epoch 45/100, Acc=0.6561, Val Loss=1.3875, lr=0.0100
[02/21 19:19:49 cifar100-global-group_norm-2.0-resnet56]: Epoch 46/100, Acc=0.6573, Val Loss=1.4076, lr=0.0100
[02/21 19:20:16 cifar100-global-group_norm-2.0-resnet56]: Epoch 47/100, Acc=0.6561, Val Loss=1.4165, lr=0.0100
[02/21 19:20:44 cifar100-global-group_norm-2.0-resnet56]: Epoch 48/100, Acc=0.6706, Val Loss=1.3514, lr=0.0100
[02/21 19:21:12 cifar100-global-group_norm-2.0-resnet56]: Epoch 49/100, Acc=0.6488, Val Loss=1.4527, lr=0.0100
[02/21 19:21:39 cifar100-global-group_norm-2.0-resnet56]: Epoch 50/100, Acc=0.6461, Val Loss=1.4854, lr=0.0100
[02/21 19:22:07 cifar100-global-group_norm-2.0-resnet56]: Epoch 51/100, Acc=0.6566, Val Loss=1.4102, lr=0.0100
[02/21 19:22:34 cifar100-global-group_norm-2.0-resnet56]: Epoch 52/100, Acc=0.6496, Val Loss=1.4158, lr=0.0100
[02/21 19:23:01 cifar100-global-group_norm-2.0-resnet56]: Epoch 53/100, Acc=0.6581, Val Loss=1.4100, lr=0.0100
[02/21 19:23:29 cifar100-global-group_norm-2.0-resnet56]: Epoch 54/100, Acc=0.6509, Val Loss=1.4950, lr=0.0100
[02/21 19:23:56 cifar100-global-group_norm-2.0-resnet56]: Epoch 55/100, Acc=0.6623, Val Loss=1.3560, lr=0.0100
[02/21 19:24:24 cifar100-global-group_norm-2.0-resnet56]: Epoch 56/100, Acc=0.6565, Val Loss=1.3677, lr=0.0100
[02/21 19:24:51 cifar100-global-group_norm-2.0-resnet56]: Epoch 57/100, Acc=0.6598, Val Loss=1.4093, lr=0.0100
[02/21 19:25:19 cifar100-global-group_norm-2.0-resnet56]: Epoch 58/100, Acc=0.6500, Val Loss=1.4189, lr=0.0100
[02/21 19:25:46 cifar100-global-group_norm-2.0-resnet56]: Epoch 59/100, Acc=0.6555, Val Loss=1.4012, lr=0.0100
[02/21 19:26:13 cifar100-global-group_norm-2.0-resnet56]: Epoch 60/100, Acc=0.7137, Val Loss=1.1455, lr=0.0010
[02/21 19:26:41 cifar100-global-group_norm-2.0-resnet56]: Epoch 61/100, Acc=0.7135, Val Loss=1.1522, lr=0.0010
[02/21 19:27:08 cifar100-global-group_norm-2.0-resnet56]: Epoch 62/100, Acc=0.7133, Val Loss=1.1562, lr=0.0010
[02/21 19:27:35 cifar100-global-group_norm-2.0-resnet56]: Epoch 63/100, Acc=0.7176, Val Loss=1.1616, lr=0.0010
[02/21 19:28:03 cifar100-global-group_norm-2.0-resnet56]: Epoch 64/100, Acc=0.7164, Val Loss=1.1667, lr=0.0010
[02/21 19:28:31 cifar100-global-group_norm-2.0-resnet56]: Epoch 65/100, Acc=0.7159, Val Loss=1.1714, lr=0.0010
[02/21 19:28:59 cifar100-global-group_norm-2.0-resnet56]: Epoch 66/100, Acc=0.7129, Val Loss=1.1787, lr=0.0010
[02/21 19:29:26 cifar100-global-group_norm-2.0-resnet56]: Epoch 67/100, Acc=0.7159, Val Loss=1.1868, lr=0.0010
[02/21 19:29:54 cifar100-global-group_norm-2.0-resnet56]: Epoch 68/100, Acc=0.7153, Val Loss=1.1851, lr=0.0010
[02/21 19:30:22 cifar100-global-group_norm-2.0-resnet56]: Epoch 69/100, Acc=0.7139, Val Loss=1.1969, lr=0.0010
[02/21 19:30:50 cifar100-global-group_norm-2.0-resnet56]: Epoch 70/100, Acc=0.7165, Val Loss=1.1969, lr=0.0010
[02/21 19:31:17 cifar100-global-group_norm-2.0-resnet56]: Epoch 71/100, Acc=0.7139, Val Loss=1.1993, lr=0.0010
[02/21 19:31:44 cifar100-global-group_norm-2.0-resnet56]: Epoch 72/100, Acc=0.7140, Val Loss=1.2011, lr=0.0010
[02/21 19:32:12 cifar100-global-group_norm-2.0-resnet56]: Epoch 73/100, Acc=0.7147, Val Loss=1.2147, lr=0.0010
[02/21 19:32:39 cifar100-global-group_norm-2.0-resnet56]: Epoch 74/100, Acc=0.7138, Val Loss=1.2155, lr=0.0010
[02/21 19:33:06 cifar100-global-group_norm-2.0-resnet56]: Epoch 75/100, Acc=0.7147, Val Loss=1.2174, lr=0.0010
[02/21 19:33:34 cifar100-global-group_norm-2.0-resnet56]: Epoch 76/100, Acc=0.7129, Val Loss=1.2253, lr=0.0010
[02/21 19:34:02 cifar100-global-group_norm-2.0-resnet56]: Epoch 77/100, Acc=0.7161, Val Loss=1.2236, lr=0.0010
[02/21 19:34:29 cifar100-global-group_norm-2.0-resnet56]: Epoch 78/100, Acc=0.7144, Val Loss=1.2315, lr=0.0010
[02/21 19:34:57 cifar100-global-group_norm-2.0-resnet56]: Epoch 79/100, Acc=0.7113, Val Loss=1.2368, lr=0.0010
[02/21 19:35:25 cifar100-global-group_norm-2.0-resnet56]: Epoch 80/100, Acc=0.7137, Val Loss=1.2268, lr=0.0001
[02/21 19:35:52 cifar100-global-group_norm-2.0-resnet56]: Epoch 81/100, Acc=0.7177, Val Loss=1.2283, lr=0.0001
[02/21 19:36:20 cifar100-global-group_norm-2.0-resnet56]: Epoch 82/100, Acc=0.7159, Val Loss=1.2267, lr=0.0001
[02/21 19:36:48 cifar100-global-group_norm-2.0-resnet56]: Epoch 83/100, Acc=0.7151, Val Loss=1.2305, lr=0.0001
[02/21 19:37:16 cifar100-global-group_norm-2.0-resnet56]: Epoch 84/100, Acc=0.7168, Val Loss=1.2323, lr=0.0001
[02/21 19:37:44 cifar100-global-group_norm-2.0-resnet56]: Epoch 85/100, Acc=0.7155, Val Loss=1.2273, lr=0.0001
[02/21 19:38:11 cifar100-global-group_norm-2.0-resnet56]: Epoch 86/100, Acc=0.7153, Val Loss=1.2319, lr=0.0001
[02/21 19:38:39 cifar100-global-group_norm-2.0-resnet56]: Epoch 87/100, Acc=0.7146, Val Loss=1.2346, lr=0.0001
[02/21 19:39:06 cifar100-global-group_norm-2.0-resnet56]: Epoch 88/100, Acc=0.7146, Val Loss=1.2284, lr=0.0001
[02/21 19:39:34 cifar100-global-group_norm-2.0-resnet56]: Epoch 89/100, Acc=0.7152, Val Loss=1.2305, lr=0.0001
[02/21 19:40:01 cifar100-global-group_norm-2.0-resnet56]: Epoch 90/100, Acc=0.7159, Val Loss=1.2264, lr=0.0001
[02/21 19:40:29 cifar100-global-group_norm-2.0-resnet56]: Epoch 91/100, Acc=0.7164, Val Loss=1.2388, lr=0.0001
[02/21 19:40:56 cifar100-global-group_norm-2.0-resnet56]: Epoch 92/100, Acc=0.7158, Val Loss=1.2291, lr=0.0001
[02/21 19:41:24 cifar100-global-group_norm-2.0-resnet56]: Epoch 93/100, Acc=0.7158, Val Loss=1.2320, lr=0.0001
[02/21 19:41:51 cifar100-global-group_norm-2.0-resnet56]: Epoch 94/100, Acc=0.7162, Val Loss=1.2295, lr=0.0001
[02/21 19:42:19 cifar100-global-group_norm-2.0-resnet56]: Epoch 95/100, Acc=0.7158, Val Loss=1.2339, lr=0.0001
[02/21 19:42:46 cifar100-global-group_norm-2.0-resnet56]: Epoch 96/100, Acc=0.7144, Val Loss=1.2375, lr=0.0001
[02/21 19:43:14 cifar100-global-group_norm-2.0-resnet56]: Epoch 97/100, Acc=0.7156, Val Loss=1.2330, lr=0.0001
[02/21 19:43:42 cifar100-global-group_norm-2.0-resnet56]: Epoch 98/100, Acc=0.7141, Val Loss=1.2311, lr=0.0001
[02/21 19:44:09 cifar100-global-group_norm-2.0-resnet56]: Epoch 99/100, Acc=0.7150, Val Loss=1.2334, lr=0.0001
[02/21 19:44:09 cifar100-global-group_norm-2.0-resnet56]: Best Acc=0.7177
[02/21 19:44:09 cifar100-global-group_norm-2.0-resnet56]: Params: 0.68 M
[02/21 19:44:09 cifar100-global-group_norm-2.0-resnet56]: ops: 63.42 M
[02/21 19:44:12 cifar100-global-group_norm-2.0-resnet56]: Acc: 0.7150 Val Loss: 1.2334

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: mode: prune
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: model: resnet56
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: verbose: False
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: dataset: cifar100
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: dataroot: data
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: batch_size: 128
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: total_epochs: 100
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: lr_decay_milestones: 60,80
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: lr_decay_gamma: 0.1
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: lr: 0.01
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: output_dir: run/cifar100/prune/cifar100-global-group_sl-2.0-resnet56
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: finetune: True
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: last_epochs: 100
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: reps: 1
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: method: group_sl
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: speed_up: 2.0
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: max_pruning_ratio: 1.0
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: reg: 1e-05
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: delta_reg: 0.0001
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: weight_decay: 0.0005
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: seed: 1
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: global_pruning: True
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: sl_total_epochs: 100
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: sl_lr: 0.01
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: sl_reg_warmup: 0
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: sl_restore: None
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: iterative_steps: 400
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: logger: <Logger cifar100-global-group_sl-2.0-resnet56 (DEBUG)>
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: device: cuda
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: num_classes: 100
[02/21 19:44:20 cifar100-global-group_sl-2.0-resnet56]: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 19:44:21 cifar100-global-group_sl-2.0-resnet56]: Regularizing...
[02/21 19:45:18 cifar100-global-group_sl-2.0-resnet56]: Epoch 0/100, Acc=0.6460, Val Loss=1.5129, lr=0.0100
[02/21 19:46:15 cifar100-global-group_sl-2.0-resnet56]: Epoch 1/100, Acc=0.6689, Val Loss=1.3476, lr=0.0100
[02/21 19:47:12 cifar100-global-group_sl-2.0-resnet56]: Epoch 2/100, Acc=0.6544, Val Loss=1.4834, lr=0.0100
[02/21 19:48:09 cifar100-global-group_sl-2.0-resnet56]: Epoch 3/100, Acc=0.6637, Val Loss=1.3945, lr=0.0100
[02/21 19:49:06 cifar100-global-group_sl-2.0-resnet56]: Epoch 4/100, Acc=0.6692, Val Loss=1.4224, lr=0.0100
[02/21 19:50:02 cifar100-global-group_sl-2.0-resnet56]: Epoch 5/100, Acc=0.6662, Val Loss=1.4349, lr=0.0100
[02/21 19:50:59 cifar100-global-group_sl-2.0-resnet56]: Epoch 6/100, Acc=0.6650, Val Loss=1.4500, lr=0.0100
[02/21 19:51:56 cifar100-global-group_sl-2.0-resnet56]: Epoch 7/100, Acc=0.6794, Val Loss=1.3545, lr=0.0100
[02/21 19:52:52 cifar100-global-group_sl-2.0-resnet56]: Epoch 8/100, Acc=0.6624, Val Loss=1.4711, lr=0.0100
[02/21 19:53:49 cifar100-global-group_sl-2.0-resnet56]: Epoch 9/100, Acc=0.6784, Val Loss=1.4285, lr=0.0100
[02/21 19:54:46 cifar100-global-group_sl-2.0-resnet56]: Epoch 10/100, Acc=0.6820, Val Loss=1.3587, lr=0.0100
[02/21 19:55:43 cifar100-global-group_sl-2.0-resnet56]: Epoch 11/100, Acc=0.6735, Val Loss=1.4463, lr=0.0100
[02/21 19:56:40 cifar100-global-group_sl-2.0-resnet56]: Epoch 12/100, Acc=0.6730, Val Loss=1.4785, lr=0.0100
[02/21 19:57:37 cifar100-global-group_sl-2.0-resnet56]: Epoch 13/100, Acc=0.6739, Val Loss=1.4501, lr=0.0100
[02/21 19:58:33 cifar100-global-group_sl-2.0-resnet56]: Epoch 14/100, Acc=0.6816, Val Loss=1.4251, lr=0.0100
[02/21 19:59:30 cifar100-global-group_sl-2.0-resnet56]: Epoch 15/100, Acc=0.6839, Val Loss=1.4052, lr=0.0100
[02/21 20:00:27 cifar100-global-group_sl-2.0-resnet56]: Epoch 16/100, Acc=0.6823, Val Loss=1.4412, lr=0.0100
[02/21 20:01:24 cifar100-global-group_sl-2.0-resnet56]: Epoch 17/100, Acc=0.6809, Val Loss=1.4346, lr=0.0100
[02/21 20:02:21 cifar100-global-group_sl-2.0-resnet56]: Epoch 18/100, Acc=0.6881, Val Loss=1.4181, lr=0.0100
[02/21 20:03:18 cifar100-global-group_sl-2.0-resnet56]: Epoch 19/100, Acc=0.6826, Val Loss=1.4438, lr=0.0100
[02/21 20:04:15 cifar100-global-group_sl-2.0-resnet56]: Epoch 20/100, Acc=0.6864, Val Loss=1.4751, lr=0.0100
[02/21 20:05:12 cifar100-global-group_sl-2.0-resnet56]: Epoch 21/100, Acc=0.6884, Val Loss=1.4355, lr=0.0100
[02/21 20:06:09 cifar100-global-group_sl-2.0-resnet56]: Epoch 22/100, Acc=0.6751, Val Loss=1.5248, lr=0.0100
[02/21 20:07:06 cifar100-global-group_sl-2.0-resnet56]: Epoch 23/100, Acc=0.6852, Val Loss=1.5057, lr=0.0100
[02/21 20:08:03 cifar100-global-group_sl-2.0-resnet56]: Epoch 24/100, Acc=0.6817, Val Loss=1.5286, lr=0.0100
[02/21 20:09:00 cifar100-global-group_sl-2.0-resnet56]: Epoch 25/100, Acc=0.6902, Val Loss=1.4554, lr=0.0100
[02/21 20:09:57 cifar100-global-group_sl-2.0-resnet56]: Epoch 26/100, Acc=0.6794, Val Loss=1.5462, lr=0.0100
[02/21 20:10:54 cifar100-global-group_sl-2.0-resnet56]: Epoch 27/100, Acc=0.6775, Val Loss=1.5435, lr=0.0100
[02/21 20:11:51 cifar100-global-group_sl-2.0-resnet56]: Epoch 28/100, Acc=0.6851, Val Loss=1.5417, lr=0.0100
[02/21 20:12:49 cifar100-global-group_sl-2.0-resnet56]: Epoch 29/100, Acc=0.6892, Val Loss=1.5175, lr=0.0100
[02/21 20:13:46 cifar100-global-group_sl-2.0-resnet56]: Epoch 30/100, Acc=0.6831, Val Loss=1.5638, lr=0.0100
[02/21 20:14:43 cifar100-global-group_sl-2.0-resnet56]: Epoch 31/100, Acc=0.6874, Val Loss=1.5464, lr=0.0100
[02/21 20:15:41 cifar100-global-group_sl-2.0-resnet56]: Epoch 32/100, Acc=0.6886, Val Loss=1.5256, lr=0.0100
[02/21 20:16:38 cifar100-global-group_sl-2.0-resnet56]: Epoch 33/100, Acc=0.6845, Val Loss=1.5287, lr=0.0100
[02/21 20:17:36 cifar100-global-group_sl-2.0-resnet56]: Epoch 34/100, Acc=0.6786, Val Loss=1.5948, lr=0.0100
[02/21 20:18:33 cifar100-global-group_sl-2.0-resnet56]: Epoch 35/100, Acc=0.6830, Val Loss=1.5888, lr=0.0100
[02/21 20:19:31 cifar100-global-group_sl-2.0-resnet56]: Epoch 36/100, Acc=0.6832, Val Loss=1.6289, lr=0.0100
[02/21 20:20:28 cifar100-global-group_sl-2.0-resnet56]: Epoch 37/100, Acc=0.6819, Val Loss=1.6168, lr=0.0100
[02/21 20:21:25 cifar100-global-group_sl-2.0-resnet56]: Epoch 38/100, Acc=0.6876, Val Loss=1.5617, lr=0.0100
[02/21 20:22:22 cifar100-global-group_sl-2.0-resnet56]: Epoch 39/100, Acc=0.6778, Val Loss=1.6857, lr=0.0100
[02/21 20:23:20 cifar100-global-group_sl-2.0-resnet56]: Epoch 40/100, Acc=0.6896, Val Loss=1.5976, lr=0.0100
[02/21 20:24:18 cifar100-global-group_sl-2.0-resnet56]: Epoch 41/100, Acc=0.6817, Val Loss=1.6490, lr=0.0100
[02/21 20:25:15 cifar100-global-group_sl-2.0-resnet56]: Epoch 42/100, Acc=0.6964, Val Loss=1.5582, lr=0.0100
[02/21 20:26:13 cifar100-global-group_sl-2.0-resnet56]: Epoch 43/100, Acc=0.6866, Val Loss=1.6320, lr=0.0100
[02/21 20:27:11 cifar100-global-group_sl-2.0-resnet56]: Epoch 44/100, Acc=0.6891, Val Loss=1.6404, lr=0.0100
[02/21 20:28:10 cifar100-global-group_sl-2.0-resnet56]: Epoch 45/100, Acc=0.6898, Val Loss=1.6134, lr=0.0100
[02/21 20:29:08 cifar100-global-group_sl-2.0-resnet56]: Epoch 46/100, Acc=0.6892, Val Loss=1.6626, lr=0.0100
[02/21 20:30:06 cifar100-global-group_sl-2.0-resnet56]: Epoch 47/100, Acc=0.6880, Val Loss=1.6549, lr=0.0100
[02/21 20:31:03 cifar100-global-group_sl-2.0-resnet56]: Epoch 48/100, Acc=0.6885, Val Loss=1.6524, lr=0.0100
[02/21 20:32:01 cifar100-global-group_sl-2.0-resnet56]: Epoch 49/100, Acc=0.6894, Val Loss=1.6180, lr=0.0100
[02/21 20:32:59 cifar100-global-group_sl-2.0-resnet56]: Epoch 50/100, Acc=0.6846, Val Loss=1.6499, lr=0.0100
[02/21 20:33:57 cifar100-global-group_sl-2.0-resnet56]: Epoch 51/100, Acc=0.6830, Val Loss=1.6937, lr=0.0100
[02/21 20:34:54 cifar100-global-group_sl-2.0-resnet56]: Epoch 52/100, Acc=0.6818, Val Loss=1.6874, lr=0.0100
[02/21 20:35:52 cifar100-global-group_sl-2.0-resnet56]: Epoch 53/100, Acc=0.6843, Val Loss=1.6635, lr=0.0100
[02/21 20:36:50 cifar100-global-group_sl-2.0-resnet56]: Epoch 54/100, Acc=0.6911, Val Loss=1.6856, lr=0.0100
[02/21 20:37:48 cifar100-global-group_sl-2.0-resnet56]: Epoch 55/100, Acc=0.6897, Val Loss=1.6351, lr=0.0100
[02/21 20:38:46 cifar100-global-group_sl-2.0-resnet56]: Epoch 56/100, Acc=0.6916, Val Loss=1.6290, lr=0.0100
[02/21 20:39:43 cifar100-global-group_sl-2.0-resnet56]: Epoch 57/100, Acc=0.6899, Val Loss=1.7066, lr=0.0100
[02/21 20:40:41 cifar100-global-group_sl-2.0-resnet56]: Epoch 58/100, Acc=0.6790, Val Loss=1.7566, lr=0.0100
[02/21 20:41:39 cifar100-global-group_sl-2.0-resnet56]: Epoch 59/100, Acc=0.6926, Val Loss=1.6460, lr=0.0100
[02/21 20:42:37 cifar100-global-group_sl-2.0-resnet56]: Epoch 60/100, Acc=0.7173, Val Loss=1.5079, lr=0.0010
[02/21 20:43:36 cifar100-global-group_sl-2.0-resnet56]: Epoch 61/100, Acc=0.7186, Val Loss=1.5073, lr=0.0010
[02/21 20:44:33 cifar100-global-group_sl-2.0-resnet56]: Epoch 62/100, Acc=0.7201, Val Loss=1.5079, lr=0.0010
[02/21 20:45:30 cifar100-global-group_sl-2.0-resnet56]: Epoch 63/100, Acc=0.7216, Val Loss=1.5140, lr=0.0010
[02/21 20:46:28 cifar100-global-group_sl-2.0-resnet56]: Epoch 64/100, Acc=0.7215, Val Loss=1.5089, lr=0.0010
[02/21 20:47:25 cifar100-global-group_sl-2.0-resnet56]: Epoch 65/100, Acc=0.7206, Val Loss=1.5195, lr=0.0010
[02/21 20:48:22 cifar100-global-group_sl-2.0-resnet56]: Epoch 66/100, Acc=0.7211, Val Loss=1.5063, lr=0.0010
[02/21 20:49:19 cifar100-global-group_sl-2.0-resnet56]: Epoch 67/100, Acc=0.7214, Val Loss=1.5106, lr=0.0010
[02/21 20:50:16 cifar100-global-group_sl-2.0-resnet56]: Epoch 68/100, Acc=0.7206, Val Loss=1.5161, lr=0.0010
[02/21 20:51:13 cifar100-global-group_sl-2.0-resnet56]: Epoch 69/100, Acc=0.7222, Val Loss=1.5174, lr=0.0010
[02/21 20:52:11 cifar100-global-group_sl-2.0-resnet56]: Epoch 70/100, Acc=0.7235, Val Loss=1.5192, lr=0.0010
[02/21 20:53:08 cifar100-global-group_sl-2.0-resnet56]: Epoch 71/100, Acc=0.7230, Val Loss=1.5232, lr=0.0010
[02/21 20:54:05 cifar100-global-group_sl-2.0-resnet56]: Epoch 72/100, Acc=0.7229, Val Loss=1.5256, lr=0.0010
[02/21 20:55:01 cifar100-global-group_sl-2.0-resnet56]: Epoch 73/100, Acc=0.7206, Val Loss=1.5406, lr=0.0010
[02/21 20:55:57 cifar100-global-group_sl-2.0-resnet56]: Epoch 74/100, Acc=0.7215, Val Loss=1.5342, lr=0.0010
[02/21 20:56:54 cifar100-global-group_sl-2.0-resnet56]: Epoch 75/100, Acc=0.7217, Val Loss=1.5418, lr=0.0010
[02/21 20:57:50 cifar100-global-group_sl-2.0-resnet56]: Epoch 76/100, Acc=0.7213, Val Loss=1.5366, lr=0.0010
[02/21 20:58:47 cifar100-global-group_sl-2.0-resnet56]: Epoch 77/100, Acc=0.7215, Val Loss=1.5391, lr=0.0010
[02/21 20:59:43 cifar100-global-group_sl-2.0-resnet56]: Epoch 78/100, Acc=0.7197, Val Loss=1.5539, lr=0.0010
[02/21 21:00:40 cifar100-global-group_sl-2.0-resnet56]: Epoch 79/100, Acc=0.7217, Val Loss=1.5481, lr=0.0010
[02/21 21:01:37 cifar100-global-group_sl-2.0-resnet56]: Epoch 80/100, Acc=0.7206, Val Loss=1.5538, lr=0.0001
[02/21 21:02:33 cifar100-global-group_sl-2.0-resnet56]: Epoch 81/100, Acc=0.7216, Val Loss=1.5447, lr=0.0001
[02/21 21:03:30 cifar100-global-group_sl-2.0-resnet56]: Epoch 82/100, Acc=0.7220, Val Loss=1.5488, lr=0.0001
[02/21 21:04:27 cifar100-global-group_sl-2.0-resnet56]: Epoch 83/100, Acc=0.7201, Val Loss=1.5486, lr=0.0001
[02/21 21:05:23 cifar100-global-group_sl-2.0-resnet56]: Epoch 84/100, Acc=0.7198, Val Loss=1.5473, lr=0.0001
[02/21 21:06:20 cifar100-global-group_sl-2.0-resnet56]: Epoch 85/100, Acc=0.7205, Val Loss=1.5523, lr=0.0001
[02/21 21:07:16 cifar100-global-group_sl-2.0-resnet56]: Epoch 86/100, Acc=0.7217, Val Loss=1.5484, lr=0.0001
[02/21 21:08:12 cifar100-global-group_sl-2.0-resnet56]: Epoch 87/100, Acc=0.7219, Val Loss=1.5401, lr=0.0001
[02/21 21:09:09 cifar100-global-group_sl-2.0-resnet56]: Epoch 88/100, Acc=0.7210, Val Loss=1.5479, lr=0.0001
[02/21 21:10:05 cifar100-global-group_sl-2.0-resnet56]: Epoch 89/100, Acc=0.7239, Val Loss=1.5460, lr=0.0001
[02/21 21:11:02 cifar100-global-group_sl-2.0-resnet56]: Epoch 90/100, Acc=0.7208, Val Loss=1.5425, lr=0.0001
[02/21 21:11:58 cifar100-global-group_sl-2.0-resnet56]: Epoch 91/100, Acc=0.7214, Val Loss=1.5466, lr=0.0001
[02/21 21:12:54 cifar100-global-group_sl-2.0-resnet56]: Epoch 92/100, Acc=0.7221, Val Loss=1.5442, lr=0.0001
[02/21 21:13:51 cifar100-global-group_sl-2.0-resnet56]: Epoch 93/100, Acc=0.7218, Val Loss=1.5433, lr=0.0001
[02/21 21:14:47 cifar100-global-group_sl-2.0-resnet56]: Epoch 94/100, Acc=0.7208, Val Loss=1.5415, lr=0.0001
[02/21 21:15:44 cifar100-global-group_sl-2.0-resnet56]: Epoch 95/100, Acc=0.7218, Val Loss=1.5466, lr=0.0001
[02/21 21:16:41 cifar100-global-group_sl-2.0-resnet56]: Epoch 96/100, Acc=0.7229, Val Loss=1.5460, lr=0.0001
[02/21 21:17:37 cifar100-global-group_sl-2.0-resnet56]: Epoch 97/100, Acc=0.7205, Val Loss=1.5537, lr=0.0001
[02/21 21:18:34 cifar100-global-group_sl-2.0-resnet56]: Epoch 98/100, Acc=0.7202, Val Loss=1.5465, lr=0.0001
[02/21 21:19:31 cifar100-global-group_sl-2.0-resnet56]: Epoch 99/100, Acc=0.7227, Val Loss=1.5460, lr=0.0001
[02/21 21:19:31 cifar100-global-group_sl-2.0-resnet56]: Best Acc=0.7239
[02/21 21:19:31 cifar100-global-group_sl-2.0-resnet56]: Loading the sparse model from run/cifar100/prune/cifar100-global-group_sl-2.0-resnet56/reg_cifar100_resnet56_group_sl_1e-05.pth...
[02/21 21:19:34 cifar100-global-group_sl-2.0-resnet56]: Pruning...
[02/21 21:19:44 cifar100-global-group_sl-2.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(8, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(8, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(8, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(8, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(8, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(8, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(8, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(8, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(8, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(8, 19, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(8, 23, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(23, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(23, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(23, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(23, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(23, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(23, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(23, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(23, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(23, 21, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(21, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(23, 61, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(61, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(61, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(61, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(61, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(61, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(61, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(58, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=61, out_features=100, bias=True)
)
[02/21 21:19:47 cifar100-global-group_sl-2.0-resnet56]: Params: 0.86 M => 0.65 M (75.14%)
[02/21 21:19:47 cifar100-global-group_sl-2.0-resnet56]: FLOPs: 127.12 M => 63.32 M (49.81%, 2.01X )
[02/21 21:19:47 cifar100-global-group_sl-2.0-resnet56]: Acc: 0.7239 => 0.0108
[02/21 21:19:47 cifar100-global-group_sl-2.0-resnet56]: Val Loss: 1.5460 => 21.2665
[02/21 21:19:47 cifar100-global-group_sl-2.0-resnet56]: Finetuning...
[02/21 21:20:15 cifar100-global-group_sl-2.0-resnet56]: Epoch 0/100, Acc=0.6132, Val Loss=1.5345, lr=0.0100
[02/21 21:20:44 cifar100-global-group_sl-2.0-resnet56]: Epoch 1/100, Acc=0.6247, Val Loss=1.6219, lr=0.0100
[02/21 21:21:12 cifar100-global-group_sl-2.0-resnet56]: Epoch 2/100, Acc=0.6440, Val Loss=1.4944, lr=0.0100
[02/21 21:21:40 cifar100-global-group_sl-2.0-resnet56]: Epoch 3/100, Acc=0.6401, Val Loss=1.5284, lr=0.0100
[02/21 21:22:08 cifar100-global-group_sl-2.0-resnet56]: Epoch 4/100, Acc=0.6413, Val Loss=1.5582, lr=0.0100
[02/21 21:22:36 cifar100-global-group_sl-2.0-resnet56]: Epoch 5/100, Acc=0.6384, Val Loss=1.5188, lr=0.0100
[02/21 21:23:05 cifar100-global-group_sl-2.0-resnet56]: Epoch 6/100, Acc=0.6478, Val Loss=1.4743, lr=0.0100
[02/21 21:23:33 cifar100-global-group_sl-2.0-resnet56]: Epoch 7/100, Acc=0.6338, Val Loss=1.5697, lr=0.0100
[02/21 21:24:01 cifar100-global-group_sl-2.0-resnet56]: Epoch 8/100, Acc=0.6384, Val Loss=1.5659, lr=0.0100
[02/21 21:24:29 cifar100-global-group_sl-2.0-resnet56]: Epoch 9/100, Acc=0.6446, Val Loss=1.4920, lr=0.0100
[02/21 21:24:57 cifar100-global-group_sl-2.0-resnet56]: Epoch 10/100, Acc=0.6392, Val Loss=1.5237, lr=0.0100
[02/21 21:25:26 cifar100-global-group_sl-2.0-resnet56]: Epoch 11/100, Acc=0.6280, Val Loss=1.5670, lr=0.0100
[02/21 21:25:54 cifar100-global-group_sl-2.0-resnet56]: Epoch 12/100, Acc=0.6501, Val Loss=1.4826, lr=0.0100
[02/21 21:26:22 cifar100-global-group_sl-2.0-resnet56]: Epoch 13/100, Acc=0.6522, Val Loss=1.4471, lr=0.0100
[02/21 21:26:49 cifar100-global-group_sl-2.0-resnet56]: Epoch 14/100, Acc=0.6525, Val Loss=1.4506, lr=0.0100
[02/21 21:27:17 cifar100-global-group_sl-2.0-resnet56]: Epoch 15/100, Acc=0.6473, Val Loss=1.4684, lr=0.0100
[02/21 21:27:45 cifar100-global-group_sl-2.0-resnet56]: Epoch 16/100, Acc=0.6498, Val Loss=1.4337, lr=0.0100
[02/21 21:28:13 cifar100-global-group_sl-2.0-resnet56]: Epoch 17/100, Acc=0.6423, Val Loss=1.5299, lr=0.0100
[02/21 21:28:41 cifar100-global-group_sl-2.0-resnet56]: Epoch 18/100, Acc=0.6516, Val Loss=1.4414, lr=0.0100
[02/21 21:29:09 cifar100-global-group_sl-2.0-resnet56]: Epoch 19/100, Acc=0.6518, Val Loss=1.4888, lr=0.0100
[02/21 21:29:36 cifar100-global-group_sl-2.0-resnet56]: Epoch 20/100, Acc=0.6357, Val Loss=1.5399, lr=0.0100
[02/21 21:30:04 cifar100-global-group_sl-2.0-resnet56]: Epoch 21/100, Acc=0.6362, Val Loss=1.5137, lr=0.0100
[02/21 21:30:32 cifar100-global-group_sl-2.0-resnet56]: Epoch 22/100, Acc=0.6366, Val Loss=1.5161, lr=0.0100
[02/21 21:31:00 cifar100-global-group_sl-2.0-resnet56]: Epoch 23/100, Acc=0.6569, Val Loss=1.4218, lr=0.0100
[02/21 21:31:28 cifar100-global-group_sl-2.0-resnet56]: Epoch 24/100, Acc=0.6318, Val Loss=1.5675, lr=0.0100
[02/21 21:31:56 cifar100-global-group_sl-2.0-resnet56]: Epoch 25/100, Acc=0.6530, Val Loss=1.4445, lr=0.0100
[02/21 21:32:24 cifar100-global-group_sl-2.0-resnet56]: Epoch 26/100, Acc=0.6484, Val Loss=1.4685, lr=0.0100
[02/21 21:32:51 cifar100-global-group_sl-2.0-resnet56]: Epoch 27/100, Acc=0.6529, Val Loss=1.4433, lr=0.0100
[02/21 21:33:19 cifar100-global-group_sl-2.0-resnet56]: Epoch 28/100, Acc=0.6414, Val Loss=1.4860, lr=0.0100
[02/21 21:33:47 cifar100-global-group_sl-2.0-resnet56]: Epoch 29/100, Acc=0.6374, Val Loss=1.5378, lr=0.0100
[02/21 21:34:15 cifar100-global-group_sl-2.0-resnet56]: Epoch 30/100, Acc=0.6316, Val Loss=1.5573, lr=0.0100
[02/21 21:34:43 cifar100-global-group_sl-2.0-resnet56]: Epoch 31/100, Acc=0.6560, Val Loss=1.4076, lr=0.0100
[02/21 21:35:10 cifar100-global-group_sl-2.0-resnet56]: Epoch 32/100, Acc=0.6499, Val Loss=1.4756, lr=0.0100
[02/21 21:35:38 cifar100-global-group_sl-2.0-resnet56]: Epoch 33/100, Acc=0.6440, Val Loss=1.4951, lr=0.0100
[02/21 21:36:06 cifar100-global-group_sl-2.0-resnet56]: Epoch 34/100, Acc=0.6595, Val Loss=1.3588, lr=0.0100
[02/21 21:36:33 cifar100-global-group_sl-2.0-resnet56]: Epoch 35/100, Acc=0.6387, Val Loss=1.5103, lr=0.0100
[02/21 21:37:01 cifar100-global-group_sl-2.0-resnet56]: Epoch 36/100, Acc=0.6350, Val Loss=1.5793, lr=0.0100
[02/21 21:37:28 cifar100-global-group_sl-2.0-resnet56]: Epoch 37/100, Acc=0.6371, Val Loss=1.5088, lr=0.0100
[02/21 21:37:56 cifar100-global-group_sl-2.0-resnet56]: Epoch 38/100, Acc=0.6462, Val Loss=1.4693, lr=0.0100
[02/21 21:38:23 cifar100-global-group_sl-2.0-resnet56]: Epoch 39/100, Acc=0.6550, Val Loss=1.4201, lr=0.0100
[02/21 21:38:51 cifar100-global-group_sl-2.0-resnet56]: Epoch 40/100, Acc=0.6551, Val Loss=1.4350, lr=0.0100
[02/21 21:39:18 cifar100-global-group_sl-2.0-resnet56]: Epoch 41/100, Acc=0.6575, Val Loss=1.4899, lr=0.0100
[02/21 21:39:45 cifar100-global-group_sl-2.0-resnet56]: Epoch 42/100, Acc=0.6488, Val Loss=1.4584, lr=0.0100
[02/21 21:40:13 cifar100-global-group_sl-2.0-resnet56]: Epoch 43/100, Acc=0.6599, Val Loss=1.4603, lr=0.0100
[02/21 21:40:41 cifar100-global-group_sl-2.0-resnet56]: Epoch 44/100, Acc=0.6456, Val Loss=1.4672, lr=0.0100
[02/21 21:41:08 cifar100-global-group_sl-2.0-resnet56]: Epoch 45/100, Acc=0.6512, Val Loss=1.4639, lr=0.0100
[02/21 21:41:36 cifar100-global-group_sl-2.0-resnet56]: Epoch 46/100, Acc=0.6554, Val Loss=1.4359, lr=0.0100
[02/21 21:42:04 cifar100-global-group_sl-2.0-resnet56]: Epoch 47/100, Acc=0.6467, Val Loss=1.4660, lr=0.0100
[02/21 21:42:32 cifar100-global-group_sl-2.0-resnet56]: Epoch 48/100, Acc=0.6511, Val Loss=1.4597, lr=0.0100
[02/21 21:43:00 cifar100-global-group_sl-2.0-resnet56]: Epoch 49/100, Acc=0.6530, Val Loss=1.4036, lr=0.0100
[02/21 21:43:28 cifar100-global-group_sl-2.0-resnet56]: Epoch 50/100, Acc=0.6322, Val Loss=1.5295, lr=0.0100
[02/21 21:43:56 cifar100-global-group_sl-2.0-resnet56]: Epoch 51/100, Acc=0.6418, Val Loss=1.4838, lr=0.0100
[02/21 21:44:23 cifar100-global-group_sl-2.0-resnet56]: Epoch 52/100, Acc=0.6433, Val Loss=1.4751, lr=0.0100
[02/21 21:44:51 cifar100-global-group_sl-2.0-resnet56]: Epoch 53/100, Acc=0.6381, Val Loss=1.4866, lr=0.0100
[02/21 21:45:19 cifar100-global-group_sl-2.0-resnet56]: Epoch 54/100, Acc=0.6557, Val Loss=1.4179, lr=0.0100
[02/21 21:45:47 cifar100-global-group_sl-2.0-resnet56]: Epoch 55/100, Acc=0.6482, Val Loss=1.4401, lr=0.0100
[02/21 21:46:15 cifar100-global-group_sl-2.0-resnet56]: Epoch 56/100, Acc=0.6395, Val Loss=1.5175, lr=0.0100
[02/21 21:46:43 cifar100-global-group_sl-2.0-resnet56]: Epoch 57/100, Acc=0.6446, Val Loss=1.4798, lr=0.0100
[02/21 21:47:11 cifar100-global-group_sl-2.0-resnet56]: Epoch 58/100, Acc=0.6507, Val Loss=1.4513, lr=0.0100
[02/21 21:47:38 cifar100-global-group_sl-2.0-resnet56]: Epoch 59/100, Acc=0.6443, Val Loss=1.5043, lr=0.0100
[02/21 21:48:05 cifar100-global-group_sl-2.0-resnet56]: Epoch 60/100, Acc=0.7075, Val Loss=1.1701, lr=0.0010
[02/21 21:48:33 cifar100-global-group_sl-2.0-resnet56]: Epoch 61/100, Acc=0.7137, Val Loss=1.1696, lr=0.0010
[02/21 21:49:01 cifar100-global-group_sl-2.0-resnet56]: Epoch 62/100, Acc=0.7131, Val Loss=1.1675, lr=0.0010
[02/21 21:49:29 cifar100-global-group_sl-2.0-resnet56]: Epoch 63/100, Acc=0.7124, Val Loss=1.1716, lr=0.0010
[02/21 21:49:58 cifar100-global-group_sl-2.0-resnet56]: Epoch 64/100, Acc=0.7155, Val Loss=1.1852, lr=0.0010
[02/21 21:50:26 cifar100-global-group_sl-2.0-resnet56]: Epoch 65/100, Acc=0.7127, Val Loss=1.1906, lr=0.0010
[02/21 21:50:54 cifar100-global-group_sl-2.0-resnet56]: Epoch 66/100, Acc=0.7154, Val Loss=1.1970, lr=0.0010
[02/21 21:51:22 cifar100-global-group_sl-2.0-resnet56]: Epoch 67/100, Acc=0.7152, Val Loss=1.2040, lr=0.0010
[02/21 21:51:50 cifar100-global-group_sl-2.0-resnet56]: Epoch 68/100, Acc=0.7125, Val Loss=1.2161, lr=0.0010
[02/21 21:52:18 cifar100-global-group_sl-2.0-resnet56]: Epoch 69/100, Acc=0.7132, Val Loss=1.2125, lr=0.0010
[02/21 21:52:46 cifar100-global-group_sl-2.0-resnet56]: Epoch 70/100, Acc=0.7146, Val Loss=1.2099, lr=0.0010
[02/21 21:53:14 cifar100-global-group_sl-2.0-resnet56]: Epoch 71/100, Acc=0.7138, Val Loss=1.2170, lr=0.0010
[02/21 21:53:43 cifar100-global-group_sl-2.0-resnet56]: Epoch 72/100, Acc=0.7141, Val Loss=1.2215, lr=0.0010
[02/21 21:54:11 cifar100-global-group_sl-2.0-resnet56]: Epoch 73/100, Acc=0.7105, Val Loss=1.2263, lr=0.0010
[02/21 21:54:39 cifar100-global-group_sl-2.0-resnet56]: Epoch 74/100, Acc=0.7113, Val Loss=1.2277, lr=0.0010
[02/21 21:55:07 cifar100-global-group_sl-2.0-resnet56]: Epoch 75/100, Acc=0.7125, Val Loss=1.2340, lr=0.0010
[02/21 21:55:34 cifar100-global-group_sl-2.0-resnet56]: Epoch 76/100, Acc=0.7111, Val Loss=1.2390, lr=0.0010
[02/21 21:56:02 cifar100-global-group_sl-2.0-resnet56]: Epoch 77/100, Acc=0.7115, Val Loss=1.2427, lr=0.0010
[02/21 21:56:30 cifar100-global-group_sl-2.0-resnet56]: Epoch 78/100, Acc=0.7123, Val Loss=1.2428, lr=0.0010
[02/21 21:56:58 cifar100-global-group_sl-2.0-resnet56]: Epoch 79/100, Acc=0.7095, Val Loss=1.2466, lr=0.0010
[02/21 21:57:26 cifar100-global-group_sl-2.0-resnet56]: Epoch 80/100, Acc=0.7133, Val Loss=1.2428, lr=0.0001
[02/21 21:57:54 cifar100-global-group_sl-2.0-resnet56]: Epoch 81/100, Acc=0.7135, Val Loss=1.2398, lr=0.0001
[02/21 21:58:21 cifar100-global-group_sl-2.0-resnet56]: Epoch 82/100, Acc=0.7139, Val Loss=1.2410, lr=0.0001
[02/21 21:58:49 cifar100-global-group_sl-2.0-resnet56]: Epoch 83/100, Acc=0.7129, Val Loss=1.2385, lr=0.0001
[02/21 21:59:17 cifar100-global-group_sl-2.0-resnet56]: Epoch 84/100, Acc=0.7136, Val Loss=1.2405, lr=0.0001
[02/21 21:59:46 cifar100-global-group_sl-2.0-resnet56]: Epoch 85/100, Acc=0.7126, Val Loss=1.2415, lr=0.0001
[02/21 22:00:14 cifar100-global-group_sl-2.0-resnet56]: Epoch 86/100, Acc=0.7138, Val Loss=1.2460, lr=0.0001
[02/21 22:00:42 cifar100-global-group_sl-2.0-resnet56]: Epoch 87/100, Acc=0.7141, Val Loss=1.2425, lr=0.0001
[02/21 22:01:10 cifar100-global-group_sl-2.0-resnet56]: Epoch 88/100, Acc=0.7137, Val Loss=1.2414, lr=0.0001
[02/21 22:01:39 cifar100-global-group_sl-2.0-resnet56]: Epoch 89/100, Acc=0.7121, Val Loss=1.2439, lr=0.0001
[02/21 22:02:08 cifar100-global-group_sl-2.0-resnet56]: Epoch 90/100, Acc=0.7119, Val Loss=1.2420, lr=0.0001
[02/21 22:02:36 cifar100-global-group_sl-2.0-resnet56]: Epoch 91/100, Acc=0.7113, Val Loss=1.2429, lr=0.0001
[02/21 22:03:04 cifar100-global-group_sl-2.0-resnet56]: Epoch 92/100, Acc=0.7128, Val Loss=1.2391, lr=0.0001
[02/21 22:03:32 cifar100-global-group_sl-2.0-resnet56]: Epoch 93/100, Acc=0.7139, Val Loss=1.2471, lr=0.0001
[02/21 22:04:00 cifar100-global-group_sl-2.0-resnet56]: Epoch 94/100, Acc=0.7125, Val Loss=1.2440, lr=0.0001
[02/21 22:04:27 cifar100-global-group_sl-2.0-resnet56]: Epoch 95/100, Acc=0.7153, Val Loss=1.2421, lr=0.0001
[02/21 22:04:55 cifar100-global-group_sl-2.0-resnet56]: Epoch 96/100, Acc=0.7124, Val Loss=1.2427, lr=0.0001
[02/21 22:05:23 cifar100-global-group_sl-2.0-resnet56]: Epoch 97/100, Acc=0.7122, Val Loss=1.2457, lr=0.0001
[02/21 22:05:51 cifar100-global-group_sl-2.0-resnet56]: Epoch 98/100, Acc=0.7134, Val Loss=1.2411, lr=0.0001
[02/21 22:06:19 cifar100-global-group_sl-2.0-resnet56]: Epoch 99/100, Acc=0.7136, Val Loss=1.2425, lr=0.0001
[02/21 22:06:19 cifar100-global-group_sl-2.0-resnet56]: Best Acc=0.7155
[02/21 22:06:19 cifar100-global-group_sl-2.0-resnet56]: Params: 0.65 M
[02/21 22:06:19 cifar100-global-group_sl-2.0-resnet56]: ops: 63.32 M
[02/21 22:06:21 cifar100-global-group_sl-2.0-resnet56]: Acc: 0.7136 Val Loss: 1.2425

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: mode: prune
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: model: resnet56
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: verbose: False
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: dataset: cifar100
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: dataroot: data
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: batch_size: 128
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: total_epochs: 100
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: lr_decay_milestones: 60,80
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: lr_decay_gamma: 0.1
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: lr: 0.01
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: output_dir: run/cifar100/prune/cifar100-global-proj-2.0-resnet56
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: finetune: True
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: last_epochs: 100
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: reps: 1
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: method: proj
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: speed_up: 2.0
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: max_pruning_ratio: 1.0
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: reg: 1e-05
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: delta_reg: 0.0001
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: weight_decay: 0.0005
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: seed: 1
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: global_pruning: True
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: sl_total_epochs: 100
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: sl_lr: 0.01
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: sl_reg_warmup: 0
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: sl_restore: None
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: iterative_steps: 400
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: logger: <Logger cifar100-global-proj-2.0-resnet56 (DEBUG)>
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: device: cuda
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: num_classes: 100
[02/21 22:06:29 cifar100-global-proj-2.0-resnet56]: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 22:06:32 cifar100-global-proj-2.0-resnet56]: Pruning...
[02/21 22:08:14 cifar100-global-proj-2.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(7, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(7, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(7, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(7, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(7, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(7, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(7, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(7, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(7, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(7, 14, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(7, 29, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(29, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(29, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(29, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(29, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(21, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(29, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(29, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(29, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(29, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(29, 58, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(58, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(29, 62, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(62, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(52, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(62, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(62, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(62, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(58, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(62, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(62, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(58, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(62, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=62, out_features=100, bias=True)
)
[02/21 22:08:17 cifar100-global-proj-2.0-resnet56]: Params: 0.86 M => 0.66 M (76.74%)
[02/21 22:08:17 cifar100-global-proj-2.0-resnet56]: FLOPs: 127.12 M => 62.84 M (49.43%, 2.02X )
[02/21 22:08:17 cifar100-global-proj-2.0-resnet56]: Acc: 0.7269 => 0.0093
[02/21 22:08:17 cifar100-global-proj-2.0-resnet56]: Val Loss: 1.1578 => 13.5115
[02/21 22:08:17 cifar100-global-proj-2.0-resnet56]: Finetuning...
[02/21 22:08:45 cifar100-global-proj-2.0-resnet56]: Epoch 0/100, Acc=0.5611, Val Loss=1.6991, lr=0.0100
[02/21 22:09:12 cifar100-global-proj-2.0-resnet56]: Epoch 1/100, Acc=0.6293, Val Loss=1.3833, lr=0.0100
[02/21 22:09:40 cifar100-global-proj-2.0-resnet56]: Epoch 2/100, Acc=0.6170, Val Loss=1.4896, lr=0.0100
[02/21 22:10:08 cifar100-global-proj-2.0-resnet56]: Epoch 3/100, Acc=0.6091, Val Loss=1.4950, lr=0.0100
[02/21 22:10:35 cifar100-global-proj-2.0-resnet56]: Epoch 4/100, Acc=0.6296, Val Loss=1.4423, lr=0.0100
[02/21 22:11:03 cifar100-global-proj-2.0-resnet56]: Epoch 5/100, Acc=0.6377, Val Loss=1.3970, lr=0.0100
[02/21 22:11:30 cifar100-global-proj-2.0-resnet56]: Epoch 6/100, Acc=0.6438, Val Loss=1.3699, lr=0.0100
[02/21 22:11:58 cifar100-global-proj-2.0-resnet56]: Epoch 7/100, Acc=0.6298, Val Loss=1.4444, lr=0.0100
[02/21 22:12:25 cifar100-global-proj-2.0-resnet56]: Epoch 8/100, Acc=0.6265, Val Loss=1.4845, lr=0.0100
[02/21 22:12:52 cifar100-global-proj-2.0-resnet56]: Epoch 9/100, Acc=0.6541, Val Loss=1.3298, lr=0.0100
[02/21 22:13:20 cifar100-global-proj-2.0-resnet56]: Epoch 10/100, Acc=0.5795, Val Loss=1.7073, lr=0.0100
[02/21 22:13:47 cifar100-global-proj-2.0-resnet56]: Epoch 11/100, Acc=0.6217, Val Loss=1.5309, lr=0.0100
[02/21 22:14:15 cifar100-global-proj-2.0-resnet56]: Epoch 12/100, Acc=0.6395, Val Loss=1.4041, lr=0.0100
[02/21 22:14:42 cifar100-global-proj-2.0-resnet56]: Epoch 13/100, Acc=0.6530, Val Loss=1.3229, lr=0.0100
[02/21 22:15:10 cifar100-global-proj-2.0-resnet56]: Epoch 14/100, Acc=0.6382, Val Loss=1.3860, lr=0.0100
[02/21 22:15:37 cifar100-global-proj-2.0-resnet56]: Epoch 15/100, Acc=0.6579, Val Loss=1.3526, lr=0.0100
[02/21 22:16:05 cifar100-global-proj-2.0-resnet56]: Epoch 16/100, Acc=0.6260, Val Loss=1.4652, lr=0.0100
[02/21 22:16:32 cifar100-global-proj-2.0-resnet56]: Epoch 17/100, Acc=0.6378, Val Loss=1.4366, lr=0.0100
[02/21 22:17:00 cifar100-global-proj-2.0-resnet56]: Epoch 18/100, Acc=0.6422, Val Loss=1.3750, lr=0.0100
[02/21 22:17:28 cifar100-global-proj-2.0-resnet56]: Epoch 19/100, Acc=0.6389, Val Loss=1.4022, lr=0.0100
[02/21 22:17:55 cifar100-global-proj-2.0-resnet56]: Epoch 20/100, Acc=0.6478, Val Loss=1.3781, lr=0.0100
[02/21 22:18:23 cifar100-global-proj-2.0-resnet56]: Epoch 21/100, Acc=0.6443, Val Loss=1.3917, lr=0.0100
[02/21 22:18:50 cifar100-global-proj-2.0-resnet56]: Epoch 22/100, Acc=0.6321, Val Loss=1.4429, lr=0.0100
[02/21 22:19:18 cifar100-global-proj-2.0-resnet56]: Epoch 23/100, Acc=0.6421, Val Loss=1.4187, lr=0.0100
[02/21 22:19:45 cifar100-global-proj-2.0-resnet56]: Epoch 24/100, Acc=0.6427, Val Loss=1.4362, lr=0.0100
[02/21 22:20:13 cifar100-global-proj-2.0-resnet56]: Epoch 25/100, Acc=0.6423, Val Loss=1.4102, lr=0.0100
[02/21 22:20:40 cifar100-global-proj-2.0-resnet56]: Epoch 26/100, Acc=0.6405, Val Loss=1.4125, lr=0.0100
[02/21 22:21:08 cifar100-global-proj-2.0-resnet56]: Epoch 27/100, Acc=0.6564, Val Loss=1.3569, lr=0.0100
[02/21 22:21:35 cifar100-global-proj-2.0-resnet56]: Epoch 28/100, Acc=0.6375, Val Loss=1.4720, lr=0.0100
[02/21 22:22:03 cifar100-global-proj-2.0-resnet56]: Epoch 29/100, Acc=0.6389, Val Loss=1.4892, lr=0.0100
[02/21 22:22:30 cifar100-global-proj-2.0-resnet56]: Epoch 30/100, Acc=0.6300, Val Loss=1.5263, lr=0.0100
[02/21 22:22:58 cifar100-global-proj-2.0-resnet56]: Epoch 31/100, Acc=0.6450, Val Loss=1.4309, lr=0.0100
[02/21 22:23:25 cifar100-global-proj-2.0-resnet56]: Epoch 32/100, Acc=0.6387, Val Loss=1.4752, lr=0.0100
[02/21 22:23:53 cifar100-global-proj-2.0-resnet56]: Epoch 33/100, Acc=0.6638, Val Loss=1.3634, lr=0.0100
[02/21 22:24:20 cifar100-global-proj-2.0-resnet56]: Epoch 34/100, Acc=0.6379, Val Loss=1.4312, lr=0.0100
[02/21 22:24:48 cifar100-global-proj-2.0-resnet56]: Epoch 35/100, Acc=0.6534, Val Loss=1.3814, lr=0.0100
[02/21 22:25:15 cifar100-global-proj-2.0-resnet56]: Epoch 36/100, Acc=0.6400, Val Loss=1.4642, lr=0.0100
[02/21 22:25:43 cifar100-global-proj-2.0-resnet56]: Epoch 37/100, Acc=0.6320, Val Loss=1.4633, lr=0.0100
[02/21 22:26:10 cifar100-global-proj-2.0-resnet56]: Epoch 38/100, Acc=0.6549, Val Loss=1.4109, lr=0.0100
[02/21 22:26:38 cifar100-global-proj-2.0-resnet56]: Epoch 39/100, Acc=0.6410, Val Loss=1.4537, lr=0.0100
[02/21 22:27:06 cifar100-global-proj-2.0-resnet56]: Epoch 40/100, Acc=0.6274, Val Loss=1.4902, lr=0.0100
[02/21 22:27:33 cifar100-global-proj-2.0-resnet56]: Epoch 41/100, Acc=0.6462, Val Loss=1.4189, lr=0.0100
[02/21 22:28:01 cifar100-global-proj-2.0-resnet56]: Epoch 42/100, Acc=0.6423, Val Loss=1.4525, lr=0.0100
[02/21 22:28:28 cifar100-global-proj-2.0-resnet56]: Epoch 43/100, Acc=0.6442, Val Loss=1.4340, lr=0.0100
[02/21 22:28:56 cifar100-global-proj-2.0-resnet56]: Epoch 44/100, Acc=0.6340, Val Loss=1.5169, lr=0.0100
[02/21 22:29:24 cifar100-global-proj-2.0-resnet56]: Epoch 45/100, Acc=0.6312, Val Loss=1.5172, lr=0.0100
[02/21 22:29:52 cifar100-global-proj-2.0-resnet56]: Epoch 46/100, Acc=0.6559, Val Loss=1.4078, lr=0.0100
[02/21 22:30:20 cifar100-global-proj-2.0-resnet56]: Epoch 47/100, Acc=0.6301, Val Loss=1.5379, lr=0.0100
[02/21 22:30:48 cifar100-global-proj-2.0-resnet56]: Epoch 48/100, Acc=0.6398, Val Loss=1.5014, lr=0.0100
[02/21 22:31:16 cifar100-global-proj-2.0-resnet56]: Epoch 49/100, Acc=0.6505, Val Loss=1.4476, lr=0.0100
[02/21 22:31:44 cifar100-global-proj-2.0-resnet56]: Epoch 50/100, Acc=0.6183, Val Loss=1.5917, lr=0.0100
[02/21 22:32:12 cifar100-global-proj-2.0-resnet56]: Epoch 51/100, Acc=0.6354, Val Loss=1.4862, lr=0.0100
[02/21 22:32:39 cifar100-global-proj-2.0-resnet56]: Epoch 52/100, Acc=0.6495, Val Loss=1.4567, lr=0.0100
[02/21 22:33:07 cifar100-global-proj-2.0-resnet56]: Epoch 53/100, Acc=0.6355, Val Loss=1.5229, lr=0.0100
[02/21 22:33:35 cifar100-global-proj-2.0-resnet56]: Epoch 54/100, Acc=0.6252, Val Loss=1.5678, lr=0.0100
[02/21 22:34:02 cifar100-global-proj-2.0-resnet56]: Epoch 55/100, Acc=0.6636, Val Loss=1.3572, lr=0.0100
[02/21 22:34:30 cifar100-global-proj-2.0-resnet56]: Epoch 56/100, Acc=0.6425, Val Loss=1.4166, lr=0.0100
[02/21 22:34:58 cifar100-global-proj-2.0-resnet56]: Epoch 57/100, Acc=0.6548, Val Loss=1.4223, lr=0.0100
[02/21 22:35:25 cifar100-global-proj-2.0-resnet56]: Epoch 58/100, Acc=0.6465, Val Loss=1.4435, lr=0.0100
[02/21 22:35:53 cifar100-global-proj-2.0-resnet56]: Epoch 59/100, Acc=0.6483, Val Loss=1.4594, lr=0.0100
[02/21 22:36:20 cifar100-global-proj-2.0-resnet56]: Epoch 60/100, Acc=0.7116, Val Loss=1.1433, lr=0.0010
[02/21 22:36:48 cifar100-global-proj-2.0-resnet56]: Epoch 61/100, Acc=0.7118, Val Loss=1.1451, lr=0.0010
[02/21 22:37:16 cifar100-global-proj-2.0-resnet56]: Epoch 62/100, Acc=0.7122, Val Loss=1.1422, lr=0.0010
[02/21 22:37:43 cifar100-global-proj-2.0-resnet56]: Epoch 63/100, Acc=0.7117, Val Loss=1.1532, lr=0.0010
[02/21 22:38:11 cifar100-global-proj-2.0-resnet56]: Epoch 64/100, Acc=0.7156, Val Loss=1.1521, lr=0.0010
[02/21 22:38:39 cifar100-global-proj-2.0-resnet56]: Epoch 65/100, Acc=0.7148, Val Loss=1.1550, lr=0.0010
[02/21 22:39:06 cifar100-global-proj-2.0-resnet56]: Epoch 66/100, Acc=0.7132, Val Loss=1.1662, lr=0.0010
[02/21 22:39:34 cifar100-global-proj-2.0-resnet56]: Epoch 67/100, Acc=0.7148, Val Loss=1.1721, lr=0.0010
[02/21 22:40:02 cifar100-global-proj-2.0-resnet56]: Epoch 68/100, Acc=0.7128, Val Loss=1.1728, lr=0.0010
[02/21 22:40:29 cifar100-global-proj-2.0-resnet56]: Epoch 69/100, Acc=0.7141, Val Loss=1.1778, lr=0.0010
[02/21 22:40:57 cifar100-global-proj-2.0-resnet56]: Epoch 70/100, Acc=0.7152, Val Loss=1.1833, lr=0.0010
[02/21 22:41:24 cifar100-global-proj-2.0-resnet56]: Epoch 71/100, Acc=0.7115, Val Loss=1.1900, lr=0.0010
[02/21 22:41:52 cifar100-global-proj-2.0-resnet56]: Epoch 72/100, Acc=0.7134, Val Loss=1.1948, lr=0.0010
[02/21 22:42:20 cifar100-global-proj-2.0-resnet56]: Epoch 73/100, Acc=0.7126, Val Loss=1.1957, lr=0.0010
[02/21 22:42:48 cifar100-global-proj-2.0-resnet56]: Epoch 74/100, Acc=0.7138, Val Loss=1.1976, lr=0.0010
[02/21 22:43:16 cifar100-global-proj-2.0-resnet56]: Epoch 75/100, Acc=0.7131, Val Loss=1.1979, lr=0.0010
[02/21 22:43:44 cifar100-global-proj-2.0-resnet56]: Epoch 76/100, Acc=0.7116, Val Loss=1.2105, lr=0.0010
[02/21 22:44:12 cifar100-global-proj-2.0-resnet56]: Epoch 77/100, Acc=0.7132, Val Loss=1.2107, lr=0.0010
[02/21 22:44:40 cifar100-global-proj-2.0-resnet56]: Epoch 78/100, Acc=0.7133, Val Loss=1.2107, lr=0.0010
[02/21 22:45:08 cifar100-global-proj-2.0-resnet56]: Epoch 79/100, Acc=0.7126, Val Loss=1.2214, lr=0.0010
[02/21 22:45:35 cifar100-global-proj-2.0-resnet56]: Epoch 80/100, Acc=0.7138, Val Loss=1.2118, lr=0.0001
[02/21 22:46:03 cifar100-global-proj-2.0-resnet56]: Epoch 81/100, Acc=0.7128, Val Loss=1.2106, lr=0.0001
[02/21 22:46:31 cifar100-global-proj-2.0-resnet56]: Epoch 82/100, Acc=0.7127, Val Loss=1.2079, lr=0.0001
[02/21 22:46:59 cifar100-global-proj-2.0-resnet56]: Epoch 83/100, Acc=0.7153, Val Loss=1.2106, lr=0.0001
[02/21 22:47:28 cifar100-global-proj-2.0-resnet56]: Epoch 84/100, Acc=0.7135, Val Loss=1.2175, lr=0.0001
[02/21 22:47:56 cifar100-global-proj-2.0-resnet56]: Epoch 85/100, Acc=0.7150, Val Loss=1.2116, lr=0.0001
[02/21 22:48:24 cifar100-global-proj-2.0-resnet56]: Epoch 86/100, Acc=0.7144, Val Loss=1.2153, lr=0.0001
[02/21 22:48:52 cifar100-global-proj-2.0-resnet56]: Epoch 87/100, Acc=0.7149, Val Loss=1.2147, lr=0.0001
[02/21 22:49:19 cifar100-global-proj-2.0-resnet56]: Epoch 88/100, Acc=0.7152, Val Loss=1.2095, lr=0.0001
[02/21 22:49:47 cifar100-global-proj-2.0-resnet56]: Epoch 89/100, Acc=0.7137, Val Loss=1.2127, lr=0.0001
[02/21 22:50:15 cifar100-global-proj-2.0-resnet56]: Epoch 90/100, Acc=0.7139, Val Loss=1.2100, lr=0.0001
[02/21 22:50:43 cifar100-global-proj-2.0-resnet56]: Epoch 91/100, Acc=0.7119, Val Loss=1.2218, lr=0.0001
[02/21 22:51:10 cifar100-global-proj-2.0-resnet56]: Epoch 92/100, Acc=0.7145, Val Loss=1.2133, lr=0.0001
[02/21 22:51:38 cifar100-global-proj-2.0-resnet56]: Epoch 93/100, Acc=0.7150, Val Loss=1.2143, lr=0.0001
[02/21 22:52:06 cifar100-global-proj-2.0-resnet56]: Epoch 94/100, Acc=0.7169, Val Loss=1.2125, lr=0.0001
[02/21 22:52:33 cifar100-global-proj-2.0-resnet56]: Epoch 95/100, Acc=0.7151, Val Loss=1.2190, lr=0.0001
[02/21 22:53:01 cifar100-global-proj-2.0-resnet56]: Epoch 96/100, Acc=0.7128, Val Loss=1.2187, lr=0.0001
[02/21 22:53:29 cifar100-global-proj-2.0-resnet56]: Epoch 97/100, Acc=0.7148, Val Loss=1.2173, lr=0.0001
[02/21 22:53:57 cifar100-global-proj-2.0-resnet56]: Epoch 98/100, Acc=0.7125, Val Loss=1.2146, lr=0.0001
[02/21 22:54:25 cifar100-global-proj-2.0-resnet56]: Epoch 99/100, Acc=0.7147, Val Loss=1.2169, lr=0.0001
[02/21 22:54:25 cifar100-global-proj-2.0-resnet56]: Best Acc=0.7169
[02/21 22:54:25 cifar100-global-proj-2.0-resnet56]: Params: 0.66 M
[02/21 22:54:25 cifar100-global-proj-2.0-resnet56]: ops: 62.84 M
[02/21 22:54:28 cifar100-global-proj-2.0-resnet56]: Acc: 0.7147 Val Loss: 1.2169

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: mode: prune
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: model: resnet56
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: verbose: False
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: dataset: cifar100
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: dataroot: data
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: batch_size: 128
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: total_epochs: 100
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: lr_decay_milestones: 60,80
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: lr_decay_gamma: 0.1
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: lr: 0.01
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: output_dir: run/cifar100/prune/cifar100-global-proj_sl-2.0-resnet56
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: finetune: True
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: last_epochs: 100
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: reps: 1
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: method: proj_sl
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: speed_up: 2.0
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: max_pruning_ratio: 1.0
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: reg: 1e-05
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: delta_reg: 0.0001
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: weight_decay: 0.0005
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: seed: 1
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: global_pruning: True
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: sl_total_epochs: 100
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: sl_lr: 0.01
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: sl_reg_warmup: 0
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: sl_restore: None
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: iterative_steps: 400
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: logger: <Logger cifar100-global-proj_sl-2.0-resnet56 (DEBUG)>
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: device: cuda
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: num_classes: 100
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 22:54:35 cifar100-global-proj_sl-2.0-resnet56]: Regularizing...
[02/21 23:01:36 cifar100-global-proj_sl-2.0-resnet56]: Epoch 0/100, Acc=0.6558, Val Loss=1.3960, lr=0.0100
[02/21 23:08:35 cifar100-global-proj_sl-2.0-resnet56]: Epoch 1/100, Acc=0.6645, Val Loss=1.4187, lr=0.0100
[02/21 23:15:36 cifar100-global-proj_sl-2.0-resnet56]: Epoch 2/100, Acc=0.6500, Val Loss=1.4914, lr=0.0100
[02/21 23:22:37 cifar100-global-proj_sl-2.0-resnet56]: Epoch 3/100, Acc=0.6697, Val Loss=1.3962, lr=0.0100
[02/21 23:29:37 cifar100-global-proj_sl-2.0-resnet56]: Epoch 4/100, Acc=0.6686, Val Loss=1.3955, lr=0.0100
[02/21 23:36:37 cifar100-global-proj_sl-2.0-resnet56]: Epoch 5/100, Acc=0.6836, Val Loss=1.3634, lr=0.0100
[02/21 23:43:38 cifar100-global-proj_sl-2.0-resnet56]: Epoch 6/100, Acc=0.6682, Val Loss=1.4074, lr=0.0100
[02/21 23:50:38 cifar100-global-proj_sl-2.0-resnet56]: Epoch 7/100, Acc=0.6855, Val Loss=1.3598, lr=0.0100
[02/21 23:57:39 cifar100-global-proj_sl-2.0-resnet56]: Epoch 8/100, Acc=0.6615, Val Loss=1.4228, lr=0.0100
[02/22 00:04:41 cifar100-global-proj_sl-2.0-resnet56]: Epoch 9/100, Acc=0.6702, Val Loss=1.4513, lr=0.0100
[02/22 00:11:42 cifar100-global-proj_sl-2.0-resnet56]: Epoch 10/100, Acc=0.6780, Val Loss=1.3927, lr=0.0100
[02/22 00:18:43 cifar100-global-proj_sl-2.0-resnet56]: Epoch 11/100, Acc=0.6725, Val Loss=1.4103, lr=0.0100
[02/22 00:25:43 cifar100-global-proj_sl-2.0-resnet56]: Epoch 12/100, Acc=0.6787, Val Loss=1.4643, lr=0.0100
[02/22 00:32:42 cifar100-global-proj_sl-2.0-resnet56]: Epoch 13/100, Acc=0.6759, Val Loss=1.4460, lr=0.0100
[02/22 00:39:42 cifar100-global-proj_sl-2.0-resnet56]: Epoch 14/100, Acc=0.6779, Val Loss=1.4502, lr=0.0100
[02/22 00:46:44 cifar100-global-proj_sl-2.0-resnet56]: Epoch 15/100, Acc=0.6785, Val Loss=1.4677, lr=0.0100
[02/22 00:53:47 cifar100-global-proj_sl-2.0-resnet56]: Epoch 16/100, Acc=0.6672, Val Loss=1.5207, lr=0.0100
[02/22 01:00:50 cifar100-global-proj_sl-2.0-resnet56]: Epoch 17/100, Acc=0.6838, Val Loss=1.4389, lr=0.0100
[02/22 01:07:49 cifar100-global-proj_sl-2.0-resnet56]: Epoch 18/100, Acc=0.6922, Val Loss=1.4163, lr=0.0100
[02/22 01:14:48 cifar100-global-proj_sl-2.0-resnet56]: Epoch 19/100, Acc=0.6825, Val Loss=1.4711, lr=0.0100
[02/22 01:21:48 cifar100-global-proj_sl-2.0-resnet56]: Epoch 20/100, Acc=0.6833, Val Loss=1.4781, lr=0.0100
[02/22 01:28:49 cifar100-global-proj_sl-2.0-resnet56]: Epoch 21/100, Acc=0.6855, Val Loss=1.4539, lr=0.0100
[02/22 01:35:52 cifar100-global-proj_sl-2.0-resnet56]: Epoch 22/100, Acc=0.6778, Val Loss=1.5269, lr=0.0100
[02/22 01:42:55 cifar100-global-proj_sl-2.0-resnet56]: Epoch 23/100, Acc=0.6894, Val Loss=1.4958, lr=0.0100
[02/22 01:49:58 cifar100-global-proj_sl-2.0-resnet56]: Epoch 24/100, Acc=0.6799, Val Loss=1.5457, lr=0.0100
[02/22 01:57:02 cifar100-global-proj_sl-2.0-resnet56]: Epoch 25/100, Acc=0.6842, Val Loss=1.5179, lr=0.0100
[02/22 02:04:06 cifar100-global-proj_sl-2.0-resnet56]: Epoch 26/100, Acc=0.6797, Val Loss=1.5976, lr=0.0100
[02/22 02:11:07 cifar100-global-proj_sl-2.0-resnet56]: Epoch 27/100, Acc=0.6796, Val Loss=1.5745, lr=0.0100
[02/22 02:18:10 cifar100-global-proj_sl-2.0-resnet56]: Epoch 28/100, Acc=0.6791, Val Loss=1.5705, lr=0.0100
[02/22 02:25:10 cifar100-global-proj_sl-2.0-resnet56]: Epoch 29/100, Acc=0.6799, Val Loss=1.5730, lr=0.0100
[02/22 02:32:10 cifar100-global-proj_sl-2.0-resnet56]: Epoch 30/100, Acc=0.6787, Val Loss=1.6259, lr=0.0100
[02/22 02:39:12 cifar100-global-proj_sl-2.0-resnet56]: Epoch 31/100, Acc=0.6805, Val Loss=1.5588, lr=0.0100
[02/22 02:46:13 cifar100-global-proj_sl-2.0-resnet56]: Epoch 32/100, Acc=0.6822, Val Loss=1.5613, lr=0.0100
[02/22 02:53:17 cifar100-global-proj_sl-2.0-resnet56]: Epoch 33/100, Acc=0.6921, Val Loss=1.5325, lr=0.0100
[02/22 03:00:21 cifar100-global-proj_sl-2.0-resnet56]: Epoch 34/100, Acc=0.6850, Val Loss=1.5799, lr=0.0100
[02/22 03:07:24 cifar100-global-proj_sl-2.0-resnet56]: Epoch 35/100, Acc=0.6738, Val Loss=1.6651, lr=0.0100
[02/22 03:14:28 cifar100-global-proj_sl-2.0-resnet56]: Epoch 36/100, Acc=0.6758, Val Loss=1.6803, lr=0.0100
[02/22 03:21:29 cifar100-global-proj_sl-2.0-resnet56]: Epoch 37/100, Acc=0.6803, Val Loss=1.6689, lr=0.0100
[02/22 03:28:30 cifar100-global-proj_sl-2.0-resnet56]: Epoch 38/100, Acc=0.6870, Val Loss=1.5959, lr=0.0100
[02/22 03:35:31 cifar100-global-proj_sl-2.0-resnet56]: Epoch 39/100, Acc=0.6832, Val Loss=1.6346, lr=0.0100
[02/22 03:42:33 cifar100-global-proj_sl-2.0-resnet56]: Epoch 40/100, Acc=0.6844, Val Loss=1.6086, lr=0.0100
[02/22 03:49:36 cifar100-global-proj_sl-2.0-resnet56]: Epoch 41/100, Acc=0.6864, Val Loss=1.6013, lr=0.0100
[02/22 03:56:39 cifar100-global-proj_sl-2.0-resnet56]: Epoch 42/100, Acc=0.6901, Val Loss=1.5819, lr=0.0100
[02/22 04:03:41 cifar100-global-proj_sl-2.0-resnet56]: Epoch 43/100, Acc=0.6946, Val Loss=1.6184, lr=0.0100
[02/22 04:10:46 cifar100-global-proj_sl-2.0-resnet56]: Epoch 44/100, Acc=0.6865, Val Loss=1.6272, lr=0.0100
[02/22 04:17:50 cifar100-global-proj_sl-2.0-resnet56]: Epoch 45/100, Acc=0.6941, Val Loss=1.5927, lr=0.0100
[02/22 04:24:52 cifar100-global-proj_sl-2.0-resnet56]: Epoch 46/100, Acc=0.6873, Val Loss=1.6008, lr=0.0100
[02/22 04:31:53 cifar100-global-proj_sl-2.0-resnet56]: Epoch 47/100, Acc=0.6856, Val Loss=1.7003, lr=0.0100
[02/22 04:38:55 cifar100-global-proj_sl-2.0-resnet56]: Epoch 48/100, Acc=0.6800, Val Loss=1.7176, lr=0.0100
[02/22 04:45:56 cifar100-global-proj_sl-2.0-resnet56]: Epoch 49/100, Acc=0.6789, Val Loss=1.7626, lr=0.0100
[02/22 04:52:58 cifar100-global-proj_sl-2.0-resnet56]: Epoch 50/100, Acc=0.6797, Val Loss=1.7312, lr=0.0100
[02/22 05:00:01 cifar100-global-proj_sl-2.0-resnet56]: Epoch 51/100, Acc=0.6869, Val Loss=1.6695, lr=0.0100
[02/22 05:07:05 cifar100-global-proj_sl-2.0-resnet56]: Epoch 52/100, Acc=0.6896, Val Loss=1.6660, lr=0.0100
[02/22 05:14:08 cifar100-global-proj_sl-2.0-resnet56]: Epoch 53/100, Acc=0.6873, Val Loss=1.6893, lr=0.0100
[02/22 05:21:09 cifar100-global-proj_sl-2.0-resnet56]: Epoch 54/100, Acc=0.6869, Val Loss=1.7168, lr=0.0100
[02/22 05:28:11 cifar100-global-proj_sl-2.0-resnet56]: Epoch 55/100, Acc=0.6872, Val Loss=1.6538, lr=0.0100
[02/22 05:35:13 cifar100-global-proj_sl-2.0-resnet56]: Epoch 56/100, Acc=0.6910, Val Loss=1.6831, lr=0.0100
[02/22 05:42:16 cifar100-global-proj_sl-2.0-resnet56]: Epoch 57/100, Acc=0.6885, Val Loss=1.7071, lr=0.0100
[02/22 05:49:20 cifar100-global-proj_sl-2.0-resnet56]: Epoch 58/100, Acc=0.6829, Val Loss=1.7361, lr=0.0100
[02/22 05:56:23 cifar100-global-proj_sl-2.0-resnet56]: Epoch 59/100, Acc=0.6917, Val Loss=1.6698, lr=0.0100
[02/22 06:03:25 cifar100-global-proj_sl-2.0-resnet56]: Epoch 60/100, Acc=0.7172, Val Loss=1.5138, lr=0.0010
[02/22 06:10:27 cifar100-global-proj_sl-2.0-resnet56]: Epoch 61/100, Acc=0.7203, Val Loss=1.5114, lr=0.0010
[02/22 06:17:29 cifar100-global-proj_sl-2.0-resnet56]: Epoch 62/100, Acc=0.7197, Val Loss=1.5106, lr=0.0010
[02/22 06:24:30 cifar100-global-proj_sl-2.0-resnet56]: Epoch 63/100, Acc=0.7195, Val Loss=1.5246, lr=0.0010
[02/22 06:31:32 cifar100-global-proj_sl-2.0-resnet56]: Epoch 64/100, Acc=0.7197, Val Loss=1.5062, lr=0.0010
[02/22 06:38:33 cifar100-global-proj_sl-2.0-resnet56]: Epoch 65/100, Acc=0.7197, Val Loss=1.5317, lr=0.0010
[02/22 06:45:35 cifar100-global-proj_sl-2.0-resnet56]: Epoch 66/100, Acc=0.7204, Val Loss=1.5234, lr=0.0010
[02/22 06:52:36 cifar100-global-proj_sl-2.0-resnet56]: Epoch 67/100, Acc=0.7219, Val Loss=1.5271, lr=0.0010
[02/22 06:59:39 cifar100-global-proj_sl-2.0-resnet56]: Epoch 68/100, Acc=0.7233, Val Loss=1.5313, lr=0.0010
[02/22 07:06:42 cifar100-global-proj_sl-2.0-resnet56]: Epoch 69/100, Acc=0.7215, Val Loss=1.5359, lr=0.0010
[02/22 07:13:45 cifar100-global-proj_sl-2.0-resnet56]: Epoch 70/100, Acc=0.7238, Val Loss=1.5382, lr=0.0010
[02/22 07:20:47 cifar100-global-proj_sl-2.0-resnet56]: Epoch 71/100, Acc=0.7230, Val Loss=1.5402, lr=0.0010
[02/22 07:27:49 cifar100-global-proj_sl-2.0-resnet56]: Epoch 72/100, Acc=0.7212, Val Loss=1.5397, lr=0.0010
[02/22 07:34:51 cifar100-global-proj_sl-2.0-resnet56]: Epoch 73/100, Acc=0.7230, Val Loss=1.5571, lr=0.0010
[02/22 07:41:53 cifar100-global-proj_sl-2.0-resnet56]: Epoch 74/100, Acc=0.7216, Val Loss=1.5533, lr=0.0010
[02/22 07:48:55 cifar100-global-proj_sl-2.0-resnet56]: Epoch 75/100, Acc=0.7256, Val Loss=1.5497, lr=0.0010
[02/22 07:55:57 cifar100-global-proj_sl-2.0-resnet56]: Epoch 76/100, Acc=0.7246, Val Loss=1.5572, lr=0.0010
[02/22 08:02:59 cifar100-global-proj_sl-2.0-resnet56]: Epoch 77/100, Acc=0.7216, Val Loss=1.5557, lr=0.0010
[02/22 08:10:02 cifar100-global-proj_sl-2.0-resnet56]: Epoch 78/100, Acc=0.7223, Val Loss=1.5642, lr=0.0010
[02/22 08:17:06 cifar100-global-proj_sl-2.0-resnet56]: Epoch 79/100, Acc=0.7233, Val Loss=1.5660, lr=0.0010
[02/22 08:24:08 cifar100-global-proj_sl-2.0-resnet56]: Epoch 80/100, Acc=0.7223, Val Loss=1.5688, lr=0.0001
[02/22 08:31:10 cifar100-global-proj_sl-2.0-resnet56]: Epoch 81/100, Acc=0.7233, Val Loss=1.5660, lr=0.0001
[02/22 08:38:10 cifar100-global-proj_sl-2.0-resnet56]: Epoch 82/100, Acc=0.7230, Val Loss=1.5707, lr=0.0001
[02/22 08:45:11 cifar100-global-proj_sl-2.0-resnet56]: Epoch 83/100, Acc=0.7231, Val Loss=1.5701, lr=0.0001
[02/22 08:52:13 cifar100-global-proj_sl-2.0-resnet56]: Epoch 84/100, Acc=0.7220, Val Loss=1.5682, lr=0.0001
[02/22 08:59:17 cifar100-global-proj_sl-2.0-resnet56]: Epoch 85/100, Acc=0.7237, Val Loss=1.5689, lr=0.0001
[02/22 09:06:21 cifar100-global-proj_sl-2.0-resnet56]: Epoch 86/100, Acc=0.7232, Val Loss=1.5701, lr=0.0001
[02/22 09:13:26 cifar100-global-proj_sl-2.0-resnet56]: Epoch 87/100, Acc=0.7228, Val Loss=1.5582, lr=0.0001
[02/22 09:20:30 cifar100-global-proj_sl-2.0-resnet56]: Epoch 88/100, Acc=0.7221, Val Loss=1.5659, lr=0.0001
[02/22 09:27:32 cifar100-global-proj_sl-2.0-resnet56]: Epoch 89/100, Acc=0.7234, Val Loss=1.5657, lr=0.0001
[02/22 09:34:35 cifar100-global-proj_sl-2.0-resnet56]: Epoch 90/100, Acc=0.7246, Val Loss=1.5577, lr=0.0001
[02/22 09:41:36 cifar100-global-proj_sl-2.0-resnet56]: Epoch 91/100, Acc=0.7240, Val Loss=1.5653, lr=0.0001
[02/22 09:48:38 cifar100-global-proj_sl-2.0-resnet56]: Epoch 92/100, Acc=0.7234, Val Loss=1.5649, lr=0.0001
[02/22 09:55:42 cifar100-global-proj_sl-2.0-resnet56]: Epoch 93/100, Acc=0.7232, Val Loss=1.5615, lr=0.0001
[02/22 10:02:44 cifar100-global-proj_sl-2.0-resnet56]: Epoch 94/100, Acc=0.7227, Val Loss=1.5626, lr=0.0001
[02/22 10:09:46 cifar100-global-proj_sl-2.0-resnet56]: Epoch 95/100, Acc=0.7235, Val Loss=1.5640, lr=0.0001
[02/22 10:16:48 cifar100-global-proj_sl-2.0-resnet56]: Epoch 96/100, Acc=0.7254, Val Loss=1.5621, lr=0.0001
[02/22 10:23:51 cifar100-global-proj_sl-2.0-resnet56]: Epoch 97/100, Acc=0.7236, Val Loss=1.5714, lr=0.0001
[02/22 10:30:53 cifar100-global-proj_sl-2.0-resnet56]: Epoch 98/100, Acc=0.7225, Val Loss=1.5634, lr=0.0001
[02/22 10:37:56 cifar100-global-proj_sl-2.0-resnet56]: Epoch 99/100, Acc=0.7248, Val Loss=1.5623, lr=0.0001
[02/22 10:37:56 cifar100-global-proj_sl-2.0-resnet56]: Best Acc=0.7256
[02/22 10:37:56 cifar100-global-proj_sl-2.0-resnet56]: Loading the sparse model from run/cifar100/prune/cifar100-global-proj_sl-2.0-resnet56/reg_cifar100_resnet56_proj_sl_1e-05.pth...
[02/22 10:37:59 cifar100-global-proj_sl-2.0-resnet56]: Pruning...
[02/22 10:39:38 cifar100-global-proj_sl-2.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(6, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(6, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(6, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(6, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(6, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(6, 26, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(26, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(26, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(26, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(26, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(26, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(26, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(26, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(26, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(26, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(21, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(26, 46, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(46, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(26, 62, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(62, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(52, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(62, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(62, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(62, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(62, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(48, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(62, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=62, out_features=100, bias=True)
)
[02/22 10:39:41 cifar100-global-proj_sl-2.0-resnet56]: Params: 0.86 M => 0.65 M (75.30%)
[02/22 10:39:41 cifar100-global-proj_sl-2.0-resnet56]: FLOPs: 127.12 M => 63.15 M (49.67%, 2.01X )
[02/22 10:39:41 cifar100-global-proj_sl-2.0-resnet56]: Acc: 0.7256 => 0.0085
[02/22 10:39:41 cifar100-global-proj_sl-2.0-resnet56]: Val Loss: 1.5497 => 34.0276
[02/22 10:39:41 cifar100-global-proj_sl-2.0-resnet56]: Finetuning...
[02/22 10:40:09 cifar100-global-proj_sl-2.0-resnet56]: Epoch 0/100, Acc=0.5514, Val Loss=1.7498, lr=0.0100
[02/22 10:40:37 cifar100-global-proj_sl-2.0-resnet56]: Epoch 1/100, Acc=0.5624, Val Loss=1.8511, lr=0.0100
[02/22 10:41:06 cifar100-global-proj_sl-2.0-resnet56]: Epoch 2/100, Acc=0.6017, Val Loss=1.5764, lr=0.0100
[02/22 10:41:34 cifar100-global-proj_sl-2.0-resnet56]: Epoch 3/100, Acc=0.6273, Val Loss=1.5291, lr=0.0100
[02/22 10:42:02 cifar100-global-proj_sl-2.0-resnet56]: Epoch 4/100, Acc=0.6254, Val Loss=1.5332, lr=0.0100
[02/22 10:42:31 cifar100-global-proj_sl-2.0-resnet56]: Epoch 5/100, Acc=0.6132, Val Loss=1.6190, lr=0.0100
[02/22 10:42:59 cifar100-global-proj_sl-2.0-resnet56]: Epoch 6/100, Acc=0.6380, Val Loss=1.4927, lr=0.0100
[02/22 10:43:27 cifar100-global-proj_sl-2.0-resnet56]: Epoch 7/100, Acc=0.6114, Val Loss=1.6294, lr=0.0100
[02/22 10:43:55 cifar100-global-proj_sl-2.0-resnet56]: Epoch 8/100, Acc=0.6474, Val Loss=1.4811, lr=0.0100
[02/22 10:44:23 cifar100-global-proj_sl-2.0-resnet56]: Epoch 9/100, Acc=0.6364, Val Loss=1.4756, lr=0.0100
[02/22 10:44:51 cifar100-global-proj_sl-2.0-resnet56]: Epoch 10/100, Acc=0.6410, Val Loss=1.5007, lr=0.0100
[02/22 10:45:19 cifar100-global-proj_sl-2.0-resnet56]: Epoch 11/100, Acc=0.6378, Val Loss=1.5114, lr=0.0100
[02/22 10:45:47 cifar100-global-proj_sl-2.0-resnet56]: Epoch 12/100, Acc=0.6378, Val Loss=1.5308, lr=0.0100
[02/22 10:46:15 cifar100-global-proj_sl-2.0-resnet56]: Epoch 13/100, Acc=0.6341, Val Loss=1.5093, lr=0.0100
[02/22 10:46:43 cifar100-global-proj_sl-2.0-resnet56]: Epoch 14/100, Acc=0.6433, Val Loss=1.4919, lr=0.0100
[02/22 10:47:11 cifar100-global-proj_sl-2.0-resnet56]: Epoch 15/100, Acc=0.6324, Val Loss=1.5409, lr=0.0100
[02/22 10:47:39 cifar100-global-proj_sl-2.0-resnet56]: Epoch 16/100, Acc=0.6389, Val Loss=1.4805, lr=0.0100
[02/22 10:48:07 cifar100-global-proj_sl-2.0-resnet56]: Epoch 17/100, Acc=0.6341, Val Loss=1.5393, lr=0.0100
[02/22 10:48:35 cifar100-global-proj_sl-2.0-resnet56]: Epoch 18/100, Acc=0.6249, Val Loss=1.5468, lr=0.0100
[02/22 10:49:04 cifar100-global-proj_sl-2.0-resnet56]: Epoch 19/100, Acc=0.6530, Val Loss=1.4570, lr=0.0100
[02/22 10:49:32 cifar100-global-proj_sl-2.0-resnet56]: Epoch 20/100, Acc=0.6195, Val Loss=1.6008, lr=0.0100
[02/22 10:50:00 cifar100-global-proj_sl-2.0-resnet56]: Epoch 21/100, Acc=0.6266, Val Loss=1.5356, lr=0.0100
[02/22 10:50:28 cifar100-global-proj_sl-2.0-resnet56]: Epoch 22/100, Acc=0.6173, Val Loss=1.6018, lr=0.0100
[02/22 10:50:56 cifar100-global-proj_sl-2.0-resnet56]: Epoch 23/100, Acc=0.6515, Val Loss=1.4053, lr=0.0100
[02/22 10:51:24 cifar100-global-proj_sl-2.0-resnet56]: Epoch 24/100, Acc=0.6434, Val Loss=1.4867, lr=0.0100
[02/22 10:51:52 cifar100-global-proj_sl-2.0-resnet56]: Epoch 25/100, Acc=0.6268, Val Loss=1.5834, lr=0.0100
[02/22 10:52:20 cifar100-global-proj_sl-2.0-resnet56]: Epoch 26/100, Acc=0.6532, Val Loss=1.4596, lr=0.0100
[02/22 10:52:48 cifar100-global-proj_sl-2.0-resnet56]: Epoch 27/100, Acc=0.6475, Val Loss=1.4602, lr=0.0100
[02/22 10:53:17 cifar100-global-proj_sl-2.0-resnet56]: Epoch 28/100, Acc=0.6321, Val Loss=1.5815, lr=0.0100
[02/22 10:53:45 cifar100-global-proj_sl-2.0-resnet56]: Epoch 29/100, Acc=0.6436, Val Loss=1.4988, lr=0.0100
[02/22 10:54:13 cifar100-global-proj_sl-2.0-resnet56]: Epoch 30/100, Acc=0.6232, Val Loss=1.6196, lr=0.0100
[02/22 10:54:41 cifar100-global-proj_sl-2.0-resnet56]: Epoch 31/100, Acc=0.6425, Val Loss=1.4792, lr=0.0100
[02/22 10:55:10 cifar100-global-proj_sl-2.0-resnet56]: Epoch 32/100, Acc=0.6353, Val Loss=1.4860, lr=0.0100
[02/22 10:55:38 cifar100-global-proj_sl-2.0-resnet56]: Epoch 33/100, Acc=0.6323, Val Loss=1.5847, lr=0.0100
[02/22 10:56:06 cifar100-global-proj_sl-2.0-resnet56]: Epoch 34/100, Acc=0.6451, Val Loss=1.4841, lr=0.0100
[02/22 10:56:35 cifar100-global-proj_sl-2.0-resnet56]: Epoch 35/100, Acc=0.6147, Val Loss=1.6185, lr=0.0100
[02/22 10:57:03 cifar100-global-proj_sl-2.0-resnet56]: Epoch 36/100, Acc=0.6484, Val Loss=1.4526, lr=0.0100
[02/22 10:57:31 cifar100-global-proj_sl-2.0-resnet56]: Epoch 37/100, Acc=0.6456, Val Loss=1.4321, lr=0.0100
[02/22 10:58:00 cifar100-global-proj_sl-2.0-resnet56]: Epoch 38/100, Acc=0.6312, Val Loss=1.5238, lr=0.0100
[02/22 10:58:28 cifar100-global-proj_sl-2.0-resnet56]: Epoch 39/100, Acc=0.6326, Val Loss=1.5584, lr=0.0100
[02/22 10:58:56 cifar100-global-proj_sl-2.0-resnet56]: Epoch 40/100, Acc=0.6504, Val Loss=1.4226, lr=0.0100
[02/22 10:59:24 cifar100-global-proj_sl-2.0-resnet56]: Epoch 41/100, Acc=0.6533, Val Loss=1.4261, lr=0.0100
[02/22 10:59:53 cifar100-global-proj_sl-2.0-resnet56]: Epoch 42/100, Acc=0.6204, Val Loss=1.5789, lr=0.0100
[02/22 11:00:21 cifar100-global-proj_sl-2.0-resnet56]: Epoch 43/100, Acc=0.6487, Val Loss=1.4928, lr=0.0100
[02/22 11:00:50 cifar100-global-proj_sl-2.0-resnet56]: Epoch 44/100, Acc=0.6106, Val Loss=1.6185, lr=0.0100
[02/22 11:01:18 cifar100-global-proj_sl-2.0-resnet56]: Epoch 45/100, Acc=0.6122, Val Loss=1.7000, lr=0.0100
[02/22 11:01:46 cifar100-global-proj_sl-2.0-resnet56]: Epoch 46/100, Acc=0.6240, Val Loss=1.5833, lr=0.0100
[02/22 11:02:15 cifar100-global-proj_sl-2.0-resnet56]: Epoch 47/100, Acc=0.6334, Val Loss=1.5291, lr=0.0100
[02/22 11:02:43 cifar100-global-proj_sl-2.0-resnet56]: Epoch 48/100, Acc=0.6392, Val Loss=1.5547, lr=0.0100
[02/22 11:03:11 cifar100-global-proj_sl-2.0-resnet56]: Epoch 49/100, Acc=0.6382, Val Loss=1.4986, lr=0.0100
[02/22 11:03:39 cifar100-global-proj_sl-2.0-resnet56]: Epoch 50/100, Acc=0.6409, Val Loss=1.4708, lr=0.0100
[02/22 11:04:08 cifar100-global-proj_sl-2.0-resnet56]: Epoch 51/100, Acc=0.6416, Val Loss=1.4825, lr=0.0100
[02/22 11:04:36 cifar100-global-proj_sl-2.0-resnet56]: Epoch 52/100, Acc=0.6509, Val Loss=1.4255, lr=0.0100
[02/22 11:05:05 cifar100-global-proj_sl-2.0-resnet56]: Epoch 53/100, Acc=0.5983, Val Loss=1.7728, lr=0.0100
[02/22 11:05:33 cifar100-global-proj_sl-2.0-resnet56]: Epoch 54/100, Acc=0.6440, Val Loss=1.4798, lr=0.0100
[02/22 11:06:02 cifar100-global-proj_sl-2.0-resnet56]: Epoch 55/100, Acc=0.6386, Val Loss=1.5472, lr=0.0100
[02/22 11:06:31 cifar100-global-proj_sl-2.0-resnet56]: Epoch 56/100, Acc=0.6134, Val Loss=1.6743, lr=0.0100
[02/22 11:06:59 cifar100-global-proj_sl-2.0-resnet56]: Epoch 57/100, Acc=0.6346, Val Loss=1.5087, lr=0.0100
[02/22 11:07:28 cifar100-global-proj_sl-2.0-resnet56]: Epoch 58/100, Acc=0.6332, Val Loss=1.5272, lr=0.0100
[02/22 11:07:57 cifar100-global-proj_sl-2.0-resnet56]: Epoch 59/100, Acc=0.6362, Val Loss=1.5142, lr=0.0100
[02/22 11:08:25 cifar100-global-proj_sl-2.0-resnet56]: Epoch 60/100, Acc=0.7031, Val Loss=1.1784, lr=0.0010
[02/22 11:08:54 cifar100-global-proj_sl-2.0-resnet56]: Epoch 61/100, Acc=0.7099, Val Loss=1.1738, lr=0.0010
[02/22 11:09:23 cifar100-global-proj_sl-2.0-resnet56]: Epoch 62/100, Acc=0.7122, Val Loss=1.1688, lr=0.0010
[02/22 11:09:52 cifar100-global-proj_sl-2.0-resnet56]: Epoch 63/100, Acc=0.7087, Val Loss=1.1782, lr=0.0010
[02/22 11:10:20 cifar100-global-proj_sl-2.0-resnet56]: Epoch 64/100, Acc=0.7112, Val Loss=1.1834, lr=0.0010
[02/22 11:10:48 cifar100-global-proj_sl-2.0-resnet56]: Epoch 65/100, Acc=0.7098, Val Loss=1.1896, lr=0.0010
[02/22 11:11:17 cifar100-global-proj_sl-2.0-resnet56]: Epoch 66/100, Acc=0.7098, Val Loss=1.2020, lr=0.0010
[02/22 11:11:45 cifar100-global-proj_sl-2.0-resnet56]: Epoch 67/100, Acc=0.7110, Val Loss=1.2003, lr=0.0010
[02/22 11:12:13 cifar100-global-proj_sl-2.0-resnet56]: Epoch 68/100, Acc=0.7119, Val Loss=1.2077, lr=0.0010
[02/22 11:12:42 cifar100-global-proj_sl-2.0-resnet56]: Epoch 69/100, Acc=0.7112, Val Loss=1.2081, lr=0.0010
[02/22 11:13:10 cifar100-global-proj_sl-2.0-resnet56]: Epoch 70/100, Acc=0.7095, Val Loss=1.2144, lr=0.0010
[02/22 11:13:37 cifar100-global-proj_sl-2.0-resnet56]: Epoch 71/100, Acc=0.7107, Val Loss=1.2179, lr=0.0010
[02/22 11:14:05 cifar100-global-proj_sl-2.0-resnet56]: Epoch 72/100, Acc=0.7098, Val Loss=1.2235, lr=0.0010
[02/22 11:14:34 cifar100-global-proj_sl-2.0-resnet56]: Epoch 73/100, Acc=0.7099, Val Loss=1.2284, lr=0.0010
[02/22 11:15:02 cifar100-global-proj_sl-2.0-resnet56]: Epoch 74/100, Acc=0.7107, Val Loss=1.2212, lr=0.0010
[02/22 11:15:30 cifar100-global-proj_sl-2.0-resnet56]: Epoch 75/100, Acc=0.7097, Val Loss=1.2417, lr=0.0010
[02/22 11:15:58 cifar100-global-proj_sl-2.0-resnet56]: Epoch 76/100, Acc=0.7100, Val Loss=1.2377, lr=0.0010
[02/22 11:16:26 cifar100-global-proj_sl-2.0-resnet56]: Epoch 77/100, Acc=0.7067, Val Loss=1.2415, lr=0.0010
[02/22 11:16:54 cifar100-global-proj_sl-2.0-resnet56]: Epoch 78/100, Acc=0.7080, Val Loss=1.2562, lr=0.0010
[02/22 11:17:22 cifar100-global-proj_sl-2.0-resnet56]: Epoch 79/100, Acc=0.7082, Val Loss=1.2566, lr=0.0010
[02/22 11:17:50 cifar100-global-proj_sl-2.0-resnet56]: Epoch 80/100, Acc=0.7077, Val Loss=1.2503, lr=0.0001
[02/22 11:18:18 cifar100-global-proj_sl-2.0-resnet56]: Epoch 81/100, Acc=0.7098, Val Loss=1.2472, lr=0.0001
[02/22 11:18:46 cifar100-global-proj_sl-2.0-resnet56]: Epoch 82/100, Acc=0.7093, Val Loss=1.2445, lr=0.0001
[02/22 11:19:14 cifar100-global-proj_sl-2.0-resnet56]: Epoch 83/100, Acc=0.7108, Val Loss=1.2407, lr=0.0001
[02/22 11:19:42 cifar100-global-proj_sl-2.0-resnet56]: Epoch 84/100, Acc=0.7096, Val Loss=1.2443, lr=0.0001
[02/22 11:20:11 cifar100-global-proj_sl-2.0-resnet56]: Epoch 85/100, Acc=0.7093, Val Loss=1.2457, lr=0.0001
[02/22 11:20:39 cifar100-global-proj_sl-2.0-resnet56]: Epoch 86/100, Acc=0.7090, Val Loss=1.2482, lr=0.0001
[02/22 11:21:07 cifar100-global-proj_sl-2.0-resnet56]: Epoch 87/100, Acc=0.7120, Val Loss=1.2452, lr=0.0001
[02/22 11:21:35 cifar100-global-proj_sl-2.0-resnet56]: Epoch 88/100, Acc=0.7108, Val Loss=1.2463, lr=0.0001
[02/22 11:22:04 cifar100-global-proj_sl-2.0-resnet56]: Epoch 89/100, Acc=0.7095, Val Loss=1.2486, lr=0.0001
[02/22 11:22:32 cifar100-global-proj_sl-2.0-resnet56]: Epoch 90/100, Acc=0.7105, Val Loss=1.2497, lr=0.0001
[02/22 11:23:00 cifar100-global-proj_sl-2.0-resnet56]: Epoch 91/100, Acc=0.7103, Val Loss=1.2475, lr=0.0001
[02/22 11:23:29 cifar100-global-proj_sl-2.0-resnet56]: Epoch 92/100, Acc=0.7117, Val Loss=1.2426, lr=0.0001
[02/22 11:23:57 cifar100-global-proj_sl-2.0-resnet56]: Epoch 93/100, Acc=0.7111, Val Loss=1.2500, lr=0.0001
[02/22 11:24:26 cifar100-global-proj_sl-2.0-resnet56]: Epoch 94/100, Acc=0.7104, Val Loss=1.2465, lr=0.0001
[02/22 11:24:54 cifar100-global-proj_sl-2.0-resnet56]: Epoch 95/100, Acc=0.7111, Val Loss=1.2444, lr=0.0001
[02/22 11:25:23 cifar100-global-proj_sl-2.0-resnet56]: Epoch 96/100, Acc=0.7117, Val Loss=1.2448, lr=0.0001
[02/22 11:25:51 cifar100-global-proj_sl-2.0-resnet56]: Epoch 97/100, Acc=0.7112, Val Loss=1.2511, lr=0.0001
[02/22 11:26:20 cifar100-global-proj_sl-2.0-resnet56]: Epoch 98/100, Acc=0.7100, Val Loss=1.2441, lr=0.0001
[02/22 11:26:48 cifar100-global-proj_sl-2.0-resnet56]: Epoch 99/100, Acc=0.7112, Val Loss=1.2463, lr=0.0001
[02/22 11:26:48 cifar100-global-proj_sl-2.0-resnet56]: Best Acc=0.7122
[02/22 11:26:48 cifar100-global-proj_sl-2.0-resnet56]: Params: 0.65 M
[02/22 11:26:48 cifar100-global-proj_sl-2.0-resnet56]: ops: 63.15 M
[02/22 11:26:51 cifar100-global-proj_sl-2.0-resnet56]: Acc: 0.7112 Val Loss: 1.2463

TIME TAKEN: 21:07:29
SLURM WORKLOAD FINISH: Sat Feb 22 11:26:52 CET 2025
