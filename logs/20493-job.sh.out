SLURM WORKLOAD START: Sun Feb 23 18:05:33 CET 2025
Sun Feb 23 18:05:33 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-PCIE-40GB          Off |   00000000:0B:00.0 Off |                    0 |
| N/A   23C    P0             30W /  250W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: mode: prune
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: model: mobilenetv2
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: verbose: False
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: dataset: cifar10
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: dataroot: data
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: batch_size: 128
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: total_epochs: 100
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: lr: 0.01
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: restore: run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: output_dir: run/cifar10/prune/cifar10-global-random-2.0-mobilenetv2
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: finetune: True
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: last_epochs: 100
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: reps: 1
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: method: random
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: speed_up: 2.0
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: reg: 1e-05
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: delta_reg: 0.0001
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: weight_decay: 0.0005
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: seed: 1
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: global_pruning: True
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: sl_total_epochs: 100
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: sl_lr: 0.01
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: sl_reg_warmup: 0
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: sl_restore: None
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: iterative_steps: 400
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: logger: <Logger cifar10-global-random-2.0-mobilenetv2 (DEBUG)>
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: device: cuda
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: num_classes: 10
[02/23 18:05:41 cifar10-global-random-2.0-mobilenetv2]: Loading model from run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/23 18:05:44 cifar10-global-random-2.0-mobilenetv2]: Pruning...
[02/23 18:05:51 cifar10-global-random-2.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 1, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 38, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(38, 38, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=38)
        (4): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(38, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 86, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(86, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=86)
        (4): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(86, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 86, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(86, 86, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=86)
        (4): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(86, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 134, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(134, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(134, 134, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=134)
        (4): BatchNorm2d(134, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(134, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 134, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(134, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(134, 134, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=134)
        (4): BatchNorm2d(134, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(134, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 134, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(134, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(134, 134, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=134)
        (4): BatchNorm2d(134, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(134, 6, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(6, 326, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(326, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(326, 326, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=326)
        (4): BatchNorm2d(326, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(326, 6, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(6, 326, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(326, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(326, 326, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=326)
        (4): BatchNorm2d(326, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(326, 6, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(6, 326, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(326, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(326, 326, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=326)
        (4): BatchNorm2d(326, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(326, 6, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(6, 326, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(326, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(326, 326, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=326)
        (4): BatchNorm2d(326, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(326, 38, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(38, 518, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(518, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(518, 518, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=518)
        (4): BatchNorm2d(518, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(518, 38, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(38, 518, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(518, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(518, 518, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=518)
        (4): BatchNorm2d(518, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(518, 38, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(38, 518, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(518, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(518, 518, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=518)
        (4): BatchNorm2d(518, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(518, 102, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(102, 902, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(902, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(902, 902, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=902)
        (4): BatchNorm2d(902, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(902, 102, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(102, 902, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(902, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(902, 902, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=902)
        (4): BatchNorm2d(902, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(902, 102, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(102, 902, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(902, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(902, 902, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=902)
      (4): BatchNorm2d(902, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(902, 262, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(262, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(262, 1222, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1222, 10, kernel_size=(1, 1), stride=(1, 1))
)
[02/23 18:05:54 cifar10-global-random-2.0-mobilenetv2]: Params: 2.25 M => 1.31 M (57.96%)
[02/23 18:05:54 cifar10-global-random-2.0-mobilenetv2]: FLOPs: 68.29 M => 34.03 M (49.84%, 2.01X )
[02/23 18:05:54 cifar10-global-random-2.0-mobilenetv2]: Acc: 0.8936 => 0.1000
[02/23 18:05:54 cifar10-global-random-2.0-mobilenetv2]: Val Loss: 0.3202 => 2.7348
[02/23 18:05:54 cifar10-global-random-2.0-mobilenetv2]: Finetuning...
[02/23 18:06:23 cifar10-global-random-2.0-mobilenetv2]: Epoch 0/100, Acc=0.1000, Val Loss=2.7438, lr=0.0100
[02/23 18:06:51 cifar10-global-random-2.0-mobilenetv2]: Epoch 1/100, Acc=0.1000, Val Loss=2.6903, lr=0.0100
[02/23 18:07:19 cifar10-global-random-2.0-mobilenetv2]: Epoch 2/100, Acc=0.1000, Val Loss=2.8587, lr=0.0100
[02/23 18:07:48 cifar10-global-random-2.0-mobilenetv2]: Epoch 3/100, Acc=0.1000, Val Loss=2.9005, lr=0.0100
[02/23 18:08:17 cifar10-global-random-2.0-mobilenetv2]: Epoch 4/100, Acc=0.1000, Val Loss=2.8779, lr=0.0100
[02/23 18:08:45 cifar10-global-random-2.0-mobilenetv2]: Epoch 5/100, Acc=0.1000, Val Loss=2.8064, lr=0.0100
[02/23 18:09:14 cifar10-global-random-2.0-mobilenetv2]: Epoch 6/100, Acc=0.1000, Val Loss=2.7889, lr=0.0100
[02/23 18:09:42 cifar10-global-random-2.0-mobilenetv2]: Epoch 7/100, Acc=0.1000, Val Loss=2.6540, lr=0.0100
[02/23 18:10:11 cifar10-global-random-2.0-mobilenetv2]: Epoch 8/100, Acc=0.1000, Val Loss=2.7383, lr=0.0100
[02/23 18:10:39 cifar10-global-random-2.0-mobilenetv2]: Epoch 9/100, Acc=0.1000, Val Loss=2.3055, lr=0.0100
[02/23 18:11:08 cifar10-global-random-2.0-mobilenetv2]: Epoch 10/100, Acc=0.1000, Val Loss=2.6188, lr=0.0100
[02/23 18:11:36 cifar10-global-random-2.0-mobilenetv2]: Epoch 11/100, Acc=0.1000, Val Loss=2.3048, lr=0.0100
[02/23 18:12:05 cifar10-global-random-2.0-mobilenetv2]: Epoch 12/100, Acc=0.1000, Val Loss=2.3328, lr=0.0100
[02/23 18:12:33 cifar10-global-random-2.0-mobilenetv2]: Epoch 13/100, Acc=0.1000, Val Loss=2.5345, lr=0.0100
[02/23 18:13:02 cifar10-global-random-2.0-mobilenetv2]: Epoch 14/100, Acc=0.1000, Val Loss=2.3824, lr=0.0100
[02/23 18:13:30 cifar10-global-random-2.0-mobilenetv2]: Epoch 15/100, Acc=0.1000, Val Loss=2.4224, lr=0.0100
[02/23 18:13:59 cifar10-global-random-2.0-mobilenetv2]: Epoch 16/100, Acc=0.1000, Val Loss=2.3826, lr=0.0100
[02/23 18:14:27 cifar10-global-random-2.0-mobilenetv2]: Epoch 17/100, Acc=0.1000, Val Loss=2.3646, lr=0.0100
[02/23 18:14:56 cifar10-global-random-2.0-mobilenetv2]: Epoch 18/100, Acc=0.1000, Val Loss=2.4549, lr=0.0100
[02/23 18:15:24 cifar10-global-random-2.0-mobilenetv2]: Epoch 19/100, Acc=0.1000, Val Loss=2.3696, lr=0.0100
[02/23 18:15:53 cifar10-global-random-2.0-mobilenetv2]: Epoch 20/100, Acc=0.1000, Val Loss=2.3511, lr=0.0100
[02/23 18:16:22 cifar10-global-random-2.0-mobilenetv2]: Epoch 21/100, Acc=0.1000, Val Loss=2.3419, lr=0.0100
[02/23 18:16:50 cifar10-global-random-2.0-mobilenetv2]: Epoch 22/100, Acc=0.1000, Val Loss=2.3553, lr=0.0100
[02/23 18:17:19 cifar10-global-random-2.0-mobilenetv2]: Epoch 23/100, Acc=0.1000, Val Loss=2.3894, lr=0.0100
[02/23 18:17:48 cifar10-global-random-2.0-mobilenetv2]: Epoch 24/100, Acc=0.1000, Val Loss=2.3422, lr=0.0100
[02/23 18:18:17 cifar10-global-random-2.0-mobilenetv2]: Epoch 25/100, Acc=0.1000, Val Loss=2.3304, lr=0.0100
[02/23 18:18:45 cifar10-global-random-2.0-mobilenetv2]: Epoch 26/100, Acc=0.1000, Val Loss=2.3281, lr=0.0100
[02/23 18:19:14 cifar10-global-random-2.0-mobilenetv2]: Epoch 27/100, Acc=0.1000, Val Loss=2.3286, lr=0.0100
[02/23 18:19:43 cifar10-global-random-2.0-mobilenetv2]: Epoch 28/100, Acc=0.1000, Val Loss=2.3464, lr=0.0100
[02/23 18:20:11 cifar10-global-random-2.0-mobilenetv2]: Epoch 29/100, Acc=0.1000, Val Loss=2.3034, lr=0.0100
[02/23 18:20:40 cifar10-global-random-2.0-mobilenetv2]: Epoch 30/100, Acc=0.1000, Val Loss=2.3203, lr=0.0100
[02/23 18:21:10 cifar10-global-random-2.0-mobilenetv2]: Epoch 31/100, Acc=0.1000, Val Loss=2.3227, lr=0.0100
[02/23 18:21:39 cifar10-global-random-2.0-mobilenetv2]: Epoch 32/100, Acc=0.1000, Val Loss=2.3093, lr=0.0100
[02/23 18:22:08 cifar10-global-random-2.0-mobilenetv2]: Epoch 33/100, Acc=0.1000, Val Loss=2.3252, lr=0.0100
[02/23 18:22:37 cifar10-global-random-2.0-mobilenetv2]: Epoch 34/100, Acc=0.1000, Val Loss=2.3070, lr=0.0100
[02/23 18:23:06 cifar10-global-random-2.0-mobilenetv2]: Epoch 35/100, Acc=0.1000, Val Loss=2.3091, lr=0.0100
[02/23 18:23:35 cifar10-global-random-2.0-mobilenetv2]: Epoch 36/100, Acc=0.1000, Val Loss=2.3059, lr=0.0100
[02/23 18:24:05 cifar10-global-random-2.0-mobilenetv2]: Epoch 37/100, Acc=0.1000, Val Loss=2.3028, lr=0.0100
[02/23 18:24:34 cifar10-global-random-2.0-mobilenetv2]: Epoch 38/100, Acc=0.1000, Val Loss=2.3032, lr=0.0100
[02/23 18:25:03 cifar10-global-random-2.0-mobilenetv2]: Epoch 39/100, Acc=0.1000, Val Loss=2.3124, lr=0.0100
[02/23 18:25:31 cifar10-global-random-2.0-mobilenetv2]: Epoch 40/100, Acc=0.1000, Val Loss=2.3029, lr=0.0100
[02/23 18:26:00 cifar10-global-random-2.0-mobilenetv2]: Epoch 41/100, Acc=0.1000, Val Loss=2.3073, lr=0.0100
[02/23 18:26:29 cifar10-global-random-2.0-mobilenetv2]: Epoch 42/100, Acc=0.1000, Val Loss=2.3079, lr=0.0100
[02/23 18:26:58 cifar10-global-random-2.0-mobilenetv2]: Epoch 43/100, Acc=0.1000, Val Loss=2.3030, lr=0.0100
[02/23 18:27:27 cifar10-global-random-2.0-mobilenetv2]: Epoch 44/100, Acc=0.1000, Val Loss=2.3030, lr=0.0100
[02/23 18:27:56 cifar10-global-random-2.0-mobilenetv2]: Epoch 45/100, Acc=0.1000, Val Loss=2.3033, lr=0.0100
[02/23 18:28:25 cifar10-global-random-2.0-mobilenetv2]: Epoch 46/100, Acc=0.1000, Val Loss=2.3031, lr=0.0100
[02/23 18:28:54 cifar10-global-random-2.0-mobilenetv2]: Epoch 47/100, Acc=0.1000, Val Loss=2.3035, lr=0.0100
[02/23 18:29:22 cifar10-global-random-2.0-mobilenetv2]: Epoch 48/100, Acc=0.1000, Val Loss=2.3027, lr=0.0100
[02/23 18:29:51 cifar10-global-random-2.0-mobilenetv2]: Epoch 49/100, Acc=0.1000, Val Loss=2.3029, lr=0.0100
[02/23 18:30:20 cifar10-global-random-2.0-mobilenetv2]: Epoch 50/100, Acc=0.1000, Val Loss=2.3029, lr=0.0100
[02/23 18:30:49 cifar10-global-random-2.0-mobilenetv2]: Epoch 51/100, Acc=0.1000, Val Loss=2.3027, lr=0.0100
[02/23 18:31:18 cifar10-global-random-2.0-mobilenetv2]: Epoch 52/100, Acc=0.1000, Val Loss=2.3029, lr=0.0100
[02/23 18:31:47 cifar10-global-random-2.0-mobilenetv2]: Epoch 53/100, Acc=0.1000, Val Loss=2.3027, lr=0.0100
[02/23 18:32:16 cifar10-global-random-2.0-mobilenetv2]: Epoch 54/100, Acc=0.1000, Val Loss=2.3027, lr=0.0100
[02/23 18:32:46 cifar10-global-random-2.0-mobilenetv2]: Epoch 55/100, Acc=0.1000, Val Loss=2.3027, lr=0.0100
[02/23 18:33:15 cifar10-global-random-2.0-mobilenetv2]: Epoch 56/100, Acc=0.1000, Val Loss=2.3027, lr=0.0100
[02/23 18:33:43 cifar10-global-random-2.0-mobilenetv2]: Epoch 57/100, Acc=0.1000, Val Loss=2.3027, lr=0.0100
[02/23 18:34:12 cifar10-global-random-2.0-mobilenetv2]: Epoch 58/100, Acc=0.1000, Val Loss=2.3029, lr=0.0100
[02/23 18:34:41 cifar10-global-random-2.0-mobilenetv2]: Epoch 59/100, Acc=0.1000, Val Loss=2.3028, lr=0.0100
[02/23 18:35:11 cifar10-global-random-2.0-mobilenetv2]: Epoch 60/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/23 18:35:39 cifar10-global-random-2.0-mobilenetv2]: Epoch 61/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/23 18:36:08 cifar10-global-random-2.0-mobilenetv2]: Epoch 62/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/23 18:36:38 cifar10-global-random-2.0-mobilenetv2]: Epoch 63/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/23 18:37:06 cifar10-global-random-2.0-mobilenetv2]: Epoch 64/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/23 18:37:36 cifar10-global-random-2.0-mobilenetv2]: Epoch 65/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/23 18:38:05 cifar10-global-random-2.0-mobilenetv2]: Epoch 66/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/23 18:38:34 cifar10-global-random-2.0-mobilenetv2]: Epoch 67/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/23 18:39:02 cifar10-global-random-2.0-mobilenetv2]: Epoch 68/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/23 18:39:31 cifar10-global-random-2.0-mobilenetv2]: Epoch 69/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/23 18:40:00 cifar10-global-random-2.0-mobilenetv2]: Epoch 70/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/23 18:40:29 cifar10-global-random-2.0-mobilenetv2]: Epoch 71/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/23 18:40:58 cifar10-global-random-2.0-mobilenetv2]: Epoch 72/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/23 18:41:27 cifar10-global-random-2.0-mobilenetv2]: Epoch 73/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/23 18:41:57 cifar10-global-random-2.0-mobilenetv2]: Epoch 74/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/23 18:42:26 cifar10-global-random-2.0-mobilenetv2]: Epoch 75/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/23 18:42:54 cifar10-global-random-2.0-mobilenetv2]: Epoch 76/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/23 18:43:23 cifar10-global-random-2.0-mobilenetv2]: Epoch 77/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/23 18:43:52 cifar10-global-random-2.0-mobilenetv2]: Epoch 78/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/23 18:44:21 cifar10-global-random-2.0-mobilenetv2]: Epoch 79/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/23 18:44:49 cifar10-global-random-2.0-mobilenetv2]: Epoch 80/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/23 18:45:18 cifar10-global-random-2.0-mobilenetv2]: Epoch 81/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/23 18:45:46 cifar10-global-random-2.0-mobilenetv2]: Epoch 82/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/23 18:46:15 cifar10-global-random-2.0-mobilenetv2]: Epoch 83/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/23 18:46:44 cifar10-global-random-2.0-mobilenetv2]: Epoch 84/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/23 18:47:12 cifar10-global-random-2.0-mobilenetv2]: Epoch 85/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/23 18:47:41 cifar10-global-random-2.0-mobilenetv2]: Epoch 86/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/23 18:48:10 cifar10-global-random-2.0-mobilenetv2]: Epoch 87/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/23 18:48:39 cifar10-global-random-2.0-mobilenetv2]: Epoch 88/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/23 18:49:08 cifar10-global-random-2.0-mobilenetv2]: Epoch 89/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/23 18:49:37 cifar10-global-random-2.0-mobilenetv2]: Epoch 90/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/23 18:50:07 cifar10-global-random-2.0-mobilenetv2]: Epoch 91/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/23 18:50:36 cifar10-global-random-2.0-mobilenetv2]: Epoch 92/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/23 18:51:05 cifar10-global-random-2.0-mobilenetv2]: Epoch 93/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/23 18:51:34 cifar10-global-random-2.0-mobilenetv2]: Epoch 94/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/23 18:52:03 cifar10-global-random-2.0-mobilenetv2]: Epoch 95/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/23 18:52:32 cifar10-global-random-2.0-mobilenetv2]: Epoch 96/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/23 18:53:00 cifar10-global-random-2.0-mobilenetv2]: Epoch 97/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/23 18:53:30 cifar10-global-random-2.0-mobilenetv2]: Epoch 98/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/23 18:53:59 cifar10-global-random-2.0-mobilenetv2]: Epoch 99/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/23 18:53:59 cifar10-global-random-2.0-mobilenetv2]: Best Acc=0.1000
[02/23 18:53:59 cifar10-global-random-2.0-mobilenetv2]: Params: 1.31 M
[02/23 18:53:59 cifar10-global-random-2.0-mobilenetv2]: ops: 34.03 M
[02/23 18:54:02 cifar10-global-random-2.0-mobilenetv2]: Acc: 0.1000 Val Loss: 2.3026

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: mode: prune
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: model: mobilenetv2
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: verbose: False
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: dataset: cifar10
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: dataroot: data
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: batch_size: 128
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: total_epochs: 100
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: lr: 0.01
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: restore: run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: output_dir: run/cifar10/prune/cifar10-global-l2-2.0-mobilenetv2
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: finetune: True
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: last_epochs: 100
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: reps: 1
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: method: l2
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: speed_up: 2.0
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: reg: 1e-05
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: delta_reg: 0.0001
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: weight_decay: 0.0005
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: seed: 1
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: global_pruning: True
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: sl_total_epochs: 100
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: sl_lr: 0.01
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: sl_reg_warmup: 0
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: sl_restore: None
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: iterative_steps: 400
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: logger: <Logger cifar10-global-l2-2.0-mobilenetv2 (DEBUG)>
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: device: cuda
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: num_classes: 10
[02/23 18:54:09 cifar10-global-l2-2.0-mobilenetv2]: Loading model from run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/23 18:54:13 cifar10-global-l2-2.0-mobilenetv2]: Pruning...
[02/23 18:54:36 cifar10-global-l2-2.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 7, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(7, 17, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(17, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=17)
      (4): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(17, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 73, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(73, 73, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=73)
        (4): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(73, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 86, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(86, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=86)
        (4): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(86, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 60, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(60, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=60)
        (4): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(60, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 88, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88)
        (4): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(88, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 52, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(52, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=52)
        (4): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(52, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 50, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(50, 50, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=50)
        (4): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(50, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 54, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(54, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=54)
        (4): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(54, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 36, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36)
        (4): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(36, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 95, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(95, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(95, 95, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=95)
        (4): BatchNorm2d(95, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(95, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 232, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(232, 232, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=232)
        (4): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(232, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 16, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 5, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=5)
        (4): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(5, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 629, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(629, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(629, 629, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=629)
      (4): BatchNorm2d(629, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(629, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 10, kernel_size=(1, 1), stride=(1, 1))
)
[02/23 18:54:39 cifar10-global-l2-2.0-mobilenetv2]: Params: 2.25 M => 1.12 M (49.90%)
[02/23 18:54:39 cifar10-global-l2-2.0-mobilenetv2]: FLOPs: 68.29 M => 34.08 M (49.91%, 2.00X )
[02/23 18:54:39 cifar10-global-l2-2.0-mobilenetv2]: Acc: 0.8936 => 0.8936
[02/23 18:54:39 cifar10-global-l2-2.0-mobilenetv2]: Val Loss: 0.3202 => 0.3202
[02/23 18:54:39 cifar10-global-l2-2.0-mobilenetv2]: Finetuning...
[02/23 18:55:08 cifar10-global-l2-2.0-mobilenetv2]: Epoch 0/100, Acc=0.8502, Val Loss=0.4397, lr=0.0100
[02/23 18:55:37 cifar10-global-l2-2.0-mobilenetv2]: Epoch 1/100, Acc=0.8431, Val Loss=0.4663, lr=0.0100
[02/23 18:56:06 cifar10-global-l2-2.0-mobilenetv2]: Epoch 2/100, Acc=0.8510, Val Loss=0.4422, lr=0.0100
[02/23 18:56:35 cifar10-global-l2-2.0-mobilenetv2]: Epoch 3/100, Acc=0.8649, Val Loss=0.3960, lr=0.0100
[02/23 18:57:04 cifar10-global-l2-2.0-mobilenetv2]: Epoch 4/100, Acc=0.8572, Val Loss=0.4133, lr=0.0100
[02/23 18:57:32 cifar10-global-l2-2.0-mobilenetv2]: Epoch 5/100, Acc=0.8293, Val Loss=0.4998, lr=0.0100
[02/23 18:58:01 cifar10-global-l2-2.0-mobilenetv2]: Epoch 6/100, Acc=0.8295, Val Loss=0.5050, lr=0.0100
[02/23 18:58:30 cifar10-global-l2-2.0-mobilenetv2]: Epoch 7/100, Acc=0.8528, Val Loss=0.4424, lr=0.0100
[02/23 18:58:59 cifar10-global-l2-2.0-mobilenetv2]: Epoch 8/100, Acc=0.8572, Val Loss=0.4162, lr=0.0100
[02/23 18:59:28 cifar10-global-l2-2.0-mobilenetv2]: Epoch 9/100, Acc=0.8579, Val Loss=0.4249, lr=0.0100
[02/23 18:59:57 cifar10-global-l2-2.0-mobilenetv2]: Epoch 10/100, Acc=0.8544, Val Loss=0.4273, lr=0.0100
[02/23 19:00:26 cifar10-global-l2-2.0-mobilenetv2]: Epoch 11/100, Acc=0.8564, Val Loss=0.4244, lr=0.0100
[02/23 19:00:55 cifar10-global-l2-2.0-mobilenetv2]: Epoch 12/100, Acc=0.8535, Val Loss=0.4261, lr=0.0100
[02/23 19:01:24 cifar10-global-l2-2.0-mobilenetv2]: Epoch 13/100, Acc=0.8602, Val Loss=0.4119, lr=0.0100
[02/23 19:01:53 cifar10-global-l2-2.0-mobilenetv2]: Epoch 14/100, Acc=0.8597, Val Loss=0.4136, lr=0.0100
[02/23 19:02:22 cifar10-global-l2-2.0-mobilenetv2]: Epoch 15/100, Acc=0.8368, Val Loss=0.4849, lr=0.0100
[02/23 19:02:52 cifar10-global-l2-2.0-mobilenetv2]: Epoch 16/100, Acc=0.8525, Val Loss=0.4344, lr=0.0100
[02/23 19:03:21 cifar10-global-l2-2.0-mobilenetv2]: Epoch 17/100, Acc=0.8555, Val Loss=0.4301, lr=0.0100
[02/23 19:03:50 cifar10-global-l2-2.0-mobilenetv2]: Epoch 18/100, Acc=0.8498, Val Loss=0.4489, lr=0.0100
[02/23 19:04:19 cifar10-global-l2-2.0-mobilenetv2]: Epoch 19/100, Acc=0.8135, Val Loss=0.5638, lr=0.0100
[02/23 19:04:48 cifar10-global-l2-2.0-mobilenetv2]: Epoch 20/100, Acc=0.8328, Val Loss=0.4842, lr=0.0100
[02/23 19:05:18 cifar10-global-l2-2.0-mobilenetv2]: Epoch 21/100, Acc=0.8551, Val Loss=0.4260, lr=0.0100
[02/23 19:05:47 cifar10-global-l2-2.0-mobilenetv2]: Epoch 22/100, Acc=0.8510, Val Loss=0.4326, lr=0.0100
[02/23 19:06:16 cifar10-global-l2-2.0-mobilenetv2]: Epoch 23/100, Acc=0.8605, Val Loss=0.4163, lr=0.0100
[02/23 19:06:45 cifar10-global-l2-2.0-mobilenetv2]: Epoch 24/100, Acc=0.8503, Val Loss=0.4544, lr=0.0100
[02/23 19:07:14 cifar10-global-l2-2.0-mobilenetv2]: Epoch 25/100, Acc=0.8560, Val Loss=0.4197, lr=0.0100
[02/23 19:07:43 cifar10-global-l2-2.0-mobilenetv2]: Epoch 26/100, Acc=0.8506, Val Loss=0.4439, lr=0.0100
[02/23 19:08:12 cifar10-global-l2-2.0-mobilenetv2]: Epoch 27/100, Acc=0.8476, Val Loss=0.4499, lr=0.0100
[02/23 19:08:41 cifar10-global-l2-2.0-mobilenetv2]: Epoch 28/100, Acc=0.8606, Val Loss=0.4194, lr=0.0100
[02/23 19:09:10 cifar10-global-l2-2.0-mobilenetv2]: Epoch 29/100, Acc=0.8440, Val Loss=0.4624, lr=0.0100
[02/23 19:09:39 cifar10-global-l2-2.0-mobilenetv2]: Epoch 30/100, Acc=0.8560, Val Loss=0.4238, lr=0.0100
[02/23 19:10:08 cifar10-global-l2-2.0-mobilenetv2]: Epoch 31/100, Acc=0.8417, Val Loss=0.4631, lr=0.0100
[02/23 19:10:38 cifar10-global-l2-2.0-mobilenetv2]: Epoch 32/100, Acc=0.8563, Val Loss=0.4386, lr=0.0100
[02/23 19:11:07 cifar10-global-l2-2.0-mobilenetv2]: Epoch 33/100, Acc=0.8581, Val Loss=0.4184, lr=0.0100
[02/23 19:11:36 cifar10-global-l2-2.0-mobilenetv2]: Epoch 34/100, Acc=0.8616, Val Loss=0.4125, lr=0.0100
[02/23 19:12:05 cifar10-global-l2-2.0-mobilenetv2]: Epoch 35/100, Acc=0.8651, Val Loss=0.3979, lr=0.0100
[02/23 19:12:33 cifar10-global-l2-2.0-mobilenetv2]: Epoch 36/100, Acc=0.8646, Val Loss=0.4110, lr=0.0100
[02/23 19:13:02 cifar10-global-l2-2.0-mobilenetv2]: Epoch 37/100, Acc=0.8727, Val Loss=0.3864, lr=0.0100
[02/23 19:13:31 cifar10-global-l2-2.0-mobilenetv2]: Epoch 38/100, Acc=0.8626, Val Loss=0.4113, lr=0.0100
[02/23 19:13:59 cifar10-global-l2-2.0-mobilenetv2]: Epoch 39/100, Acc=0.8539, Val Loss=0.4399, lr=0.0100
[02/23 19:14:29 cifar10-global-l2-2.0-mobilenetv2]: Epoch 40/100, Acc=0.8516, Val Loss=0.4440, lr=0.0100
[02/23 19:14:58 cifar10-global-l2-2.0-mobilenetv2]: Epoch 41/100, Acc=0.8625, Val Loss=0.4003, lr=0.0100
[02/23 19:15:27 cifar10-global-l2-2.0-mobilenetv2]: Epoch 42/100, Acc=0.8620, Val Loss=0.4095, lr=0.0100
[02/23 19:15:56 cifar10-global-l2-2.0-mobilenetv2]: Epoch 43/100, Acc=0.8441, Val Loss=0.4649, lr=0.0100
[02/23 19:16:25 cifar10-global-l2-2.0-mobilenetv2]: Epoch 44/100, Acc=0.8543, Val Loss=0.4338, lr=0.0100
[02/23 19:16:54 cifar10-global-l2-2.0-mobilenetv2]: Epoch 45/100, Acc=0.8496, Val Loss=0.4518, lr=0.0100
[02/23 19:17:22 cifar10-global-l2-2.0-mobilenetv2]: Epoch 46/100, Acc=0.8498, Val Loss=0.4426, lr=0.0100
[02/23 19:17:51 cifar10-global-l2-2.0-mobilenetv2]: Epoch 47/100, Acc=0.8509, Val Loss=0.4458, lr=0.0100
[02/23 19:18:21 cifar10-global-l2-2.0-mobilenetv2]: Epoch 48/100, Acc=0.8458, Val Loss=0.4473, lr=0.0100
[02/23 19:18:49 cifar10-global-l2-2.0-mobilenetv2]: Epoch 49/100, Acc=0.8631, Val Loss=0.4075, lr=0.0100
[02/23 19:19:18 cifar10-global-l2-2.0-mobilenetv2]: Epoch 50/100, Acc=0.8682, Val Loss=0.3900, lr=0.0100
[02/23 19:19:47 cifar10-global-l2-2.0-mobilenetv2]: Epoch 51/100, Acc=0.8532, Val Loss=0.4418, lr=0.0100
[02/23 19:20:16 cifar10-global-l2-2.0-mobilenetv2]: Epoch 52/100, Acc=0.8505, Val Loss=0.4498, lr=0.0100
[02/23 19:20:45 cifar10-global-l2-2.0-mobilenetv2]: Epoch 53/100, Acc=0.8565, Val Loss=0.4262, lr=0.0100
[02/23 19:21:14 cifar10-global-l2-2.0-mobilenetv2]: Epoch 54/100, Acc=0.8646, Val Loss=0.3985, lr=0.0100
[02/23 19:21:43 cifar10-global-l2-2.0-mobilenetv2]: Epoch 55/100, Acc=0.8628, Val Loss=0.4008, lr=0.0100
[02/23 19:22:12 cifar10-global-l2-2.0-mobilenetv2]: Epoch 56/100, Acc=0.8682, Val Loss=0.4013, lr=0.0100
[02/23 19:22:40 cifar10-global-l2-2.0-mobilenetv2]: Epoch 57/100, Acc=0.8496, Val Loss=0.4527, lr=0.0100
[02/23 19:23:09 cifar10-global-l2-2.0-mobilenetv2]: Epoch 58/100, Acc=0.8559, Val Loss=0.4342, lr=0.0100
[02/23 19:23:38 cifar10-global-l2-2.0-mobilenetv2]: Epoch 59/100, Acc=0.8625, Val Loss=0.4098, lr=0.0100
[02/23 19:24:07 cifar10-global-l2-2.0-mobilenetv2]: Epoch 60/100, Acc=0.8920, Val Loss=0.3223, lr=0.0010
[02/23 19:24:37 cifar10-global-l2-2.0-mobilenetv2]: Epoch 61/100, Acc=0.8933, Val Loss=0.3159, lr=0.0010
[02/23 19:25:06 cifar10-global-l2-2.0-mobilenetv2]: Epoch 62/100, Acc=0.8931, Val Loss=0.3114, lr=0.0010
[02/23 19:25:36 cifar10-global-l2-2.0-mobilenetv2]: Epoch 63/100, Acc=0.8942, Val Loss=0.3116, lr=0.0010
[02/23 19:26:06 cifar10-global-l2-2.0-mobilenetv2]: Epoch 64/100, Acc=0.8956, Val Loss=0.3097, lr=0.0010
[02/23 19:26:36 cifar10-global-l2-2.0-mobilenetv2]: Epoch 65/100, Acc=0.8948, Val Loss=0.3097, lr=0.0010
[02/23 19:27:06 cifar10-global-l2-2.0-mobilenetv2]: Epoch 66/100, Acc=0.8954, Val Loss=0.3113, lr=0.0010
[02/23 19:27:35 cifar10-global-l2-2.0-mobilenetv2]: Epoch 67/100, Acc=0.8951, Val Loss=0.3102, lr=0.0010
[02/23 19:28:05 cifar10-global-l2-2.0-mobilenetv2]: Epoch 68/100, Acc=0.8951, Val Loss=0.3111, lr=0.0010
[02/23 19:28:34 cifar10-global-l2-2.0-mobilenetv2]: Epoch 69/100, Acc=0.8961, Val Loss=0.3115, lr=0.0010
[02/23 19:29:04 cifar10-global-l2-2.0-mobilenetv2]: Epoch 70/100, Acc=0.8970, Val Loss=0.3107, lr=0.0010
[02/23 19:29:34 cifar10-global-l2-2.0-mobilenetv2]: Epoch 71/100, Acc=0.8955, Val Loss=0.3071, lr=0.0010
[02/23 19:30:04 cifar10-global-l2-2.0-mobilenetv2]: Epoch 72/100, Acc=0.8970, Val Loss=0.3124, lr=0.0010
[02/23 19:30:33 cifar10-global-l2-2.0-mobilenetv2]: Epoch 73/100, Acc=0.8969, Val Loss=0.3116, lr=0.0010
[02/23 19:31:03 cifar10-global-l2-2.0-mobilenetv2]: Epoch 74/100, Acc=0.8966, Val Loss=0.3102, lr=0.0010
[02/23 19:31:33 cifar10-global-l2-2.0-mobilenetv2]: Epoch 75/100, Acc=0.8977, Val Loss=0.3099, lr=0.0010
[02/23 19:32:02 cifar10-global-l2-2.0-mobilenetv2]: Epoch 76/100, Acc=0.8957, Val Loss=0.3094, lr=0.0010
[02/23 19:32:32 cifar10-global-l2-2.0-mobilenetv2]: Epoch 77/100, Acc=0.8969, Val Loss=0.3104, lr=0.0010
[02/23 19:33:01 cifar10-global-l2-2.0-mobilenetv2]: Epoch 78/100, Acc=0.8988, Val Loss=0.3086, lr=0.0010
[02/23 19:33:31 cifar10-global-l2-2.0-mobilenetv2]: Epoch 79/100, Acc=0.8953, Val Loss=0.3128, lr=0.0010
[02/23 19:34:00 cifar10-global-l2-2.0-mobilenetv2]: Epoch 80/100, Acc=0.8963, Val Loss=0.3071, lr=0.0001
[02/23 19:34:30 cifar10-global-l2-2.0-mobilenetv2]: Epoch 81/100, Acc=0.8968, Val Loss=0.3069, lr=0.0001
[02/23 19:34:59 cifar10-global-l2-2.0-mobilenetv2]: Epoch 82/100, Acc=0.8990, Val Loss=0.3078, lr=0.0001
[02/23 19:35:29 cifar10-global-l2-2.0-mobilenetv2]: Epoch 83/100, Acc=0.8983, Val Loss=0.3071, lr=0.0001
[02/23 19:35:58 cifar10-global-l2-2.0-mobilenetv2]: Epoch 84/100, Acc=0.8978, Val Loss=0.3058, lr=0.0001
[02/23 19:36:27 cifar10-global-l2-2.0-mobilenetv2]: Epoch 85/100, Acc=0.8981, Val Loss=0.3056, lr=0.0001
[02/23 19:36:56 cifar10-global-l2-2.0-mobilenetv2]: Epoch 86/100, Acc=0.8971, Val Loss=0.3079, lr=0.0001
[02/23 19:37:26 cifar10-global-l2-2.0-mobilenetv2]: Epoch 87/100, Acc=0.8997, Val Loss=0.3051, lr=0.0001
[02/23 19:37:56 cifar10-global-l2-2.0-mobilenetv2]: Epoch 88/100, Acc=0.8975, Val Loss=0.3059, lr=0.0001
[02/23 19:38:26 cifar10-global-l2-2.0-mobilenetv2]: Epoch 89/100, Acc=0.8994, Val Loss=0.3062, lr=0.0001
[02/23 19:38:55 cifar10-global-l2-2.0-mobilenetv2]: Epoch 90/100, Acc=0.8977, Val Loss=0.3080, lr=0.0001
[02/23 19:39:25 cifar10-global-l2-2.0-mobilenetv2]: Epoch 91/100, Acc=0.8970, Val Loss=0.3064, lr=0.0001
[02/23 19:39:55 cifar10-global-l2-2.0-mobilenetv2]: Epoch 92/100, Acc=0.8993, Val Loss=0.3040, lr=0.0001
[02/23 19:40:24 cifar10-global-l2-2.0-mobilenetv2]: Epoch 93/100, Acc=0.8977, Val Loss=0.3069, lr=0.0001
[02/23 19:40:54 cifar10-global-l2-2.0-mobilenetv2]: Epoch 94/100, Acc=0.8994, Val Loss=0.3051, lr=0.0001
[02/23 19:41:24 cifar10-global-l2-2.0-mobilenetv2]: Epoch 95/100, Acc=0.8989, Val Loss=0.3055, lr=0.0001
[02/23 19:41:54 cifar10-global-l2-2.0-mobilenetv2]: Epoch 96/100, Acc=0.8989, Val Loss=0.3050, lr=0.0001
[02/23 19:42:24 cifar10-global-l2-2.0-mobilenetv2]: Epoch 97/100, Acc=0.8969, Val Loss=0.3053, lr=0.0001
[02/23 19:42:54 cifar10-global-l2-2.0-mobilenetv2]: Epoch 98/100, Acc=0.8984, Val Loss=0.3057, lr=0.0001
[02/23 19:43:24 cifar10-global-l2-2.0-mobilenetv2]: Epoch 99/100, Acc=0.8992, Val Loss=0.3056, lr=0.0001
[02/23 19:43:24 cifar10-global-l2-2.0-mobilenetv2]: Best Acc=0.8997
[02/23 19:43:24 cifar10-global-l2-2.0-mobilenetv2]: Params: 1.12 M
[02/23 19:43:24 cifar10-global-l2-2.0-mobilenetv2]: ops: 34.08 M
[02/23 19:43:27 cifar10-global-l2-2.0-mobilenetv2]: Acc: 0.8992 Val Loss: 0.3056

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: mode: prune
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: model: mobilenetv2
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: verbose: False
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: dataset: cifar10
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: dataroot: data
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: batch_size: 128
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: total_epochs: 100
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: lr: 0.01
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: restore: run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: output_dir: run/cifar10/prune/cifar10-global-fpgm-2.0-mobilenetv2
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: finetune: True
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: last_epochs: 100
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: reps: 1
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: method: fpgm
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: speed_up: 2.0
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: reg: 1e-05
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: delta_reg: 0.0001
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: weight_decay: 0.0005
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: seed: 1
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: global_pruning: True
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: sl_total_epochs: 100
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: sl_lr: 0.01
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: sl_reg_warmup: 0
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: sl_restore: None
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: iterative_steps: 400
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: logger: <Logger cifar10-global-fpgm-2.0-mobilenetv2 (DEBUG)>
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: device: cuda
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: num_classes: 10
[02/23 19:43:34 cifar10-global-fpgm-2.0-mobilenetv2]: Loading model from run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/23 19:43:38 cifar10-global-fpgm-2.0-mobilenetv2]: Pruning...
[02/23 19:44:00 cifar10-global-fpgm-2.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 7, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(7, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 45, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(45, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=45)
        (4): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(45, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 46, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(46, 46, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=46)
        (4): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(46, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 86, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(86, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=86)
        (4): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(86, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 46, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(46, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=46)
        (4): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(46, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 30, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(30, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=30)
        (4): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(30, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 89, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=89)
        (4): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(89, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 5, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=5)
        (4): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(5, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=6)
        (4): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(6, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 206, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(206, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(206, 206, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=206)
        (4): BatchNorm2d(206, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(206, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 16, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 5, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=5)
        (4): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(5, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 564, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(564, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(564, 564, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=564)
      (4): BatchNorm2d(564, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(564, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 10, kernel_size=(1, 1), stride=(1, 1))
)
[02/23 19:44:03 cifar10-global-fpgm-2.0-mobilenetv2]: Params: 2.25 M => 0.86 M (38.31%)
[02/23 19:44:03 cifar10-global-fpgm-2.0-mobilenetv2]: FLOPs: 68.29 M => 31.19 M (45.67%, 2.19X )
[02/23 19:44:03 cifar10-global-fpgm-2.0-mobilenetv2]: Acc: 0.8936 => 0.8935
[02/23 19:44:03 cifar10-global-fpgm-2.0-mobilenetv2]: Val Loss: 0.3202 => 0.3202
[02/23 19:44:03 cifar10-global-fpgm-2.0-mobilenetv2]: Finetuning...
[02/23 19:44:31 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 0/100, Acc=0.8566, Val Loss=0.4284, lr=0.0100
[02/23 19:44:59 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 1/100, Acc=0.8397, Val Loss=0.4808, lr=0.0100
[02/23 19:45:28 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 2/100, Acc=0.8372, Val Loss=0.4797, lr=0.0100
[02/23 19:45:56 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 3/100, Acc=0.8574, Val Loss=0.4162, lr=0.0100
[02/23 19:46:25 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 4/100, Acc=0.8564, Val Loss=0.4166, lr=0.0100
[02/23 19:46:53 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 5/100, Acc=0.8424, Val Loss=0.4558, lr=0.0100
[02/23 19:47:21 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 6/100, Acc=0.8538, Val Loss=0.4312, lr=0.0100
[02/23 19:47:50 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 7/100, Acc=0.8404, Val Loss=0.4670, lr=0.0100
[02/23 19:48:19 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 8/100, Acc=0.8600, Val Loss=0.4249, lr=0.0100
[02/23 19:48:47 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 9/100, Acc=0.8516, Val Loss=0.4296, lr=0.0100
[02/23 19:49:16 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 10/100, Acc=0.8464, Val Loss=0.4587, lr=0.0100
[02/23 19:49:44 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 11/100, Acc=0.8572, Val Loss=0.4243, lr=0.0100
[02/23 19:50:12 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 12/100, Acc=0.8551, Val Loss=0.4225, lr=0.0100
[02/23 19:50:41 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 13/100, Acc=0.8388, Val Loss=0.4787, lr=0.0100
[02/23 19:51:10 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 14/100, Acc=0.8389, Val Loss=0.4766, lr=0.0100
[02/23 19:51:39 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 15/100, Acc=0.8245, Val Loss=0.5357, lr=0.0100
[02/23 19:52:08 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 16/100, Acc=0.8146, Val Loss=0.5544, lr=0.0100
[02/23 19:52:36 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 17/100, Acc=0.8572, Val Loss=0.4186, lr=0.0100
[02/23 19:53:05 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 18/100, Acc=0.8494, Val Loss=0.4332, lr=0.0100
[02/23 19:53:34 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 19/100, Acc=0.8410, Val Loss=0.4630, lr=0.0100
[02/23 19:54:03 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 20/100, Acc=0.8474, Val Loss=0.4557, lr=0.0100
[02/23 19:54:31 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 21/100, Acc=0.8552, Val Loss=0.4270, lr=0.0100
[02/23 19:55:00 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 22/100, Acc=0.8568, Val Loss=0.4361, lr=0.0100
[02/23 19:55:29 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 23/100, Acc=0.8566, Val Loss=0.4201, lr=0.0100
[02/23 19:55:57 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 24/100, Acc=0.8583, Val Loss=0.4202, lr=0.0100
[02/23 19:56:26 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 25/100, Acc=0.8552, Val Loss=0.4312, lr=0.0100
[02/23 19:56:54 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 26/100, Acc=0.8553, Val Loss=0.4229, lr=0.0100
[02/23 19:57:22 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 27/100, Acc=0.8500, Val Loss=0.4549, lr=0.0100
[02/23 19:57:50 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 28/100, Acc=0.8548, Val Loss=0.4364, lr=0.0100
[02/23 19:58:19 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 29/100, Acc=0.8633, Val Loss=0.3958, lr=0.0100
[02/23 19:58:47 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 30/100, Acc=0.8512, Val Loss=0.4350, lr=0.0100
[02/23 19:59:16 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 31/100, Acc=0.8478, Val Loss=0.4461, lr=0.0100
[02/23 19:59:44 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 32/100, Acc=0.8250, Val Loss=0.5389, lr=0.0100
[02/23 20:00:12 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 33/100, Acc=0.8593, Val Loss=0.4287, lr=0.0100
[02/23 20:00:41 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 34/100, Acc=0.8554, Val Loss=0.4344, lr=0.0100
[02/23 20:01:10 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 35/100, Acc=0.8452, Val Loss=0.4531, lr=0.0100
[02/23 20:01:39 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 36/100, Acc=0.8519, Val Loss=0.4348, lr=0.0100
[02/23 20:02:08 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 37/100, Acc=0.8514, Val Loss=0.4508, lr=0.0100
[02/23 20:02:37 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 38/100, Acc=0.8663, Val Loss=0.4082, lr=0.0100
[02/23 20:03:06 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 39/100, Acc=0.8606, Val Loss=0.4140, lr=0.0100
[02/23 20:03:35 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 40/100, Acc=0.8643, Val Loss=0.4128, lr=0.0100
[02/23 20:04:03 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 41/100, Acc=0.8595, Val Loss=0.4155, lr=0.0100
[02/23 20:04:32 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 42/100, Acc=0.8415, Val Loss=0.4739, lr=0.0100
[02/23 20:05:01 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 43/100, Acc=0.8473, Val Loss=0.4490, lr=0.0100
[02/23 20:05:30 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 44/100, Acc=0.8600, Val Loss=0.4105, lr=0.0100
[02/23 20:05:58 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 45/100, Acc=0.8409, Val Loss=0.4790, lr=0.0100
[02/23 20:06:27 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 46/100, Acc=0.8516, Val Loss=0.4418, lr=0.0100
[02/23 20:06:53 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 47/100, Acc=0.8574, Val Loss=0.4279, lr=0.0100
[02/23 20:07:20 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 48/100, Acc=0.8317, Val Loss=0.4977, lr=0.0100
[02/23 20:07:47 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 49/100, Acc=0.8517, Val Loss=0.4319, lr=0.0100
[02/23 20:08:14 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 50/100, Acc=0.8636, Val Loss=0.4029, lr=0.0100
[02/23 20:08:41 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 51/100, Acc=0.8601, Val Loss=0.4200, lr=0.0100
[02/23 20:09:08 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 52/100, Acc=0.8587, Val Loss=0.4118, lr=0.0100
[02/23 20:09:35 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 53/100, Acc=0.8599, Val Loss=0.4066, lr=0.0100
[02/23 20:10:02 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 54/100, Acc=0.8529, Val Loss=0.4307, lr=0.0100
[02/23 20:10:30 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 55/100, Acc=0.8420, Val Loss=0.4596, lr=0.0100
[02/23 20:10:57 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 56/100, Acc=0.8456, Val Loss=0.4491, lr=0.0100
[02/23 20:11:24 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 57/100, Acc=0.8513, Val Loss=0.4329, lr=0.0100
[02/23 20:11:52 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 58/100, Acc=0.8625, Val Loss=0.4026, lr=0.0100
[02/23 20:12:19 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 59/100, Acc=0.8577, Val Loss=0.4218, lr=0.0100
[02/23 20:12:46 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 60/100, Acc=0.8899, Val Loss=0.3209, lr=0.0010
[02/23 20:13:13 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 61/100, Acc=0.8932, Val Loss=0.3132, lr=0.0010
[02/23 20:13:41 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 62/100, Acc=0.8958, Val Loss=0.3078, lr=0.0010
[02/23 20:14:08 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 63/100, Acc=0.8965, Val Loss=0.3089, lr=0.0010
[02/23 20:14:36 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 64/100, Acc=0.8950, Val Loss=0.3062, lr=0.0010
[02/23 20:15:03 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 65/100, Acc=0.8988, Val Loss=0.3074, lr=0.0010
[02/23 20:15:31 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 66/100, Acc=0.8974, Val Loss=0.3066, lr=0.0010
[02/23 20:15:58 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 67/100, Acc=0.8987, Val Loss=0.3051, lr=0.0010
[02/23 20:16:26 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 68/100, Acc=0.8993, Val Loss=0.3103, lr=0.0010
[02/23 20:16:53 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 69/100, Acc=0.8970, Val Loss=0.3094, lr=0.0010
[02/23 20:17:21 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 70/100, Acc=0.8992, Val Loss=0.3090, lr=0.0010
[02/23 20:17:49 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 71/100, Acc=0.8986, Val Loss=0.3091, lr=0.0010
[02/23 20:18:16 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 72/100, Acc=0.8987, Val Loss=0.3050, lr=0.0010
[02/23 20:18:44 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 73/100, Acc=0.9002, Val Loss=0.3073, lr=0.0010
[02/23 20:19:12 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 74/100, Acc=0.9008, Val Loss=0.3050, lr=0.0010
[02/23 20:19:40 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 75/100, Acc=0.9000, Val Loss=0.3066, lr=0.0010
[02/23 20:20:07 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 76/100, Acc=0.8987, Val Loss=0.3091, lr=0.0010
[02/23 20:20:35 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 77/100, Acc=0.8995, Val Loss=0.3068, lr=0.0010
[02/23 20:21:03 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 78/100, Acc=0.8985, Val Loss=0.3075, lr=0.0010
[02/23 20:21:30 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 79/100, Acc=0.8975, Val Loss=0.3089, lr=0.0010
[02/23 20:21:58 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 80/100, Acc=0.8998, Val Loss=0.3043, lr=0.0001
[02/23 20:22:25 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 81/100, Acc=0.9006, Val Loss=0.3033, lr=0.0001
[02/23 20:22:53 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 82/100, Acc=0.9021, Val Loss=0.3041, lr=0.0001
[02/23 20:23:21 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 83/100, Acc=0.9003, Val Loss=0.3037, lr=0.0001
[02/23 20:23:48 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 84/100, Acc=0.9018, Val Loss=0.3029, lr=0.0001
[02/23 20:24:15 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 85/100, Acc=0.9004, Val Loss=0.3033, lr=0.0001
[02/23 20:24:43 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 86/100, Acc=0.9011, Val Loss=0.3034, lr=0.0001
[02/23 20:25:10 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 87/100, Acc=0.9002, Val Loss=0.3027, lr=0.0001
[02/23 20:25:38 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 88/100, Acc=0.9019, Val Loss=0.3023, lr=0.0001
[02/23 20:26:05 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 89/100, Acc=0.9011, Val Loss=0.3029, lr=0.0001
[02/23 20:26:32 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 90/100, Acc=0.9001, Val Loss=0.3048, lr=0.0001
[02/23 20:26:59 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 91/100, Acc=0.9021, Val Loss=0.3026, lr=0.0001
[02/23 20:27:26 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 92/100, Acc=0.9025, Val Loss=0.3013, lr=0.0001
[02/23 20:27:54 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 93/100, Acc=0.9009, Val Loss=0.3033, lr=0.0001
[02/23 20:28:21 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 94/100, Acc=0.9020, Val Loss=0.3026, lr=0.0001
[02/23 20:28:48 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 95/100, Acc=0.9012, Val Loss=0.3037, lr=0.0001
[02/23 20:29:15 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 96/100, Acc=0.9024, Val Loss=0.3020, lr=0.0001
[02/23 20:29:42 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 97/100, Acc=0.9019, Val Loss=0.3026, lr=0.0001
[02/23 20:30:09 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 98/100, Acc=0.9009, Val Loss=0.3020, lr=0.0001
[02/23 20:30:36 cifar10-global-fpgm-2.0-mobilenetv2]: Epoch 99/100, Acc=0.9017, Val Loss=0.3023, lr=0.0001
[02/23 20:30:36 cifar10-global-fpgm-2.0-mobilenetv2]: Best Acc=0.9025
[02/23 20:30:36 cifar10-global-fpgm-2.0-mobilenetv2]: Params: 0.86 M
[02/23 20:30:36 cifar10-global-fpgm-2.0-mobilenetv2]: ops: 31.19 M
[02/23 20:30:39 cifar10-global-fpgm-2.0-mobilenetv2]: Acc: 0.9017 Val Loss: 0.3023

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: mode: prune
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: model: mobilenetv2
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: verbose: False
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: dataset: cifar10
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: dataroot: data
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: batch_size: 128
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: total_epochs: 100
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: lr: 0.01
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: restore: run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: output_dir: run/cifar10/prune/cifar10-global-obdc-2.0-mobilenetv2
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: finetune: True
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: last_epochs: 100
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: reps: 1
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: method: obdc
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: speed_up: 2.0
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: reg: 1e-05
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: delta_reg: 0.0001
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: weight_decay: 0.0005
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: seed: 1
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: global_pruning: True
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: sl_total_epochs: 100
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: sl_lr: 0.01
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: sl_reg_warmup: 0
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: sl_restore: None
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: iterative_steps: 400
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: logger: <Logger cifar10-global-obdc-2.0-mobilenetv2 (DEBUG)>
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: device: cuda
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: num_classes: 10
[02/23 20:30:46 cifar10-global-obdc-2.0-mobilenetv2]: Loading model from run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/23 20:30:50 cifar10-global-obdc-2.0-mobilenetv2]: Pruning...
Traceback (most recent call last):
  File "/home/REDACTED/paper1/main.py", line 429, in <module>
    main()
  File "/home/REDACTED/paper1/main.py", line 369, in main
    progressive_pruning(
  File "/home/REDACTED/paper1/main.py", line 65, in progressive_pruning
    imp._prepare_model(model, pruner)
  File "/home/REDACTED/paper1/torch_pruning/pruner/importance.py", line 872, in _prepare_model
    for i, (dep, idxs) in enumerate(group):
TypeError: 'NoneType' object is not iterable
/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: mode: prune
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: model: mobilenetv2
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: verbose: False
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: dataset: cifar10
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: dataroot: data
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: batch_size: 128
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: total_epochs: 100
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: lr: 0.01
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: restore: run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: output_dir: run/cifar10/prune/cifar10-global-lamp-2.0-mobilenetv2
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: finetune: True
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: last_epochs: 100
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: reps: 1
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: method: lamp
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: speed_up: 2.0
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: reg: 1e-05
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: delta_reg: 0.0001
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: weight_decay: 0.0005
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: seed: 1
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: global_pruning: True
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: sl_total_epochs: 100
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: sl_lr: 0.01
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: sl_reg_warmup: 0
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: sl_restore: None
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: iterative_steps: 400
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: logger: <Logger cifar10-global-lamp-2.0-mobilenetv2 (DEBUG)>
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: device: cuda
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: num_classes: 10
[02/23 20:30:59 cifar10-global-lamp-2.0-mobilenetv2]: Loading model from run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/23 20:31:04 cifar10-global-lamp-2.0-mobilenetv2]: Pruning...
[02/23 20:31:36 cifar10-global-lamp-2.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 8, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(8, 17, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(17, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=17)
      (4): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(17, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 73, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(73, 73, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=73)
        (4): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(73, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 86, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(86, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=86)
        (4): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(86, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 62, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(62, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=62)
        (4): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(62, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 88, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88)
        (4): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(88, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 55, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=55)
        (4): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(55, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 51, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(51, 51, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=51)
        (4): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(51, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 99, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(99, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(99, 99, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=99)
        (4): BatchNorm2d(99, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(99, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 61, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=61)
        (4): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(61, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 40, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40)
        (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(40, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 97, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(97, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=97)
        (4): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(97, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 568, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(568, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(568, 568, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=568)
        (4): BatchNorm2d(568, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(568, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 231, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(231, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(231, 231, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=231)
        (4): BatchNorm2d(231, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(231, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 16, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 5, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=5)
        (4): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(5, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 615, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(615, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(615, 615, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=615)
      (4): BatchNorm2d(615, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(615, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 10, kernel_size=(1, 1), stride=(1, 1))
)
[02/23 20:31:39 cifar10-global-lamp-2.0-mobilenetv2]: Params: 2.25 M => 1.12 M (49.63%)
[02/23 20:31:39 cifar10-global-lamp-2.0-mobilenetv2]: FLOPs: 68.29 M => 34.00 M (49.78%, 2.01X )
[02/23 20:31:39 cifar10-global-lamp-2.0-mobilenetv2]: Acc: 0.8936 => 0.8936
[02/23 20:31:39 cifar10-global-lamp-2.0-mobilenetv2]: Val Loss: 0.3202 => 0.3202
[02/23 20:31:39 cifar10-global-lamp-2.0-mobilenetv2]: Finetuning...
[02/23 20:32:06 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 0/100, Acc=0.8529, Val Loss=0.4331, lr=0.0100
[02/23 20:32:33 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 1/100, Acc=0.8522, Val Loss=0.4362, lr=0.0100
[02/23 20:33:00 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 2/100, Acc=0.8429, Val Loss=0.4692, lr=0.0100
[02/23 20:33:28 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 3/100, Acc=0.8597, Val Loss=0.4127, lr=0.0100
[02/23 20:33:55 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 4/100, Acc=0.8600, Val Loss=0.4115, lr=0.0100
[02/23 20:34:22 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 5/100, Acc=0.8452, Val Loss=0.4555, lr=0.0100
[02/23 20:34:50 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 6/100, Acc=0.8477, Val Loss=0.4438, lr=0.0100
[02/23 20:35:17 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 7/100, Acc=0.8433, Val Loss=0.4604, lr=0.0100
[02/23 20:35:45 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 8/100, Acc=0.8611, Val Loss=0.4190, lr=0.0100
[02/23 20:36:13 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 9/100, Acc=0.8491, Val Loss=0.4560, lr=0.0100
[02/23 20:36:40 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 10/100, Acc=0.8553, Val Loss=0.4331, lr=0.0100
[02/23 20:37:09 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 11/100, Acc=0.8502, Val Loss=0.4409, lr=0.0100
[02/23 20:37:36 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 12/100, Acc=0.8603, Val Loss=0.4098, lr=0.0100
[02/23 20:38:04 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 13/100, Acc=0.8518, Val Loss=0.4318, lr=0.0100
[02/23 20:38:32 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 14/100, Acc=0.8334, Val Loss=0.4820, lr=0.0100
[02/23 20:39:00 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 15/100, Acc=0.8508, Val Loss=0.4347, lr=0.0100
[02/23 20:39:28 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 16/100, Acc=0.8510, Val Loss=0.4372, lr=0.0100
[02/23 20:39:56 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 17/100, Acc=0.8565, Val Loss=0.4276, lr=0.0100
[02/23 20:40:24 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 18/100, Acc=0.8562, Val Loss=0.4331, lr=0.0100
[02/23 20:40:52 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 19/100, Acc=0.8434, Val Loss=0.4532, lr=0.0100
[02/23 20:41:20 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 20/100, Acc=0.8309, Val Loss=0.5055, lr=0.0100
[02/23 20:41:48 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 21/100, Acc=0.8582, Val Loss=0.4137, lr=0.0100
[02/23 20:42:16 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 22/100, Acc=0.8605, Val Loss=0.4024, lr=0.0100
[02/23 20:42:45 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 23/100, Acc=0.8570, Val Loss=0.4264, lr=0.0100
[02/23 20:43:13 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 24/100, Acc=0.8628, Val Loss=0.3996, lr=0.0100
[02/23 20:43:41 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 25/100, Acc=0.8675, Val Loss=0.3975, lr=0.0100
[02/23 20:44:09 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 26/100, Acc=0.8479, Val Loss=0.4465, lr=0.0100
[02/23 20:44:38 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 27/100, Acc=0.8574, Val Loss=0.4242, lr=0.0100
[02/23 20:45:06 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 28/100, Acc=0.8535, Val Loss=0.4369, lr=0.0100
[02/23 20:45:34 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 29/100, Acc=0.8464, Val Loss=0.4550, lr=0.0100
[02/23 20:46:02 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 30/100, Acc=0.8556, Val Loss=0.4149, lr=0.0100
[02/23 20:46:29 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 31/100, Acc=0.8489, Val Loss=0.4482, lr=0.0100
[02/23 20:46:57 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 32/100, Acc=0.8406, Val Loss=0.4617, lr=0.0100
[02/23 20:47:26 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 33/100, Acc=0.8580, Val Loss=0.4167, lr=0.0100
[02/23 20:47:54 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 34/100, Acc=0.8573, Val Loss=0.4197, lr=0.0100
[02/23 20:48:22 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 35/100, Acc=0.8592, Val Loss=0.4129, lr=0.0100
[02/23 20:48:50 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 36/100, Acc=0.8419, Val Loss=0.4681, lr=0.0100
[02/23 20:49:18 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 37/100, Acc=0.8579, Val Loss=0.4165, lr=0.0100
[02/23 20:49:46 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 38/100, Acc=0.8613, Val Loss=0.4118, lr=0.0100
[02/23 20:50:15 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 39/100, Acc=0.8561, Val Loss=0.4180, lr=0.0100
[02/23 20:50:42 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 40/100, Acc=0.8530, Val Loss=0.4394, lr=0.0100
[02/23 20:51:10 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 41/100, Acc=0.8500, Val Loss=0.4504, lr=0.0100
[02/23 20:51:37 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 42/100, Acc=0.8619, Val Loss=0.4126, lr=0.0100
[02/23 20:52:05 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 43/100, Acc=0.8470, Val Loss=0.4581, lr=0.0100
[02/23 20:52:32 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 44/100, Acc=0.8475, Val Loss=0.4464, lr=0.0100
[02/23 20:53:00 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 45/100, Acc=0.8627, Val Loss=0.4075, lr=0.0100
[02/23 20:53:28 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 46/100, Acc=0.8595, Val Loss=0.4139, lr=0.0100
[02/23 20:53:55 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 47/100, Acc=0.8575, Val Loss=0.4184, lr=0.0100
[02/23 20:54:23 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 48/100, Acc=0.8412, Val Loss=0.4784, lr=0.0100
[02/23 20:54:51 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 49/100, Acc=0.8549, Val Loss=0.4401, lr=0.0100
[02/23 20:55:18 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 50/100, Acc=0.8597, Val Loss=0.4228, lr=0.0100
[02/23 20:55:46 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 51/100, Acc=0.8596, Val Loss=0.4188, lr=0.0100
[02/23 20:56:14 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 52/100, Acc=0.8660, Val Loss=0.3970, lr=0.0100
[02/23 20:56:41 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 53/100, Acc=0.8588, Val Loss=0.4242, lr=0.0100
[02/23 20:57:09 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 54/100, Acc=0.8511, Val Loss=0.4415, lr=0.0100
[02/23 20:57:37 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 55/100, Acc=0.8618, Val Loss=0.4153, lr=0.0100
[02/23 20:58:05 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 56/100, Acc=0.8658, Val Loss=0.4043, lr=0.0100
[02/23 20:58:32 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 57/100, Acc=0.8663, Val Loss=0.3967, lr=0.0100
[02/23 20:59:00 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 58/100, Acc=0.8587, Val Loss=0.4191, lr=0.0100
[02/23 20:59:28 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 59/100, Acc=0.8446, Val Loss=0.4529, lr=0.0100
[02/23 20:59:56 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 60/100, Acc=0.8902, Val Loss=0.3215, lr=0.0010
[02/23 21:00:24 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 61/100, Acc=0.8956, Val Loss=0.3113, lr=0.0010
[02/23 21:00:52 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 62/100, Acc=0.8955, Val Loss=0.3115, lr=0.0010
[02/23 21:01:20 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 63/100, Acc=0.8965, Val Loss=0.3099, lr=0.0010
[02/23 21:01:48 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 64/100, Acc=0.8981, Val Loss=0.3067, lr=0.0010
[02/23 21:02:17 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 65/100, Acc=0.8978, Val Loss=0.3092, lr=0.0010
[02/23 21:02:45 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 66/100, Acc=0.9003, Val Loss=0.3054, lr=0.0010
[02/23 21:03:13 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 67/100, Acc=0.8962, Val Loss=0.3061, lr=0.0010
[02/23 21:03:41 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 68/100, Acc=0.9001, Val Loss=0.3063, lr=0.0010
[02/23 21:04:09 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 69/100, Acc=0.8976, Val Loss=0.3066, lr=0.0010
[02/23 21:04:38 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 70/100, Acc=0.8971, Val Loss=0.3090, lr=0.0010
[02/23 21:05:06 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 71/100, Acc=0.8974, Val Loss=0.3074, lr=0.0010
[02/23 21:05:33 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 72/100, Acc=0.8992, Val Loss=0.3039, lr=0.0010
[02/23 21:06:01 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 73/100, Acc=0.8987, Val Loss=0.3044, lr=0.0010
[02/23 21:06:29 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 74/100, Acc=0.9012, Val Loss=0.3031, lr=0.0010
[02/23 21:06:57 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 75/100, Acc=0.8996, Val Loss=0.3065, lr=0.0010
[02/23 21:07:25 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 76/100, Acc=0.8976, Val Loss=0.3104, lr=0.0010
[02/23 21:07:52 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 77/100, Acc=0.8999, Val Loss=0.3096, lr=0.0010
[02/23 21:08:20 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 78/100, Acc=0.8980, Val Loss=0.3100, lr=0.0010
[02/23 21:08:48 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 79/100, Acc=0.8983, Val Loss=0.3116, lr=0.0010
[02/23 21:09:16 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 80/100, Acc=0.9005, Val Loss=0.3054, lr=0.0001
[02/23 21:09:44 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 81/100, Acc=0.8983, Val Loss=0.3051, lr=0.0001
[02/23 21:10:12 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 82/100, Acc=0.9001, Val Loss=0.3063, lr=0.0001
[02/23 21:10:40 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 83/100, Acc=0.8993, Val Loss=0.3049, lr=0.0001
[02/23 21:11:08 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 84/100, Acc=0.8999, Val Loss=0.3043, lr=0.0001
[02/23 21:11:36 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 85/100, Acc=0.8992, Val Loss=0.3052, lr=0.0001
[02/23 21:12:04 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 86/100, Acc=0.9005, Val Loss=0.3046, lr=0.0001
[02/23 21:12:33 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 87/100, Acc=0.8989, Val Loss=0.3022, lr=0.0001
[02/23 21:13:01 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 88/100, Acc=0.9006, Val Loss=0.3026, lr=0.0001
[02/23 21:13:30 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 89/100, Acc=0.9005, Val Loss=0.3029, lr=0.0001
[02/23 21:13:58 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 90/100, Acc=0.8995, Val Loss=0.3039, lr=0.0001
[02/23 21:14:26 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 91/100, Acc=0.8994, Val Loss=0.3027, lr=0.0001
[02/23 21:14:55 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 92/100, Acc=0.8999, Val Loss=0.3006, lr=0.0001
[02/23 21:15:23 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 93/100, Acc=0.9009, Val Loss=0.3028, lr=0.0001
[02/23 21:15:52 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 94/100, Acc=0.9018, Val Loss=0.3021, lr=0.0001
[02/23 21:16:20 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 95/100, Acc=0.9019, Val Loss=0.3025, lr=0.0001
[02/23 21:16:48 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 96/100, Acc=0.9009, Val Loss=0.3022, lr=0.0001
[02/23 21:17:17 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 97/100, Acc=0.9009, Val Loss=0.3018, lr=0.0001
[02/23 21:17:45 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 98/100, Acc=0.9004, Val Loss=0.3017, lr=0.0001
[02/23 21:18:13 cifar10-global-lamp-2.0-mobilenetv2]: Epoch 99/100, Acc=0.9010, Val Loss=0.3016, lr=0.0001
[02/23 21:18:13 cifar10-global-lamp-2.0-mobilenetv2]: Best Acc=0.9019
[02/23 21:18:13 cifar10-global-lamp-2.0-mobilenetv2]: Params: 1.12 M
[02/23 21:18:13 cifar10-global-lamp-2.0-mobilenetv2]: ops: 34.00 M
[02/23 21:18:16 cifar10-global-lamp-2.0-mobilenetv2]: Acc: 0.9010 Val Loss: 0.3016

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: mode: prune
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: model: mobilenetv2
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: verbose: False
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: dataset: cifar10
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: dataroot: data
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: batch_size: 128
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: total_epochs: 100
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: lr: 0.01
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: restore: run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: output_dir: run/cifar10/prune/cifar10-global-slim-2.0-mobilenetv2
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: finetune: True
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: last_epochs: 100
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: reps: 1
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: method: slim
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: speed_up: 2.0
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: reg: 1e-05
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: delta_reg: 0.0001
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: weight_decay: 0.0005
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: seed: 1
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: global_pruning: True
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: sl_total_epochs: 100
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: sl_lr: 0.01
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: sl_reg_warmup: 0
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: sl_restore: None
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: iterative_steps: 400
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: logger: <Logger cifar10-global-slim-2.0-mobilenetv2 (DEBUG)>
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: device: cuda
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: num_classes: 10
[02/23 21:18:24 cifar10-global-slim-2.0-mobilenetv2]: Loading model from run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/23 21:18:25 cifar10-global-slim-2.0-mobilenetv2]: Regularizing...
[02/23 21:19:06 cifar10-global-slim-2.0-mobilenetv2]: Epoch 0/100, Acc=0.8527, Val Loss=0.4365, lr=0.0100
[02/23 21:19:33 cifar10-global-slim-2.0-mobilenetv2]: Epoch 1/100, Acc=0.8519, Val Loss=0.4381, lr=0.0100
[02/23 21:20:01 cifar10-global-slim-2.0-mobilenetv2]: Epoch 2/100, Acc=0.8566, Val Loss=0.4242, lr=0.0100
[02/23 21:20:29 cifar10-global-slim-2.0-mobilenetv2]: Epoch 3/100, Acc=0.8474, Val Loss=0.4567, lr=0.0100
[02/23 21:20:57 cifar10-global-slim-2.0-mobilenetv2]: Epoch 4/100, Acc=0.8531, Val Loss=0.4475, lr=0.0100
[02/23 21:21:25 cifar10-global-slim-2.0-mobilenetv2]: Epoch 5/100, Acc=0.8559, Val Loss=0.4162, lr=0.0100
[02/23 21:21:53 cifar10-global-slim-2.0-mobilenetv2]: Epoch 6/100, Acc=0.8622, Val Loss=0.4001, lr=0.0100
[02/23 21:22:21 cifar10-global-slim-2.0-mobilenetv2]: Epoch 7/100, Acc=0.8618, Val Loss=0.4110, lr=0.0100
[02/23 21:22:49 cifar10-global-slim-2.0-mobilenetv2]: Epoch 8/100, Acc=0.8661, Val Loss=0.4083, lr=0.0100
[02/23 21:23:17 cifar10-global-slim-2.0-mobilenetv2]: Epoch 9/100, Acc=0.8614, Val Loss=0.4175, lr=0.0100
[02/23 21:23:45 cifar10-global-slim-2.0-mobilenetv2]: Epoch 10/100, Acc=0.8582, Val Loss=0.4323, lr=0.0100
[02/23 21:24:13 cifar10-global-slim-2.0-mobilenetv2]: Epoch 11/100, Acc=0.8730, Val Loss=0.3771, lr=0.0100
[02/23 21:24:41 cifar10-global-slim-2.0-mobilenetv2]: Epoch 12/100, Acc=0.8664, Val Loss=0.3991, lr=0.0100
[02/23 21:25:09 cifar10-global-slim-2.0-mobilenetv2]: Epoch 13/100, Acc=0.8653, Val Loss=0.3996, lr=0.0100
[02/23 21:25:37 cifar10-global-slim-2.0-mobilenetv2]: Epoch 14/100, Acc=0.8723, Val Loss=0.3860, lr=0.0100
[02/23 21:26:05 cifar10-global-slim-2.0-mobilenetv2]: Epoch 15/100, Acc=0.8677, Val Loss=0.4009, lr=0.0100
[02/23 21:26:33 cifar10-global-slim-2.0-mobilenetv2]: Epoch 16/100, Acc=0.8692, Val Loss=0.4023, lr=0.0100
[02/23 21:27:01 cifar10-global-slim-2.0-mobilenetv2]: Epoch 17/100, Acc=0.8690, Val Loss=0.4024, lr=0.0100
[02/23 21:27:29 cifar10-global-slim-2.0-mobilenetv2]: Epoch 18/100, Acc=0.8763, Val Loss=0.3730, lr=0.0100
[02/23 21:27:57 cifar10-global-slim-2.0-mobilenetv2]: Epoch 19/100, Acc=0.8773, Val Loss=0.3710, lr=0.0100
[02/23 21:28:26 cifar10-global-slim-2.0-mobilenetv2]: Epoch 20/100, Acc=0.8785, Val Loss=0.3617, lr=0.0100
[02/23 21:28:53 cifar10-global-slim-2.0-mobilenetv2]: Epoch 21/100, Acc=0.8630, Val Loss=0.4226, lr=0.0100
[02/23 21:29:21 cifar10-global-slim-2.0-mobilenetv2]: Epoch 22/100, Acc=0.8710, Val Loss=0.3946, lr=0.0100
[02/23 21:29:49 cifar10-global-slim-2.0-mobilenetv2]: Epoch 23/100, Acc=0.8702, Val Loss=0.3888, lr=0.0100
[02/23 21:30:17 cifar10-global-slim-2.0-mobilenetv2]: Epoch 24/100, Acc=0.8723, Val Loss=0.3808, lr=0.0100
[02/23 21:30:45 cifar10-global-slim-2.0-mobilenetv2]: Epoch 25/100, Acc=0.8783, Val Loss=0.3640, lr=0.0100
[02/23 21:31:12 cifar10-global-slim-2.0-mobilenetv2]: Epoch 26/100, Acc=0.8678, Val Loss=0.3925, lr=0.0100
[02/23 21:31:40 cifar10-global-slim-2.0-mobilenetv2]: Epoch 27/100, Acc=0.8726, Val Loss=0.3839, lr=0.0100
[02/23 21:32:09 cifar10-global-slim-2.0-mobilenetv2]: Epoch 28/100, Acc=0.8671, Val Loss=0.4030, lr=0.0100
[02/23 21:32:37 cifar10-global-slim-2.0-mobilenetv2]: Epoch 29/100, Acc=0.8736, Val Loss=0.3932, lr=0.0100
[02/23 21:33:05 cifar10-global-slim-2.0-mobilenetv2]: Epoch 30/100, Acc=0.8783, Val Loss=0.3782, lr=0.0100
[02/23 21:33:33 cifar10-global-slim-2.0-mobilenetv2]: Epoch 31/100, Acc=0.8718, Val Loss=0.3969, lr=0.0100
[02/23 21:34:02 cifar10-global-slim-2.0-mobilenetv2]: Epoch 32/100, Acc=0.8775, Val Loss=0.3677, lr=0.0100
[02/23 21:34:30 cifar10-global-slim-2.0-mobilenetv2]: Epoch 33/100, Acc=0.8780, Val Loss=0.3742, lr=0.0100
[02/23 21:34:58 cifar10-global-slim-2.0-mobilenetv2]: Epoch 34/100, Acc=0.8708, Val Loss=0.3958, lr=0.0100
[02/23 21:35:27 cifar10-global-slim-2.0-mobilenetv2]: Epoch 35/100, Acc=0.8733, Val Loss=0.3850, lr=0.0100
[02/23 21:35:55 cifar10-global-slim-2.0-mobilenetv2]: Epoch 36/100, Acc=0.8800, Val Loss=0.3652, lr=0.0100
[02/23 21:36:23 cifar10-global-slim-2.0-mobilenetv2]: Epoch 37/100, Acc=0.8761, Val Loss=0.3855, lr=0.0100
[02/23 21:36:51 cifar10-global-slim-2.0-mobilenetv2]: Epoch 38/100, Acc=0.8844, Val Loss=0.3594, lr=0.0100
[02/23 21:37:19 cifar10-global-slim-2.0-mobilenetv2]: Epoch 39/100, Acc=0.8750, Val Loss=0.3829, lr=0.0100
[02/23 21:37:47 cifar10-global-slim-2.0-mobilenetv2]: Epoch 40/100, Acc=0.8797, Val Loss=0.3682, lr=0.0100
[02/23 21:38:14 cifar10-global-slim-2.0-mobilenetv2]: Epoch 41/100, Acc=0.8803, Val Loss=0.3725, lr=0.0100
[02/23 21:38:42 cifar10-global-slim-2.0-mobilenetv2]: Epoch 42/100, Acc=0.8800, Val Loss=0.3701, lr=0.0100
[02/23 21:39:10 cifar10-global-slim-2.0-mobilenetv2]: Epoch 43/100, Acc=0.8770, Val Loss=0.3772, lr=0.0100
[02/23 21:39:38 cifar10-global-slim-2.0-mobilenetv2]: Epoch 44/100, Acc=0.8814, Val Loss=0.3582, lr=0.0100
[02/23 21:40:05 cifar10-global-slim-2.0-mobilenetv2]: Epoch 45/100, Acc=0.8829, Val Loss=0.3526, lr=0.0100
[02/23 21:40:33 cifar10-global-slim-2.0-mobilenetv2]: Epoch 46/100, Acc=0.8793, Val Loss=0.3608, lr=0.0100
[02/23 21:41:01 cifar10-global-slim-2.0-mobilenetv2]: Epoch 47/100, Acc=0.8777, Val Loss=0.3704, lr=0.0100
[02/23 21:41:29 cifar10-global-slim-2.0-mobilenetv2]: Epoch 48/100, Acc=0.8794, Val Loss=0.3729, lr=0.0100
[02/23 21:41:57 cifar10-global-slim-2.0-mobilenetv2]: Epoch 49/100, Acc=0.8835, Val Loss=0.3612, lr=0.0100
[02/23 21:42:24 cifar10-global-slim-2.0-mobilenetv2]: Epoch 50/100, Acc=0.8842, Val Loss=0.3495, lr=0.0100
[02/23 21:42:52 cifar10-global-slim-2.0-mobilenetv2]: Epoch 51/100, Acc=0.8833, Val Loss=0.3626, lr=0.0100
[02/23 21:43:20 cifar10-global-slim-2.0-mobilenetv2]: Epoch 52/100, Acc=0.8755, Val Loss=0.3842, lr=0.0100
[02/23 21:43:49 cifar10-global-slim-2.0-mobilenetv2]: Epoch 53/100, Acc=0.8807, Val Loss=0.3738, lr=0.0100
[02/23 21:44:17 cifar10-global-slim-2.0-mobilenetv2]: Epoch 54/100, Acc=0.8764, Val Loss=0.3744, lr=0.0100
[02/23 21:44:45 cifar10-global-slim-2.0-mobilenetv2]: Epoch 55/100, Acc=0.8779, Val Loss=0.3677, lr=0.0100
[02/23 21:45:14 cifar10-global-slim-2.0-mobilenetv2]: Epoch 56/100, Acc=0.8854, Val Loss=0.3527, lr=0.0100
[02/23 21:45:42 cifar10-global-slim-2.0-mobilenetv2]: Epoch 57/100, Acc=0.8849, Val Loss=0.3526, lr=0.0100
[02/23 21:46:11 cifar10-global-slim-2.0-mobilenetv2]: Epoch 58/100, Acc=0.8808, Val Loss=0.3663, lr=0.0100
[02/23 21:46:39 cifar10-global-slim-2.0-mobilenetv2]: Epoch 59/100, Acc=0.8839, Val Loss=0.3499, lr=0.0100
[02/23 21:47:07 cifar10-global-slim-2.0-mobilenetv2]: Epoch 60/100, Acc=0.8936, Val Loss=0.3179, lr=0.0010
[02/23 21:47:36 cifar10-global-slim-2.0-mobilenetv2]: Epoch 61/100, Acc=0.8975, Val Loss=0.3131, lr=0.0010
[02/23 21:48:05 cifar10-global-slim-2.0-mobilenetv2]: Epoch 62/100, Acc=0.8979, Val Loss=0.3124, lr=0.0010
[02/23 21:48:33 cifar10-global-slim-2.0-mobilenetv2]: Epoch 63/100, Acc=0.8984, Val Loss=0.3172, lr=0.0010
[02/23 21:49:01 cifar10-global-slim-2.0-mobilenetv2]: Epoch 64/100, Acc=0.8979, Val Loss=0.3156, lr=0.0010
[02/23 21:49:29 cifar10-global-slim-2.0-mobilenetv2]: Epoch 65/100, Acc=0.8985, Val Loss=0.3161, lr=0.0010
[02/23 21:49:57 cifar10-global-slim-2.0-mobilenetv2]: Epoch 66/100, Acc=0.8995, Val Loss=0.3138, lr=0.0010
[02/23 21:50:26 cifar10-global-slim-2.0-mobilenetv2]: Epoch 67/100, Acc=0.8989, Val Loss=0.3119, lr=0.0010
[02/23 21:50:54 cifar10-global-slim-2.0-mobilenetv2]: Epoch 68/100, Acc=0.8996, Val Loss=0.3147, lr=0.0010
[02/23 21:51:22 cifar10-global-slim-2.0-mobilenetv2]: Epoch 69/100, Acc=0.8982, Val Loss=0.3177, lr=0.0010
[02/23 21:51:50 cifar10-global-slim-2.0-mobilenetv2]: Epoch 70/100, Acc=0.8989, Val Loss=0.3138, lr=0.0010
[02/23 21:52:18 cifar10-global-slim-2.0-mobilenetv2]: Epoch 71/100, Acc=0.8981, Val Loss=0.3166, lr=0.0010
[02/23 21:52:46 cifar10-global-slim-2.0-mobilenetv2]: Epoch 72/100, Acc=0.8989, Val Loss=0.3167, lr=0.0010
[02/23 21:53:14 cifar10-global-slim-2.0-mobilenetv2]: Epoch 73/100, Acc=0.8991, Val Loss=0.3183, lr=0.0010
[02/23 21:53:42 cifar10-global-slim-2.0-mobilenetv2]: Epoch 74/100, Acc=0.8988, Val Loss=0.3169, lr=0.0010
[02/23 21:54:10 cifar10-global-slim-2.0-mobilenetv2]: Epoch 75/100, Acc=0.8992, Val Loss=0.3190, lr=0.0010
[02/23 21:54:37 cifar10-global-slim-2.0-mobilenetv2]: Epoch 76/100, Acc=0.8991, Val Loss=0.3197, lr=0.0010
[02/23 21:55:06 cifar10-global-slim-2.0-mobilenetv2]: Epoch 77/100, Acc=0.8981, Val Loss=0.3215, lr=0.0010
[02/23 21:55:34 cifar10-global-slim-2.0-mobilenetv2]: Epoch 78/100, Acc=0.8993, Val Loss=0.3220, lr=0.0010
[02/23 21:56:02 cifar10-global-slim-2.0-mobilenetv2]: Epoch 79/100, Acc=0.8989, Val Loss=0.3203, lr=0.0010
[02/23 21:56:30 cifar10-global-slim-2.0-mobilenetv2]: Epoch 80/100, Acc=0.8995, Val Loss=0.3176, lr=0.0001
[02/23 21:56:58 cifar10-global-slim-2.0-mobilenetv2]: Epoch 81/100, Acc=0.8979, Val Loss=0.3200, lr=0.0001
[02/23 21:57:26 cifar10-global-slim-2.0-mobilenetv2]: Epoch 82/100, Acc=0.8989, Val Loss=0.3181, lr=0.0001
[02/23 21:57:53 cifar10-global-slim-2.0-mobilenetv2]: Epoch 83/100, Acc=0.8995, Val Loss=0.3199, lr=0.0001
[02/23 21:58:21 cifar10-global-slim-2.0-mobilenetv2]: Epoch 84/100, Acc=0.8990, Val Loss=0.3191, lr=0.0001
[02/23 21:58:49 cifar10-global-slim-2.0-mobilenetv2]: Epoch 85/100, Acc=0.8988, Val Loss=0.3187, lr=0.0001
[02/23 21:59:18 cifar10-global-slim-2.0-mobilenetv2]: Epoch 86/100, Acc=0.8999, Val Loss=0.3201, lr=0.0001
[02/23 21:59:45 cifar10-global-slim-2.0-mobilenetv2]: Epoch 87/100, Acc=0.8994, Val Loss=0.3191, lr=0.0001
[02/23 22:00:14 cifar10-global-slim-2.0-mobilenetv2]: Epoch 88/100, Acc=0.8983, Val Loss=0.3188, lr=0.0001
[02/23 22:00:42 cifar10-global-slim-2.0-mobilenetv2]: Epoch 89/100, Acc=0.8977, Val Loss=0.3190, lr=0.0001
[02/23 22:01:10 cifar10-global-slim-2.0-mobilenetv2]: Epoch 90/100, Acc=0.9005, Val Loss=0.3179, lr=0.0001
[02/23 22:01:39 cifar10-global-slim-2.0-mobilenetv2]: Epoch 91/100, Acc=0.8985, Val Loss=0.3182, lr=0.0001
[02/23 22:02:07 cifar10-global-slim-2.0-mobilenetv2]: Epoch 92/100, Acc=0.8986, Val Loss=0.3195, lr=0.0001
[02/23 22:02:35 cifar10-global-slim-2.0-mobilenetv2]: Epoch 93/100, Acc=0.8983, Val Loss=0.3179, lr=0.0001
[02/23 22:03:04 cifar10-global-slim-2.0-mobilenetv2]: Epoch 94/100, Acc=0.8985, Val Loss=0.3185, lr=0.0001
[02/23 22:03:32 cifar10-global-slim-2.0-mobilenetv2]: Epoch 95/100, Acc=0.8985, Val Loss=0.3199, lr=0.0001
[02/23 22:04:00 cifar10-global-slim-2.0-mobilenetv2]: Epoch 96/100, Acc=0.8990, Val Loss=0.3192, lr=0.0001
[02/23 22:04:29 cifar10-global-slim-2.0-mobilenetv2]: Epoch 97/100, Acc=0.8994, Val Loss=0.3187, lr=0.0001
[02/23 22:04:57 cifar10-global-slim-2.0-mobilenetv2]: Epoch 98/100, Acc=0.8992, Val Loss=0.3194, lr=0.0001
[02/23 22:05:25 cifar10-global-slim-2.0-mobilenetv2]: Epoch 99/100, Acc=0.8989, Val Loss=0.3196, lr=0.0001
[02/23 22:05:25 cifar10-global-slim-2.0-mobilenetv2]: Best Acc=0.9005
[02/23 22:05:25 cifar10-global-slim-2.0-mobilenetv2]: Loading the sparse model from run/cifar10/prune/cifar10-global-slim-2.0-mobilenetv2/reg_cifar10_mobilenetv2_slim_1e-05.pth...
[02/23 22:05:28 cifar10-global-slim-2.0-mobilenetv2]: Pruning...
[02/23 22:05:47 cifar10-global-slim-2.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 7, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(7, 17, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(17, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=17)
      (4): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(17, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 69, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(69, 69, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=69)
        (4): BatchNorm2d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(69, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 74, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(74, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(74, 74, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=74)
        (4): BatchNorm2d(74, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(74, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 50, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(50, 50, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=50)
        (4): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(50, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 69, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(69, 69, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=69)
        (4): BatchNorm2d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(69, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 40, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40)
        (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(40, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 51, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(51, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=51)
        (4): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(51, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 31, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=31)
        (4): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(31, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 12, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=12)
        (4): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(12, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 73, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(73, 73, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=73)
        (4): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(73, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 144, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 7, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=7)
        (4): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(7, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 525, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(525, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(525, 525, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=525)
        (4): BatchNorm2d(525, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(525, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 418, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(418, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(418, 418, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=418)
      (4): BatchNorm2d(418, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(418, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 10, kernel_size=(1, 1), stride=(1, 1))
)
[02/23 22:05:50 cifar10-global-slim-2.0-mobilenetv2]: Params: 2.25 M => 1.14 M (50.78%)
[02/23 22:05:50 cifar10-global-slim-2.0-mobilenetv2]: FLOPs: 68.29 M => 33.95 M (49.72%, 2.01X )
[02/23 22:05:50 cifar10-global-slim-2.0-mobilenetv2]: Acc: 0.9005 => 0.9004
[02/23 22:05:50 cifar10-global-slim-2.0-mobilenetv2]: Val Loss: 0.3179 => 0.3179
[02/23 22:05:50 cifar10-global-slim-2.0-mobilenetv2]: Finetuning...
[02/23 22:06:18 cifar10-global-slim-2.0-mobilenetv2]: Epoch 0/100, Acc=0.8863, Val Loss=0.3445, lr=0.0100
[02/23 22:06:46 cifar10-global-slim-2.0-mobilenetv2]: Epoch 1/100, Acc=0.8848, Val Loss=0.3530, lr=0.0100
[02/23 22:07:15 cifar10-global-slim-2.0-mobilenetv2]: Epoch 2/100, Acc=0.8781, Val Loss=0.3626, lr=0.0100
[02/23 22:07:43 cifar10-global-slim-2.0-mobilenetv2]: Epoch 3/100, Acc=0.8747, Val Loss=0.3751, lr=0.0100
[02/23 22:08:11 cifar10-global-slim-2.0-mobilenetv2]: Epoch 4/100, Acc=0.8808, Val Loss=0.3607, lr=0.0100
[02/23 22:08:39 cifar10-global-slim-2.0-mobilenetv2]: Epoch 5/100, Acc=0.8822, Val Loss=0.3589, lr=0.0100
[02/23 22:09:07 cifar10-global-slim-2.0-mobilenetv2]: Epoch 6/100, Acc=0.8839, Val Loss=0.3534, lr=0.0100
[02/23 22:09:35 cifar10-global-slim-2.0-mobilenetv2]: Epoch 7/100, Acc=0.8781, Val Loss=0.3871, lr=0.0100
[02/23 22:10:03 cifar10-global-slim-2.0-mobilenetv2]: Epoch 8/100, Acc=0.8824, Val Loss=0.3565, lr=0.0100
[02/23 22:10:31 cifar10-global-slim-2.0-mobilenetv2]: Epoch 9/100, Acc=0.8790, Val Loss=0.3622, lr=0.0100
[02/23 22:10:59 cifar10-global-slim-2.0-mobilenetv2]: Epoch 10/100, Acc=0.8780, Val Loss=0.3776, lr=0.0100
[02/23 22:11:27 cifar10-global-slim-2.0-mobilenetv2]: Epoch 11/100, Acc=0.8746, Val Loss=0.3849, lr=0.0100
[02/23 22:11:55 cifar10-global-slim-2.0-mobilenetv2]: Epoch 12/100, Acc=0.8822, Val Loss=0.3618, lr=0.0100
[02/23 22:12:23 cifar10-global-slim-2.0-mobilenetv2]: Epoch 13/100, Acc=0.8763, Val Loss=0.3723, lr=0.0100
[02/23 22:12:51 cifar10-global-slim-2.0-mobilenetv2]: Epoch 14/100, Acc=0.8773, Val Loss=0.3723, lr=0.0100
[02/23 22:13:18 cifar10-global-slim-2.0-mobilenetv2]: Epoch 15/100, Acc=0.8642, Val Loss=0.4169, lr=0.0100
[02/23 22:13:46 cifar10-global-slim-2.0-mobilenetv2]: Epoch 16/100, Acc=0.8672, Val Loss=0.4040, lr=0.0100
[02/23 22:14:14 cifar10-global-slim-2.0-mobilenetv2]: Epoch 17/100, Acc=0.8725, Val Loss=0.3835, lr=0.0100
[02/23 22:14:42 cifar10-global-slim-2.0-mobilenetv2]: Epoch 18/100, Acc=0.8765, Val Loss=0.3721, lr=0.0100
[02/23 22:15:10 cifar10-global-slim-2.0-mobilenetv2]: Epoch 19/100, Acc=0.8561, Val Loss=0.4336, lr=0.0100
[02/23 22:15:38 cifar10-global-slim-2.0-mobilenetv2]: Epoch 20/100, Acc=0.8655, Val Loss=0.4050, lr=0.0100
[02/23 22:16:06 cifar10-global-slim-2.0-mobilenetv2]: Epoch 21/100, Acc=0.8791, Val Loss=0.3647, lr=0.0100
[02/23 22:16:34 cifar10-global-slim-2.0-mobilenetv2]: Epoch 22/100, Acc=0.8736, Val Loss=0.3821, lr=0.0100
[02/23 22:17:02 cifar10-global-slim-2.0-mobilenetv2]: Epoch 23/100, Acc=0.8664, Val Loss=0.4070, lr=0.0100
[02/23 22:17:31 cifar10-global-slim-2.0-mobilenetv2]: Epoch 24/100, Acc=0.8642, Val Loss=0.4108, lr=0.0100
[02/23 22:17:59 cifar10-global-slim-2.0-mobilenetv2]: Epoch 25/100, Acc=0.8785, Val Loss=0.3669, lr=0.0100
[02/23 22:18:27 cifar10-global-slim-2.0-mobilenetv2]: Epoch 26/100, Acc=0.8592, Val Loss=0.4175, lr=0.0100
[02/23 22:18:56 cifar10-global-slim-2.0-mobilenetv2]: Epoch 27/100, Acc=0.8765, Val Loss=0.3715, lr=0.0100
[02/23 22:19:24 cifar10-global-slim-2.0-mobilenetv2]: Epoch 28/100, Acc=0.8682, Val Loss=0.4047, lr=0.0100
[02/23 22:19:52 cifar10-global-slim-2.0-mobilenetv2]: Epoch 29/100, Acc=0.8698, Val Loss=0.3891, lr=0.0100
[02/23 22:20:21 cifar10-global-slim-2.0-mobilenetv2]: Epoch 30/100, Acc=0.8661, Val Loss=0.4040, lr=0.0100
[02/23 22:20:49 cifar10-global-slim-2.0-mobilenetv2]: Epoch 31/100, Acc=0.8710, Val Loss=0.3888, lr=0.0100
[02/23 22:21:17 cifar10-global-slim-2.0-mobilenetv2]: Epoch 32/100, Acc=0.8731, Val Loss=0.3791, lr=0.0100
[02/23 22:21:46 cifar10-global-slim-2.0-mobilenetv2]: Epoch 33/100, Acc=0.8546, Val Loss=0.4437, lr=0.0100
[02/23 22:22:14 cifar10-global-slim-2.0-mobilenetv2]: Epoch 34/100, Acc=0.8647, Val Loss=0.4052, lr=0.0100
[02/23 22:22:42 cifar10-global-slim-2.0-mobilenetv2]: Epoch 35/100, Acc=0.8508, Val Loss=0.4561, lr=0.0100
[02/23 22:23:10 cifar10-global-slim-2.0-mobilenetv2]: Epoch 36/100, Acc=0.8683, Val Loss=0.3970, lr=0.0100
[02/23 22:23:38 cifar10-global-slim-2.0-mobilenetv2]: Epoch 37/100, Acc=0.8681, Val Loss=0.4006, lr=0.0100
[02/23 22:24:06 cifar10-global-slim-2.0-mobilenetv2]: Epoch 38/100, Acc=0.8561, Val Loss=0.4312, lr=0.0100
[02/23 22:24:34 cifar10-global-slim-2.0-mobilenetv2]: Epoch 39/100, Acc=0.8579, Val Loss=0.4160, lr=0.0100
[02/23 22:25:02 cifar10-global-slim-2.0-mobilenetv2]: Epoch 40/100, Acc=0.8708, Val Loss=0.3892, lr=0.0100
[02/23 22:25:31 cifar10-global-slim-2.0-mobilenetv2]: Epoch 41/100, Acc=0.8648, Val Loss=0.3956, lr=0.0100
[02/23 22:25:59 cifar10-global-slim-2.0-mobilenetv2]: Epoch 42/100, Acc=0.8575, Val Loss=0.4293, lr=0.0100
[02/23 22:26:27 cifar10-global-slim-2.0-mobilenetv2]: Epoch 43/100, Acc=0.8630, Val Loss=0.3968, lr=0.0100
[02/23 22:26:56 cifar10-global-slim-2.0-mobilenetv2]: Epoch 44/100, Acc=0.8609, Val Loss=0.4082, lr=0.0100
[02/23 22:27:24 cifar10-global-slim-2.0-mobilenetv2]: Epoch 45/100, Acc=0.8639, Val Loss=0.4231, lr=0.0100
[02/23 22:27:52 cifar10-global-slim-2.0-mobilenetv2]: Epoch 46/100, Acc=0.8755, Val Loss=0.3814, lr=0.0100
[02/23 22:28:21 cifar10-global-slim-2.0-mobilenetv2]: Epoch 47/100, Acc=0.8642, Val Loss=0.4142, lr=0.0100
[02/23 22:28:49 cifar10-global-slim-2.0-mobilenetv2]: Epoch 48/100, Acc=0.8706, Val Loss=0.3872, lr=0.0100
[02/23 22:29:18 cifar10-global-slim-2.0-mobilenetv2]: Epoch 49/100, Acc=0.8533, Val Loss=0.4400, lr=0.0100
[02/23 22:29:46 cifar10-global-slim-2.0-mobilenetv2]: Epoch 50/100, Acc=0.8547, Val Loss=0.4256, lr=0.0100
[02/23 22:30:14 cifar10-global-slim-2.0-mobilenetv2]: Epoch 51/100, Acc=0.8613, Val Loss=0.4119, lr=0.0100
[02/23 22:30:43 cifar10-global-slim-2.0-mobilenetv2]: Epoch 52/100, Acc=0.8605, Val Loss=0.4037, lr=0.0100
[02/23 22:31:11 cifar10-global-slim-2.0-mobilenetv2]: Epoch 53/100, Acc=0.8631, Val Loss=0.4061, lr=0.0100
[02/23 22:31:40 cifar10-global-slim-2.0-mobilenetv2]: Epoch 54/100, Acc=0.8720, Val Loss=0.3948, lr=0.0100
[02/23 22:32:08 cifar10-global-slim-2.0-mobilenetv2]: Epoch 55/100, Acc=0.8558, Val Loss=0.4222, lr=0.0100
[02/23 22:32:37 cifar10-global-slim-2.0-mobilenetv2]: Epoch 56/100, Acc=0.8501, Val Loss=0.4434, lr=0.0100
[02/23 22:33:05 cifar10-global-slim-2.0-mobilenetv2]: Epoch 57/100, Acc=0.8615, Val Loss=0.4222, lr=0.0100
[02/23 22:33:34 cifar10-global-slim-2.0-mobilenetv2]: Epoch 58/100, Acc=0.8695, Val Loss=0.3998, lr=0.0100
[02/23 22:34:02 cifar10-global-slim-2.0-mobilenetv2]: Epoch 59/100, Acc=0.8644, Val Loss=0.4078, lr=0.0100
[02/23 22:34:31 cifar10-global-slim-2.0-mobilenetv2]: Epoch 60/100, Acc=0.8971, Val Loss=0.3075, lr=0.0010
[02/23 22:34:59 cifar10-global-slim-2.0-mobilenetv2]: Epoch 61/100, Acc=0.8984, Val Loss=0.2996, lr=0.0010
[02/23 22:35:28 cifar10-global-slim-2.0-mobilenetv2]: Epoch 62/100, Acc=0.9022, Val Loss=0.2948, lr=0.0010
[02/23 22:35:57 cifar10-global-slim-2.0-mobilenetv2]: Epoch 63/100, Acc=0.9027, Val Loss=0.2949, lr=0.0010
[02/23 22:36:25 cifar10-global-slim-2.0-mobilenetv2]: Epoch 64/100, Acc=0.9005, Val Loss=0.2941, lr=0.0010
[02/23 22:36:54 cifar10-global-slim-2.0-mobilenetv2]: Epoch 65/100, Acc=0.9002, Val Loss=0.2953, lr=0.0010
[02/23 22:37:22 cifar10-global-slim-2.0-mobilenetv2]: Epoch 66/100, Acc=0.9008, Val Loss=0.2969, lr=0.0010
[02/23 22:37:50 cifar10-global-slim-2.0-mobilenetv2]: Epoch 67/100, Acc=0.9007, Val Loss=0.2960, lr=0.0010
[02/23 22:38:19 cifar10-global-slim-2.0-mobilenetv2]: Epoch 68/100, Acc=0.9030, Val Loss=0.2945, lr=0.0010
[02/23 22:38:47 cifar10-global-slim-2.0-mobilenetv2]: Epoch 69/100, Acc=0.9022, Val Loss=0.2977, lr=0.0010
[02/23 22:39:15 cifar10-global-slim-2.0-mobilenetv2]: Epoch 70/100, Acc=0.9056, Val Loss=0.2945, lr=0.0010
[02/23 22:39:44 cifar10-global-slim-2.0-mobilenetv2]: Epoch 71/100, Acc=0.9022, Val Loss=0.2979, lr=0.0010
[02/23 22:40:12 cifar10-global-slim-2.0-mobilenetv2]: Epoch 72/100, Acc=0.9032, Val Loss=0.3001, lr=0.0010
[02/23 22:40:40 cifar10-global-slim-2.0-mobilenetv2]: Epoch 73/100, Acc=0.9057, Val Loss=0.2960, lr=0.0010
[02/23 22:41:08 cifar10-global-slim-2.0-mobilenetv2]: Epoch 74/100, Acc=0.9042, Val Loss=0.2973, lr=0.0010
[02/23 22:41:36 cifar10-global-slim-2.0-mobilenetv2]: Epoch 75/100, Acc=0.9029, Val Loss=0.3021, lr=0.0010
[02/23 22:42:04 cifar10-global-slim-2.0-mobilenetv2]: Epoch 76/100, Acc=0.9036, Val Loss=0.3016, lr=0.0010
[02/23 22:42:32 cifar10-global-slim-2.0-mobilenetv2]: Epoch 77/100, Acc=0.9008, Val Loss=0.3047, lr=0.0010
[02/23 22:43:00 cifar10-global-slim-2.0-mobilenetv2]: Epoch 78/100, Acc=0.9032, Val Loss=0.3012, lr=0.0010
[02/23 22:43:28 cifar10-global-slim-2.0-mobilenetv2]: Epoch 79/100, Acc=0.9035, Val Loss=0.2981, lr=0.0010
[02/23 22:43:56 cifar10-global-slim-2.0-mobilenetv2]: Epoch 80/100, Acc=0.9033, Val Loss=0.2981, lr=0.0001
[02/23 22:44:24 cifar10-global-slim-2.0-mobilenetv2]: Epoch 81/100, Acc=0.9048, Val Loss=0.2965, lr=0.0001
[02/23 22:44:52 cifar10-global-slim-2.0-mobilenetv2]: Epoch 82/100, Acc=0.9046, Val Loss=0.2955, lr=0.0001
[02/23 22:45:20 cifar10-global-slim-2.0-mobilenetv2]: Epoch 83/100, Acc=0.9037, Val Loss=0.3000, lr=0.0001
[02/23 22:45:49 cifar10-global-slim-2.0-mobilenetv2]: Epoch 84/100, Acc=0.9041, Val Loss=0.2969, lr=0.0001
[02/23 22:46:18 cifar10-global-slim-2.0-mobilenetv2]: Epoch 85/100, Acc=0.9053, Val Loss=0.2971, lr=0.0001
[02/23 22:46:46 cifar10-global-slim-2.0-mobilenetv2]: Epoch 86/100, Acc=0.9046, Val Loss=0.2977, lr=0.0001
[02/23 22:47:15 cifar10-global-slim-2.0-mobilenetv2]: Epoch 87/100, Acc=0.9039, Val Loss=0.2974, lr=0.0001
[02/23 22:47:44 cifar10-global-slim-2.0-mobilenetv2]: Epoch 88/100, Acc=0.9026, Val Loss=0.2979, lr=0.0001
[02/23 22:48:12 cifar10-global-slim-2.0-mobilenetv2]: Epoch 89/100, Acc=0.9047, Val Loss=0.2960, lr=0.0001
[02/23 22:48:40 cifar10-global-slim-2.0-mobilenetv2]: Epoch 90/100, Acc=0.9039, Val Loss=0.2967, lr=0.0001
[02/23 22:49:09 cifar10-global-slim-2.0-mobilenetv2]: Epoch 91/100, Acc=0.9058, Val Loss=0.2995, lr=0.0001
[02/23 22:49:37 cifar10-global-slim-2.0-mobilenetv2]: Epoch 92/100, Acc=0.9049, Val Loss=0.2975, lr=0.0001
[02/23 22:50:05 cifar10-global-slim-2.0-mobilenetv2]: Epoch 93/100, Acc=0.9064, Val Loss=0.2974, lr=0.0001
[02/23 22:50:34 cifar10-global-slim-2.0-mobilenetv2]: Epoch 94/100, Acc=0.9051, Val Loss=0.2972, lr=0.0001
[02/23 22:51:02 cifar10-global-slim-2.0-mobilenetv2]: Epoch 95/100, Acc=0.9043, Val Loss=0.2978, lr=0.0001
[02/23 22:51:30 cifar10-global-slim-2.0-mobilenetv2]: Epoch 96/100, Acc=0.9057, Val Loss=0.2981, lr=0.0001
[02/23 22:51:58 cifar10-global-slim-2.0-mobilenetv2]: Epoch 97/100, Acc=0.9061, Val Loss=0.2967, lr=0.0001
[02/23 22:52:26 cifar10-global-slim-2.0-mobilenetv2]: Epoch 98/100, Acc=0.9048, Val Loss=0.2980, lr=0.0001
[02/23 22:52:54 cifar10-global-slim-2.0-mobilenetv2]: Epoch 99/100, Acc=0.9039, Val Loss=0.2967, lr=0.0001
[02/23 22:52:54 cifar10-global-slim-2.0-mobilenetv2]: Best Acc=0.9064
[02/23 22:52:54 cifar10-global-slim-2.0-mobilenetv2]: Params: 1.14 M
[02/23 22:52:54 cifar10-global-slim-2.0-mobilenetv2]: ops: 33.95 M
[02/23 22:52:57 cifar10-global-slim-2.0-mobilenetv2]: Acc: 0.9039 Val Loss: 0.2967

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: mode: prune
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: model: mobilenetv2
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: verbose: False
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: dataset: cifar10
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: dataroot: data
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: batch_size: 128
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: total_epochs: 100
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: lr: 0.01
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: restore: run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: output_dir: run/cifar10/prune/cifar10-global-group_norm-2.0-mobilenetv2
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: finetune: True
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: last_epochs: 100
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: reps: 1
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: method: group_norm
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: speed_up: 2.0
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: reg: 1e-05
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: delta_reg: 0.0001
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: weight_decay: 0.0005
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: seed: 1
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: global_pruning: True
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: sl_total_epochs: 100
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: sl_lr: 0.01
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: sl_reg_warmup: 0
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: sl_restore: None
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: iterative_steps: 400
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: logger: <Logger cifar10-global-group_norm-2.0-mobilenetv2 (DEBUG)>
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: device: cuda
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: num_classes: 10
[02/23 22:53:04 cifar10-global-group_norm-2.0-mobilenetv2]: Loading model from run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/23 22:53:08 cifar10-global-group_norm-2.0-mobilenetv2]: Pruning...
[02/23 22:53:30 cifar10-global-group_norm-2.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 7, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(7, 17, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(17, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=17)
      (4): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(17, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 73, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(73, 73, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=73)
        (4): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(73, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 86, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(86, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=86)
        (4): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(86, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 60, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(60, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=60)
        (4): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(60, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 88, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88)
        (4): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(88, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 52, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(52, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=52)
        (4): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(52, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 50, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(50, 50, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=50)
        (4): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(50, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 54, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(54, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=54)
        (4): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(54, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 36, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36)
        (4): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(36, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 95, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(95, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(95, 95, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=95)
        (4): BatchNorm2d(95, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(95, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 232, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(232, 232, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=232)
        (4): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(232, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 16, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 5, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=5)
        (4): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(5, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 629, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(629, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(629, 629, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=629)
      (4): BatchNorm2d(629, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(629, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 10, kernel_size=(1, 1), stride=(1, 1))
)
[02/23 22:53:33 cifar10-global-group_norm-2.0-mobilenetv2]: Params: 2.25 M => 1.12 M (49.90%)
[02/23 22:53:33 cifar10-global-group_norm-2.0-mobilenetv2]: FLOPs: 68.29 M => 34.08 M (49.91%, 2.00X )
[02/23 22:53:33 cifar10-global-group_norm-2.0-mobilenetv2]: Acc: 0.8936 => 0.8936
[02/23 22:53:33 cifar10-global-group_norm-2.0-mobilenetv2]: Val Loss: 0.3202 => 0.3202
[02/23 22:53:33 cifar10-global-group_norm-2.0-mobilenetv2]: Finetuning...
[02/23 22:54:00 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 0/100, Acc=0.8429, Val Loss=0.4582, lr=0.0100
[02/23 22:54:28 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 1/100, Acc=0.8462, Val Loss=0.4585, lr=0.0100
[02/23 22:54:55 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 2/100, Acc=0.8427, Val Loss=0.4592, lr=0.0100
[02/23 22:55:23 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 3/100, Acc=0.8555, Val Loss=0.4330, lr=0.0100
[02/23 22:55:51 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 4/100, Acc=0.8578, Val Loss=0.4171, lr=0.0100
[02/23 22:56:19 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 5/100, Acc=0.8262, Val Loss=0.5163, lr=0.0100
[02/23 22:56:46 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 6/100, Acc=0.8476, Val Loss=0.4417, lr=0.0100
[02/23 22:57:14 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 7/100, Acc=0.8491, Val Loss=0.4421, lr=0.0100
[02/23 22:57:41 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 8/100, Acc=0.8557, Val Loss=0.4233, lr=0.0100
[02/23 22:58:09 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 9/100, Acc=0.8513, Val Loss=0.4275, lr=0.0100
[02/23 22:58:36 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 10/100, Acc=0.8512, Val Loss=0.4523, lr=0.0100
[02/23 22:59:04 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 11/100, Acc=0.8619, Val Loss=0.4145, lr=0.0100
[02/23 22:59:31 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 12/100, Acc=0.8526, Val Loss=0.4319, lr=0.0100
[02/23 22:59:59 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 13/100, Acc=0.8570, Val Loss=0.4162, lr=0.0100
[02/23 23:00:27 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 14/100, Acc=0.8419, Val Loss=0.4636, lr=0.0100
[02/23 23:00:55 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 15/100, Acc=0.8545, Val Loss=0.4326, lr=0.0100
[02/23 23:01:22 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 16/100, Acc=0.8561, Val Loss=0.4356, lr=0.0100
[02/23 23:01:50 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 17/100, Acc=0.8545, Val Loss=0.4317, lr=0.0100
[02/23 23:02:17 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 18/100, Acc=0.8544, Val Loss=0.4279, lr=0.0100
[02/23 23:02:44 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 19/100, Acc=0.8489, Val Loss=0.4402, lr=0.0100
[02/23 23:03:12 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 20/100, Acc=0.8579, Val Loss=0.4168, lr=0.0100
[02/23 23:03:39 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 21/100, Acc=0.8620, Val Loss=0.4019, lr=0.0100
[02/23 23:04:07 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 22/100, Acc=0.8501, Val Loss=0.4512, lr=0.0100
[02/23 23:04:34 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 23/100, Acc=0.8587, Val Loss=0.4162, lr=0.0100
[02/23 23:05:02 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 24/100, Acc=0.8571, Val Loss=0.4329, lr=0.0100
[02/23 23:05:30 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 25/100, Acc=0.8554, Val Loss=0.4256, lr=0.0100
[02/23 23:05:57 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 26/100, Acc=0.8470, Val Loss=0.4608, lr=0.0100
[02/23 23:06:25 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 27/100, Acc=0.8541, Val Loss=0.4344, lr=0.0100
[02/23 23:06:53 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 28/100, Acc=0.8590, Val Loss=0.4232, lr=0.0100
[02/23 23:07:20 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 29/100, Acc=0.8543, Val Loss=0.4320, lr=0.0100
[02/23 23:07:48 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 30/100, Acc=0.8554, Val Loss=0.4357, lr=0.0100
[02/23 23:08:16 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 31/100, Acc=0.8447, Val Loss=0.4632, lr=0.0100
[02/23 23:08:44 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 32/100, Acc=0.8428, Val Loss=0.4728, lr=0.0100
[02/23 23:09:12 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 33/100, Acc=0.8528, Val Loss=0.4461, lr=0.0100
[02/23 23:09:40 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 34/100, Acc=0.8648, Val Loss=0.4045, lr=0.0100
[02/23 23:10:07 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 35/100, Acc=0.8505, Val Loss=0.4364, lr=0.0100
[02/23 23:10:35 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 36/100, Acc=0.8555, Val Loss=0.4268, lr=0.0100
[02/23 23:11:03 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 37/100, Acc=0.8628, Val Loss=0.4077, lr=0.0100
[02/23 23:11:31 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 38/100, Acc=0.8647, Val Loss=0.3919, lr=0.0100
[02/23 23:11:59 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 39/100, Acc=0.8581, Val Loss=0.4133, lr=0.0100
[02/23 23:12:27 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 40/100, Acc=0.8481, Val Loss=0.4578, lr=0.0100
[02/23 23:12:55 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 41/100, Acc=0.8654, Val Loss=0.4029, lr=0.0100
[02/23 23:13:23 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 42/100, Acc=0.8589, Val Loss=0.4258, lr=0.0100
[02/23 23:13:50 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 43/100, Acc=0.8457, Val Loss=0.4458, lr=0.0100
[02/23 23:14:18 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 44/100, Acc=0.8492, Val Loss=0.4407, lr=0.0100
[02/23 23:14:46 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 45/100, Acc=0.8562, Val Loss=0.4338, lr=0.0100
[02/23 23:15:14 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 46/100, Acc=0.8609, Val Loss=0.4065, lr=0.0100
[02/23 23:15:42 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 47/100, Acc=0.8429, Val Loss=0.4687, lr=0.0100
[02/23 23:16:09 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 48/100, Acc=0.8467, Val Loss=0.4613, lr=0.0100
[02/23 23:16:37 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 49/100, Acc=0.8611, Val Loss=0.4116, lr=0.0100
[02/23 23:17:04 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 50/100, Acc=0.8575, Val Loss=0.4261, lr=0.0100
[02/23 23:17:32 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 51/100, Acc=0.8568, Val Loss=0.4220, lr=0.0100
[02/23 23:17:59 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 52/100, Acc=0.8526, Val Loss=0.4300, lr=0.0100
[02/23 23:18:26 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 53/100, Acc=0.8531, Val Loss=0.4456, lr=0.0100
[02/23 23:18:54 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 54/100, Acc=0.8509, Val Loss=0.4467, lr=0.0100
[02/23 23:19:21 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 55/100, Acc=0.8572, Val Loss=0.4174, lr=0.0100
[02/23 23:19:49 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 56/100, Acc=0.8621, Val Loss=0.4112, lr=0.0100
[02/23 23:20:16 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 57/100, Acc=0.8490, Val Loss=0.4415, lr=0.0100
[02/23 23:20:43 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 58/100, Acc=0.8668, Val Loss=0.3950, lr=0.0100
[02/23 23:21:11 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 59/100, Acc=0.8502, Val Loss=0.4468, lr=0.0100
[02/23 23:21:38 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 60/100, Acc=0.8901, Val Loss=0.3239, lr=0.0010
[02/23 23:22:05 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 61/100, Acc=0.8942, Val Loss=0.3153, lr=0.0010
[02/23 23:22:33 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 62/100, Acc=0.8962, Val Loss=0.3099, lr=0.0010
[02/23 23:23:00 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 63/100, Acc=0.8960, Val Loss=0.3091, lr=0.0010
[02/23 23:23:27 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 64/100, Acc=0.8969, Val Loss=0.3072, lr=0.0010
[02/23 23:23:55 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 65/100, Acc=0.8954, Val Loss=0.3106, lr=0.0010
[02/23 23:24:22 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 66/100, Acc=0.8952, Val Loss=0.3118, lr=0.0010
[02/23 23:24:50 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 67/100, Acc=0.8967, Val Loss=0.3106, lr=0.0010
[02/23 23:25:17 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 68/100, Acc=0.8973, Val Loss=0.3113, lr=0.0010
[02/23 23:25:45 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 69/100, Acc=0.8996, Val Loss=0.3079, lr=0.0010
[02/23 23:26:12 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 70/100, Acc=0.8969, Val Loss=0.3146, lr=0.0010
[02/23 23:26:40 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 71/100, Acc=0.8988, Val Loss=0.3078, lr=0.0010
[02/23 23:27:07 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 72/100, Acc=0.8993, Val Loss=0.3076, lr=0.0010
[02/23 23:27:34 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 73/100, Acc=0.8995, Val Loss=0.3087, lr=0.0010
[02/23 23:28:02 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 74/100, Acc=0.8975, Val Loss=0.3091, lr=0.0010
[02/23 23:28:29 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 75/100, Acc=0.8990, Val Loss=0.3080, lr=0.0010
[02/23 23:28:57 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 76/100, Acc=0.8968, Val Loss=0.3078, lr=0.0010
[02/23 23:29:24 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 77/100, Acc=0.8960, Val Loss=0.3091, lr=0.0010
[02/23 23:29:51 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 78/100, Acc=0.8994, Val Loss=0.3107, lr=0.0010
[02/23 23:30:19 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 79/100, Acc=0.8982, Val Loss=0.3134, lr=0.0010
[02/23 23:30:46 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 80/100, Acc=0.8992, Val Loss=0.3066, lr=0.0001
[02/23 23:31:14 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 81/100, Acc=0.8992, Val Loss=0.3077, lr=0.0001
[02/23 23:31:41 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 82/100, Acc=0.8999, Val Loss=0.3083, lr=0.0001
[02/23 23:32:09 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 83/100, Acc=0.9002, Val Loss=0.3068, lr=0.0001
[02/23 23:32:37 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 84/100, Acc=0.8999, Val Loss=0.3060, lr=0.0001
[02/23 23:33:04 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 85/100, Acc=0.9010, Val Loss=0.3061, lr=0.0001
[02/23 23:33:31 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 86/100, Acc=0.8992, Val Loss=0.3075, lr=0.0001
[02/23 23:33:59 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 87/100, Acc=0.8991, Val Loss=0.3057, lr=0.0001
[02/23 23:34:26 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 88/100, Acc=0.9009, Val Loss=0.3041, lr=0.0001
[02/23 23:34:53 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 89/100, Acc=0.9002, Val Loss=0.3046, lr=0.0001
[02/23 23:35:21 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 90/100, Acc=0.8998, Val Loss=0.3063, lr=0.0001
[02/23 23:35:48 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 91/100, Acc=0.9009, Val Loss=0.3052, lr=0.0001
[02/23 23:36:16 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 92/100, Acc=0.9001, Val Loss=0.3035, lr=0.0001
[02/23 23:36:43 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 93/100, Acc=0.9008, Val Loss=0.3054, lr=0.0001
[02/23 23:37:11 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 94/100, Acc=0.8998, Val Loss=0.3053, lr=0.0001
[02/23 23:37:39 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 95/100, Acc=0.8999, Val Loss=0.3054, lr=0.0001
[02/23 23:38:06 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 96/100, Acc=0.9003, Val Loss=0.3055, lr=0.0001
[02/23 23:38:34 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 97/100, Acc=0.8994, Val Loss=0.3056, lr=0.0001
[02/23 23:39:01 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 98/100, Acc=0.9002, Val Loss=0.3050, lr=0.0001
[02/23 23:39:29 cifar10-global-group_norm-2.0-mobilenetv2]: Epoch 99/100, Acc=0.8996, Val Loss=0.3045, lr=0.0001
[02/23 23:39:29 cifar10-global-group_norm-2.0-mobilenetv2]: Best Acc=0.9010
[02/23 23:39:29 cifar10-global-group_norm-2.0-mobilenetv2]: Params: 1.12 M
[02/23 23:39:29 cifar10-global-group_norm-2.0-mobilenetv2]: ops: 34.08 M
[02/23 23:39:32 cifar10-global-group_norm-2.0-mobilenetv2]: Acc: 0.8996 Val Loss: 0.3045

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: mode: prune
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: model: mobilenetv2
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: verbose: False
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: dataset: cifar10
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: dataroot: data
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: batch_size: 128
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: total_epochs: 100
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: lr: 0.01
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: restore: run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: output_dir: run/cifar10/prune/cifar10-global-group_sl-2.0-mobilenetv2
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: finetune: True
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: last_epochs: 100
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: reps: 1
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: method: group_sl
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: speed_up: 2.0
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: reg: 1e-05
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: delta_reg: 0.0001
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: weight_decay: 0.0005
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: seed: 1
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: global_pruning: True
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: sl_total_epochs: 100
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: sl_lr: 0.01
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: sl_reg_warmup: 0
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: sl_restore: None
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: iterative_steps: 400
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: logger: <Logger cifar10-global-group_sl-2.0-mobilenetv2 (DEBUG)>
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: device: cuda
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: num_classes: 10
[02/23 23:39:41 cifar10-global-group_sl-2.0-mobilenetv2]: Loading model from run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/23 23:39:42 cifar10-global-group_sl-2.0-mobilenetv2]: Regularizing...
[02/23 23:40:50 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 0/100, Acc=0.8530, Val Loss=0.4319, lr=0.0100
[02/23 23:41:52 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 1/100, Acc=0.8472, Val Loss=0.4596, lr=0.0100
[02/23 23:42:54 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 2/100, Acc=0.8593, Val Loss=0.4155, lr=0.0100
[02/23 23:43:56 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 3/100, Acc=0.8580, Val Loss=0.4211, lr=0.0100
[02/23 23:44:58 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 4/100, Acc=0.8601, Val Loss=0.4190, lr=0.0100
[02/23 23:46:00 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 5/100, Acc=0.8635, Val Loss=0.3988, lr=0.0100
[02/23 23:47:03 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 6/100, Acc=0.8653, Val Loss=0.3978, lr=0.0100
[02/23 23:48:06 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 7/100, Acc=0.8587, Val Loss=0.4212, lr=0.0100
[02/23 23:49:08 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 8/100, Acc=0.8626, Val Loss=0.4134, lr=0.0100
[02/23 23:50:11 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 9/100, Acc=0.8653, Val Loss=0.4005, lr=0.0100
[02/23 23:51:13 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 10/100, Acc=0.8558, Val Loss=0.4356, lr=0.0100
[02/23 23:52:16 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 11/100, Acc=0.8611, Val Loss=0.4135, lr=0.0100
[02/23 23:53:18 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 12/100, Acc=0.8737, Val Loss=0.3859, lr=0.0100
[02/23 23:54:21 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 13/100, Acc=0.8660, Val Loss=0.4042, lr=0.0100
[02/23 23:55:24 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 14/100, Acc=0.8664, Val Loss=0.4073, lr=0.0100
[02/23 23:56:26 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 15/100, Acc=0.8678, Val Loss=0.3940, lr=0.0100
[02/23 23:57:29 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 16/100, Acc=0.8476, Val Loss=0.4608, lr=0.0100
[02/23 23:58:32 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 17/100, Acc=0.8642, Val Loss=0.4037, lr=0.0100
[02/23 23:59:34 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 18/100, Acc=0.8756, Val Loss=0.3757, lr=0.0100
[02/24 00:00:37 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 19/100, Acc=0.8774, Val Loss=0.3728, lr=0.0100
[02/24 00:01:39 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 20/100, Acc=0.8738, Val Loss=0.3770, lr=0.0100
[02/24 00:02:41 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 21/100, Acc=0.8677, Val Loss=0.3962, lr=0.0100
[02/24 00:03:44 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 22/100, Acc=0.8710, Val Loss=0.3831, lr=0.0100
[02/24 00:04:46 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 23/100, Acc=0.8711, Val Loss=0.3837, lr=0.0100
[02/24 00:05:48 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 24/100, Acc=0.8663, Val Loss=0.3949, lr=0.0100
[02/24 00:06:51 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 25/100, Acc=0.8645, Val Loss=0.3957, lr=0.0100
[02/24 00:07:53 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 26/100, Acc=0.8660, Val Loss=0.4005, lr=0.0100
[02/24 00:08:55 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 27/100, Acc=0.8679, Val Loss=0.3905, lr=0.0100
[02/24 00:09:58 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 28/100, Acc=0.8659, Val Loss=0.4134, lr=0.0100
[02/24 00:11:00 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 29/100, Acc=0.8726, Val Loss=0.3861, lr=0.0100
[02/24 00:12:03 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 30/100, Acc=0.8762, Val Loss=0.3655, lr=0.0100
[02/24 00:13:06 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 31/100, Acc=0.8793, Val Loss=0.3727, lr=0.0100
[02/24 00:14:08 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 32/100, Acc=0.8758, Val Loss=0.3729, lr=0.0100
[02/24 00:15:11 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 33/100, Acc=0.8753, Val Loss=0.3681, lr=0.0100
[02/24 00:16:14 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 34/100, Acc=0.8806, Val Loss=0.3662, lr=0.0100
[02/24 00:17:16 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 35/100, Acc=0.8746, Val Loss=0.3657, lr=0.0100
[02/24 00:18:19 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 36/100, Acc=0.8779, Val Loss=0.3629, lr=0.0100
[02/24 00:19:22 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 37/100, Acc=0.8695, Val Loss=0.3812, lr=0.0100
[02/24 00:20:25 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 38/100, Acc=0.8778, Val Loss=0.3679, lr=0.0100
[02/24 00:21:27 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 39/100, Acc=0.8682, Val Loss=0.3863, lr=0.0100
[02/24 00:22:30 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 40/100, Acc=0.8701, Val Loss=0.3926, lr=0.0100
[02/24 00:23:33 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 41/100, Acc=0.8692, Val Loss=0.3856, lr=0.0100
[02/24 00:24:36 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 42/100, Acc=0.8740, Val Loss=0.3747, lr=0.0100
[02/24 00:25:39 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 43/100, Acc=0.8772, Val Loss=0.3607, lr=0.0100
[02/24 00:26:41 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 44/100, Acc=0.8833, Val Loss=0.3569, lr=0.0100
[02/24 00:27:45 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 45/100, Acc=0.8822, Val Loss=0.3519, lr=0.0100
[02/24 00:28:47 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 46/100, Acc=0.8759, Val Loss=0.3727, lr=0.0100
[02/24 00:29:50 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 47/100, Acc=0.8829, Val Loss=0.3629, lr=0.0100
[02/24 00:30:53 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 48/100, Acc=0.8839, Val Loss=0.3494, lr=0.0100
[02/24 00:31:57 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 49/100, Acc=0.8815, Val Loss=0.3735, lr=0.0100
[02/24 00:33:01 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 50/100, Acc=0.8829, Val Loss=0.3565, lr=0.0100
[02/24 00:34:05 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 51/100, Acc=0.8874, Val Loss=0.3542, lr=0.0100
[02/24 00:35:08 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 52/100, Acc=0.8780, Val Loss=0.3647, lr=0.0100
[02/24 00:36:10 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 53/100, Acc=0.8781, Val Loss=0.3660, lr=0.0100
[02/24 00:37:13 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 54/100, Acc=0.8772, Val Loss=0.3776, lr=0.0100
[02/24 00:38:16 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 55/100, Acc=0.8827, Val Loss=0.3512, lr=0.0100
[02/24 00:39:18 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 56/100, Acc=0.8860, Val Loss=0.3526, lr=0.0100
[02/24 00:40:20 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 57/100, Acc=0.8795, Val Loss=0.3677, lr=0.0100
[02/24 00:41:23 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 58/100, Acc=0.8840, Val Loss=0.3557, lr=0.0100
[02/24 00:42:25 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 59/100, Acc=0.8821, Val Loss=0.3494, lr=0.0100
[02/24 00:43:27 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 60/100, Acc=0.8928, Val Loss=0.3129, lr=0.0010
[02/24 00:44:29 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 61/100, Acc=0.8965, Val Loss=0.3086, lr=0.0010
[02/24 00:45:31 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 62/100, Acc=0.8975, Val Loss=0.3068, lr=0.0010
[02/24 00:46:34 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 63/100, Acc=0.8969, Val Loss=0.3099, lr=0.0010
[02/24 00:47:36 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 64/100, Acc=0.8977, Val Loss=0.3106, lr=0.0010
[02/24 00:48:39 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 65/100, Acc=0.8987, Val Loss=0.3092, lr=0.0010
[02/24 00:49:41 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 66/100, Acc=0.8977, Val Loss=0.3102, lr=0.0010
[02/24 00:50:44 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 67/100, Acc=0.8987, Val Loss=0.3071, lr=0.0010
[02/24 00:51:46 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 68/100, Acc=0.8997, Val Loss=0.3108, lr=0.0010
[02/24 00:52:49 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 69/100, Acc=0.8972, Val Loss=0.3112, lr=0.0010
[02/24 00:53:52 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 70/100, Acc=0.9007, Val Loss=0.3082, lr=0.0010
[02/24 00:54:55 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 71/100, Acc=0.9018, Val Loss=0.3118, lr=0.0010
[02/24 00:55:58 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 72/100, Acc=0.8994, Val Loss=0.3124, lr=0.0010
[02/24 00:57:02 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 73/100, Acc=0.8993, Val Loss=0.3139, lr=0.0010
[02/24 00:58:05 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 74/100, Acc=0.9015, Val Loss=0.3107, lr=0.0010
[02/24 00:59:08 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 75/100, Acc=0.9003, Val Loss=0.3119, lr=0.0010
[02/24 01:00:11 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 76/100, Acc=0.8993, Val Loss=0.3137, lr=0.0010
[02/24 01:01:14 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 77/100, Acc=0.8993, Val Loss=0.3128, lr=0.0010
[02/24 01:02:17 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 78/100, Acc=0.8995, Val Loss=0.3169, lr=0.0010
[02/24 01:03:20 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 79/100, Acc=0.8989, Val Loss=0.3156, lr=0.0010
[02/24 01:04:24 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 80/100, Acc=0.8994, Val Loss=0.3117, lr=0.0001
[02/24 01:05:27 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 81/100, Acc=0.9009, Val Loss=0.3144, lr=0.0001
[02/24 01:06:30 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 82/100, Acc=0.9002, Val Loss=0.3116, lr=0.0001
[02/24 01:07:33 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 83/100, Acc=0.8995, Val Loss=0.3141, lr=0.0001
[02/24 01:08:36 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 84/100, Acc=0.9007, Val Loss=0.3137, lr=0.0001
[02/24 01:09:39 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 85/100, Acc=0.8998, Val Loss=0.3131, lr=0.0001
[02/24 01:10:43 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 86/100, Acc=0.9000, Val Loss=0.3130, lr=0.0001
[02/24 01:11:46 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 87/100, Acc=0.9001, Val Loss=0.3129, lr=0.0001
[02/24 01:12:49 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 88/100, Acc=0.9008, Val Loss=0.3122, lr=0.0001
[02/24 01:13:52 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 89/100, Acc=0.9005, Val Loss=0.3132, lr=0.0001
[02/24 01:14:55 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 90/100, Acc=0.8989, Val Loss=0.3132, lr=0.0001
[02/24 01:15:57 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 91/100, Acc=0.9002, Val Loss=0.3127, lr=0.0001
[02/24 01:17:00 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 92/100, Acc=0.8991, Val Loss=0.3144, lr=0.0001
[02/24 01:18:03 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 93/100, Acc=0.9003, Val Loss=0.3133, lr=0.0001
[02/24 01:19:06 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 94/100, Acc=0.9009, Val Loss=0.3142, lr=0.0001
[02/24 01:20:09 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 95/100, Acc=0.8996, Val Loss=0.3143, lr=0.0001
[02/24 01:21:12 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 96/100, Acc=0.9015, Val Loss=0.3148, lr=0.0001
[02/24 01:22:14 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 97/100, Acc=0.8997, Val Loss=0.3145, lr=0.0001
[02/24 01:23:17 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 98/100, Acc=0.8987, Val Loss=0.3152, lr=0.0001
[02/24 01:24:20 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 99/100, Acc=0.8996, Val Loss=0.3145, lr=0.0001
[02/24 01:24:20 cifar10-global-group_sl-2.0-mobilenetv2]: Best Acc=0.9018
[02/24 01:24:20 cifar10-global-group_sl-2.0-mobilenetv2]: Loading the sparse model from run/cifar10/prune/cifar10-global-group_sl-2.0-mobilenetv2/reg_cifar10_mobilenetv2_group_sl_1e-05.pth...
[02/24 01:24:23 cifar10-global-group_sl-2.0-mobilenetv2]: Pruning...
[02/24 01:24:48 cifar10-global-group_sl-2.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 8, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(8, 17, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(17, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=17)
      (4): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(17, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 73, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(73, 73, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=73)
        (4): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(73, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88)
        (4): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 65, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(65, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(65, 65, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=65)
        (4): BatchNorm2d(65, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(65, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 88, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88)
        (4): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(88, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 57, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(57, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=57)
        (4): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(57, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 51, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(51, 51, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=51)
        (4): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(51, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 101, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(101, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(101, 101, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=101)
        (4): BatchNorm2d(101, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(101, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 39, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(39, 39, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=39)
        (4): BatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(39, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 104, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(104, 104, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=104)
        (4): BatchNorm2d(104, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(104, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 458, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(458, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(458, 458, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=458)
        (4): BatchNorm2d(458, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(458, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 250, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(250, 250, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=250)
        (4): BatchNorm2d(250, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(250, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 16, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 5, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=5)
        (4): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(5, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 642, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(642, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(642, 642, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=642)
      (4): BatchNorm2d(642, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(642, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 10, kernel_size=(1, 1), stride=(1, 1))
)
[02/24 01:24:51 cifar10-global-group_sl-2.0-mobilenetv2]: Params: 2.25 M => 1.12 M (49.54%)
[02/24 01:24:51 cifar10-global-group_sl-2.0-mobilenetv2]: FLOPs: 68.29 M => 34.02 M (49.82%, 2.01X )
[02/24 01:24:51 cifar10-global-group_sl-2.0-mobilenetv2]: Acc: 0.9018 => 0.9019
[02/24 01:24:51 cifar10-global-group_sl-2.0-mobilenetv2]: Val Loss: 0.3118 => 0.3118
[02/24 01:24:51 cifar10-global-group_sl-2.0-mobilenetv2]: Finetuning...
[02/24 01:25:19 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 0/100, Acc=0.8868, Val Loss=0.3509, lr=0.0100
[02/24 01:25:47 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 1/100, Acc=0.8863, Val Loss=0.3447, lr=0.0100
[02/24 01:26:15 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 2/100, Acc=0.8784, Val Loss=0.3674, lr=0.0100
[02/24 01:26:43 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 3/100, Acc=0.8753, Val Loss=0.3694, lr=0.0100
[02/24 01:27:11 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 4/100, Acc=0.8835, Val Loss=0.3599, lr=0.0100
[02/24 01:27:39 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 5/100, Acc=0.8817, Val Loss=0.3620, lr=0.0100
[02/24 01:28:07 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 6/100, Acc=0.8828, Val Loss=0.3467, lr=0.0100
[02/24 01:28:35 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 7/100, Acc=0.8800, Val Loss=0.3705, lr=0.0100
[02/24 01:29:03 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 8/100, Acc=0.8756, Val Loss=0.3661, lr=0.0100
[02/24 01:29:31 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 9/100, Acc=0.8767, Val Loss=0.3627, lr=0.0100
[02/24 01:29:59 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 10/100, Acc=0.8742, Val Loss=0.3799, lr=0.0100
[02/24 01:30:27 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 11/100, Acc=0.8797, Val Loss=0.3604, lr=0.0100
[02/24 01:30:55 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 12/100, Acc=0.8772, Val Loss=0.3594, lr=0.0100
[02/24 01:31:23 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 13/100, Acc=0.8727, Val Loss=0.3902, lr=0.0100
[02/24 01:31:51 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 14/100, Acc=0.8793, Val Loss=0.3705, lr=0.0100
[02/24 01:32:19 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 15/100, Acc=0.8573, Val Loss=0.4402, lr=0.0100
[02/24 01:32:47 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 16/100, Acc=0.8754, Val Loss=0.3839, lr=0.0100
[02/24 01:33:15 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 17/100, Acc=0.8707, Val Loss=0.3951, lr=0.0100
[02/24 01:33:44 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 18/100, Acc=0.8569, Val Loss=0.4305, lr=0.0100
[02/24 01:34:12 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 19/100, Acc=0.8603, Val Loss=0.4262, lr=0.0100
[02/24 01:34:40 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 20/100, Acc=0.8722, Val Loss=0.3836, lr=0.0100
[02/24 01:35:09 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 21/100, Acc=0.8654, Val Loss=0.3991, lr=0.0100
[02/24 01:35:37 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 22/100, Acc=0.8759, Val Loss=0.3806, lr=0.0100
[02/24 01:36:06 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 23/100, Acc=0.8711, Val Loss=0.3886, lr=0.0100
[02/24 01:36:34 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 24/100, Acc=0.8692, Val Loss=0.4038, lr=0.0100
[02/24 01:37:02 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 25/100, Acc=0.8664, Val Loss=0.4003, lr=0.0100
[02/24 01:37:31 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 26/100, Acc=0.8657, Val Loss=0.3987, lr=0.0100
[02/24 01:37:59 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 27/100, Acc=0.8596, Val Loss=0.4222, lr=0.0100
[02/24 01:38:28 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 28/100, Acc=0.8693, Val Loss=0.4018, lr=0.0100
[02/24 01:38:56 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 29/100, Acc=0.8673, Val Loss=0.3836, lr=0.0100
[02/24 01:39:24 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 30/100, Acc=0.8666, Val Loss=0.4082, lr=0.0100
[02/24 01:39:53 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 31/100, Acc=0.8587, Val Loss=0.4308, lr=0.0100
[02/24 01:40:21 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 32/100, Acc=0.8729, Val Loss=0.3804, lr=0.0100
[02/24 01:40:49 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 33/100, Acc=0.8520, Val Loss=0.4309, lr=0.0100
[02/24 01:41:17 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 34/100, Acc=0.8656, Val Loss=0.3995, lr=0.0100
[02/24 01:41:45 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 35/100, Acc=0.8566, Val Loss=0.4216, lr=0.0100
[02/24 01:42:13 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 36/100, Acc=0.8541, Val Loss=0.4336, lr=0.0100
[02/24 01:42:41 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 37/100, Acc=0.8634, Val Loss=0.4092, lr=0.0100
[02/24 01:43:09 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 38/100, Acc=0.8617, Val Loss=0.4083, lr=0.0100
[02/24 01:43:38 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 39/100, Acc=0.8669, Val Loss=0.3906, lr=0.0100
[02/24 01:44:06 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 40/100, Acc=0.8569, Val Loss=0.4275, lr=0.0100
[02/24 01:44:34 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 41/100, Acc=0.8618, Val Loss=0.4148, lr=0.0100
[02/24 01:45:02 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 42/100, Acc=0.8669, Val Loss=0.4038, lr=0.0100
[02/24 01:45:30 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 43/100, Acc=0.8557, Val Loss=0.4200, lr=0.0100
[02/24 01:45:59 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 44/100, Acc=0.8566, Val Loss=0.4147, lr=0.0100
[02/24 01:46:27 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 45/100, Acc=0.8657, Val Loss=0.4012, lr=0.0100
[02/24 01:46:55 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 46/100, Acc=0.8672, Val Loss=0.4011, lr=0.0100
[02/24 01:47:23 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 47/100, Acc=0.8615, Val Loss=0.4150, lr=0.0100
[02/24 01:47:52 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 48/100, Acc=0.8606, Val Loss=0.4053, lr=0.0100
[02/24 01:48:20 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 49/100, Acc=0.8645, Val Loss=0.4047, lr=0.0100
[02/24 01:48:48 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 50/100, Acc=0.8673, Val Loss=0.3914, lr=0.0100
[02/24 01:49:17 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 51/100, Acc=0.8595, Val Loss=0.4240, lr=0.0100
[02/24 01:49:45 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 52/100, Acc=0.8681, Val Loss=0.3805, lr=0.0100
[02/24 01:50:13 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 53/100, Acc=0.8646, Val Loss=0.3950, lr=0.0100
[02/24 01:50:41 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 54/100, Acc=0.8607, Val Loss=0.4151, lr=0.0100
[02/24 01:51:09 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 55/100, Acc=0.8611, Val Loss=0.4176, lr=0.0100
[02/24 01:51:37 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 56/100, Acc=0.8758, Val Loss=0.3763, lr=0.0100
[02/24 01:52:05 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 57/100, Acc=0.8675, Val Loss=0.4028, lr=0.0100
[02/24 01:52:33 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 58/100, Acc=0.8656, Val Loss=0.3921, lr=0.0100
[02/24 01:53:01 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 59/100, Acc=0.8711, Val Loss=0.3766, lr=0.0100
[02/24 01:53:29 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 60/100, Acc=0.8967, Val Loss=0.3093, lr=0.0010
[02/24 01:53:58 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 61/100, Acc=0.9009, Val Loss=0.3016, lr=0.0010
[02/24 01:54:26 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 62/100, Acc=0.9012, Val Loss=0.2989, lr=0.0010
[02/24 01:54:54 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 63/100, Acc=0.9021, Val Loss=0.2977, lr=0.0010
[02/24 01:55:23 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 64/100, Acc=0.9045, Val Loss=0.2967, lr=0.0010
[02/24 01:55:51 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 65/100, Acc=0.9011, Val Loss=0.2969, lr=0.0010
[02/24 01:56:19 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 66/100, Acc=0.8998, Val Loss=0.2988, lr=0.0010
[02/24 01:56:47 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 67/100, Acc=0.9033, Val Loss=0.2985, lr=0.0010
[02/24 01:57:15 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 68/100, Acc=0.9020, Val Loss=0.2989, lr=0.0010
[02/24 01:57:43 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 69/100, Acc=0.9014, Val Loss=0.3011, lr=0.0010
[02/24 01:58:11 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 70/100, Acc=0.9030, Val Loss=0.3007, lr=0.0010
[02/24 01:58:39 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 71/100, Acc=0.9017, Val Loss=0.3022, lr=0.0010
[02/24 01:59:06 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 72/100, Acc=0.8986, Val Loss=0.3101, lr=0.0010
[02/24 01:59:34 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 73/100, Acc=0.9016, Val Loss=0.3032, lr=0.0010
[02/24 02:00:02 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 74/100, Acc=0.9008, Val Loss=0.3024, lr=0.0010
[02/24 02:00:30 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 75/100, Acc=0.9005, Val Loss=0.3065, lr=0.0010
[02/24 02:00:57 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 76/100, Acc=0.8992, Val Loss=0.3091, lr=0.0010
[02/24 02:01:25 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 77/100, Acc=0.8982, Val Loss=0.3103, lr=0.0010
[02/24 02:01:53 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 78/100, Acc=0.8993, Val Loss=0.3095, lr=0.0010
[02/24 02:02:21 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 79/100, Acc=0.8995, Val Loss=0.3087, lr=0.0010
[02/24 02:02:49 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 80/100, Acc=0.9002, Val Loss=0.3064, lr=0.0001
[02/24 02:03:16 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 81/100, Acc=0.9002, Val Loss=0.3053, lr=0.0001
[02/24 02:03:44 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 82/100, Acc=0.9022, Val Loss=0.3029, lr=0.0001
[02/24 02:04:12 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 83/100, Acc=0.9018, Val Loss=0.3070, lr=0.0001
[02/24 02:04:40 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 84/100, Acc=0.9020, Val Loss=0.3034, lr=0.0001
[02/24 02:05:08 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 85/100, Acc=0.9022, Val Loss=0.3041, lr=0.0001
[02/24 02:05:36 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 86/100, Acc=0.9012, Val Loss=0.3048, lr=0.0001
[02/24 02:06:04 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 87/100, Acc=0.9023, Val Loss=0.3035, lr=0.0001
[02/24 02:06:31 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 88/100, Acc=0.9015, Val Loss=0.3045, lr=0.0001
[02/24 02:06:59 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 89/100, Acc=0.9022, Val Loss=0.3029, lr=0.0001
[02/24 02:07:27 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 90/100, Acc=0.9021, Val Loss=0.3027, lr=0.0001
[02/24 02:07:55 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 91/100, Acc=0.9023, Val Loss=0.3064, lr=0.0001
[02/24 02:08:23 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 92/100, Acc=0.9027, Val Loss=0.3031, lr=0.0001
[02/24 02:08:50 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 93/100, Acc=0.9023, Val Loss=0.3031, lr=0.0001
[02/24 02:09:18 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 94/100, Acc=0.9036, Val Loss=0.3037, lr=0.0001
[02/24 02:09:46 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 95/100, Acc=0.9016, Val Loss=0.3043, lr=0.0001
[02/24 02:10:14 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 96/100, Acc=0.9020, Val Loss=0.3046, lr=0.0001
[02/24 02:10:42 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 97/100, Acc=0.9017, Val Loss=0.3038, lr=0.0001
[02/24 02:11:10 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 98/100, Acc=0.9021, Val Loss=0.3046, lr=0.0001
[02/24 02:11:38 cifar10-global-group_sl-2.0-mobilenetv2]: Epoch 99/100, Acc=0.9005, Val Loss=0.3034, lr=0.0001
[02/24 02:11:38 cifar10-global-group_sl-2.0-mobilenetv2]: Best Acc=0.9045
[02/24 02:11:38 cifar10-global-group_sl-2.0-mobilenetv2]: Params: 1.12 M
[02/24 02:11:38 cifar10-global-group_sl-2.0-mobilenetv2]: ops: 34.02 M
[02/24 02:11:41 cifar10-global-group_sl-2.0-mobilenetv2]: Acc: 0.9005 Val Loss: 0.3034

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: mode: prune
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: model: mobilenetv2
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: verbose: False
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: dataset: cifar10
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: dataroot: data
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: batch_size: 128
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: total_epochs: 100
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: lr: 0.01
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: restore: run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: output_dir: run/cifar10/prune/cifar10-global-proj-2.0-mobilenetv2
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: finetune: True
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: last_epochs: 100
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: reps: 1
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: method: proj
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: speed_up: 2.0
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: reg: 1e-05
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: delta_reg: 0.0001
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: weight_decay: 0.0005
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: seed: 1
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: global_pruning: True
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: sl_total_epochs: 100
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: sl_lr: 0.01
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: sl_reg_warmup: 0
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: sl_restore: None
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: iterative_steps: 400
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: logger: <Logger cifar10-global-proj-2.0-mobilenetv2 (DEBUG)>
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: device: cuda
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: num_classes: 10
[02/24 02:11:52 cifar10-global-proj-2.0-mobilenetv2]: Loading model from run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/24 02:11:56 cifar10-global-proj-2.0-mobilenetv2]: Pruning...
[02/24 02:22:40 cifar10-global-proj-2.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 7, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(7, 17, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(17, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=17)
      (4): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(17, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 73, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(73, 73, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=73)
        (4): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(73, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 84, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(84, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=84)
        (4): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(84, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 57, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(57, 57, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=57)
        (4): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(57, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 88, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88)
        (4): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(88, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 51, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(51, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=51)
        (4): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(51, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 48, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48)
        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 97, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(97, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=97)
        (4): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(97, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 54, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(54, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=54)
        (4): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(54, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 37, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(37, 37, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=37)
        (4): BatchNorm2d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(37, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 95, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(95, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(95, 95, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=95)
        (4): BatchNorm2d(95, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(95, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 235, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(235, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(235, 235, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=235)
        (4): BatchNorm2d(235, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(235, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 16, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 5, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=5)
        (4): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(5, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 632, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(632, 632, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=632)
      (4): BatchNorm2d(632, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(632, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 10, kernel_size=(1, 1), stride=(1, 1))
)
[02/24 02:22:43 cifar10-global-proj-2.0-mobilenetv2]: Params: 2.25 M => 1.13 M (49.98%)
[02/24 02:22:43 cifar10-global-proj-2.0-mobilenetv2]: FLOPs: 68.29 M => 34.06 M (49.88%, 2.00X )
[02/24 02:22:43 cifar10-global-proj-2.0-mobilenetv2]: Acc: 0.8936 => 0.8936
[02/24 02:22:43 cifar10-global-proj-2.0-mobilenetv2]: Val Loss: 0.3202 => 0.3202
[02/24 02:22:43 cifar10-global-proj-2.0-mobilenetv2]: Finetuning...
[02/24 02:23:10 cifar10-global-proj-2.0-mobilenetv2]: Epoch 0/100, Acc=0.8591, Val Loss=0.4112, lr=0.0100
[02/24 02:23:38 cifar10-global-proj-2.0-mobilenetv2]: Epoch 1/100, Acc=0.8316, Val Loss=0.4957, lr=0.0100
[02/24 02:24:06 cifar10-global-proj-2.0-mobilenetv2]: Epoch 2/100, Acc=0.8452, Val Loss=0.4597, lr=0.0100
[02/24 02:24:34 cifar10-global-proj-2.0-mobilenetv2]: Epoch 3/100, Acc=0.8526, Val Loss=0.4221, lr=0.0100
[02/24 02:25:01 cifar10-global-proj-2.0-mobilenetv2]: Epoch 4/100, Acc=0.8623, Val Loss=0.4005, lr=0.0100
[02/24 02:25:29 cifar10-global-proj-2.0-mobilenetv2]: Epoch 5/100, Acc=0.8456, Val Loss=0.4500, lr=0.0100
[02/24 02:25:57 cifar10-global-proj-2.0-mobilenetv2]: Epoch 6/100, Acc=0.8545, Val Loss=0.4357, lr=0.0100
[02/24 02:26:25 cifar10-global-proj-2.0-mobilenetv2]: Epoch 7/100, Acc=0.8411, Val Loss=0.4802, lr=0.0100
[02/24 02:26:53 cifar10-global-proj-2.0-mobilenetv2]: Epoch 8/100, Acc=0.8532, Val Loss=0.4475, lr=0.0100
[02/24 02:27:20 cifar10-global-proj-2.0-mobilenetv2]: Epoch 9/100, Acc=0.8511, Val Loss=0.4389, lr=0.0100
[02/24 02:27:48 cifar10-global-proj-2.0-mobilenetv2]: Epoch 10/100, Acc=0.8468, Val Loss=0.4686, lr=0.0100
[02/24 02:28:16 cifar10-global-proj-2.0-mobilenetv2]: Epoch 11/100, Acc=0.8518, Val Loss=0.4347, lr=0.0100
[02/24 02:28:44 cifar10-global-proj-2.0-mobilenetv2]: Epoch 12/100, Acc=0.8653, Val Loss=0.4004, lr=0.0100
[02/24 02:29:13 cifar10-global-proj-2.0-mobilenetv2]: Epoch 13/100, Acc=0.8525, Val Loss=0.4233, lr=0.0100
[02/24 02:29:41 cifar10-global-proj-2.0-mobilenetv2]: Epoch 14/100, Acc=0.8425, Val Loss=0.4659, lr=0.0100
[02/24 02:30:09 cifar10-global-proj-2.0-mobilenetv2]: Epoch 15/100, Acc=0.8604, Val Loss=0.4180, lr=0.0100
[02/24 02:30:37 cifar10-global-proj-2.0-mobilenetv2]: Epoch 16/100, Acc=0.8469, Val Loss=0.4507, lr=0.0100
[02/24 02:31:05 cifar10-global-proj-2.0-mobilenetv2]: Epoch 17/100, Acc=0.8519, Val Loss=0.4413, lr=0.0100
[02/24 02:31:33 cifar10-global-proj-2.0-mobilenetv2]: Epoch 18/100, Acc=0.8596, Val Loss=0.4197, lr=0.0100
[02/24 02:32:02 cifar10-global-proj-2.0-mobilenetv2]: Epoch 19/100, Acc=0.8514, Val Loss=0.4401, lr=0.0100
[02/24 02:32:30 cifar10-global-proj-2.0-mobilenetv2]: Epoch 20/100, Acc=0.8532, Val Loss=0.4462, lr=0.0100
[02/24 02:32:58 cifar10-global-proj-2.0-mobilenetv2]: Epoch 21/100, Acc=0.8556, Val Loss=0.4293, lr=0.0100
[02/24 02:33:26 cifar10-global-proj-2.0-mobilenetv2]: Epoch 22/100, Acc=0.8579, Val Loss=0.4166, lr=0.0100
[02/24 02:33:53 cifar10-global-proj-2.0-mobilenetv2]: Epoch 23/100, Acc=0.8605, Val Loss=0.4168, lr=0.0100
[02/24 02:34:21 cifar10-global-proj-2.0-mobilenetv2]: Epoch 24/100, Acc=0.8622, Val Loss=0.4108, lr=0.0100
[02/24 02:34:49 cifar10-global-proj-2.0-mobilenetv2]: Epoch 25/100, Acc=0.8647, Val Loss=0.4043, lr=0.0100
[02/24 02:35:17 cifar10-global-proj-2.0-mobilenetv2]: Epoch 26/100, Acc=0.8479, Val Loss=0.4507, lr=0.0100
[02/24 02:35:45 cifar10-global-proj-2.0-mobilenetv2]: Epoch 27/100, Acc=0.8540, Val Loss=0.4361, lr=0.0100
[02/24 02:36:13 cifar10-global-proj-2.0-mobilenetv2]: Epoch 28/100, Acc=0.8585, Val Loss=0.4128, lr=0.0100
[02/24 02:36:41 cifar10-global-proj-2.0-mobilenetv2]: Epoch 29/100, Acc=0.8261, Val Loss=0.5169, lr=0.0100
[02/24 02:37:09 cifar10-global-proj-2.0-mobilenetv2]: Epoch 30/100, Acc=0.8559, Val Loss=0.4146, lr=0.0100
[02/24 02:37:36 cifar10-global-proj-2.0-mobilenetv2]: Epoch 31/100, Acc=0.8511, Val Loss=0.4317, lr=0.0100
[02/24 02:38:04 cifar10-global-proj-2.0-mobilenetv2]: Epoch 32/100, Acc=0.8334, Val Loss=0.4979, lr=0.0100
[02/24 02:38:32 cifar10-global-proj-2.0-mobilenetv2]: Epoch 33/100, Acc=0.8374, Val Loss=0.4848, lr=0.0100
[02/24 02:39:00 cifar10-global-proj-2.0-mobilenetv2]: Epoch 34/100, Acc=0.8550, Val Loss=0.4312, lr=0.0100
[02/24 02:39:28 cifar10-global-proj-2.0-mobilenetv2]: Epoch 35/100, Acc=0.8539, Val Loss=0.4254, lr=0.0100
[02/24 02:39:56 cifar10-global-proj-2.0-mobilenetv2]: Epoch 36/100, Acc=0.8552, Val Loss=0.4343, lr=0.0100
[02/24 02:40:24 cifar10-global-proj-2.0-mobilenetv2]: Epoch 37/100, Acc=0.8545, Val Loss=0.4432, lr=0.0100
[02/24 02:40:52 cifar10-global-proj-2.0-mobilenetv2]: Epoch 38/100, Acc=0.8604, Val Loss=0.4194, lr=0.0100
[02/24 02:41:21 cifar10-global-proj-2.0-mobilenetv2]: Epoch 39/100, Acc=0.8581, Val Loss=0.4193, lr=0.0100
[02/24 02:41:49 cifar10-global-proj-2.0-mobilenetv2]: Epoch 40/100, Acc=0.8516, Val Loss=0.4409, lr=0.0100
[02/24 02:42:18 cifar10-global-proj-2.0-mobilenetv2]: Epoch 41/100, Acc=0.8643, Val Loss=0.4093, lr=0.0100
[02/24 02:42:46 cifar10-global-proj-2.0-mobilenetv2]: Epoch 42/100, Acc=0.8518, Val Loss=0.4517, lr=0.0100
[02/24 02:43:14 cifar10-global-proj-2.0-mobilenetv2]: Epoch 43/100, Acc=0.8592, Val Loss=0.4206, lr=0.0100
[02/24 02:43:42 cifar10-global-proj-2.0-mobilenetv2]: Epoch 44/100, Acc=0.8659, Val Loss=0.3985, lr=0.0100
[02/24 02:44:11 cifar10-global-proj-2.0-mobilenetv2]: Epoch 45/100, Acc=0.8530, Val Loss=0.4364, lr=0.0100
[02/24 02:44:39 cifar10-global-proj-2.0-mobilenetv2]: Epoch 46/100, Acc=0.8443, Val Loss=0.4731, lr=0.0100
[02/24 02:45:07 cifar10-global-proj-2.0-mobilenetv2]: Epoch 47/100, Acc=0.8628, Val Loss=0.3962, lr=0.0100
[02/24 02:45:36 cifar10-global-proj-2.0-mobilenetv2]: Epoch 48/100, Acc=0.8569, Val Loss=0.4193, lr=0.0100
[02/24 02:46:04 cifar10-global-proj-2.0-mobilenetv2]: Epoch 49/100, Acc=0.8559, Val Loss=0.4376, lr=0.0100
[02/24 02:46:32 cifar10-global-proj-2.0-mobilenetv2]: Epoch 50/100, Acc=0.8631, Val Loss=0.4043, lr=0.0100
[02/24 02:47:01 cifar10-global-proj-2.0-mobilenetv2]: Epoch 51/100, Acc=0.8497, Val Loss=0.4424, lr=0.0100
[02/24 02:47:29 cifar10-global-proj-2.0-mobilenetv2]: Epoch 52/100, Acc=0.8531, Val Loss=0.4333, lr=0.0100
[02/24 02:47:57 cifar10-global-proj-2.0-mobilenetv2]: Epoch 53/100, Acc=0.8610, Val Loss=0.4289, lr=0.0100
[02/24 02:48:26 cifar10-global-proj-2.0-mobilenetv2]: Epoch 54/100, Acc=0.8486, Val Loss=0.4552, lr=0.0100
[02/24 02:48:54 cifar10-global-proj-2.0-mobilenetv2]: Epoch 55/100, Acc=0.8580, Val Loss=0.4305, lr=0.0100
[02/24 02:49:22 cifar10-global-proj-2.0-mobilenetv2]: Epoch 56/100, Acc=0.8612, Val Loss=0.4137, lr=0.0100
[02/24 02:49:51 cifar10-global-proj-2.0-mobilenetv2]: Epoch 57/100, Acc=0.8590, Val Loss=0.4057, lr=0.0100
[02/24 02:50:19 cifar10-global-proj-2.0-mobilenetv2]: Epoch 58/100, Acc=0.8589, Val Loss=0.4343, lr=0.0100
[02/24 02:50:48 cifar10-global-proj-2.0-mobilenetv2]: Epoch 59/100, Acc=0.8635, Val Loss=0.4058, lr=0.0100
[02/24 02:51:16 cifar10-global-proj-2.0-mobilenetv2]: Epoch 60/100, Acc=0.8966, Val Loss=0.3159, lr=0.0010
[02/24 02:51:45 cifar10-global-proj-2.0-mobilenetv2]: Epoch 61/100, Acc=0.8964, Val Loss=0.3104, lr=0.0010
[02/24 02:52:13 cifar10-global-proj-2.0-mobilenetv2]: Epoch 62/100, Acc=0.8995, Val Loss=0.3085, lr=0.0010
[02/24 02:52:41 cifar10-global-proj-2.0-mobilenetv2]: Epoch 63/100, Acc=0.8978, Val Loss=0.3098, lr=0.0010
[02/24 02:53:10 cifar10-global-proj-2.0-mobilenetv2]: Epoch 64/100, Acc=0.8989, Val Loss=0.3077, lr=0.0010
[02/24 02:53:38 cifar10-global-proj-2.0-mobilenetv2]: Epoch 65/100, Acc=0.8990, Val Loss=0.3087, lr=0.0010
[02/24 02:54:06 cifar10-global-proj-2.0-mobilenetv2]: Epoch 66/100, Acc=0.9001, Val Loss=0.3053, lr=0.0010
[02/24 02:54:35 cifar10-global-proj-2.0-mobilenetv2]: Epoch 67/100, Acc=0.8996, Val Loss=0.3068, lr=0.0010
[02/24 02:55:03 cifar10-global-proj-2.0-mobilenetv2]: Epoch 68/100, Acc=0.9009, Val Loss=0.3080, lr=0.0010
[02/24 02:55:31 cifar10-global-proj-2.0-mobilenetv2]: Epoch 69/100, Acc=0.9038, Val Loss=0.3058, lr=0.0010
[02/24 02:56:00 cifar10-global-proj-2.0-mobilenetv2]: Epoch 70/100, Acc=0.8991, Val Loss=0.3117, lr=0.0010
[02/24 02:56:28 cifar10-global-proj-2.0-mobilenetv2]: Epoch 71/100, Acc=0.8991, Val Loss=0.3072, lr=0.0010
[02/24 02:56:56 cifar10-global-proj-2.0-mobilenetv2]: Epoch 72/100, Acc=0.8983, Val Loss=0.3102, lr=0.0010
[02/24 02:57:24 cifar10-global-proj-2.0-mobilenetv2]: Epoch 73/100, Acc=0.9007, Val Loss=0.3116, lr=0.0010
[02/24 02:57:52 cifar10-global-proj-2.0-mobilenetv2]: Epoch 74/100, Acc=0.9019, Val Loss=0.3069, lr=0.0010
[02/24 02:58:20 cifar10-global-proj-2.0-mobilenetv2]: Epoch 75/100, Acc=0.8974, Val Loss=0.3105, lr=0.0010
[02/24 02:58:48 cifar10-global-proj-2.0-mobilenetv2]: Epoch 76/100, Acc=0.8984, Val Loss=0.3109, lr=0.0010
[02/24 02:59:17 cifar10-global-proj-2.0-mobilenetv2]: Epoch 77/100, Acc=0.8992, Val Loss=0.3130, lr=0.0010
[02/24 02:59:45 cifar10-global-proj-2.0-mobilenetv2]: Epoch 78/100, Acc=0.8998, Val Loss=0.3065, lr=0.0010
[02/24 03:00:13 cifar10-global-proj-2.0-mobilenetv2]: Epoch 79/100, Acc=0.8988, Val Loss=0.3098, lr=0.0010
[02/24 03:00:41 cifar10-global-proj-2.0-mobilenetv2]: Epoch 80/100, Acc=0.9007, Val Loss=0.3050, lr=0.0001
[02/24 03:01:09 cifar10-global-proj-2.0-mobilenetv2]: Epoch 81/100, Acc=0.9011, Val Loss=0.3051, lr=0.0001
[02/24 03:01:38 cifar10-global-proj-2.0-mobilenetv2]: Epoch 82/100, Acc=0.9008, Val Loss=0.3057, lr=0.0001
[02/24 03:02:06 cifar10-global-proj-2.0-mobilenetv2]: Epoch 83/100, Acc=0.9012, Val Loss=0.3056, lr=0.0001
[02/24 03:02:34 cifar10-global-proj-2.0-mobilenetv2]: Epoch 84/100, Acc=0.9015, Val Loss=0.3056, lr=0.0001
[02/24 03:03:02 cifar10-global-proj-2.0-mobilenetv2]: Epoch 85/100, Acc=0.9014, Val Loss=0.3044, lr=0.0001
[02/24 03:03:31 cifar10-global-proj-2.0-mobilenetv2]: Epoch 86/100, Acc=0.9020, Val Loss=0.3063, lr=0.0001
[02/24 03:03:59 cifar10-global-proj-2.0-mobilenetv2]: Epoch 87/100, Acc=0.9026, Val Loss=0.3039, lr=0.0001
[02/24 03:04:27 cifar10-global-proj-2.0-mobilenetv2]: Epoch 88/100, Acc=0.9031, Val Loss=0.3036, lr=0.0001
[02/24 03:04:55 cifar10-global-proj-2.0-mobilenetv2]: Epoch 89/100, Acc=0.9030, Val Loss=0.3039, lr=0.0001
[02/24 03:05:24 cifar10-global-proj-2.0-mobilenetv2]: Epoch 90/100, Acc=0.9003, Val Loss=0.3061, lr=0.0001
[02/24 03:05:52 cifar10-global-proj-2.0-mobilenetv2]: Epoch 91/100, Acc=0.9015, Val Loss=0.3045, lr=0.0001
[02/24 03:06:20 cifar10-global-proj-2.0-mobilenetv2]: Epoch 92/100, Acc=0.9020, Val Loss=0.3024, lr=0.0001
[02/24 03:06:49 cifar10-global-proj-2.0-mobilenetv2]: Epoch 93/100, Acc=0.9016, Val Loss=0.3052, lr=0.0001
[02/24 03:07:17 cifar10-global-proj-2.0-mobilenetv2]: Epoch 94/100, Acc=0.9020, Val Loss=0.3041, lr=0.0001
[02/24 03:07:45 cifar10-global-proj-2.0-mobilenetv2]: Epoch 95/100, Acc=0.9011, Val Loss=0.3054, lr=0.0001
[02/24 03:08:14 cifar10-global-proj-2.0-mobilenetv2]: Epoch 96/100, Acc=0.9019, Val Loss=0.3045, lr=0.0001
[02/24 03:08:42 cifar10-global-proj-2.0-mobilenetv2]: Epoch 97/100, Acc=0.9027, Val Loss=0.3047, lr=0.0001
[02/24 03:09:10 cifar10-global-proj-2.0-mobilenetv2]: Epoch 98/100, Acc=0.9014, Val Loss=0.3047, lr=0.0001
[02/24 03:09:39 cifar10-global-proj-2.0-mobilenetv2]: Epoch 99/100, Acc=0.9033, Val Loss=0.3044, lr=0.0001
[02/24 03:09:39 cifar10-global-proj-2.0-mobilenetv2]: Best Acc=0.9038
[02/24 03:09:39 cifar10-global-proj-2.0-mobilenetv2]: Params: 1.13 M
[02/24 03:09:39 cifar10-global-proj-2.0-mobilenetv2]: ops: 34.06 M
[02/24 03:09:41 cifar10-global-proj-2.0-mobilenetv2]: Acc: 0.9033 Val Loss: 0.3044

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: mode: prune
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: model: mobilenetv2
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: verbose: False
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: dataset: cifar10
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: dataroot: data
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: batch_size: 128
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: total_epochs: 100
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: lr: 0.01
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: restore: run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: output_dir: run/cifar10/prune/cifar10-global-proj_sl-2.0-mobilenetv2
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: finetune: True
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: last_epochs: 100
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: reps: 1
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: method: proj_sl
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: speed_up: 2.0
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: reg: 1e-05
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: delta_reg: 0.0001
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: weight_decay: 0.0005
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: seed: 1
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: global_pruning: True
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: sl_total_epochs: 100
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: sl_lr: 0.01
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: sl_reg_warmup: 0
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: sl_restore: None
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: iterative_steps: 400
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: logger: <Logger cifar10-global-proj_sl-2.0-mobilenetv2 (DEBUG)>
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: device: cuda
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: num_classes: 10
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: Loading model from run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/24 03:09:50 cifar10-global-proj_sl-2.0-mobilenetv2]: Regularizing...
[02/24 03:38:21 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 0/100, Acc=0.8518, Val Loss=0.4478, lr=0.0100
[02/24 04:06:47 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 1/100, Acc=0.8550, Val Loss=0.4276, lr=0.0100
[02/24 04:35:16 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 2/100, Acc=0.8605, Val Loss=0.4009, lr=0.0100
[02/24 05:03:39 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 3/100, Acc=0.8516, Val Loss=0.4386, lr=0.0100
[02/24 05:32:05 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 4/100, Acc=0.8632, Val Loss=0.4100, lr=0.0100
[02/24 06:00:28 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 5/100, Acc=0.8640, Val Loss=0.4035, lr=0.0100
[02/24 06:28:50 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 6/100, Acc=0.8572, Val Loss=0.4162, lr=0.0100
[02/24 06:57:17 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 7/100, Acc=0.8581, Val Loss=0.4197, lr=0.0100
[02/24 07:25:43 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 8/100, Acc=0.8535, Val Loss=0.4431, lr=0.0100
[02/24 07:54:07 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 9/100, Acc=0.8683, Val Loss=0.3910, lr=0.0100
[02/24 08:22:32 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 10/100, Acc=0.8624, Val Loss=0.4151, lr=0.0100
[02/24 08:51:01 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 11/100, Acc=0.8649, Val Loss=0.3976, lr=0.0100
[02/24 09:19:32 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 12/100, Acc=0.8665, Val Loss=0.3956, lr=0.0100
[02/24 09:48:01 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 13/100, Acc=0.8667, Val Loss=0.3954, lr=0.0100
[02/24 10:16:27 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 14/100, Acc=0.8683, Val Loss=0.3901, lr=0.0100
[02/24 10:44:55 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 15/100, Acc=0.8548, Val Loss=0.4293, lr=0.0100
[02/24 11:13:21 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 16/100, Acc=0.8501, Val Loss=0.4543, lr=0.0100
[02/24 11:41:44 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 17/100, Acc=0.8582, Val Loss=0.4395, lr=0.0100
[02/24 12:10:12 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 18/100, Acc=0.8684, Val Loss=0.4055, lr=0.0100
[02/24 12:38:38 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 19/100, Acc=0.8683, Val Loss=0.3992, lr=0.0100
[02/24 13:07:02 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 20/100, Acc=0.8721, Val Loss=0.3831, lr=0.0100
[02/24 13:35:20 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 21/100, Acc=0.8640, Val Loss=0.4123, lr=0.0100
[02/24 14:03:36 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 22/100, Acc=0.8702, Val Loss=0.3939, lr=0.0100
[02/24 14:31:57 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 23/100, Acc=0.8738, Val Loss=0.3798, lr=0.0100
[02/24 15:00:12 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 24/100, Acc=0.8645, Val Loss=0.4123, lr=0.0100
[02/24 15:28:28 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 25/100, Acc=0.8747, Val Loss=0.3795, lr=0.0100
[02/24 15:56:49 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 26/100, Acc=0.8677, Val Loss=0.4071, lr=0.0100
[02/24 16:25:07 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 27/100, Acc=0.8765, Val Loss=0.3730, lr=0.0100
[02/24 16:53:25 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 28/100, Acc=0.8746, Val Loss=0.3788, lr=0.0100
[02/24 17:21:37 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 29/100, Acc=0.8722, Val Loss=0.3877, lr=0.0100
[02/24 17:49:57 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 30/100, Acc=0.8766, Val Loss=0.3711, lr=0.0100
[02/24 18:18:15 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 31/100, Acc=0.8664, Val Loss=0.4152, lr=0.0100
[02/24 18:46:28 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 32/100, Acc=0.8780, Val Loss=0.3767, lr=0.0100
[02/24 19:14:42 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 33/100, Acc=0.8722, Val Loss=0.3884, lr=0.0100
[02/24 19:42:52 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 34/100, Acc=0.8667, Val Loss=0.4123, lr=0.0100
[02/24 20:11:07 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 35/100, Acc=0.8727, Val Loss=0.3816, lr=0.0100
[02/24 20:39:24 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 36/100, Acc=0.8804, Val Loss=0.3692, lr=0.0100
[02/24 21:07:42 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 37/100, Acc=0.8807, Val Loss=0.3693, lr=0.0100
[02/24 21:36:01 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 38/100, Acc=0.8788, Val Loss=0.3716, lr=0.0100
[02/24 22:04:27 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 39/100, Acc=0.8741, Val Loss=0.3862, lr=0.0100
[02/24 22:32:48 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 40/100, Acc=0.8734, Val Loss=0.3851, lr=0.0100
[02/24 23:01:09 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 41/100, Acc=0.8774, Val Loss=0.3672, lr=0.0100
[02/24 23:29:24 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 42/100, Acc=0.8789, Val Loss=0.3652, lr=0.0100
[02/24 23:57:37 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 43/100, Acc=0.8709, Val Loss=0.3840, lr=0.0100
[02/25 00:25:52 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 44/100, Acc=0.8837, Val Loss=0.3606, lr=0.0100
[02/25 00:54:10 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 45/100, Acc=0.8807, Val Loss=0.3645, lr=0.0100
[02/25 01:22:29 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 46/100, Acc=0.8762, Val Loss=0.3848, lr=0.0100
[02/25 01:50:44 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 47/100, Acc=0.8724, Val Loss=0.3794, lr=0.0100
[02/25 02:19:02 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 48/100, Acc=0.8757, Val Loss=0.3825, lr=0.0100
[02/25 02:47:21 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 49/100, Acc=0.8858, Val Loss=0.3592, lr=0.0100
[02/25 03:15:38 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 50/100, Acc=0.8772, Val Loss=0.3782, lr=0.0100
[02/25 03:43:55 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 51/100, Acc=0.8846, Val Loss=0.3677, lr=0.0100
[02/25 04:12:12 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 52/100, Acc=0.8786, Val Loss=0.3766, lr=0.0100
[02/25 04:40:33 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 53/100, Acc=0.8738, Val Loss=0.3859, lr=0.0100
[02/25 05:08:52 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 54/100, Acc=0.8746, Val Loss=0.3792, lr=0.0100
[02/25 05:37:14 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 55/100, Acc=0.8823, Val Loss=0.3587, lr=0.0100
[02/25 06:05:32 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 56/100, Acc=0.8838, Val Loss=0.3566, lr=0.0100
[02/25 06:33:52 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 57/100, Acc=0.8859, Val Loss=0.3551, lr=0.0100
[02/25 07:02:15 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 58/100, Acc=0.8818, Val Loss=0.3676, lr=0.0100
[02/25 07:30:30 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 59/100, Acc=0.8799, Val Loss=0.3767, lr=0.0100
[02/25 07:58:43 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 60/100, Acc=0.8934, Val Loss=0.3273, lr=0.0010
[02/25 08:26:56 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 61/100, Acc=0.8946, Val Loss=0.3224, lr=0.0010
[02/25 08:55:12 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 62/100, Acc=0.8947, Val Loss=0.3219, lr=0.0010
[02/25 09:23:31 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 63/100, Acc=0.8952, Val Loss=0.3239, lr=0.0010
[02/25 09:51:44 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 64/100, Acc=0.8965, Val Loss=0.3232, lr=0.0010
[02/25 10:19:56 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 65/100, Acc=0.8955, Val Loss=0.3233, lr=0.0010
[02/25 10:48:10 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 66/100, Acc=0.8963, Val Loss=0.3235, lr=0.0010
[02/25 11:16:24 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 67/100, Acc=0.8949, Val Loss=0.3204, lr=0.0010
[02/25 11:44:36 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 68/100, Acc=0.8955, Val Loss=0.3221, lr=0.0010
[02/25 12:12:55 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 69/100, Acc=0.8960, Val Loss=0.3234, lr=0.0010
[02/25 12:41:16 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 70/100, Acc=0.8945, Val Loss=0.3235, lr=0.0010
[02/25 13:09:39 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 71/100, Acc=0.8957, Val Loss=0.3259, lr=0.0010
[02/25 13:37:56 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 72/100, Acc=0.8951, Val Loss=0.3254, lr=0.0010
[02/25 14:06:29 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 73/100, Acc=0.8945, Val Loss=0.3273, lr=0.0010
[02/25 14:34:47 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 74/100, Acc=0.8933, Val Loss=0.3257, lr=0.0010
[02/25 15:03:04 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 75/100, Acc=0.8973, Val Loss=0.3281, lr=0.0010
[02/25 15:31:25 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 76/100, Acc=0.8949, Val Loss=0.3289, lr=0.0010
[02/25 15:59:43 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 77/100, Acc=0.8947, Val Loss=0.3307, lr=0.0010
[02/25 16:28:04 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 78/100, Acc=0.8946, Val Loss=0.3344, lr=0.0010
[02/25 16:56:21 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 79/100, Acc=0.8938, Val Loss=0.3309, lr=0.0010
[02/25 17:24:38 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 80/100, Acc=0.8939, Val Loss=0.3260, lr=0.0001
[02/25 17:52:53 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 81/100, Acc=0.8945, Val Loss=0.3282, lr=0.0001
[02/25 18:21:11 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 82/100, Acc=0.8954, Val Loss=0.3264, lr=0.0001
[02/25 18:49:29 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 83/100, Acc=0.8945, Val Loss=0.3292, lr=0.0001
[02/25 19:17:51 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 84/100, Acc=0.8955, Val Loss=0.3286, lr=0.0001
[02/25 19:46:14 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 85/100, Acc=0.8954, Val Loss=0.3265, lr=0.0001
[02/25 20:14:30 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 86/100, Acc=0.8948, Val Loss=0.3277, lr=0.0001
[02/25 20:42:52 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 87/100, Acc=0.8941, Val Loss=0.3278, lr=0.0001
[02/25 21:11:06 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 88/100, Acc=0.8950, Val Loss=0.3264, lr=0.0001
[02/25 21:39:49 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 89/100, Acc=0.8952, Val Loss=0.3270, lr=0.0001
[02/25 22:08:07 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 90/100, Acc=0.8952, Val Loss=0.3275, lr=0.0001
[02/25 22:36:24 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 91/100, Acc=0.8948, Val Loss=0.3281, lr=0.0001
[02/25 23:04:40 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 92/100, Acc=0.8951, Val Loss=0.3291, lr=0.0001
[02/25 23:32:59 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 93/100, Acc=0.8963, Val Loss=0.3277, lr=0.0001
[02/26 00:01:17 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 94/100, Acc=0.8944, Val Loss=0.3282, lr=0.0001
[02/26 00:29:38 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 95/100, Acc=0.8951, Val Loss=0.3288, lr=0.0001
[02/26 00:58:00 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 96/100, Acc=0.8964, Val Loss=0.3286, lr=0.0001
[02/26 01:26:16 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 97/100, Acc=0.8951, Val Loss=0.3275, lr=0.0001
[02/26 01:54:31 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 98/100, Acc=0.8958, Val Loss=0.3286, lr=0.0001
[02/26 02:22:54 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 99/100, Acc=0.8947, Val Loss=0.3291, lr=0.0001
[02/26 02:22:54 cifar10-global-proj_sl-2.0-mobilenetv2]: Best Acc=0.8973
[02/26 02:22:54 cifar10-global-proj_sl-2.0-mobilenetv2]: Loading the sparse model from run/cifar10/prune/cifar10-global-proj_sl-2.0-mobilenetv2/reg_cifar10_mobilenetv2_proj_sl_1e-05.pth...
[02/26 02:22:57 cifar10-global-proj_sl-2.0-mobilenetv2]: Pruning...
[02/26 02:34:06 cifar10-global-proj_sl-2.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 8, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(8, 17, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(17, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=17)
      (4): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(17, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 73, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(73, 73, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=73)
        (4): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(73, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 87, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(87, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(87, 87, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=87)
        (4): BatchNorm2d(87, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(87, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 61, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(61, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=61)
        (4): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(61, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 88, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88)
        (4): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(88, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 55, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=55)
        (4): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(55, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 51, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(51, 51, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=51)
        (4): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(51, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 100, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=100)
        (4): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(100, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 62, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=62)
        (4): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(62, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 38, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(38, 38, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=38)
        (4): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(38, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 101, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(101, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(101, 101, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=101)
        (4): BatchNorm2d(101, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(101, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 498, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(498, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(498, 498, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=498)
        (4): BatchNorm2d(498, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(498, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 249, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(249, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(249, 249, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=249)
        (4): BatchNorm2d(249, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(249, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 16, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 5, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=5)
        (4): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(5, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 640, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)
      (4): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 10, kernel_size=(1, 1), stride=(1, 1))
)
[02/26 02:34:09 cifar10-global-proj_sl-2.0-mobilenetv2]: Params: 2.25 M => 1.12 M (49.78%)
[02/26 02:34:09 cifar10-global-proj_sl-2.0-mobilenetv2]: FLOPs: 68.29 M => 34.09 M (49.92%, 2.00X )
[02/26 02:34:09 cifar10-global-proj_sl-2.0-mobilenetv2]: Acc: 0.8973 => 0.8974
[02/26 02:34:09 cifar10-global-proj_sl-2.0-mobilenetv2]: Val Loss: 0.3281 => 0.3281
[02/26 02:34:09 cifar10-global-proj_sl-2.0-mobilenetv2]: Finetuning...
[02/26 02:34:37 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 0/100, Acc=0.8800, Val Loss=0.3700, lr=0.0100
[02/26 02:35:05 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 1/100, Acc=0.8812, Val Loss=0.3553, lr=0.0100
[02/26 02:35:33 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 2/100, Acc=0.8780, Val Loss=0.3656, lr=0.0100
[02/26 02:36:01 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 3/100, Acc=0.8786, Val Loss=0.3784, lr=0.0100
[02/26 02:36:28 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 4/100, Acc=0.8815, Val Loss=0.3521, lr=0.0100
[02/26 02:36:56 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 5/100, Acc=0.8760, Val Loss=0.3705, lr=0.0100
[02/26 02:37:24 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 6/100, Acc=0.8817, Val Loss=0.3521, lr=0.0100
[02/26 02:37:52 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 7/100, Acc=0.8792, Val Loss=0.3859, lr=0.0100
[02/26 02:38:20 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 8/100, Acc=0.8753, Val Loss=0.3781, lr=0.0100
[02/26 02:38:48 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 9/100, Acc=0.8716, Val Loss=0.4008, lr=0.0100
[02/26 02:39:16 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 10/100, Acc=0.8735, Val Loss=0.3797, lr=0.0100
[02/26 02:39:44 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 11/100, Acc=0.8722, Val Loss=0.3846, lr=0.0100
[02/26 02:40:12 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 12/100, Acc=0.8708, Val Loss=0.3949, lr=0.0100
[02/26 02:40:40 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 13/100, Acc=0.8728, Val Loss=0.3970, lr=0.0100
[02/26 02:41:08 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 14/100, Acc=0.8647, Val Loss=0.4108, lr=0.0100
[02/26 02:41:35 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 15/100, Acc=0.8605, Val Loss=0.4326, lr=0.0100
[02/26 02:42:03 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 16/100, Acc=0.8613, Val Loss=0.4281, lr=0.0100
[02/26 02:42:31 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 17/100, Acc=0.8699, Val Loss=0.3908, lr=0.0100
[02/26 02:42:59 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 18/100, Acc=0.8843, Val Loss=0.3579, lr=0.0100
[02/26 02:43:27 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 19/100, Acc=0.8696, Val Loss=0.4028, lr=0.0100
[02/26 02:43:55 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 20/100, Acc=0.8631, Val Loss=0.4058, lr=0.0100
[02/26 02:44:23 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 21/100, Acc=0.8664, Val Loss=0.4045, lr=0.0100
[02/26 02:44:51 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 22/100, Acc=0.8734, Val Loss=0.3762, lr=0.0100
[02/26 02:45:19 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 23/100, Acc=0.8391, Val Loss=0.4864, lr=0.0100
[02/26 02:45:46 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 24/100, Acc=0.8665, Val Loss=0.4150, lr=0.0100
[02/26 02:46:14 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 25/100, Acc=0.8760, Val Loss=0.3760, lr=0.0100
[02/26 02:46:42 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 26/100, Acc=0.8665, Val Loss=0.3936, lr=0.0100
[02/26 02:47:10 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 27/100, Acc=0.8736, Val Loss=0.3762, lr=0.0100
[02/26 02:47:38 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 28/100, Acc=0.8680, Val Loss=0.4083, lr=0.0100
[02/26 02:48:06 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 29/100, Acc=0.8602, Val Loss=0.3987, lr=0.0100
[02/26 02:48:34 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 30/100, Acc=0.8688, Val Loss=0.3943, lr=0.0100
[02/26 02:49:02 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 31/100, Acc=0.8700, Val Loss=0.3859, lr=0.0100
[02/26 02:49:30 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 32/100, Acc=0.8685, Val Loss=0.3835, lr=0.0100
[02/26 02:49:58 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 33/100, Acc=0.8234, Val Loss=0.5548, lr=0.0100
[02/26 02:50:26 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 34/100, Acc=0.8704, Val Loss=0.3857, lr=0.0100
[02/26 02:50:54 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 35/100, Acc=0.8572, Val Loss=0.4341, lr=0.0100
[02/26 02:51:22 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 36/100, Acc=0.8528, Val Loss=0.4473, lr=0.0100
[02/26 02:51:50 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 37/100, Acc=0.8455, Val Loss=0.4703, lr=0.0100
[02/26 02:52:18 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 38/100, Acc=0.8586, Val Loss=0.4204, lr=0.0100
[02/26 02:52:46 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 39/100, Acc=0.8657, Val Loss=0.3920, lr=0.0100
[02/26 02:53:14 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 40/100, Acc=0.8649, Val Loss=0.4108, lr=0.0100
[02/26 02:53:43 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 41/100, Acc=0.8679, Val Loss=0.3985, lr=0.0100
[02/26 02:54:11 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 42/100, Acc=0.8661, Val Loss=0.4082, lr=0.0100
[02/26 02:54:39 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 43/100, Acc=0.8655, Val Loss=0.3953, lr=0.0100
[02/26 02:55:07 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 44/100, Acc=0.8571, Val Loss=0.4333, lr=0.0100
[02/26 02:55:35 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 45/100, Acc=0.8688, Val Loss=0.4018, lr=0.0100
[02/26 02:56:04 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 46/100, Acc=0.8552, Val Loss=0.4359, lr=0.0100
[02/26 02:56:32 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 47/100, Acc=0.8532, Val Loss=0.4461, lr=0.0100
[02/26 02:57:00 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 48/100, Acc=0.8731, Val Loss=0.3872, lr=0.0100
[02/26 02:57:28 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 49/100, Acc=0.8648, Val Loss=0.4094, lr=0.0100
[02/26 02:57:56 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 50/100, Acc=0.8547, Val Loss=0.4349, lr=0.0100
[02/26 02:58:25 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 51/100, Acc=0.8554, Val Loss=0.4405, lr=0.0100
[02/26 02:58:53 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 52/100, Acc=0.8633, Val Loss=0.4021, lr=0.0100
[02/26 02:59:21 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 53/100, Acc=0.8466, Val Loss=0.4497, lr=0.0100
[02/26 02:59:49 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 54/100, Acc=0.8604, Val Loss=0.4273, lr=0.0100
[02/26 03:00:18 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 55/100, Acc=0.8533, Val Loss=0.4424, lr=0.0100
[02/26 03:00:46 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 56/100, Acc=0.8638, Val Loss=0.3964, lr=0.0100
[02/26 03:01:14 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 57/100, Acc=0.8698, Val Loss=0.3908, lr=0.0100
[02/26 03:01:42 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 58/100, Acc=0.8631, Val Loss=0.4019, lr=0.0100
[02/26 03:02:10 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 59/100, Acc=0.8544, Val Loss=0.4194, lr=0.0100
[02/26 03:02:39 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 60/100, Acc=0.8955, Val Loss=0.3101, lr=0.0010
[02/26 03:03:07 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 61/100, Acc=0.8968, Val Loss=0.3066, lr=0.0010
[02/26 03:03:35 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 62/100, Acc=0.8981, Val Loss=0.3014, lr=0.0010
[02/26 03:04:03 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 63/100, Acc=0.8942, Val Loss=0.3014, lr=0.0010
[02/26 03:04:31 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 64/100, Acc=0.8976, Val Loss=0.3003, lr=0.0010
[02/26 03:05:00 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 65/100, Acc=0.8974, Val Loss=0.3010, lr=0.0010
[02/26 03:05:28 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 66/100, Acc=0.8979, Val Loss=0.3025, lr=0.0010
[02/26 03:05:56 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 67/100, Acc=0.8968, Val Loss=0.3029, lr=0.0010
[02/26 03:06:24 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 68/100, Acc=0.8980, Val Loss=0.2995, lr=0.0010
[02/26 03:06:52 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 69/100, Acc=0.8958, Val Loss=0.3068, lr=0.0010
[02/26 03:07:20 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 70/100, Acc=0.8974, Val Loss=0.3067, lr=0.0010
[02/26 03:07:48 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 71/100, Acc=0.8988, Val Loss=0.3046, lr=0.0010
[02/26 03:08:16 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 72/100, Acc=0.8989, Val Loss=0.3045, lr=0.0010
[02/26 03:08:44 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 73/100, Acc=0.8982, Val Loss=0.3062, lr=0.0010
[02/26 03:09:12 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 74/100, Acc=0.8975, Val Loss=0.3060, lr=0.0010
[02/26 03:09:39 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 75/100, Acc=0.8973, Val Loss=0.3103, lr=0.0010
[02/26 03:10:07 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 76/100, Acc=0.8978, Val Loss=0.3085, lr=0.0010
[02/26 03:10:35 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 77/100, Acc=0.8970, Val Loss=0.3114, lr=0.0010
[02/26 03:11:03 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 78/100, Acc=0.8990, Val Loss=0.3073, lr=0.0010
[02/26 03:11:31 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 79/100, Acc=0.8998, Val Loss=0.3121, lr=0.0010
[02/26 03:11:58 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 80/100, Acc=0.8985, Val Loss=0.3098, lr=0.0001
[02/26 03:12:26 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 81/100, Acc=0.8995, Val Loss=0.3080, lr=0.0001
[02/26 03:12:54 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 82/100, Acc=0.9002, Val Loss=0.3070, lr=0.0001
[02/26 03:13:22 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 83/100, Acc=0.9010, Val Loss=0.3089, lr=0.0001
[02/26 03:13:49 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 84/100, Acc=0.9010, Val Loss=0.3066, lr=0.0001
[02/26 03:14:17 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 85/100, Acc=0.9005, Val Loss=0.3058, lr=0.0001
[02/26 03:14:44 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 86/100, Acc=0.9006, Val Loss=0.3067, lr=0.0001
[02/26 03:15:12 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 87/100, Acc=0.9006, Val Loss=0.3049, lr=0.0001
[02/26 03:15:40 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 88/100, Acc=0.8996, Val Loss=0.3063, lr=0.0001
[02/26 03:16:08 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 89/100, Acc=0.8989, Val Loss=0.3051, lr=0.0001
[02/26 03:16:35 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 90/100, Acc=0.9015, Val Loss=0.3039, lr=0.0001
[02/26 03:17:03 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 91/100, Acc=0.9013, Val Loss=0.3070, lr=0.0001
[02/26 03:17:31 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 92/100, Acc=0.9011, Val Loss=0.3053, lr=0.0001
[02/26 03:17:59 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 93/100, Acc=0.9002, Val Loss=0.3057, lr=0.0001
[02/26 03:18:26 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 94/100, Acc=0.9014, Val Loss=0.3059, lr=0.0001
[02/26 03:18:54 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 95/100, Acc=0.9015, Val Loss=0.3067, lr=0.0001
[02/26 03:19:22 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 96/100, Acc=0.9011, Val Loss=0.3072, lr=0.0001
[02/26 03:19:49 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 97/100, Acc=0.9010, Val Loss=0.3049, lr=0.0001
[02/26 03:20:17 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 98/100, Acc=0.9009, Val Loss=0.3062, lr=0.0001
[02/26 03:20:45 cifar10-global-proj_sl-2.0-mobilenetv2]: Epoch 99/100, Acc=0.9015, Val Loss=0.3048, lr=0.0001
[02/26 03:20:45 cifar10-global-proj_sl-2.0-mobilenetv2]: Best Acc=0.9015
[02/26 03:20:45 cifar10-global-proj_sl-2.0-mobilenetv2]: Params: 1.12 M
[02/26 03:20:45 cifar10-global-proj_sl-2.0-mobilenetv2]: ops: 34.09 M
[02/26 03:20:48 cifar10-global-proj_sl-2.0-mobilenetv2]: Acc: 0.9015 Val Loss: 0.3048

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: mode: prune
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: model: mobilenetv2
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: verbose: False
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: dataset: cifar10
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: dataroot: data
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: batch_size: 128
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: total_epochs: 100
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: lr: 0.01
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: restore: run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: output_dir: run/cifar10/prune/cifar10-global-random-3.0-mobilenetv2
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: finetune: True
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: last_epochs: 100
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: reps: 1
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: method: random
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: speed_up: 3.0
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: reg: 1e-05
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: delta_reg: 0.0001
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: weight_decay: 0.0005
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: seed: 1
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: global_pruning: True
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: sl_total_epochs: 100
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: sl_lr: 0.01
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: sl_reg_warmup: 0
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: sl_restore: None
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: iterative_steps: 400
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: logger: <Logger cifar10-global-random-3.0-mobilenetv2 (DEBUG)>
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: device: cuda
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: num_classes: 10
[02/26 03:20:55 cifar10-global-random-3.0-mobilenetv2]: Loading model from run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/26 03:20:59 cifar10-global-random-3.0-mobilenetv2]: Pruning...
[02/26 03:21:10 cifar10-global-random-3.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 1, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (4): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 49, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(49, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=49)
        (4): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(49, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 49, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(49, 49, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=49)
        (4): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(49, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 97, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(97, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=97)
        (4): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(97, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 97, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(97, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=97)
        (4): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(97, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 97, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(97, 97, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=97)
        (4): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(97, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 289, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(289, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(289, 289, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=289)
        (4): BatchNorm2d(289, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(289, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 289, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(289, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(289, 289, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=289)
        (4): BatchNorm2d(289, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(289, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 289, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(289, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(289, 289, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=289)
        (4): BatchNorm2d(289, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(289, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 289, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(289, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(289, 289, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=289)
        (4): BatchNorm2d(289, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(289, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 481, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(481, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(481, 481, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=481)
        (4): BatchNorm2d(481, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(481, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 481, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(481, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(481, 481, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=481)
        (4): BatchNorm2d(481, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(481, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 481, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(481, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(481, 481, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=481)
        (4): BatchNorm2d(481, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(481, 65, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(65, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(65, 865, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(865, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(865, 865, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=865)
        (4): BatchNorm2d(865, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(865, 65, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(65, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(65, 865, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(865, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(865, 865, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=865)
        (4): BatchNorm2d(865, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(865, 65, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(65, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(65, 865, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(865, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(865, 865, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=865)
      (4): BatchNorm2d(865, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(865, 225, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(225, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(225, 1185, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1185, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1185, 10, kernel_size=(1, 1), stride=(1, 1))
)
[02/26 03:21:13 cifar10-global-random-3.0-mobilenetv2]: Params: 2.25 M => 0.88 M (39.02%)
[02/26 03:21:13 cifar10-global-random-3.0-mobilenetv2]: FLOPs: 68.29 M => 22.64 M (33.16%, 3.02X )
[02/26 03:21:13 cifar10-global-random-3.0-mobilenetv2]: Acc: 0.8936 => 0.1000
[02/26 03:21:13 cifar10-global-random-3.0-mobilenetv2]: Val Loss: 0.3202 => 2.8661
[02/26 03:21:13 cifar10-global-random-3.0-mobilenetv2]: Finetuning...
[02/26 03:21:39 cifar10-global-random-3.0-mobilenetv2]: Epoch 0/100, Acc=0.1000, Val Loss=2.3058, lr=0.0100
[02/26 03:22:06 cifar10-global-random-3.0-mobilenetv2]: Epoch 1/100, Acc=0.1000, Val Loss=2.3082, lr=0.0100
[02/26 03:22:33 cifar10-global-random-3.0-mobilenetv2]: Epoch 2/100, Acc=0.1000, Val Loss=2.3064, lr=0.0100
[02/26 03:22:59 cifar10-global-random-3.0-mobilenetv2]: Epoch 3/100, Acc=0.1000, Val Loss=2.3051, lr=0.0100
[02/26 03:23:26 cifar10-global-random-3.0-mobilenetv2]: Epoch 4/100, Acc=0.1000, Val Loss=2.3049, lr=0.0100
[02/26 03:23:53 cifar10-global-random-3.0-mobilenetv2]: Epoch 5/100, Acc=0.1000, Val Loss=2.3047, lr=0.0100
[02/26 03:24:20 cifar10-global-random-3.0-mobilenetv2]: Epoch 6/100, Acc=0.1000, Val Loss=2.3062, lr=0.0100
[02/26 03:24:46 cifar10-global-random-3.0-mobilenetv2]: Epoch 7/100, Acc=0.1000, Val Loss=2.3038, lr=0.0100
[02/26 03:25:13 cifar10-global-random-3.0-mobilenetv2]: Epoch 8/100, Acc=0.1000, Val Loss=2.3043, lr=0.0100
[02/26 03:25:40 cifar10-global-random-3.0-mobilenetv2]: Epoch 9/100, Acc=0.1000, Val Loss=2.3041, lr=0.0100
[02/26 03:26:07 cifar10-global-random-3.0-mobilenetv2]: Epoch 10/100, Acc=0.1000, Val Loss=2.3046, lr=0.0100
[02/26 03:26:33 cifar10-global-random-3.0-mobilenetv2]: Epoch 11/100, Acc=0.1000, Val Loss=2.3053, lr=0.0100
[02/26 03:27:00 cifar10-global-random-3.0-mobilenetv2]: Epoch 12/100, Acc=0.1000, Val Loss=2.3054, lr=0.0100
[02/26 03:27:26 cifar10-global-random-3.0-mobilenetv2]: Epoch 13/100, Acc=0.1000, Val Loss=2.3033, lr=0.0100
[02/26 03:27:53 cifar10-global-random-3.0-mobilenetv2]: Epoch 14/100, Acc=0.1000, Val Loss=2.3044, lr=0.0100
[02/26 03:28:20 cifar10-global-random-3.0-mobilenetv2]: Epoch 15/100, Acc=0.1000, Val Loss=2.3046, lr=0.0100
[02/26 03:28:47 cifar10-global-random-3.0-mobilenetv2]: Epoch 16/100, Acc=0.1000, Val Loss=2.3036, lr=0.0100
[02/26 03:29:14 cifar10-global-random-3.0-mobilenetv2]: Epoch 17/100, Acc=0.1000, Val Loss=2.3039, lr=0.0100
[02/26 03:29:41 cifar10-global-random-3.0-mobilenetv2]: Epoch 18/100, Acc=0.1000, Val Loss=2.3039, lr=0.0100
[02/26 03:30:08 cifar10-global-random-3.0-mobilenetv2]: Epoch 19/100, Acc=0.1000, Val Loss=2.3048, lr=0.0100
[02/26 03:30:36 cifar10-global-random-3.0-mobilenetv2]: Epoch 20/100, Acc=0.1000, Val Loss=2.3041, lr=0.0100
[02/26 03:31:03 cifar10-global-random-3.0-mobilenetv2]: Epoch 21/100, Acc=0.1000, Val Loss=2.3031, lr=0.0100
[02/26 03:31:30 cifar10-global-random-3.0-mobilenetv2]: Epoch 22/100, Acc=0.1000, Val Loss=2.3035, lr=0.0100
[02/26 03:31:57 cifar10-global-random-3.0-mobilenetv2]: Epoch 23/100, Acc=0.1000, Val Loss=2.3031, lr=0.0100
[02/26 03:32:24 cifar10-global-random-3.0-mobilenetv2]: Epoch 24/100, Acc=0.1000, Val Loss=2.3033, lr=0.0100
[02/26 03:32:51 cifar10-global-random-3.0-mobilenetv2]: Epoch 25/100, Acc=0.1000, Val Loss=2.3034, lr=0.0100
[02/26 03:33:18 cifar10-global-random-3.0-mobilenetv2]: Epoch 26/100, Acc=0.1000, Val Loss=2.3030, lr=0.0100
[02/26 03:33:45 cifar10-global-random-3.0-mobilenetv2]: Epoch 27/100, Acc=0.1000, Val Loss=2.3035, lr=0.0100
[02/26 03:34:13 cifar10-global-random-3.0-mobilenetv2]: Epoch 28/100, Acc=0.1000, Val Loss=2.3038, lr=0.0100
[02/26 03:34:40 cifar10-global-random-3.0-mobilenetv2]: Epoch 29/100, Acc=0.1000, Val Loss=2.3035, lr=0.0100
[02/26 03:35:07 cifar10-global-random-3.0-mobilenetv2]: Epoch 30/100, Acc=0.1000, Val Loss=2.3034, lr=0.0100
[02/26 03:35:34 cifar10-global-random-3.0-mobilenetv2]: Epoch 31/100, Acc=0.1000, Val Loss=2.3037, lr=0.0100
[02/26 03:36:01 cifar10-global-random-3.0-mobilenetv2]: Epoch 32/100, Acc=0.1000, Val Loss=2.3029, lr=0.0100
[02/26 03:36:28 cifar10-global-random-3.0-mobilenetv2]: Epoch 33/100, Acc=0.1000, Val Loss=2.3030, lr=0.0100
[02/26 03:36:55 cifar10-global-random-3.0-mobilenetv2]: Epoch 34/100, Acc=0.1000, Val Loss=2.3029, lr=0.0100
[02/26 03:37:22 cifar10-global-random-3.0-mobilenetv2]: Epoch 35/100, Acc=0.1000, Val Loss=2.3032, lr=0.0100
[02/26 03:37:50 cifar10-global-random-3.0-mobilenetv2]: Epoch 36/100, Acc=0.1000, Val Loss=2.3028, lr=0.0100
[02/26 03:38:17 cifar10-global-random-3.0-mobilenetv2]: Epoch 37/100, Acc=0.1000, Val Loss=2.3027, lr=0.0100
[02/26 03:38:44 cifar10-global-random-3.0-mobilenetv2]: Epoch 38/100, Acc=0.1000, Val Loss=2.3032, lr=0.0100
[02/26 03:39:11 cifar10-global-random-3.0-mobilenetv2]: Epoch 39/100, Acc=0.1000, Val Loss=2.3030, lr=0.0100
[02/26 03:39:38 cifar10-global-random-3.0-mobilenetv2]: Epoch 40/100, Acc=0.1000, Val Loss=2.3029, lr=0.0100
[02/26 03:40:05 cifar10-global-random-3.0-mobilenetv2]: Epoch 41/100, Acc=0.1000, Val Loss=2.3029, lr=0.0100
[02/26 03:40:32 cifar10-global-random-3.0-mobilenetv2]: Epoch 42/100, Acc=0.1000, Val Loss=2.3028, lr=0.0100
[02/26 03:40:59 cifar10-global-random-3.0-mobilenetv2]: Epoch 43/100, Acc=0.1000, Val Loss=2.3028, lr=0.0100
[02/26 03:41:26 cifar10-global-random-3.0-mobilenetv2]: Epoch 44/100, Acc=0.1000, Val Loss=2.3028, lr=0.0100
[02/26 03:41:53 cifar10-global-random-3.0-mobilenetv2]: Epoch 45/100, Acc=0.1000, Val Loss=2.3028, lr=0.0100
[02/26 03:42:20 cifar10-global-random-3.0-mobilenetv2]: Epoch 46/100, Acc=0.1000, Val Loss=2.3028, lr=0.0100
[02/26 03:42:47 cifar10-global-random-3.0-mobilenetv2]: Epoch 47/100, Acc=0.1000, Val Loss=2.3030, lr=0.0100
[02/26 03:43:14 cifar10-global-random-3.0-mobilenetv2]: Epoch 48/100, Acc=0.1000, Val Loss=2.3026, lr=0.0100
[02/26 03:43:41 cifar10-global-random-3.0-mobilenetv2]: Epoch 49/100, Acc=0.1000, Val Loss=2.3027, lr=0.0100
[02/26 03:44:08 cifar10-global-random-3.0-mobilenetv2]: Epoch 50/100, Acc=0.1000, Val Loss=2.3028, lr=0.0100
[02/26 03:44:35 cifar10-global-random-3.0-mobilenetv2]: Epoch 51/100, Acc=0.1000, Val Loss=2.3027, lr=0.0100
[02/26 03:45:02 cifar10-global-random-3.0-mobilenetv2]: Epoch 52/100, Acc=0.1000, Val Loss=2.3028, lr=0.0100
[02/26 03:45:29 cifar10-global-random-3.0-mobilenetv2]: Epoch 53/100, Acc=0.1000, Val Loss=2.3027, lr=0.0100
[02/26 03:45:56 cifar10-global-random-3.0-mobilenetv2]: Epoch 54/100, Acc=0.1000, Val Loss=2.3027, lr=0.0100
[02/26 03:46:23 cifar10-global-random-3.0-mobilenetv2]: Epoch 55/100, Acc=0.1000, Val Loss=2.3027, lr=0.0100
[02/26 03:46:50 cifar10-global-random-3.0-mobilenetv2]: Epoch 56/100, Acc=0.1000, Val Loss=2.3027, lr=0.0100
[02/26 03:47:17 cifar10-global-random-3.0-mobilenetv2]: Epoch 57/100, Acc=0.1000, Val Loss=2.3027, lr=0.0100
[02/26 03:47:44 cifar10-global-random-3.0-mobilenetv2]: Epoch 58/100, Acc=0.1000, Val Loss=2.3029, lr=0.0100
[02/26 03:48:11 cifar10-global-random-3.0-mobilenetv2]: Epoch 59/100, Acc=0.1000, Val Loss=2.3027, lr=0.0100
[02/26 03:48:38 cifar10-global-random-3.0-mobilenetv2]: Epoch 60/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/26 03:49:05 cifar10-global-random-3.0-mobilenetv2]: Epoch 61/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/26 03:49:32 cifar10-global-random-3.0-mobilenetv2]: Epoch 62/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/26 03:49:59 cifar10-global-random-3.0-mobilenetv2]: Epoch 63/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/26 03:50:26 cifar10-global-random-3.0-mobilenetv2]: Epoch 64/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/26 03:50:52 cifar10-global-random-3.0-mobilenetv2]: Epoch 65/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/26 03:51:19 cifar10-global-random-3.0-mobilenetv2]: Epoch 66/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/26 03:51:46 cifar10-global-random-3.0-mobilenetv2]: Epoch 67/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/26 03:52:13 cifar10-global-random-3.0-mobilenetv2]: Epoch 68/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/26 03:52:40 cifar10-global-random-3.0-mobilenetv2]: Epoch 69/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/26 03:53:07 cifar10-global-random-3.0-mobilenetv2]: Epoch 70/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/26 03:53:34 cifar10-global-random-3.0-mobilenetv2]: Epoch 71/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/26 03:54:01 cifar10-global-random-3.0-mobilenetv2]: Epoch 72/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/26 03:54:28 cifar10-global-random-3.0-mobilenetv2]: Epoch 73/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/26 03:54:55 cifar10-global-random-3.0-mobilenetv2]: Epoch 74/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/26 03:55:22 cifar10-global-random-3.0-mobilenetv2]: Epoch 75/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/26 03:55:49 cifar10-global-random-3.0-mobilenetv2]: Epoch 76/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/26 03:56:16 cifar10-global-random-3.0-mobilenetv2]: Epoch 77/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/26 03:56:43 cifar10-global-random-3.0-mobilenetv2]: Epoch 78/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/26 03:57:10 cifar10-global-random-3.0-mobilenetv2]: Epoch 79/100, Acc=0.1000, Val Loss=2.3026, lr=0.0010
[02/26 03:57:37 cifar10-global-random-3.0-mobilenetv2]: Epoch 80/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/26 03:58:04 cifar10-global-random-3.0-mobilenetv2]: Epoch 81/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/26 03:58:31 cifar10-global-random-3.0-mobilenetv2]: Epoch 82/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/26 03:58:58 cifar10-global-random-3.0-mobilenetv2]: Epoch 83/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/26 03:59:25 cifar10-global-random-3.0-mobilenetv2]: Epoch 84/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/26 03:59:53 cifar10-global-random-3.0-mobilenetv2]: Epoch 85/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/26 04:00:20 cifar10-global-random-3.0-mobilenetv2]: Epoch 86/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/26 04:00:48 cifar10-global-random-3.0-mobilenetv2]: Epoch 87/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/26 04:01:15 cifar10-global-random-3.0-mobilenetv2]: Epoch 88/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/26 04:01:42 cifar10-global-random-3.0-mobilenetv2]: Epoch 89/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/26 04:02:09 cifar10-global-random-3.0-mobilenetv2]: Epoch 90/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/26 04:02:36 cifar10-global-random-3.0-mobilenetv2]: Epoch 91/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/26 04:03:04 cifar10-global-random-3.0-mobilenetv2]: Epoch 92/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/26 04:03:31 cifar10-global-random-3.0-mobilenetv2]: Epoch 93/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/26 04:03:58 cifar10-global-random-3.0-mobilenetv2]: Epoch 94/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/26 04:04:25 cifar10-global-random-3.0-mobilenetv2]: Epoch 95/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/26 04:04:52 cifar10-global-random-3.0-mobilenetv2]: Epoch 96/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/26 04:05:20 cifar10-global-random-3.0-mobilenetv2]: Epoch 97/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/26 04:05:47 cifar10-global-random-3.0-mobilenetv2]: Epoch 98/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/26 04:06:14 cifar10-global-random-3.0-mobilenetv2]: Epoch 99/100, Acc=0.1000, Val Loss=2.3026, lr=0.0001
[02/26 04:06:14 cifar10-global-random-3.0-mobilenetv2]: Best Acc=0.1000
[02/26 04:06:14 cifar10-global-random-3.0-mobilenetv2]: Params: 0.88 M
[02/26 04:06:14 cifar10-global-random-3.0-mobilenetv2]: ops: 22.64 M
[02/26 04:06:17 cifar10-global-random-3.0-mobilenetv2]: Acc: 0.1000 Val Loss: 2.3026

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: mode: prune
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: model: mobilenetv2
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: verbose: False
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: dataset: cifar10
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: dataroot: data
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: batch_size: 128
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: total_epochs: 100
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: lr: 0.01
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: restore: run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: output_dir: run/cifar10/prune/cifar10-global-l2-3.0-mobilenetv2
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: finetune: True
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: last_epochs: 100
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: reps: 1
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: method: l2
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: speed_up: 3.0
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: reg: 1e-05
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: delta_reg: 0.0001
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: weight_decay: 0.0005
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: seed: 1
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: global_pruning: True
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: sl_total_epochs: 100
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: sl_lr: 0.01
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: sl_reg_warmup: 0
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: sl_restore: None
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: iterative_steps: 400
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: logger: <Logger cifar10-global-l2-3.0-mobilenetv2 (DEBUG)>
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: device: cuda
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: num_classes: 10
[02/26 04:06:24 cifar10-global-l2-3.0-mobilenetv2]: Loading model from run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/26 04:06:27 cifar10-global-l2-3.0-mobilenetv2]: Pruning...
[02/26 04:06:58 cifar10-global-l2-3.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 7, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(7, 16, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 67, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(67, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(67, 67, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=67)
        (4): BatchNorm2d(67, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(67, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72)
        (4): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 50, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(50, 50, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=50)
        (4): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(50, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 66, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(66, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(66, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=66)
        (4): BatchNorm2d(66, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(66, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 31, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=31)
        (4): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(31, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 39, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(39, 39, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=39)
        (4): BatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(39, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 31, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=31)
        (4): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(31, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 11, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(11, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=11)
        (4): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(11, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 72, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72)
        (4): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(72, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 2, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)
        (4): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(2, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (4): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(1, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 132, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(132, 132, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=132)
        (4): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(132, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 7, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=7)
        (4): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(7, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 1, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (4): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(1, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 378, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(378, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(378, 378, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=378)
      (4): BatchNorm2d(378, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(378, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1258, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1258, 10, kernel_size=(1, 1), stride=(1, 1))
)
[02/26 04:07:01 cifar10-global-l2-3.0-mobilenetv2]: Params: 2.25 M => 0.70 M (31.03%)
[02/26 04:07:01 cifar10-global-l2-3.0-mobilenetv2]: FLOPs: 68.29 M => 22.58 M (33.06%, 3.02X )
[02/26 04:07:01 cifar10-global-l2-3.0-mobilenetv2]: Acc: 0.8936 => 0.8949
[02/26 04:07:01 cifar10-global-l2-3.0-mobilenetv2]: Val Loss: 0.3202 => 0.3194
[02/26 04:07:01 cifar10-global-l2-3.0-mobilenetv2]: Finetuning...
[02/26 04:07:28 cifar10-global-l2-3.0-mobilenetv2]: Epoch 0/100, Acc=0.8636, Val Loss=0.4085, lr=0.0100
[02/26 04:07:55 cifar10-global-l2-3.0-mobilenetv2]: Epoch 1/100, Acc=0.8477, Val Loss=0.4379, lr=0.0100
[02/26 04:08:22 cifar10-global-l2-3.0-mobilenetv2]: Epoch 2/100, Acc=0.8440, Val Loss=0.4600, lr=0.0100
[02/26 04:08:49 cifar10-global-l2-3.0-mobilenetv2]: Epoch 3/100, Acc=0.8548, Val Loss=0.4220, lr=0.0100
[02/26 04:09:17 cifar10-global-l2-3.0-mobilenetv2]: Epoch 4/100, Acc=0.8608, Val Loss=0.4079, lr=0.0100
[02/26 04:09:44 cifar10-global-l2-3.0-mobilenetv2]: Epoch 5/100, Acc=0.8429, Val Loss=0.4613, lr=0.0100
[02/26 04:10:11 cifar10-global-l2-3.0-mobilenetv2]: Epoch 6/100, Acc=0.8525, Val Loss=0.4344, lr=0.0100
[02/26 04:10:38 cifar10-global-l2-3.0-mobilenetv2]: Epoch 7/100, Acc=0.8491, Val Loss=0.4443, lr=0.0100
[02/26 04:11:06 cifar10-global-l2-3.0-mobilenetv2]: Epoch 8/100, Acc=0.8587, Val Loss=0.4312, lr=0.0100
[02/26 04:11:33 cifar10-global-l2-3.0-mobilenetv2]: Epoch 9/100, Acc=0.8418, Val Loss=0.4782, lr=0.0100
[02/26 04:12:00 cifar10-global-l2-3.0-mobilenetv2]: Epoch 10/100, Acc=0.8437, Val Loss=0.4545, lr=0.0100
[02/26 04:12:27 cifar10-global-l2-3.0-mobilenetv2]: Epoch 11/100, Acc=0.8579, Val Loss=0.4228, lr=0.0100
[02/26 04:12:54 cifar10-global-l2-3.0-mobilenetv2]: Epoch 12/100, Acc=0.8496, Val Loss=0.4371, lr=0.0100
[02/26 04:13:21 cifar10-global-l2-3.0-mobilenetv2]: Epoch 13/100, Acc=0.8486, Val Loss=0.4501, lr=0.0100
[02/26 04:13:48 cifar10-global-l2-3.0-mobilenetv2]: Epoch 14/100, Acc=0.8360, Val Loss=0.4900, lr=0.0100
[02/26 04:14:15 cifar10-global-l2-3.0-mobilenetv2]: Epoch 15/100, Acc=0.8514, Val Loss=0.4364, lr=0.0100
[02/26 04:14:42 cifar10-global-l2-3.0-mobilenetv2]: Epoch 16/100, Acc=0.8382, Val Loss=0.4741, lr=0.0100
[02/26 04:15:09 cifar10-global-l2-3.0-mobilenetv2]: Epoch 17/100, Acc=0.8473, Val Loss=0.4574, lr=0.0100
[02/26 04:15:37 cifar10-global-l2-3.0-mobilenetv2]: Epoch 18/100, Acc=0.8591, Val Loss=0.4268, lr=0.0100
[02/26 04:16:04 cifar10-global-l2-3.0-mobilenetv2]: Epoch 19/100, Acc=0.8554, Val Loss=0.4197, lr=0.0100
[02/26 04:16:32 cifar10-global-l2-3.0-mobilenetv2]: Epoch 20/100, Acc=0.8408, Val Loss=0.4813, lr=0.0100
[02/26 04:16:59 cifar10-global-l2-3.0-mobilenetv2]: Epoch 21/100, Acc=0.8664, Val Loss=0.3925, lr=0.0100
[02/26 04:17:26 cifar10-global-l2-3.0-mobilenetv2]: Epoch 22/100, Acc=0.8567, Val Loss=0.4224, lr=0.0100
[02/26 04:17:53 cifar10-global-l2-3.0-mobilenetv2]: Epoch 23/100, Acc=0.8646, Val Loss=0.3987, lr=0.0100
[02/26 04:18:20 cifar10-global-l2-3.0-mobilenetv2]: Epoch 24/100, Acc=0.8615, Val Loss=0.4094, lr=0.0100
[02/26 04:18:47 cifar10-global-l2-3.0-mobilenetv2]: Epoch 25/100, Acc=0.8621, Val Loss=0.4139, lr=0.0100
[02/26 04:19:14 cifar10-global-l2-3.0-mobilenetv2]: Epoch 26/100, Acc=0.8436, Val Loss=0.4748, lr=0.0100
[02/26 04:19:41 cifar10-global-l2-3.0-mobilenetv2]: Epoch 27/100, Acc=0.8576, Val Loss=0.4324, lr=0.0100
[02/26 04:20:07 cifar10-global-l2-3.0-mobilenetv2]: Epoch 28/100, Acc=0.8541, Val Loss=0.4286, lr=0.0100
[02/26 04:20:34 cifar10-global-l2-3.0-mobilenetv2]: Epoch 29/100, Acc=0.8428, Val Loss=0.4764, lr=0.0100
[02/26 04:21:01 cifar10-global-l2-3.0-mobilenetv2]: Epoch 30/100, Acc=0.8378, Val Loss=0.4881, lr=0.0100
[02/26 04:21:28 cifar10-global-l2-3.0-mobilenetv2]: Epoch 31/100, Acc=0.8502, Val Loss=0.4448, lr=0.0100
[02/26 04:21:55 cifar10-global-l2-3.0-mobilenetv2]: Epoch 32/100, Acc=0.8394, Val Loss=0.4892, lr=0.0100
[02/26 04:22:21 cifar10-global-l2-3.0-mobilenetv2]: Epoch 33/100, Acc=0.8582, Val Loss=0.4283, lr=0.0100
[02/26 04:22:48 cifar10-global-l2-3.0-mobilenetv2]: Epoch 34/100, Acc=0.8601, Val Loss=0.4313, lr=0.0100
[02/26 04:23:15 cifar10-global-l2-3.0-mobilenetv2]: Epoch 35/100, Acc=0.8579, Val Loss=0.4294, lr=0.0100
[02/26 04:23:42 cifar10-global-l2-3.0-mobilenetv2]: Epoch 36/100, Acc=0.8403, Val Loss=0.4870, lr=0.0100
[02/26 04:24:08 cifar10-global-l2-3.0-mobilenetv2]: Epoch 37/100, Acc=0.8583, Val Loss=0.4207, lr=0.0100
[02/26 04:24:35 cifar10-global-l2-3.0-mobilenetv2]: Epoch 38/100, Acc=0.8621, Val Loss=0.4023, lr=0.0100
[02/26 04:25:02 cifar10-global-l2-3.0-mobilenetv2]: Epoch 39/100, Acc=0.8652, Val Loss=0.4059, lr=0.0100
[02/26 04:25:29 cifar10-global-l2-3.0-mobilenetv2]: Epoch 40/100, Acc=0.8530, Val Loss=0.4280, lr=0.0100
[02/26 04:25:56 cifar10-global-l2-3.0-mobilenetv2]: Epoch 41/100, Acc=0.8544, Val Loss=0.4341, lr=0.0100
[02/26 04:26:22 cifar10-global-l2-3.0-mobilenetv2]: Epoch 42/100, Acc=0.8623, Val Loss=0.4159, lr=0.0100
[02/26 04:26:49 cifar10-global-l2-3.0-mobilenetv2]: Epoch 43/100, Acc=0.8444, Val Loss=0.4626, lr=0.0100
[02/26 04:27:16 cifar10-global-l2-3.0-mobilenetv2]: Epoch 44/100, Acc=0.8637, Val Loss=0.4165, lr=0.0100
[02/26 04:27:43 cifar10-global-l2-3.0-mobilenetv2]: Epoch 45/100, Acc=0.8616, Val Loss=0.4144, lr=0.0100
[02/26 04:28:10 cifar10-global-l2-3.0-mobilenetv2]: Epoch 46/100, Acc=0.8581, Val Loss=0.4254, lr=0.0100
[02/26 04:28:37 cifar10-global-l2-3.0-mobilenetv2]: Epoch 47/100, Acc=0.8347, Val Loss=0.4931, lr=0.0100
[02/26 04:29:03 cifar10-global-l2-3.0-mobilenetv2]: Epoch 48/100, Acc=0.8494, Val Loss=0.4462, lr=0.0100
[02/26 04:29:30 cifar10-global-l2-3.0-mobilenetv2]: Epoch 49/100, Acc=0.8586, Val Loss=0.4237, lr=0.0100
[02/26 04:29:57 cifar10-global-l2-3.0-mobilenetv2]: Epoch 50/100, Acc=0.8444, Val Loss=0.4656, lr=0.0100
[02/26 04:30:24 cifar10-global-l2-3.0-mobilenetv2]: Epoch 51/100, Acc=0.8509, Val Loss=0.4471, lr=0.0100
[02/26 04:30:51 cifar10-global-l2-3.0-mobilenetv2]: Epoch 52/100, Acc=0.8577, Val Loss=0.4230, lr=0.0100
[02/26 04:31:18 cifar10-global-l2-3.0-mobilenetv2]: Epoch 53/100, Acc=0.8585, Val Loss=0.4229, lr=0.0100
[02/26 04:31:45 cifar10-global-l2-3.0-mobilenetv2]: Epoch 54/100, Acc=0.8638, Val Loss=0.4035, lr=0.0100
[02/26 04:32:12 cifar10-global-l2-3.0-mobilenetv2]: Epoch 55/100, Acc=0.8706, Val Loss=0.3918, lr=0.0100
[02/26 04:32:38 cifar10-global-l2-3.0-mobilenetv2]: Epoch 56/100, Acc=0.8451, Val Loss=0.4666, lr=0.0100
[02/26 04:33:05 cifar10-global-l2-3.0-mobilenetv2]: Epoch 57/100, Acc=0.8469, Val Loss=0.4516, lr=0.0100
[02/26 04:33:32 cifar10-global-l2-3.0-mobilenetv2]: Epoch 58/100, Acc=0.8620, Val Loss=0.4153, lr=0.0100
[02/26 04:33:59 cifar10-global-l2-3.0-mobilenetv2]: Epoch 59/100, Acc=0.8540, Val Loss=0.4320, lr=0.0100
[02/26 04:34:26 cifar10-global-l2-3.0-mobilenetv2]: Epoch 60/100, Acc=0.8936, Val Loss=0.3132, lr=0.0010
[02/26 04:34:53 cifar10-global-l2-3.0-mobilenetv2]: Epoch 61/100, Acc=0.8946, Val Loss=0.3084, lr=0.0010
[02/26 04:35:20 cifar10-global-l2-3.0-mobilenetv2]: Epoch 62/100, Acc=0.8954, Val Loss=0.3048, lr=0.0010
[02/26 04:35:47 cifar10-global-l2-3.0-mobilenetv2]: Epoch 63/100, Acc=0.8973, Val Loss=0.3064, lr=0.0010
[02/26 04:36:14 cifar10-global-l2-3.0-mobilenetv2]: Epoch 64/100, Acc=0.8973, Val Loss=0.3025, lr=0.0010
[02/26 04:36:41 cifar10-global-l2-3.0-mobilenetv2]: Epoch 65/100, Acc=0.8963, Val Loss=0.3050, lr=0.0010
[02/26 04:37:08 cifar10-global-l2-3.0-mobilenetv2]: Epoch 66/100, Acc=0.8980, Val Loss=0.3036, lr=0.0010
[02/26 04:37:35 cifar10-global-l2-3.0-mobilenetv2]: Epoch 67/100, Acc=0.8956, Val Loss=0.3078, lr=0.0010
[02/26 04:38:02 cifar10-global-l2-3.0-mobilenetv2]: Epoch 68/100, Acc=0.8967, Val Loss=0.3082, lr=0.0010
[02/26 04:38:29 cifar10-global-l2-3.0-mobilenetv2]: Epoch 69/100, Acc=0.8984, Val Loss=0.3067, lr=0.0010
[02/26 04:38:56 cifar10-global-l2-3.0-mobilenetv2]: Epoch 70/100, Acc=0.8987, Val Loss=0.3067, lr=0.0010
[02/26 04:39:23 cifar10-global-l2-3.0-mobilenetv2]: Epoch 71/100, Acc=0.8972, Val Loss=0.3034, lr=0.0010
[02/26 04:39:50 cifar10-global-l2-3.0-mobilenetv2]: Epoch 72/100, Acc=0.8999, Val Loss=0.3048, lr=0.0010
[02/26 04:40:17 cifar10-global-l2-3.0-mobilenetv2]: Epoch 73/100, Acc=0.8998, Val Loss=0.3039, lr=0.0010
[02/26 04:40:44 cifar10-global-l2-3.0-mobilenetv2]: Epoch 74/100, Acc=0.9012, Val Loss=0.3032, lr=0.0010
[02/26 04:41:11 cifar10-global-l2-3.0-mobilenetv2]: Epoch 75/100, Acc=0.8999, Val Loss=0.3048, lr=0.0010
[02/26 04:41:38 cifar10-global-l2-3.0-mobilenetv2]: Epoch 76/100, Acc=0.8995, Val Loss=0.3047, lr=0.0010
[02/26 04:42:05 cifar10-global-l2-3.0-mobilenetv2]: Epoch 77/100, Acc=0.8994, Val Loss=0.3062, lr=0.0010
[02/26 04:42:32 cifar10-global-l2-3.0-mobilenetv2]: Epoch 78/100, Acc=0.9012, Val Loss=0.3040, lr=0.0010
[02/26 04:42:59 cifar10-global-l2-3.0-mobilenetv2]: Epoch 79/100, Acc=0.9004, Val Loss=0.3048, lr=0.0010
[02/26 04:43:26 cifar10-global-l2-3.0-mobilenetv2]: Epoch 80/100, Acc=0.9017, Val Loss=0.2997, lr=0.0001
[02/26 04:43:53 cifar10-global-l2-3.0-mobilenetv2]: Epoch 81/100, Acc=0.9014, Val Loss=0.2995, lr=0.0001
[02/26 04:44:21 cifar10-global-l2-3.0-mobilenetv2]: Epoch 82/100, Acc=0.9012, Val Loss=0.3012, lr=0.0001
[02/26 04:44:48 cifar10-global-l2-3.0-mobilenetv2]: Epoch 83/100, Acc=0.9027, Val Loss=0.3001, lr=0.0001
[02/26 04:45:16 cifar10-global-l2-3.0-mobilenetv2]: Epoch 84/100, Acc=0.9021, Val Loss=0.2991, lr=0.0001
[02/26 04:45:43 cifar10-global-l2-3.0-mobilenetv2]: Epoch 85/100, Acc=0.9034, Val Loss=0.2991, lr=0.0001
[02/26 04:46:11 cifar10-global-l2-3.0-mobilenetv2]: Epoch 86/100, Acc=0.9021, Val Loss=0.3001, lr=0.0001
[02/26 04:46:38 cifar10-global-l2-3.0-mobilenetv2]: Epoch 87/100, Acc=0.9023, Val Loss=0.2981, lr=0.0001
[02/26 04:47:05 cifar10-global-l2-3.0-mobilenetv2]: Epoch 88/100, Acc=0.9014, Val Loss=0.2980, lr=0.0001
[02/26 04:47:33 cifar10-global-l2-3.0-mobilenetv2]: Epoch 89/100, Acc=0.9023, Val Loss=0.2985, lr=0.0001
[02/26 04:48:00 cifar10-global-l2-3.0-mobilenetv2]: Epoch 90/100, Acc=0.9028, Val Loss=0.3008, lr=0.0001
[02/26 04:48:28 cifar10-global-l2-3.0-mobilenetv2]: Epoch 91/100, Acc=0.9031, Val Loss=0.2990, lr=0.0001
[02/26 04:48:55 cifar10-global-l2-3.0-mobilenetv2]: Epoch 92/100, Acc=0.9027, Val Loss=0.2978, lr=0.0001
[02/26 04:49:23 cifar10-global-l2-3.0-mobilenetv2]: Epoch 93/100, Acc=0.9035, Val Loss=0.3000, lr=0.0001
[02/26 04:49:51 cifar10-global-l2-3.0-mobilenetv2]: Epoch 94/100, Acc=0.9022, Val Loss=0.2999, lr=0.0001
[02/26 04:50:18 cifar10-global-l2-3.0-mobilenetv2]: Epoch 95/100, Acc=0.9038, Val Loss=0.3003, lr=0.0001
[02/26 04:50:45 cifar10-global-l2-3.0-mobilenetv2]: Epoch 96/100, Acc=0.9035, Val Loss=0.2986, lr=0.0001
[02/26 04:51:12 cifar10-global-l2-3.0-mobilenetv2]: Epoch 97/100, Acc=0.9032, Val Loss=0.2995, lr=0.0001
[02/26 04:51:40 cifar10-global-l2-3.0-mobilenetv2]: Epoch 98/100, Acc=0.9039, Val Loss=0.2996, lr=0.0001
[02/26 04:52:07 cifar10-global-l2-3.0-mobilenetv2]: Epoch 99/100, Acc=0.9041, Val Loss=0.2990, lr=0.0001
[02/26 04:52:07 cifar10-global-l2-3.0-mobilenetv2]: Best Acc=0.9041
[02/26 04:52:07 cifar10-global-l2-3.0-mobilenetv2]: Params: 0.70 M
[02/26 04:52:07 cifar10-global-l2-3.0-mobilenetv2]: ops: 22.58 M
[02/26 04:52:10 cifar10-global-l2-3.0-mobilenetv2]: Acc: 0.9041 Val Loss: 0.2990

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: mode: prune
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: model: mobilenetv2
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: verbose: False
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: dataset: cifar10
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: dataroot: data
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: batch_size: 128
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: total_epochs: 100
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: lr: 0.01
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: restore: run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: output_dir: run/cifar10/prune/cifar10-global-fpgm-3.0-mobilenetv2
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: finetune: True
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: last_epochs: 100
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: reps: 1
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: method: fpgm
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: speed_up: 3.0
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: reg: 1e-05
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: delta_reg: 0.0001
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: weight_decay: 0.0005
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: seed: 1
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: global_pruning: True
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: sl_total_epochs: 100
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: sl_lr: 0.01
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: sl_reg_warmup: 0
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: sl_restore: None
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: iterative_steps: 400
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: logger: <Logger cifar10-global-fpgm-3.0-mobilenetv2 (DEBUG)>
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: device: cuda
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: num_classes: 10
[02/26 04:52:17 cifar10-global-fpgm-3.0-mobilenetv2]: Loading model from run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/26 04:52:20 cifar10-global-fpgm-3.0-mobilenetv2]: Pruning...
[02/26 04:52:45 cifar10-global-fpgm-3.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 7, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(7, 17, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(17, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=17)
      (4): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(17, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 80, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80)
        (4): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(80, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 55, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(55, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=55)
        (4): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(55, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 84, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(84, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=84)
        (4): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(84, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 45, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(45, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=45)
        (4): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(45, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 46, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(46, 46, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=46)
        (4): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(46, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 51, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(51, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=51)
        (4): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(51, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 46, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(46, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=46)
        (4): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(46, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 13, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(13, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=13)
        (4): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(13, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 89, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=89)
        (4): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(89, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4)
        (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 2, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)
        (4): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(2, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 162, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(162, 162, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=162)
        (4): BatchNorm2d(162, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(162, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 7, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=7)
        (4): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(7, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 3, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3)
        (4): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(3, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 393, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(393, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(393, 393, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=393)
      (4): BatchNorm2d(393, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(393, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1019, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1019, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1019, 10, kernel_size=(1, 1), stride=(1, 1))
)
[02/26 04:52:48 cifar10-global-fpgm-3.0-mobilenetv2]: Params: 2.25 M => 0.65 M (28.77%)
[02/26 04:52:48 cifar10-global-fpgm-3.0-mobilenetv2]: FLOPs: 68.29 M => 22.69 M (33.23%, 3.01X )
[02/26 04:52:48 cifar10-global-fpgm-3.0-mobilenetv2]: Acc: 0.8936 => 0.8916
[02/26 04:52:48 cifar10-global-fpgm-3.0-mobilenetv2]: Val Loss: 0.3202 => 0.3225
[02/26 04:52:48 cifar10-global-fpgm-3.0-mobilenetv2]: Finetuning...
[02/26 04:53:15 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 0/100, Acc=0.8614, Val Loss=0.4022, lr=0.0100
[02/26 04:53:41 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 1/100, Acc=0.8555, Val Loss=0.4287, lr=0.0100
[02/26 04:54:08 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 2/100, Acc=0.8410, Val Loss=0.4681, lr=0.0100
[02/26 04:54:35 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 3/100, Acc=0.8658, Val Loss=0.3917, lr=0.0100
[02/26 04:55:02 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 4/100, Acc=0.8512, Val Loss=0.4388, lr=0.0100
[02/26 04:55:28 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 5/100, Acc=0.8580, Val Loss=0.4176, lr=0.0100
[02/26 04:55:55 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 6/100, Acc=0.8605, Val Loss=0.4120, lr=0.0100
[02/26 04:56:22 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 7/100, Acc=0.8472, Val Loss=0.4480, lr=0.0100
[02/26 04:56:48 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 8/100, Acc=0.8642, Val Loss=0.4055, lr=0.0100
[02/26 04:57:15 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 9/100, Acc=0.8477, Val Loss=0.4411, lr=0.0100
[02/26 04:57:43 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 10/100, Acc=0.8527, Val Loss=0.4255, lr=0.0100
[02/26 04:58:10 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 11/100, Acc=0.8513, Val Loss=0.4487, lr=0.0100
[02/26 04:58:37 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 12/100, Acc=0.8508, Val Loss=0.4408, lr=0.0100
[02/26 04:59:05 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 13/100, Acc=0.8468, Val Loss=0.4548, lr=0.0100
[02/26 04:59:32 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 14/100, Acc=0.8464, Val Loss=0.4554, lr=0.0100
[02/26 04:59:59 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 15/100, Acc=0.8539, Val Loss=0.4277, lr=0.0100
[02/26 05:00:26 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 16/100, Acc=0.8557, Val Loss=0.4287, lr=0.0100
[02/26 05:00:54 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 17/100, Acc=0.8587, Val Loss=0.4117, lr=0.0100
[02/26 05:01:21 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 18/100, Acc=0.8599, Val Loss=0.4263, lr=0.0100
[02/26 05:01:48 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 19/100, Acc=0.8521, Val Loss=0.4459, lr=0.0100
[02/26 05:02:15 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 20/100, Acc=0.8557, Val Loss=0.4445, lr=0.0100
[02/26 05:02:42 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 21/100, Acc=0.8499, Val Loss=0.4317, lr=0.0100
[02/26 05:03:09 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 22/100, Acc=0.8663, Val Loss=0.3922, lr=0.0100
[02/26 05:03:37 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 23/100, Acc=0.8592, Val Loss=0.4079, lr=0.0100
[02/26 05:04:04 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 24/100, Acc=0.8471, Val Loss=0.4641, lr=0.0100
[02/26 05:04:31 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 25/100, Acc=0.8619, Val Loss=0.4049, lr=0.0100
[02/26 05:04:58 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 26/100, Acc=0.8561, Val Loss=0.4318, lr=0.0100
[02/26 05:05:25 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 27/100, Acc=0.8466, Val Loss=0.4587, lr=0.0100
[02/26 05:05:52 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 28/100, Acc=0.8550, Val Loss=0.4332, lr=0.0100
[02/26 05:06:19 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 29/100, Acc=0.8416, Val Loss=0.4765, lr=0.0100
[02/26 05:06:47 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 30/100, Acc=0.8508, Val Loss=0.4318, lr=0.0100
[02/26 05:07:14 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 31/100, Acc=0.8350, Val Loss=0.4868, lr=0.0100
[02/26 05:07:41 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 32/100, Acc=0.8457, Val Loss=0.4543, lr=0.0100
[02/26 05:08:08 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 33/100, Acc=0.8591, Val Loss=0.4197, lr=0.0100
[02/26 05:08:35 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 34/100, Acc=0.8453, Val Loss=0.4551, lr=0.0100
[02/26 05:09:02 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 35/100, Acc=0.8680, Val Loss=0.3863, lr=0.0100
[02/26 05:09:30 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 36/100, Acc=0.8570, Val Loss=0.4208, lr=0.0100
[02/26 05:09:57 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 37/100, Acc=0.8649, Val Loss=0.3951, lr=0.0100
[02/26 05:10:24 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 38/100, Acc=0.8666, Val Loss=0.3999, lr=0.0100
[02/26 05:10:51 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 39/100, Acc=0.8653, Val Loss=0.3995, lr=0.0100
[02/26 05:11:19 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 40/100, Acc=0.8587, Val Loss=0.4149, lr=0.0100
[02/26 05:11:46 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 41/100, Acc=0.8551, Val Loss=0.4348, lr=0.0100
[02/26 05:12:13 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 42/100, Acc=0.8585, Val Loss=0.4138, lr=0.0100
[02/26 05:12:40 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 43/100, Acc=0.8407, Val Loss=0.4741, lr=0.0100
[02/26 05:13:08 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 44/100, Acc=0.8546, Val Loss=0.4328, lr=0.0100
[02/26 05:13:35 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 45/100, Acc=0.8520, Val Loss=0.4363, lr=0.0100
[02/26 05:14:03 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 46/100, Acc=0.8485, Val Loss=0.4516, lr=0.0100
[02/26 05:14:30 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 47/100, Acc=0.8291, Val Loss=0.4899, lr=0.0100
[02/26 05:14:58 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 48/100, Acc=0.8558, Val Loss=0.4270, lr=0.0100
[02/26 05:15:25 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 49/100, Acc=0.8576, Val Loss=0.4319, lr=0.0100
[02/26 05:15:53 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 50/100, Acc=0.8559, Val Loss=0.4221, lr=0.0100
[02/26 05:16:20 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 51/100, Acc=0.8451, Val Loss=0.4497, lr=0.0100
[02/26 05:16:48 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 52/100, Acc=0.8647, Val Loss=0.4041, lr=0.0100
[02/26 05:17:16 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 53/100, Acc=0.8574, Val Loss=0.4351, lr=0.0100
[02/26 05:17:43 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 54/100, Acc=0.8689, Val Loss=0.3810, lr=0.0100
[02/26 05:18:11 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 55/100, Acc=0.8412, Val Loss=0.4703, lr=0.0100
[02/26 05:18:38 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 56/100, Acc=0.8611, Val Loss=0.4068, lr=0.0100
[02/26 05:19:06 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 57/100, Acc=0.8583, Val Loss=0.4209, lr=0.0100
[02/26 05:19:33 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 58/100, Acc=0.8639, Val Loss=0.4035, lr=0.0100
[02/26 05:20:01 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 59/100, Acc=0.8707, Val Loss=0.3857, lr=0.0100
[02/26 05:20:28 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 60/100, Acc=0.8916, Val Loss=0.3232, lr=0.0010
[02/26 05:20:56 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 61/100, Acc=0.8924, Val Loss=0.3160, lr=0.0010
[02/26 05:21:23 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 62/100, Acc=0.8927, Val Loss=0.3132, lr=0.0010
[02/26 05:21:50 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 63/100, Acc=0.8954, Val Loss=0.3089, lr=0.0010
[02/26 05:22:17 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 64/100, Acc=0.8950, Val Loss=0.3092, lr=0.0010
[02/26 05:22:45 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 65/100, Acc=0.8966, Val Loss=0.3107, lr=0.0010
[02/26 05:23:12 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 66/100, Acc=0.8981, Val Loss=0.3063, lr=0.0010
[02/26 05:23:39 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 67/100, Acc=0.8965, Val Loss=0.3082, lr=0.0010
[02/26 05:24:07 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 68/100, Acc=0.8964, Val Loss=0.3112, lr=0.0010
[02/26 05:24:34 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 69/100, Acc=0.8961, Val Loss=0.3089, lr=0.0010
[02/26 05:25:01 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 70/100, Acc=0.8965, Val Loss=0.3098, lr=0.0010
[02/26 05:25:29 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 71/100, Acc=0.8964, Val Loss=0.3080, lr=0.0010
[02/26 05:25:56 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 72/100, Acc=0.8964, Val Loss=0.3087, lr=0.0010
[02/26 05:26:24 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 73/100, Acc=0.8963, Val Loss=0.3088, lr=0.0010
[02/26 05:26:52 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 74/100, Acc=0.8986, Val Loss=0.3062, lr=0.0010
[02/26 05:27:19 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 75/100, Acc=0.8977, Val Loss=0.3081, lr=0.0010
[02/26 05:27:47 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 76/100, Acc=0.8979, Val Loss=0.3070, lr=0.0010
[02/26 05:28:14 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 77/100, Acc=0.8977, Val Loss=0.3088, lr=0.0010
[02/26 05:28:42 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 78/100, Acc=0.8951, Val Loss=0.3113, lr=0.0010
[02/26 05:29:09 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 79/100, Acc=0.8972, Val Loss=0.3097, lr=0.0010
[02/26 05:29:37 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 80/100, Acc=0.8984, Val Loss=0.3052, lr=0.0001
[02/26 05:30:04 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 81/100, Acc=0.8993, Val Loss=0.3062, lr=0.0001
[02/26 05:30:32 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 82/100, Acc=0.8995, Val Loss=0.3067, lr=0.0001
[02/26 05:31:00 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 83/100, Acc=0.8981, Val Loss=0.3052, lr=0.0001
[02/26 05:31:27 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 84/100, Acc=0.8978, Val Loss=0.3052, lr=0.0001
[02/26 05:31:55 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 85/100, Acc=0.8976, Val Loss=0.3050, lr=0.0001
[02/26 05:32:22 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 86/100, Acc=0.8988, Val Loss=0.3055, lr=0.0001
[02/26 05:32:50 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 87/100, Acc=0.8998, Val Loss=0.3025, lr=0.0001
[02/26 05:33:17 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 88/100, Acc=0.8998, Val Loss=0.3025, lr=0.0001
[02/26 05:33:44 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 89/100, Acc=0.9004, Val Loss=0.3028, lr=0.0001
[02/26 05:34:12 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 90/100, Acc=0.9005, Val Loss=0.3046, lr=0.0001
[02/26 05:34:39 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 91/100, Acc=0.9003, Val Loss=0.3032, lr=0.0001
[02/26 05:35:07 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 92/100, Acc=0.9002, Val Loss=0.3023, lr=0.0001
[02/26 05:35:35 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 93/100, Acc=0.8996, Val Loss=0.3039, lr=0.0001
[02/26 05:36:02 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 94/100, Acc=0.8997, Val Loss=0.3028, lr=0.0001
[02/26 05:36:30 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 95/100, Acc=0.9011, Val Loss=0.3027, lr=0.0001
[02/26 05:36:57 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 96/100, Acc=0.8996, Val Loss=0.3022, lr=0.0001
[02/26 05:37:25 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 97/100, Acc=0.8987, Val Loss=0.3035, lr=0.0001
[02/26 05:37:52 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 98/100, Acc=0.8983, Val Loss=0.3027, lr=0.0001
[02/26 05:38:20 cifar10-global-fpgm-3.0-mobilenetv2]: Epoch 99/100, Acc=0.8991, Val Loss=0.3021, lr=0.0001
[02/26 05:38:20 cifar10-global-fpgm-3.0-mobilenetv2]: Best Acc=0.9011
[02/26 05:38:20 cifar10-global-fpgm-3.0-mobilenetv2]: Params: 0.65 M
[02/26 05:38:20 cifar10-global-fpgm-3.0-mobilenetv2]: ops: 22.69 M
[02/26 05:38:23 cifar10-global-fpgm-3.0-mobilenetv2]: Acc: 0.8991 Val Loss: 0.3021

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: mode: prune
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: model: mobilenetv2
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: verbose: False
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: dataset: cifar10
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: dataroot: data
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: batch_size: 128
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: total_epochs: 100
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: lr: 0.01
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: restore: run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: output_dir: run/cifar10/prune/cifar10-global-obdc-3.0-mobilenetv2
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: finetune: True
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: last_epochs: 100
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: reps: 1
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: method: obdc
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: speed_up: 3.0
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: reg: 1e-05
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: delta_reg: 0.0001
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: weight_decay: 0.0005
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: seed: 1
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: global_pruning: True
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: sl_total_epochs: 100
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: sl_lr: 0.01
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: sl_reg_warmup: 0
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: sl_restore: None
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: iterative_steps: 400
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: logger: <Logger cifar10-global-obdc-3.0-mobilenetv2 (DEBUG)>
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: device: cuda
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: num_classes: 10
[02/26 05:38:30 cifar10-global-obdc-3.0-mobilenetv2]: Loading model from run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/26 05:38:33 cifar10-global-obdc-3.0-mobilenetv2]: Pruning...
Traceback (most recent call last):
  File "/home/REDACTED/paper1/main.py", line 429, in <module>
    main()
  File "/home/REDACTED/paper1/main.py", line 369, in main
    progressive_pruning(
  File "/home/REDACTED/paper1/main.py", line 65, in progressive_pruning
    imp._prepare_model(model, pruner)
  File "/home/REDACTED/paper1/torch_pruning/pruner/importance.py", line 872, in _prepare_model
    for i, (dep, idxs) in enumerate(group):
TypeError: 'NoneType' object is not iterable
/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: mode: prune
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: model: mobilenetv2
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: verbose: False
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: dataset: cifar10
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: dataroot: data
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: batch_size: 128
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: total_epochs: 100
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: lr: 0.01
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: restore: run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: output_dir: run/cifar10/prune/cifar10-global-lamp-3.0-mobilenetv2
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: finetune: True
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: last_epochs: 100
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: reps: 1
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: method: lamp
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: speed_up: 3.0
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: reg: 1e-05
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: delta_reg: 0.0001
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: weight_decay: 0.0005
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: seed: 1
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: global_pruning: True
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: sl_total_epochs: 100
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: sl_lr: 0.01
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: sl_reg_warmup: 0
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: sl_restore: None
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: iterative_steps: 400
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: logger: <Logger cifar10-global-lamp-3.0-mobilenetv2 (DEBUG)>
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: device: cuda
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: num_classes: 10
[02/26 05:38:38 cifar10-global-lamp-3.0-mobilenetv2]: Loading model from run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/26 05:38:42 cifar10-global-lamp-3.0-mobilenetv2]: Pruning...
[02/26 05:39:14 cifar10-global-lamp-3.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 7, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(7, 17, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(17, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=17)
      (4): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(17, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 68, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(68, 68, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=68)
        (4): BatchNorm2d(68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(68, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 73, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(73, 73, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=73)
        (4): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(73, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 50, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(50, 50, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=50)
        (4): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(50, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 70, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(70, 70, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=70)
        (4): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(70, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 33, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(33, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=33)
        (4): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(33, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 40, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40)
        (4): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(40, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 51, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(51, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=51)
        (4): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(51, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 33, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(33, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=33)
        (4): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(33, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 14, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(14, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14)
        (4): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(14, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 73, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(73, 73, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=73)
        (4): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(73, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 17, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(17, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=17)
        (4): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(17, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 3, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=3)
        (4): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(3, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 142, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(142, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(142, 142, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=142)
        (4): BatchNorm2d(142, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(142, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 7, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=7)
        (4): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(7, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 4, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=4)
        (4): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(4, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 404, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(404, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(404, 404, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=404)
      (4): BatchNorm2d(404, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(404, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1181, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1181, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1181, 10, kernel_size=(1, 1), stride=(1, 1))
)
[02/26 05:39:17 cifar10-global-lamp-3.0-mobilenetv2]: Params: 2.25 M => 0.70 M (30.88%)
[02/26 05:39:17 cifar10-global-lamp-3.0-mobilenetv2]: FLOPs: 68.29 M => 22.63 M (33.14%, 3.02X )
[02/26 05:39:17 cifar10-global-lamp-3.0-mobilenetv2]: Acc: 0.8936 => 0.8936
[02/26 05:39:17 cifar10-global-lamp-3.0-mobilenetv2]: Val Loss: 0.3202 => 0.3194
[02/26 05:39:17 cifar10-global-lamp-3.0-mobilenetv2]: Finetuning...
[02/26 05:39:44 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 0/100, Acc=0.8475, Val Loss=0.4487, lr=0.0100
[02/26 05:40:12 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 1/100, Acc=0.8350, Val Loss=0.4832, lr=0.0100
[02/26 05:40:39 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 2/100, Acc=0.8408, Val Loss=0.4690, lr=0.0100
[02/26 05:41:06 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 3/100, Acc=0.8487, Val Loss=0.4411, lr=0.0100
[02/26 05:41:33 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 4/100, Acc=0.8595, Val Loss=0.4196, lr=0.0100
[02/26 05:42:01 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 5/100, Acc=0.8500, Val Loss=0.4321, lr=0.0100
[02/26 05:42:28 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 6/100, Acc=0.8433, Val Loss=0.4633, lr=0.0100
[02/26 05:42:55 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 7/100, Acc=0.8523, Val Loss=0.4282, lr=0.0100
[02/26 05:43:22 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 8/100, Acc=0.8596, Val Loss=0.4185, lr=0.0100
[02/26 05:43:50 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 9/100, Acc=0.8495, Val Loss=0.4383, lr=0.0100
[02/26 05:44:17 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 10/100, Acc=0.8620, Val Loss=0.4023, lr=0.0100
[02/26 05:44:44 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 11/100, Acc=0.8640, Val Loss=0.3984, lr=0.0100
[02/26 05:45:11 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 12/100, Acc=0.8514, Val Loss=0.4341, lr=0.0100
[02/26 05:45:38 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 13/100, Acc=0.8442, Val Loss=0.4425, lr=0.0100
[02/26 05:46:06 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 14/100, Acc=0.8473, Val Loss=0.4578, lr=0.0100
[02/26 05:46:33 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 15/100, Acc=0.8561, Val Loss=0.4328, lr=0.0100
[02/26 05:47:00 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 16/100, Acc=0.8674, Val Loss=0.3868, lr=0.0100
[02/26 05:47:28 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 17/100, Acc=0.8575, Val Loss=0.4183, lr=0.0100
[02/26 05:47:55 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 18/100, Acc=0.8561, Val Loss=0.4236, lr=0.0100
[02/26 05:48:22 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 19/100, Acc=0.8544, Val Loss=0.4343, lr=0.0100
[02/26 05:48:50 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 20/100, Acc=0.8417, Val Loss=0.4676, lr=0.0100
[02/26 05:49:17 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 21/100, Acc=0.8607, Val Loss=0.4061, lr=0.0100
[02/26 05:49:44 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 22/100, Acc=0.8559, Val Loss=0.4269, lr=0.0100
[02/26 05:50:12 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 23/100, Acc=0.8595, Val Loss=0.4168, lr=0.0100
[02/26 05:50:39 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 24/100, Acc=0.8634, Val Loss=0.4130, lr=0.0100
[02/26 05:51:07 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 25/100, Acc=0.8607, Val Loss=0.4215, lr=0.0100
[02/26 05:51:34 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 26/100, Acc=0.8524, Val Loss=0.4398, lr=0.0100
[02/26 05:52:01 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 27/100, Acc=0.8329, Val Loss=0.4932, lr=0.0100
[02/26 05:52:29 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 28/100, Acc=0.8501, Val Loss=0.4529, lr=0.0100
[02/26 05:52:56 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 29/100, Acc=0.8316, Val Loss=0.4936, lr=0.0100
[02/26 05:53:23 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 30/100, Acc=0.8507, Val Loss=0.4373, lr=0.0100
[02/26 05:53:51 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 31/100, Acc=0.8501, Val Loss=0.4466, lr=0.0100
[02/26 05:54:18 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 32/100, Acc=0.8471, Val Loss=0.4526, lr=0.0100
[02/26 05:54:46 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 33/100, Acc=0.8636, Val Loss=0.3955, lr=0.0100
[02/26 05:55:13 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 34/100, Acc=0.8665, Val Loss=0.3947, lr=0.0100
[02/26 05:55:41 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 35/100, Acc=0.8629, Val Loss=0.4149, lr=0.0100
[02/26 05:56:08 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 36/100, Acc=0.8532, Val Loss=0.4320, lr=0.0100
[02/26 05:56:35 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 37/100, Acc=0.8611, Val Loss=0.4022, lr=0.0100
[02/26 05:57:02 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 38/100, Acc=0.8594, Val Loss=0.4290, lr=0.0100
[02/26 05:57:29 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 39/100, Acc=0.8597, Val Loss=0.4088, lr=0.0100
[02/26 05:57:56 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 40/100, Acc=0.8632, Val Loss=0.4098, lr=0.0100
[02/26 05:58:23 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 41/100, Acc=0.8646, Val Loss=0.4061, lr=0.0100
[02/26 05:58:50 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 42/100, Acc=0.8620, Val Loss=0.4031, lr=0.0100
[02/26 05:59:17 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 43/100, Acc=0.8636, Val Loss=0.4100, lr=0.0100
[02/26 05:59:44 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 44/100, Acc=0.8650, Val Loss=0.4001, lr=0.0100
[02/26 06:00:11 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 45/100, Acc=0.8643, Val Loss=0.4031, lr=0.0100
[02/26 06:00:39 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 46/100, Acc=0.8525, Val Loss=0.4289, lr=0.0100
[02/26 06:01:06 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 47/100, Acc=0.8354, Val Loss=0.4849, lr=0.0100
[02/26 06:01:33 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 48/100, Acc=0.8466, Val Loss=0.4439, lr=0.0100
[02/26 06:02:00 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 49/100, Acc=0.8556, Val Loss=0.4237, lr=0.0100
[02/26 06:02:27 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 50/100, Acc=0.8617, Val Loss=0.4155, lr=0.0100
[02/26 06:02:54 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 51/100, Acc=0.8577, Val Loss=0.4234, lr=0.0100
[02/26 06:03:21 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 52/100, Acc=0.8537, Val Loss=0.4274, lr=0.0100
[02/26 06:03:48 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 53/100, Acc=0.8509, Val Loss=0.4389, lr=0.0100
[02/26 06:04:15 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 54/100, Acc=0.8645, Val Loss=0.4095, lr=0.0100
[02/26 06:04:43 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 55/100, Acc=0.8598, Val Loss=0.4117, lr=0.0100
[02/26 06:05:10 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 56/100, Acc=0.8461, Val Loss=0.4608, lr=0.0100
[02/26 06:05:37 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 57/100, Acc=0.8466, Val Loss=0.4494, lr=0.0100
[02/26 06:06:04 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 58/100, Acc=0.8548, Val Loss=0.4392, lr=0.0100
[02/26 06:06:32 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 59/100, Acc=0.8575, Val Loss=0.4132, lr=0.0100
[02/26 06:06:59 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 60/100, Acc=0.8927, Val Loss=0.3154, lr=0.0010
[02/26 06:07:26 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 61/100, Acc=0.8955, Val Loss=0.3075, lr=0.0010
[02/26 06:07:53 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 62/100, Acc=0.8946, Val Loss=0.3067, lr=0.0010
[02/26 06:08:20 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 63/100, Acc=0.8967, Val Loss=0.3072, lr=0.0010
[02/26 06:08:48 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 64/100, Acc=0.8969, Val Loss=0.3025, lr=0.0010
[02/26 06:09:15 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 65/100, Acc=0.8957, Val Loss=0.3056, lr=0.0010
[02/26 06:09:42 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 66/100, Acc=0.8968, Val Loss=0.3053, lr=0.0010
[02/26 06:10:09 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 67/100, Acc=0.8957, Val Loss=0.3075, lr=0.0010
[02/26 06:10:37 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 68/100, Acc=0.8990, Val Loss=0.3070, lr=0.0010
[02/26 06:11:04 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 69/100, Acc=0.8978, Val Loss=0.3050, lr=0.0010
[02/26 06:11:31 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 70/100, Acc=0.8989, Val Loss=0.3085, lr=0.0010
[02/26 06:11:58 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 71/100, Acc=0.8990, Val Loss=0.3025, lr=0.0010
[02/26 06:12:25 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 72/100, Acc=0.8978, Val Loss=0.3060, lr=0.0010
[02/26 06:12:52 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 73/100, Acc=0.8982, Val Loss=0.3067, lr=0.0010
[02/26 06:13:19 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 74/100, Acc=0.9001, Val Loss=0.3046, lr=0.0010
[02/26 06:13:47 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 75/100, Acc=0.8976, Val Loss=0.3068, lr=0.0010
[02/26 06:14:14 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 76/100, Acc=0.8977, Val Loss=0.3086, lr=0.0010
[02/26 06:14:41 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 77/100, Acc=0.8980, Val Loss=0.3102, lr=0.0010
[02/26 06:15:08 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 78/100, Acc=0.8975, Val Loss=0.3090, lr=0.0010
[02/26 06:15:35 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 79/100, Acc=0.8976, Val Loss=0.3062, lr=0.0010
[02/26 06:16:02 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 80/100, Acc=0.9001, Val Loss=0.3004, lr=0.0001
[02/26 06:16:29 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 81/100, Acc=0.9010, Val Loss=0.3016, lr=0.0001
[02/26 06:16:57 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 82/100, Acc=0.8997, Val Loss=0.3036, lr=0.0001
[02/26 06:17:23 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 83/100, Acc=0.8985, Val Loss=0.3044, lr=0.0001
[02/26 06:17:50 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 84/100, Acc=0.8999, Val Loss=0.3028, lr=0.0001
[02/26 06:18:17 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 85/100, Acc=0.8989, Val Loss=0.3041, lr=0.0001
[02/26 06:18:45 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 86/100, Acc=0.8993, Val Loss=0.3037, lr=0.0001
[02/26 06:19:12 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 87/100, Acc=0.8999, Val Loss=0.3017, lr=0.0001
[02/26 06:19:39 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 88/100, Acc=0.9001, Val Loss=0.3012, lr=0.0001
[02/26 06:20:06 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 89/100, Acc=0.9010, Val Loss=0.3019, lr=0.0001
[02/26 06:20:33 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 90/100, Acc=0.9005, Val Loss=0.3040, lr=0.0001
[02/26 06:21:01 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 91/100, Acc=0.9006, Val Loss=0.3020, lr=0.0001
[02/26 06:21:28 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 92/100, Acc=0.9021, Val Loss=0.3013, lr=0.0001
[02/26 06:21:55 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 93/100, Acc=0.8999, Val Loss=0.3026, lr=0.0001
[02/26 06:22:22 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 94/100, Acc=0.9007, Val Loss=0.3020, lr=0.0001
[02/26 06:22:49 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 95/100, Acc=0.9003, Val Loss=0.3031, lr=0.0001
[02/26 06:23:17 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 96/100, Acc=0.9000, Val Loss=0.3027, lr=0.0001
[02/26 06:23:44 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 97/100, Acc=0.8988, Val Loss=0.3035, lr=0.0001
[02/26 06:24:11 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 98/100, Acc=0.8982, Val Loss=0.3034, lr=0.0001
[02/26 06:24:38 cifar10-global-lamp-3.0-mobilenetv2]: Epoch 99/100, Acc=0.8994, Val Loss=0.3029, lr=0.0001
[02/26 06:24:38 cifar10-global-lamp-3.0-mobilenetv2]: Best Acc=0.9021
[02/26 06:24:38 cifar10-global-lamp-3.0-mobilenetv2]: Params: 0.70 M
[02/26 06:24:38 cifar10-global-lamp-3.0-mobilenetv2]: ops: 22.63 M
[02/26 06:24:41 cifar10-global-lamp-3.0-mobilenetv2]: Acc: 0.8994 Val Loss: 0.3029

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: mode: prune
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: model: mobilenetv2
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: verbose: False
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: dataset: cifar10
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: dataroot: data
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: batch_size: 128
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: total_epochs: 100
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: lr: 0.01
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: restore: run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: output_dir: run/cifar10/prune/cifar10-global-slim-3.0-mobilenetv2
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: finetune: True
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: last_epochs: 100
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: reps: 1
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: method: slim
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: speed_up: 3.0
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: reg: 1e-05
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: delta_reg: 0.0001
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: weight_decay: 0.0005
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: seed: 1
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: global_pruning: True
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: sl_total_epochs: 100
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: sl_lr: 0.01
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: sl_reg_warmup: 0
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: sl_restore: None
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: iterative_steps: 400
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: logger: <Logger cifar10-global-slim-3.0-mobilenetv2 (DEBUG)>
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: device: cuda
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: num_classes: 10
[02/26 06:24:48 cifar10-global-slim-3.0-mobilenetv2]: Loading model from run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/26 06:24:49 cifar10-global-slim-3.0-mobilenetv2]: Regularizing...
[02/26 06:25:16 cifar10-global-slim-3.0-mobilenetv2]: Epoch 0/100, Acc=0.8527, Val Loss=0.4365, lr=0.0100
[02/26 06:25:43 cifar10-global-slim-3.0-mobilenetv2]: Epoch 1/100, Acc=0.8519, Val Loss=0.4381, lr=0.0100
[02/26 06:26:10 cifar10-global-slim-3.0-mobilenetv2]: Epoch 2/100, Acc=0.8566, Val Loss=0.4242, lr=0.0100
[02/26 06:26:38 cifar10-global-slim-3.0-mobilenetv2]: Epoch 3/100, Acc=0.8474, Val Loss=0.4567, lr=0.0100
[02/26 06:27:05 cifar10-global-slim-3.0-mobilenetv2]: Epoch 4/100, Acc=0.8531, Val Loss=0.4475, lr=0.0100
[02/26 06:27:33 cifar10-global-slim-3.0-mobilenetv2]: Epoch 5/100, Acc=0.8559, Val Loss=0.4162, lr=0.0100
[02/26 06:28:00 cifar10-global-slim-3.0-mobilenetv2]: Epoch 6/100, Acc=0.8622, Val Loss=0.4001, lr=0.0100
[02/26 06:28:27 cifar10-global-slim-3.0-mobilenetv2]: Epoch 7/100, Acc=0.8618, Val Loss=0.4110, lr=0.0100
[02/26 06:28:54 cifar10-global-slim-3.0-mobilenetv2]: Epoch 8/100, Acc=0.8661, Val Loss=0.4083, lr=0.0100
[02/26 06:29:22 cifar10-global-slim-3.0-mobilenetv2]: Epoch 9/100, Acc=0.8614, Val Loss=0.4175, lr=0.0100
[02/26 06:29:49 cifar10-global-slim-3.0-mobilenetv2]: Epoch 10/100, Acc=0.8582, Val Loss=0.4323, lr=0.0100
[02/26 06:30:16 cifar10-global-slim-3.0-mobilenetv2]: Epoch 11/100, Acc=0.8730, Val Loss=0.3771, lr=0.0100
[02/26 06:30:44 cifar10-global-slim-3.0-mobilenetv2]: Epoch 12/100, Acc=0.8664, Val Loss=0.3991, lr=0.0100
[02/26 06:31:11 cifar10-global-slim-3.0-mobilenetv2]: Epoch 13/100, Acc=0.8653, Val Loss=0.3996, lr=0.0100
[02/26 06:31:39 cifar10-global-slim-3.0-mobilenetv2]: Epoch 14/100, Acc=0.8723, Val Loss=0.3860, lr=0.0100
[02/26 06:32:06 cifar10-global-slim-3.0-mobilenetv2]: Epoch 15/100, Acc=0.8677, Val Loss=0.4009, lr=0.0100
[02/26 06:32:34 cifar10-global-slim-3.0-mobilenetv2]: Epoch 16/100, Acc=0.8692, Val Loss=0.4023, lr=0.0100
[02/26 06:33:01 cifar10-global-slim-3.0-mobilenetv2]: Epoch 17/100, Acc=0.8690, Val Loss=0.4024, lr=0.0100
[02/26 06:33:29 cifar10-global-slim-3.0-mobilenetv2]: Epoch 18/100, Acc=0.8763, Val Loss=0.3730, lr=0.0100
[02/26 06:33:57 cifar10-global-slim-3.0-mobilenetv2]: Epoch 19/100, Acc=0.8773, Val Loss=0.3710, lr=0.0100
[02/26 06:34:24 cifar10-global-slim-3.0-mobilenetv2]: Epoch 20/100, Acc=0.8785, Val Loss=0.3617, lr=0.0100
[02/26 06:34:52 cifar10-global-slim-3.0-mobilenetv2]: Epoch 21/100, Acc=0.8630, Val Loss=0.4226, lr=0.0100
[02/26 06:35:20 cifar10-global-slim-3.0-mobilenetv2]: Epoch 22/100, Acc=0.8710, Val Loss=0.3946, lr=0.0100
[02/26 06:35:48 cifar10-global-slim-3.0-mobilenetv2]: Epoch 23/100, Acc=0.8702, Val Loss=0.3888, lr=0.0100
[02/26 06:36:16 cifar10-global-slim-3.0-mobilenetv2]: Epoch 24/100, Acc=0.8723, Val Loss=0.3808, lr=0.0100
[02/26 06:36:43 cifar10-global-slim-3.0-mobilenetv2]: Epoch 25/100, Acc=0.8783, Val Loss=0.3640, lr=0.0100
[02/26 06:37:11 cifar10-global-slim-3.0-mobilenetv2]: Epoch 26/100, Acc=0.8678, Val Loss=0.3925, lr=0.0100
[02/26 06:37:39 cifar10-global-slim-3.0-mobilenetv2]: Epoch 27/100, Acc=0.8726, Val Loss=0.3839, lr=0.0100
[02/26 06:38:06 cifar10-global-slim-3.0-mobilenetv2]: Epoch 28/100, Acc=0.8671, Val Loss=0.4030, lr=0.0100
[02/26 06:38:34 cifar10-global-slim-3.0-mobilenetv2]: Epoch 29/100, Acc=0.8736, Val Loss=0.3932, lr=0.0100
[02/26 06:39:02 cifar10-global-slim-3.0-mobilenetv2]: Epoch 30/100, Acc=0.8783, Val Loss=0.3782, lr=0.0100
[02/26 06:39:30 cifar10-global-slim-3.0-mobilenetv2]: Epoch 31/100, Acc=0.8718, Val Loss=0.3969, lr=0.0100
[02/26 06:39:58 cifar10-global-slim-3.0-mobilenetv2]: Epoch 32/100, Acc=0.8775, Val Loss=0.3677, lr=0.0100
[02/26 06:40:25 cifar10-global-slim-3.0-mobilenetv2]: Epoch 33/100, Acc=0.8780, Val Loss=0.3742, lr=0.0100
[02/26 06:40:53 cifar10-global-slim-3.0-mobilenetv2]: Epoch 34/100, Acc=0.8708, Val Loss=0.3958, lr=0.0100
[02/26 06:41:21 cifar10-global-slim-3.0-mobilenetv2]: Epoch 35/100, Acc=0.8733, Val Loss=0.3850, lr=0.0100
[02/26 06:41:49 cifar10-global-slim-3.0-mobilenetv2]: Epoch 36/100, Acc=0.8800, Val Loss=0.3652, lr=0.0100
[02/26 06:42:17 cifar10-global-slim-3.0-mobilenetv2]: Epoch 37/100, Acc=0.8761, Val Loss=0.3855, lr=0.0100
[02/26 06:42:45 cifar10-global-slim-3.0-mobilenetv2]: Epoch 38/100, Acc=0.8844, Val Loss=0.3594, lr=0.0100
[02/26 06:43:13 cifar10-global-slim-3.0-mobilenetv2]: Epoch 39/100, Acc=0.8750, Val Loss=0.3829, lr=0.0100
[02/26 06:43:41 cifar10-global-slim-3.0-mobilenetv2]: Epoch 40/100, Acc=0.8797, Val Loss=0.3682, lr=0.0100
[02/26 06:44:08 cifar10-global-slim-3.0-mobilenetv2]: Epoch 41/100, Acc=0.8803, Val Loss=0.3725, lr=0.0100
[02/26 06:44:36 cifar10-global-slim-3.0-mobilenetv2]: Epoch 42/100, Acc=0.8800, Val Loss=0.3701, lr=0.0100
[02/26 06:45:04 cifar10-global-slim-3.0-mobilenetv2]: Epoch 43/100, Acc=0.8770, Val Loss=0.3772, lr=0.0100
[02/26 06:45:31 cifar10-global-slim-3.0-mobilenetv2]: Epoch 44/100, Acc=0.8814, Val Loss=0.3582, lr=0.0100
[02/26 06:45:59 cifar10-global-slim-3.0-mobilenetv2]: Epoch 45/100, Acc=0.8829, Val Loss=0.3526, lr=0.0100
[02/26 06:46:27 cifar10-global-slim-3.0-mobilenetv2]: Epoch 46/100, Acc=0.8793, Val Loss=0.3608, lr=0.0100
[02/26 06:46:55 cifar10-global-slim-3.0-mobilenetv2]: Epoch 47/100, Acc=0.8777, Val Loss=0.3704, lr=0.0100
[02/26 06:47:22 cifar10-global-slim-3.0-mobilenetv2]: Epoch 48/100, Acc=0.8794, Val Loss=0.3729, lr=0.0100
[02/26 06:47:50 cifar10-global-slim-3.0-mobilenetv2]: Epoch 49/100, Acc=0.8835, Val Loss=0.3612, lr=0.0100
[02/26 06:48:18 cifar10-global-slim-3.0-mobilenetv2]: Epoch 50/100, Acc=0.8842, Val Loss=0.3495, lr=0.0100
[02/26 06:48:46 cifar10-global-slim-3.0-mobilenetv2]: Epoch 51/100, Acc=0.8833, Val Loss=0.3626, lr=0.0100
[02/26 06:49:14 cifar10-global-slim-3.0-mobilenetv2]: Epoch 52/100, Acc=0.8755, Val Loss=0.3842, lr=0.0100
[02/26 06:49:41 cifar10-global-slim-3.0-mobilenetv2]: Epoch 53/100, Acc=0.8807, Val Loss=0.3738, lr=0.0100
[02/26 06:50:09 cifar10-global-slim-3.0-mobilenetv2]: Epoch 54/100, Acc=0.8764, Val Loss=0.3744, lr=0.0100
[02/26 06:50:37 cifar10-global-slim-3.0-mobilenetv2]: Epoch 55/100, Acc=0.8779, Val Loss=0.3677, lr=0.0100
[02/26 06:51:04 cifar10-global-slim-3.0-mobilenetv2]: Epoch 56/100, Acc=0.8854, Val Loss=0.3527, lr=0.0100
[02/26 06:51:32 cifar10-global-slim-3.0-mobilenetv2]: Epoch 57/100, Acc=0.8849, Val Loss=0.3526, lr=0.0100
[02/26 06:51:59 cifar10-global-slim-3.0-mobilenetv2]: Epoch 58/100, Acc=0.8808, Val Loss=0.3663, lr=0.0100
[02/26 06:52:27 cifar10-global-slim-3.0-mobilenetv2]: Epoch 59/100, Acc=0.8839, Val Loss=0.3499, lr=0.0100
[02/26 06:52:54 cifar10-global-slim-3.0-mobilenetv2]: Epoch 60/100, Acc=0.8936, Val Loss=0.3179, lr=0.0010
[02/26 06:53:23 cifar10-global-slim-3.0-mobilenetv2]: Epoch 61/100, Acc=0.8975, Val Loss=0.3131, lr=0.0010
[02/26 06:53:51 cifar10-global-slim-3.0-mobilenetv2]: Epoch 62/100, Acc=0.8979, Val Loss=0.3124, lr=0.0010
[02/26 06:54:19 cifar10-global-slim-3.0-mobilenetv2]: Epoch 63/100, Acc=0.8984, Val Loss=0.3172, lr=0.0010
[02/26 06:54:46 cifar10-global-slim-3.0-mobilenetv2]: Epoch 64/100, Acc=0.8979, Val Loss=0.3156, lr=0.0010
[02/26 06:55:14 cifar10-global-slim-3.0-mobilenetv2]: Epoch 65/100, Acc=0.8985, Val Loss=0.3161, lr=0.0010
[02/26 06:55:42 cifar10-global-slim-3.0-mobilenetv2]: Epoch 66/100, Acc=0.8995, Val Loss=0.3138, lr=0.0010
[02/26 06:56:09 cifar10-global-slim-3.0-mobilenetv2]: Epoch 67/100, Acc=0.8989, Val Loss=0.3119, lr=0.0010
[02/26 06:56:37 cifar10-global-slim-3.0-mobilenetv2]: Epoch 68/100, Acc=0.8996, Val Loss=0.3147, lr=0.0010
[02/26 06:57:05 cifar10-global-slim-3.0-mobilenetv2]: Epoch 69/100, Acc=0.8982, Val Loss=0.3177, lr=0.0010
[02/26 06:57:32 cifar10-global-slim-3.0-mobilenetv2]: Epoch 70/100, Acc=0.8989, Val Loss=0.3138, lr=0.0010
[02/26 06:58:00 cifar10-global-slim-3.0-mobilenetv2]: Epoch 71/100, Acc=0.8981, Val Loss=0.3166, lr=0.0010
[02/26 06:58:27 cifar10-global-slim-3.0-mobilenetv2]: Epoch 72/100, Acc=0.8989, Val Loss=0.3167, lr=0.0010
[02/26 06:58:55 cifar10-global-slim-3.0-mobilenetv2]: Epoch 73/100, Acc=0.8991, Val Loss=0.3183, lr=0.0010
[02/26 06:59:22 cifar10-global-slim-3.0-mobilenetv2]: Epoch 74/100, Acc=0.8988, Val Loss=0.3169, lr=0.0010
[02/26 06:59:50 cifar10-global-slim-3.0-mobilenetv2]: Epoch 75/100, Acc=0.8992, Val Loss=0.3190, lr=0.0010
[02/26 07:00:18 cifar10-global-slim-3.0-mobilenetv2]: Epoch 76/100, Acc=0.8991, Val Loss=0.3197, lr=0.0010
[02/26 07:00:45 cifar10-global-slim-3.0-mobilenetv2]: Epoch 77/100, Acc=0.8981, Val Loss=0.3215, lr=0.0010
[02/26 07:01:13 cifar10-global-slim-3.0-mobilenetv2]: Epoch 78/100, Acc=0.8993, Val Loss=0.3220, lr=0.0010
[02/26 07:01:40 cifar10-global-slim-3.0-mobilenetv2]: Epoch 79/100, Acc=0.8989, Val Loss=0.3203, lr=0.0010
[02/26 07:02:08 cifar10-global-slim-3.0-mobilenetv2]: Epoch 80/100, Acc=0.8995, Val Loss=0.3176, lr=0.0001
[02/26 07:02:35 cifar10-global-slim-3.0-mobilenetv2]: Epoch 81/100, Acc=0.8979, Val Loss=0.3200, lr=0.0001
[02/26 07:03:03 cifar10-global-slim-3.0-mobilenetv2]: Epoch 82/100, Acc=0.8989, Val Loss=0.3181, lr=0.0001
[02/26 07:03:30 cifar10-global-slim-3.0-mobilenetv2]: Epoch 83/100, Acc=0.8995, Val Loss=0.3199, lr=0.0001
[02/26 07:03:58 cifar10-global-slim-3.0-mobilenetv2]: Epoch 84/100, Acc=0.8990, Val Loss=0.3191, lr=0.0001
[02/26 07:04:25 cifar10-global-slim-3.0-mobilenetv2]: Epoch 85/100, Acc=0.8988, Val Loss=0.3187, lr=0.0001
[02/26 07:04:53 cifar10-global-slim-3.0-mobilenetv2]: Epoch 86/100, Acc=0.8999, Val Loss=0.3201, lr=0.0001
[02/26 07:05:20 cifar10-global-slim-3.0-mobilenetv2]: Epoch 87/100, Acc=0.8994, Val Loss=0.3191, lr=0.0001
[02/26 07:05:48 cifar10-global-slim-3.0-mobilenetv2]: Epoch 88/100, Acc=0.8983, Val Loss=0.3188, lr=0.0001
[02/26 07:06:15 cifar10-global-slim-3.0-mobilenetv2]: Epoch 89/100, Acc=0.8977, Val Loss=0.3190, lr=0.0001
[02/26 07:06:43 cifar10-global-slim-3.0-mobilenetv2]: Epoch 90/100, Acc=0.9005, Val Loss=0.3179, lr=0.0001
[02/26 07:07:11 cifar10-global-slim-3.0-mobilenetv2]: Epoch 91/100, Acc=0.8985, Val Loss=0.3182, lr=0.0001
[02/26 07:07:38 cifar10-global-slim-3.0-mobilenetv2]: Epoch 92/100, Acc=0.8986, Val Loss=0.3195, lr=0.0001
[02/26 07:08:06 cifar10-global-slim-3.0-mobilenetv2]: Epoch 93/100, Acc=0.8983, Val Loss=0.3179, lr=0.0001
[02/26 07:08:33 cifar10-global-slim-3.0-mobilenetv2]: Epoch 94/100, Acc=0.8985, Val Loss=0.3185, lr=0.0001
[02/26 07:09:01 cifar10-global-slim-3.0-mobilenetv2]: Epoch 95/100, Acc=0.8985, Val Loss=0.3199, lr=0.0001
[02/26 07:09:28 cifar10-global-slim-3.0-mobilenetv2]: Epoch 96/100, Acc=0.8990, Val Loss=0.3192, lr=0.0001
[02/26 07:09:56 cifar10-global-slim-3.0-mobilenetv2]: Epoch 97/100, Acc=0.8994, Val Loss=0.3187, lr=0.0001
[02/26 07:10:24 cifar10-global-slim-3.0-mobilenetv2]: Epoch 98/100, Acc=0.8992, Val Loss=0.3194, lr=0.0001
[02/26 07:10:52 cifar10-global-slim-3.0-mobilenetv2]: Epoch 99/100, Acc=0.8989, Val Loss=0.3196, lr=0.0001
[02/26 07:10:52 cifar10-global-slim-3.0-mobilenetv2]: Best Acc=0.9005
[02/26 07:10:52 cifar10-global-slim-3.0-mobilenetv2]: Loading the sparse model from run/cifar10/prune/cifar10-global-slim-3.0-mobilenetv2/reg_cifar10_mobilenetv2_slim_1e-05.pth...
[02/26 07:10:55 cifar10-global-slim-3.0-mobilenetv2]: Pruning...
[02/26 07:11:18 cifar10-global-slim-3.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 7, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(7, 16, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 67, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(67, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(67, 67, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=67)
        (4): BatchNorm2d(67, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(67, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72)
        (4): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 50, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(50, 50, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=50)
        (4): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(50, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 65, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(65, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(65, 65, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=65)
        (4): BatchNorm2d(65, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(65, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 31, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=31)
        (4): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(31, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 39, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(39, 39, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=39)
        (4): BatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(39, 62, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(62, 47, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(47, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=47)
        (4): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(47, 62, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(62, 31, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=31)
        (4): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(31, 62, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(62, 11, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(11, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=11)
        (4): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(11, 62, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(62, 71, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(71, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(71, 71, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=71)
        (4): BatchNorm2d(71, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(71, 89, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(89, 428, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(428, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(428, 428, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=428)
        (4): BatchNorm2d(428, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(428, 89, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(89, 550, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(550, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(550, 550, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=550)
        (4): BatchNorm2d(550, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(550, 89, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(89, 119, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(119, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(119, 119, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=119)
        (4): BatchNorm2d(119, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(119, 130, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(130, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(130, 7, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=7)
        (4): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(7, 130, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(130, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(130, 1, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (4): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(1, 130, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(130, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(130, 295, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(295, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(295, 295, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=295)
      (4): BatchNorm2d(295, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(295, 281, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(281, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(281, 1032, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1032, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1032, 10, kernel_size=(1, 1), stride=(1, 1))
)
[02/26 07:11:21 cifar10-global-slim-3.0-mobilenetv2]: Params: 2.25 M => 0.70 M (31.07%)
[02/26 07:11:21 cifar10-global-slim-3.0-mobilenetv2]: FLOPs: 68.29 M => 22.68 M (33.22%, 3.01X )
[02/26 07:11:21 cifar10-global-slim-3.0-mobilenetv2]: Acc: 0.9005 => 0.8499
[02/26 07:11:21 cifar10-global-slim-3.0-mobilenetv2]: Val Loss: 0.3179 => 0.4730
[02/26 07:11:21 cifar10-global-slim-3.0-mobilenetv2]: Finetuning...
[02/26 07:11:48 cifar10-global-slim-3.0-mobilenetv2]: Epoch 0/100, Acc=0.8836, Val Loss=0.3541, lr=0.0100
[02/26 07:12:15 cifar10-global-slim-3.0-mobilenetv2]: Epoch 1/100, Acc=0.8824, Val Loss=0.3630, lr=0.0100
[02/26 07:12:43 cifar10-global-slim-3.0-mobilenetv2]: Epoch 2/100, Acc=0.8782, Val Loss=0.3657, lr=0.0100
[02/26 07:13:10 cifar10-global-slim-3.0-mobilenetv2]: Epoch 3/100, Acc=0.8814, Val Loss=0.3567, lr=0.0100
[02/26 07:13:38 cifar10-global-slim-3.0-mobilenetv2]: Epoch 4/100, Acc=0.8823, Val Loss=0.3681, lr=0.0100
[02/26 07:14:05 cifar10-global-slim-3.0-mobilenetv2]: Epoch 5/100, Acc=0.8832, Val Loss=0.3605, lr=0.0100
[02/26 07:14:32 cifar10-global-slim-3.0-mobilenetv2]: Epoch 6/100, Acc=0.8790, Val Loss=0.3643, lr=0.0100
[02/26 07:15:00 cifar10-global-slim-3.0-mobilenetv2]: Epoch 7/100, Acc=0.8770, Val Loss=0.3751, lr=0.0100
[02/26 07:15:27 cifar10-global-slim-3.0-mobilenetv2]: Epoch 8/100, Acc=0.8738, Val Loss=0.3771, lr=0.0100
[02/26 07:15:55 cifar10-global-slim-3.0-mobilenetv2]: Epoch 9/100, Acc=0.8823, Val Loss=0.3555, lr=0.0100
[02/26 07:16:22 cifar10-global-slim-3.0-mobilenetv2]: Epoch 10/100, Acc=0.8755, Val Loss=0.3778, lr=0.0100
[02/26 07:16:49 cifar10-global-slim-3.0-mobilenetv2]: Epoch 11/100, Acc=0.8760, Val Loss=0.3814, lr=0.0100
[02/26 07:17:17 cifar10-global-slim-3.0-mobilenetv2]: Epoch 12/100, Acc=0.8804, Val Loss=0.3623, lr=0.0100
[02/26 07:17:44 cifar10-global-slim-3.0-mobilenetv2]: Epoch 13/100, Acc=0.8709, Val Loss=0.3930, lr=0.0100
[02/26 07:18:11 cifar10-global-slim-3.0-mobilenetv2]: Epoch 14/100, Acc=0.8792, Val Loss=0.3672, lr=0.0100
[02/26 07:18:39 cifar10-global-slim-3.0-mobilenetv2]: Epoch 15/100, Acc=0.8616, Val Loss=0.4219, lr=0.0100
[02/26 07:19:06 cifar10-global-slim-3.0-mobilenetv2]: Epoch 16/100, Acc=0.8641, Val Loss=0.4181, lr=0.0100
[02/26 07:19:34 cifar10-global-slim-3.0-mobilenetv2]: Epoch 17/100, Acc=0.8720, Val Loss=0.3868, lr=0.0100
[02/26 07:20:01 cifar10-global-slim-3.0-mobilenetv2]: Epoch 18/100, Acc=0.8736, Val Loss=0.3700, lr=0.0100
[02/26 07:20:28 cifar10-global-slim-3.0-mobilenetv2]: Epoch 19/100, Acc=0.8666, Val Loss=0.3984, lr=0.0100
[02/26 07:20:56 cifar10-global-slim-3.0-mobilenetv2]: Epoch 20/100, Acc=0.8583, Val Loss=0.4208, lr=0.0100
[02/26 07:21:23 cifar10-global-slim-3.0-mobilenetv2]: Epoch 21/100, Acc=0.8766, Val Loss=0.3720, lr=0.0100
[02/26 07:21:50 cifar10-global-slim-3.0-mobilenetv2]: Epoch 22/100, Acc=0.8768, Val Loss=0.3732, lr=0.0100
[02/26 07:22:18 cifar10-global-slim-3.0-mobilenetv2]: Epoch 23/100, Acc=0.8694, Val Loss=0.3933, lr=0.0100
[02/26 07:22:45 cifar10-global-slim-3.0-mobilenetv2]: Epoch 24/100, Acc=0.8665, Val Loss=0.4030, lr=0.0100
[02/26 07:23:12 cifar10-global-slim-3.0-mobilenetv2]: Epoch 25/100, Acc=0.8722, Val Loss=0.3770, lr=0.0100
[02/26 07:23:40 cifar10-global-slim-3.0-mobilenetv2]: Epoch 26/100, Acc=0.8590, Val Loss=0.4212, lr=0.0100
[02/26 07:24:07 cifar10-global-slim-3.0-mobilenetv2]: Epoch 27/100, Acc=0.8763, Val Loss=0.3644, lr=0.0100
[02/26 07:24:35 cifar10-global-slim-3.0-mobilenetv2]: Epoch 28/100, Acc=0.8737, Val Loss=0.3792, lr=0.0100
[02/26 07:25:02 cifar10-global-slim-3.0-mobilenetv2]: Epoch 29/100, Acc=0.8579, Val Loss=0.4054, lr=0.0100
[02/26 07:25:29 cifar10-global-slim-3.0-mobilenetv2]: Epoch 30/100, Acc=0.8750, Val Loss=0.3804, lr=0.0100
[02/26 07:25:57 cifar10-global-slim-3.0-mobilenetv2]: Epoch 31/100, Acc=0.8653, Val Loss=0.3984, lr=0.0100
[02/26 07:26:24 cifar10-global-slim-3.0-mobilenetv2]: Epoch 32/100, Acc=0.8640, Val Loss=0.4110, lr=0.0100
[02/26 07:26:52 cifar10-global-slim-3.0-mobilenetv2]: Epoch 33/100, Acc=0.8405, Val Loss=0.4869, lr=0.0100
[02/26 07:27:19 cifar10-global-slim-3.0-mobilenetv2]: Epoch 34/100, Acc=0.8613, Val Loss=0.4182, lr=0.0100
[02/26 07:27:47 cifar10-global-slim-3.0-mobilenetv2]: Epoch 35/100, Acc=0.8584, Val Loss=0.4261, lr=0.0100
[02/26 07:28:14 cifar10-global-slim-3.0-mobilenetv2]: Epoch 36/100, Acc=0.8556, Val Loss=0.4384, lr=0.0100
[02/26 07:28:42 cifar10-global-slim-3.0-mobilenetv2]: Epoch 37/100, Acc=0.8670, Val Loss=0.4050, lr=0.0100
[02/26 07:29:10 cifar10-global-slim-3.0-mobilenetv2]: Epoch 38/100, Acc=0.8721, Val Loss=0.3951, lr=0.0100
[02/26 07:29:38 cifar10-global-slim-3.0-mobilenetv2]: Epoch 39/100, Acc=0.8599, Val Loss=0.4156, lr=0.0100
[02/26 07:30:06 cifar10-global-slim-3.0-mobilenetv2]: Epoch 40/100, Acc=0.8694, Val Loss=0.3869, lr=0.0100
[02/26 07:30:33 cifar10-global-slim-3.0-mobilenetv2]: Epoch 41/100, Acc=0.8538, Val Loss=0.4311, lr=0.0100
[02/26 07:31:01 cifar10-global-slim-3.0-mobilenetv2]: Epoch 42/100, Acc=0.8624, Val Loss=0.4106, lr=0.0100
[02/26 07:31:30 cifar10-global-slim-3.0-mobilenetv2]: Epoch 43/100, Acc=0.8568, Val Loss=0.4225, lr=0.0100
[02/26 07:31:58 cifar10-global-slim-3.0-mobilenetv2]: Epoch 44/100, Acc=0.8612, Val Loss=0.4174, lr=0.0100
[02/26 07:32:26 cifar10-global-slim-3.0-mobilenetv2]: Epoch 45/100, Acc=0.8619, Val Loss=0.4085, lr=0.0100
[02/26 07:32:54 cifar10-global-slim-3.0-mobilenetv2]: Epoch 46/100, Acc=0.8667, Val Loss=0.3906, lr=0.0100
[02/26 07:33:22 cifar10-global-slim-3.0-mobilenetv2]: Epoch 47/100, Acc=0.8628, Val Loss=0.4173, lr=0.0100
[02/26 07:33:50 cifar10-global-slim-3.0-mobilenetv2]: Epoch 48/100, Acc=0.8581, Val Loss=0.4376, lr=0.0100
[02/26 07:34:18 cifar10-global-slim-3.0-mobilenetv2]: Epoch 49/100, Acc=0.8648, Val Loss=0.3941, lr=0.0100
[02/26 07:34:46 cifar10-global-slim-3.0-mobilenetv2]: Epoch 50/100, Acc=0.8561, Val Loss=0.4205, lr=0.0100
[02/26 07:35:14 cifar10-global-slim-3.0-mobilenetv2]: Epoch 51/100, Acc=0.8585, Val Loss=0.4234, lr=0.0100
[02/26 07:35:42 cifar10-global-slim-3.0-mobilenetv2]: Epoch 52/100, Acc=0.8613, Val Loss=0.4047, lr=0.0100
[02/26 07:36:10 cifar10-global-slim-3.0-mobilenetv2]: Epoch 53/100, Acc=0.8656, Val Loss=0.3977, lr=0.0100
[02/26 07:36:38 cifar10-global-slim-3.0-mobilenetv2]: Epoch 54/100, Acc=0.8633, Val Loss=0.4099, lr=0.0100
[02/26 07:37:06 cifar10-global-slim-3.0-mobilenetv2]: Epoch 55/100, Acc=0.8570, Val Loss=0.4217, lr=0.0100
[02/26 07:37:34 cifar10-global-slim-3.0-mobilenetv2]: Epoch 56/100, Acc=0.8614, Val Loss=0.4049, lr=0.0100
[02/26 07:38:02 cifar10-global-slim-3.0-mobilenetv2]: Epoch 57/100, Acc=0.8688, Val Loss=0.3944, lr=0.0100
[02/26 07:38:30 cifar10-global-slim-3.0-mobilenetv2]: Epoch 58/100, Acc=0.8618, Val Loss=0.4185, lr=0.0100
[02/26 07:38:58 cifar10-global-slim-3.0-mobilenetv2]: Epoch 59/100, Acc=0.8428, Val Loss=0.4732, lr=0.0100
[02/26 07:39:26 cifar10-global-slim-3.0-mobilenetv2]: Epoch 60/100, Acc=0.8969, Val Loss=0.3073, lr=0.0010
[02/26 07:39:54 cifar10-global-slim-3.0-mobilenetv2]: Epoch 61/100, Acc=0.9006, Val Loss=0.3011, lr=0.0010
[02/26 07:40:22 cifar10-global-slim-3.0-mobilenetv2]: Epoch 62/100, Acc=0.9011, Val Loss=0.2957, lr=0.0010
[02/26 07:40:50 cifar10-global-slim-3.0-mobilenetv2]: Epoch 63/100, Acc=0.9018, Val Loss=0.2968, lr=0.0010
[02/26 07:41:18 cifar10-global-slim-3.0-mobilenetv2]: Epoch 64/100, Acc=0.9026, Val Loss=0.2951, lr=0.0010
[02/26 07:41:46 cifar10-global-slim-3.0-mobilenetv2]: Epoch 65/100, Acc=0.9007, Val Loss=0.2971, lr=0.0010
[02/26 07:42:14 cifar10-global-slim-3.0-mobilenetv2]: Epoch 66/100, Acc=0.8978, Val Loss=0.3002, lr=0.0010
[02/26 07:42:42 cifar10-global-slim-3.0-mobilenetv2]: Epoch 67/100, Acc=0.9015, Val Loss=0.2985, lr=0.0010
[02/26 07:43:09 cifar10-global-slim-3.0-mobilenetv2]: Epoch 68/100, Acc=0.9018, Val Loss=0.2955, lr=0.0010
[02/26 07:43:37 cifar10-global-slim-3.0-mobilenetv2]: Epoch 69/100, Acc=0.9004, Val Loss=0.3003, lr=0.0010
[02/26 07:44:05 cifar10-global-slim-3.0-mobilenetv2]: Epoch 70/100, Acc=0.9005, Val Loss=0.3004, lr=0.0010
[02/26 07:44:32 cifar10-global-slim-3.0-mobilenetv2]: Epoch 71/100, Acc=0.8996, Val Loss=0.3025, lr=0.0010
[02/26 07:45:00 cifar10-global-slim-3.0-mobilenetv2]: Epoch 72/100, Acc=0.9002, Val Loss=0.3044, lr=0.0010
[02/26 07:45:28 cifar10-global-slim-3.0-mobilenetv2]: Epoch 73/100, Acc=0.8998, Val Loss=0.3017, lr=0.0010
[02/26 07:45:56 cifar10-global-slim-3.0-mobilenetv2]: Epoch 74/100, Acc=0.9010, Val Loss=0.3054, lr=0.0010
[02/26 07:46:23 cifar10-global-slim-3.0-mobilenetv2]: Epoch 75/100, Acc=0.9020, Val Loss=0.3058, lr=0.0010
[02/26 07:46:51 cifar10-global-slim-3.0-mobilenetv2]: Epoch 76/100, Acc=0.9013, Val Loss=0.3070, lr=0.0010
[02/26 07:47:19 cifar10-global-slim-3.0-mobilenetv2]: Epoch 77/100, Acc=0.9013, Val Loss=0.3061, lr=0.0010
[02/26 07:47:46 cifar10-global-slim-3.0-mobilenetv2]: Epoch 78/100, Acc=0.9010, Val Loss=0.3068, lr=0.0010
[02/26 07:48:14 cifar10-global-slim-3.0-mobilenetv2]: Epoch 79/100, Acc=0.9002, Val Loss=0.3042, lr=0.0010
[02/26 07:48:41 cifar10-global-slim-3.0-mobilenetv2]: Epoch 80/100, Acc=0.9013, Val Loss=0.3043, lr=0.0001
[02/26 07:49:09 cifar10-global-slim-3.0-mobilenetv2]: Epoch 81/100, Acc=0.9021, Val Loss=0.3031, lr=0.0001
[02/26 07:49:36 cifar10-global-slim-3.0-mobilenetv2]: Epoch 82/100, Acc=0.9025, Val Loss=0.3012, lr=0.0001
[02/26 07:50:04 cifar10-global-slim-3.0-mobilenetv2]: Epoch 83/100, Acc=0.9029, Val Loss=0.3043, lr=0.0001
[02/26 07:50:32 cifar10-global-slim-3.0-mobilenetv2]: Epoch 84/100, Acc=0.9030, Val Loss=0.3022, lr=0.0001
[02/26 07:50:59 cifar10-global-slim-3.0-mobilenetv2]: Epoch 85/100, Acc=0.9013, Val Loss=0.3028, lr=0.0001
[02/26 07:51:27 cifar10-global-slim-3.0-mobilenetv2]: Epoch 86/100, Acc=0.9027, Val Loss=0.3043, lr=0.0001
[02/26 07:51:55 cifar10-global-slim-3.0-mobilenetv2]: Epoch 87/100, Acc=0.9030, Val Loss=0.3019, lr=0.0001
[02/26 07:52:23 cifar10-global-slim-3.0-mobilenetv2]: Epoch 88/100, Acc=0.9020, Val Loss=0.3029, lr=0.0001
[02/26 07:52:51 cifar10-global-slim-3.0-mobilenetv2]: Epoch 89/100, Acc=0.9008, Val Loss=0.3013, lr=0.0001
[02/26 07:53:18 cifar10-global-slim-3.0-mobilenetv2]: Epoch 90/100, Acc=0.9019, Val Loss=0.3012, lr=0.0001
[02/26 07:53:46 cifar10-global-slim-3.0-mobilenetv2]: Epoch 91/100, Acc=0.9011, Val Loss=0.3043, lr=0.0001
[02/26 07:54:13 cifar10-global-slim-3.0-mobilenetv2]: Epoch 92/100, Acc=0.9030, Val Loss=0.3032, lr=0.0001
[02/26 07:54:41 cifar10-global-slim-3.0-mobilenetv2]: Epoch 93/100, Acc=0.9027, Val Loss=0.3015, lr=0.0001
[02/26 07:55:09 cifar10-global-slim-3.0-mobilenetv2]: Epoch 94/100, Acc=0.9011, Val Loss=0.3019, lr=0.0001
[02/26 07:55:36 cifar10-global-slim-3.0-mobilenetv2]: Epoch 95/100, Acc=0.9022, Val Loss=0.3029, lr=0.0001
[02/26 07:56:04 cifar10-global-slim-3.0-mobilenetv2]: Epoch 96/100, Acc=0.9008, Val Loss=0.3046, lr=0.0001
[02/26 07:56:31 cifar10-global-slim-3.0-mobilenetv2]: Epoch 97/100, Acc=0.9026, Val Loss=0.3031, lr=0.0001
[02/26 07:56:59 cifar10-global-slim-3.0-mobilenetv2]: Epoch 98/100, Acc=0.9030, Val Loss=0.3042, lr=0.0001
[02/26 07:57:26 cifar10-global-slim-3.0-mobilenetv2]: Epoch 99/100, Acc=0.9023, Val Loss=0.3018, lr=0.0001
[02/26 07:57:26 cifar10-global-slim-3.0-mobilenetv2]: Best Acc=0.9030
[02/26 07:57:26 cifar10-global-slim-3.0-mobilenetv2]: Params: 0.70 M
[02/26 07:57:26 cifar10-global-slim-3.0-mobilenetv2]: ops: 22.68 M
[02/26 07:57:29 cifar10-global-slim-3.0-mobilenetv2]: Acc: 0.9023 Val Loss: 0.3018

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: mode: prune
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: model: mobilenetv2
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: verbose: False
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: dataset: cifar10
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: dataroot: data
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: batch_size: 128
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: total_epochs: 100
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: lr: 0.01
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: restore: run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: output_dir: run/cifar10/prune/cifar10-global-group_norm-3.0-mobilenetv2
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: finetune: True
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: last_epochs: 100
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: reps: 1
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: method: group_norm
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: speed_up: 3.0
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: reg: 1e-05
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: delta_reg: 0.0001
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: weight_decay: 0.0005
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: seed: 1
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: global_pruning: True
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: sl_total_epochs: 100
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: sl_lr: 0.01
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: sl_reg_warmup: 0
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: sl_restore: None
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: iterative_steps: 400
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: logger: <Logger cifar10-global-group_norm-3.0-mobilenetv2 (DEBUG)>
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: device: cuda
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: num_classes: 10
[02/26 07:57:37 cifar10-global-group_norm-3.0-mobilenetv2]: Loading model from run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/26 07:57:40 cifar10-global-group_norm-3.0-mobilenetv2]: Pruning...
[02/26 07:58:11 cifar10-global-group_norm-3.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 7, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(7, 16, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 67, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(67, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(67, 67, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=67)
        (4): BatchNorm2d(67, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(67, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72)
        (4): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 50, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(50, 50, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=50)
        (4): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(50, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 66, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(66, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(66, 66, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=66)
        (4): BatchNorm2d(66, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(66, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 31, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=31)
        (4): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(31, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 39, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(39, 39, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=39)
        (4): BatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(39, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)
        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 31, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(31, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=31)
        (4): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(31, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 11, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(11, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=11)
        (4): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(11, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 72, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72)
        (4): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(72, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 2, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)
        (4): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(2, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 1, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (4): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(1, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 132, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(132, 132, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=132)
        (4): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(132, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 7, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=7)
        (4): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(7, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 1, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (4): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(1, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 378, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(378, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(378, 378, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=378)
      (4): BatchNorm2d(378, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(378, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1258, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1258, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1258, 10, kernel_size=(1, 1), stride=(1, 1))
)
[02/26 07:58:14 cifar10-global-group_norm-3.0-mobilenetv2]: Params: 2.25 M => 0.70 M (31.03%)
[02/26 07:58:14 cifar10-global-group_norm-3.0-mobilenetv2]: FLOPs: 68.29 M => 22.58 M (33.06%, 3.02X )
[02/26 07:58:14 cifar10-global-group_norm-3.0-mobilenetv2]: Acc: 0.8936 => 0.8949
[02/26 07:58:14 cifar10-global-group_norm-3.0-mobilenetv2]: Val Loss: 0.3202 => 0.3194
[02/26 07:58:14 cifar10-global-group_norm-3.0-mobilenetv2]: Finetuning...
[02/26 07:58:41 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 0/100, Acc=0.8612, Val Loss=0.4127, lr=0.0100
[02/26 07:59:08 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 1/100, Acc=0.8454, Val Loss=0.4557, lr=0.0100
[02/26 07:59:36 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 2/100, Acc=0.8454, Val Loss=0.4651, lr=0.0100
[02/26 08:00:03 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 3/100, Acc=0.8519, Val Loss=0.4302, lr=0.0100
[02/26 08:00:31 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 4/100, Acc=0.8494, Val Loss=0.4301, lr=0.0100
[02/26 08:00:58 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 5/100, Acc=0.8541, Val Loss=0.4319, lr=0.0100
[02/26 08:01:26 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 6/100, Acc=0.8492, Val Loss=0.4590, lr=0.0100
[02/26 08:01:53 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 7/100, Acc=0.8535, Val Loss=0.4294, lr=0.0100
[02/26 08:02:21 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 8/100, Acc=0.8614, Val Loss=0.4279, lr=0.0100
[02/26 08:02:48 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 9/100, Acc=0.8491, Val Loss=0.4465, lr=0.0100
[02/26 08:03:16 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 10/100, Acc=0.8605, Val Loss=0.4246, lr=0.0100
[02/26 08:03:44 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 11/100, Acc=0.8567, Val Loss=0.4249, lr=0.0100
[02/26 08:04:11 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 12/100, Acc=0.8474, Val Loss=0.4398, lr=0.0100
[02/26 08:04:39 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 13/100, Acc=0.8525, Val Loss=0.4323, lr=0.0100
[02/26 08:05:06 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 14/100, Acc=0.8510, Val Loss=0.4572, lr=0.0100
[02/26 08:05:34 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 15/100, Acc=0.8469, Val Loss=0.4592, lr=0.0100
[02/26 08:06:01 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 16/100, Acc=0.8487, Val Loss=0.4460, lr=0.0100
[02/26 08:06:28 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 17/100, Acc=0.8495, Val Loss=0.4342, lr=0.0100
[02/26 08:06:56 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 18/100, Acc=0.8540, Val Loss=0.4326, lr=0.0100
[02/26 08:07:24 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 19/100, Acc=0.8503, Val Loss=0.4456, lr=0.0100
[02/26 08:07:51 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 20/100, Acc=0.8356, Val Loss=0.4864, lr=0.0100
[02/26 08:08:19 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 21/100, Acc=0.8608, Val Loss=0.4119, lr=0.0100
[02/26 08:08:47 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 22/100, Acc=0.8516, Val Loss=0.4397, lr=0.0100
[02/26 08:09:14 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 23/100, Acc=0.8543, Val Loss=0.4342, lr=0.0100
[02/26 08:09:42 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 24/100, Acc=0.8588, Val Loss=0.4069, lr=0.0100
[02/26 08:10:10 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 25/100, Acc=0.8588, Val Loss=0.4308, lr=0.0100
[02/26 08:10:37 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 26/100, Acc=0.8548, Val Loss=0.4286, lr=0.0100
[02/26 08:11:05 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 27/100, Acc=0.8541, Val Loss=0.4306, lr=0.0100
[02/26 08:11:32 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 28/100, Acc=0.8604, Val Loss=0.4151, lr=0.0100
[02/26 08:12:00 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 29/100, Acc=0.8532, Val Loss=0.4357, lr=0.0100
[02/26 08:12:28 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 30/100, Acc=0.8577, Val Loss=0.4165, lr=0.0100
[02/26 08:12:55 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 31/100, Acc=0.8472, Val Loss=0.4512, lr=0.0100
[02/26 08:13:23 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 32/100, Acc=0.8525, Val Loss=0.4252, lr=0.0100
[02/26 08:13:51 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 33/100, Acc=0.8455, Val Loss=0.4475, lr=0.0100
[02/26 08:14:19 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 34/100, Acc=0.8564, Val Loss=0.4334, lr=0.0100
[02/26 08:14:46 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 35/100, Acc=0.8532, Val Loss=0.4412, lr=0.0100
[02/26 08:15:13 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 36/100, Acc=0.8603, Val Loss=0.4110, lr=0.0100
[02/26 08:15:41 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 37/100, Acc=0.8548, Val Loss=0.4318, lr=0.0100
[02/26 08:16:09 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 38/100, Acc=0.8627, Val Loss=0.4070, lr=0.0100
[02/26 08:16:37 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 39/100, Acc=0.8570, Val Loss=0.4261, lr=0.0100
[02/26 08:17:05 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 40/100, Acc=0.8623, Val Loss=0.4050, lr=0.0100
[02/26 08:17:33 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 41/100, Acc=0.8606, Val Loss=0.4163, lr=0.0100
[02/26 08:18:00 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 42/100, Acc=0.8623, Val Loss=0.4159, lr=0.0100
[02/26 08:18:28 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 43/100, Acc=0.8552, Val Loss=0.4293, lr=0.0100
[02/26 08:18:56 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 44/100, Acc=0.8598, Val Loss=0.4057, lr=0.0100
[02/26 08:19:24 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 45/100, Acc=0.8457, Val Loss=0.4529, lr=0.0100
[02/26 08:19:51 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 46/100, Acc=0.8422, Val Loss=0.4699, lr=0.0100
[02/26 08:20:19 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 47/100, Acc=0.8615, Val Loss=0.4224, lr=0.0100
[02/26 08:20:47 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 48/100, Acc=0.8530, Val Loss=0.4373, lr=0.0100
[02/26 08:21:14 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 49/100, Acc=0.8524, Val Loss=0.4307, lr=0.0100
[02/26 08:21:42 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 50/100, Acc=0.8629, Val Loss=0.4099, lr=0.0100
[02/26 08:22:10 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 51/100, Acc=0.8648, Val Loss=0.4098, lr=0.0100
[02/26 08:22:38 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 52/100, Acc=0.8587, Val Loss=0.4289, lr=0.0100
[02/26 08:23:06 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 53/100, Acc=0.8555, Val Loss=0.4309, lr=0.0100
[02/26 08:23:33 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 54/100, Acc=0.8642, Val Loss=0.4065, lr=0.0100
[02/26 08:24:01 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 55/100, Acc=0.8499, Val Loss=0.4406, lr=0.0100
[02/26 08:24:29 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 56/100, Acc=0.8461, Val Loss=0.4760, lr=0.0100
[02/26 08:24:56 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 57/100, Acc=0.8582, Val Loss=0.4211, lr=0.0100
[02/26 08:25:24 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 58/100, Acc=0.8645, Val Loss=0.3969, lr=0.0100
[02/26 08:25:52 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 59/100, Acc=0.8694, Val Loss=0.3878, lr=0.0100
[02/26 08:26:20 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 60/100, Acc=0.8905, Val Loss=0.3170, lr=0.0010
[02/26 08:26:47 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 61/100, Acc=0.8944, Val Loss=0.3089, lr=0.0010
[02/26 08:27:15 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 62/100, Acc=0.8967, Val Loss=0.3077, lr=0.0010
[02/26 08:27:43 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 63/100, Acc=0.8933, Val Loss=0.3089, lr=0.0010
[02/26 08:28:11 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 64/100, Acc=0.8959, Val Loss=0.3093, lr=0.0010
[02/26 08:28:38 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 65/100, Acc=0.8976, Val Loss=0.3070, lr=0.0010
[02/26 08:29:06 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 66/100, Acc=0.8979, Val Loss=0.3067, lr=0.0010
[02/26 08:29:34 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 67/100, Acc=0.8964, Val Loss=0.3061, lr=0.0010
[02/26 08:30:02 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 68/100, Acc=0.8985, Val Loss=0.3082, lr=0.0010
[02/26 08:30:29 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 69/100, Acc=0.8985, Val Loss=0.3054, lr=0.0010
[02/26 08:30:57 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 70/100, Acc=0.8980, Val Loss=0.3088, lr=0.0010
[02/26 08:31:25 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 71/100, Acc=0.8957, Val Loss=0.3094, lr=0.0010
[02/26 08:31:52 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 72/100, Acc=0.8968, Val Loss=0.3082, lr=0.0010
[02/26 08:32:20 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 73/100, Acc=0.8972, Val Loss=0.3109, lr=0.0010
[02/26 08:32:48 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 74/100, Acc=0.8975, Val Loss=0.3076, lr=0.0010
[02/26 08:33:15 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 75/100, Acc=0.8978, Val Loss=0.3062, lr=0.0010
[02/26 08:33:43 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 76/100, Acc=0.8971, Val Loss=0.3126, lr=0.0010
[02/26 08:34:11 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 77/100, Acc=0.8987, Val Loss=0.3110, lr=0.0010
[02/26 08:34:39 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 78/100, Acc=0.8954, Val Loss=0.3120, lr=0.0010
[02/26 08:35:07 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 79/100, Acc=0.8963, Val Loss=0.3126, lr=0.0010
[02/26 08:35:34 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 80/100, Acc=0.8976, Val Loss=0.3073, lr=0.0001
[02/26 08:36:02 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 81/100, Acc=0.8982, Val Loss=0.3076, lr=0.0001
[02/26 08:36:30 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 82/100, Acc=0.8984, Val Loss=0.3073, lr=0.0001
[02/26 08:36:58 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 83/100, Acc=0.8986, Val Loss=0.3076, lr=0.0001
[02/26 08:37:26 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 84/100, Acc=0.8984, Val Loss=0.3068, lr=0.0001
[02/26 08:37:54 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 85/100, Acc=0.8980, Val Loss=0.3075, lr=0.0001
[02/26 08:38:22 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 86/100, Acc=0.8976, Val Loss=0.3077, lr=0.0001
[02/26 08:38:49 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 87/100, Acc=0.8993, Val Loss=0.3053, lr=0.0001
[02/26 08:39:18 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 88/100, Acc=0.8996, Val Loss=0.3055, lr=0.0001
[02/26 08:39:46 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 89/100, Acc=0.8992, Val Loss=0.3054, lr=0.0001
[02/26 08:40:14 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 90/100, Acc=0.8980, Val Loss=0.3073, lr=0.0001
[02/26 08:40:42 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 91/100, Acc=0.8988, Val Loss=0.3052, lr=0.0001
[02/26 08:41:09 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 92/100, Acc=0.8991, Val Loss=0.3040, lr=0.0001
[02/26 08:41:37 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 93/100, Acc=0.8988, Val Loss=0.3063, lr=0.0001
[02/26 08:42:05 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 94/100, Acc=0.8985, Val Loss=0.3050, lr=0.0001
[02/26 08:42:33 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 95/100, Acc=0.8991, Val Loss=0.3056, lr=0.0001
[02/26 08:43:00 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 96/100, Acc=0.8996, Val Loss=0.3039, lr=0.0001
[02/26 08:43:28 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 97/100, Acc=0.8988, Val Loss=0.3051, lr=0.0001
[02/26 08:43:56 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 98/100, Acc=0.8999, Val Loss=0.3045, lr=0.0001
[02/26 08:44:24 cifar10-global-group_norm-3.0-mobilenetv2]: Epoch 99/100, Acc=0.9000, Val Loss=0.3044, lr=0.0001
[02/26 08:44:24 cifar10-global-group_norm-3.0-mobilenetv2]: Best Acc=0.9000
[02/26 08:44:24 cifar10-global-group_norm-3.0-mobilenetv2]: Params: 0.70 M
[02/26 08:44:24 cifar10-global-group_norm-3.0-mobilenetv2]: ops: 22.58 M
[02/26 08:44:27 cifar10-global-group_norm-3.0-mobilenetv2]: Acc: 0.9000 Val Loss: 0.3044

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: mode: prune
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: model: mobilenetv2
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: verbose: False
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: dataset: cifar10
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: dataroot: data
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: batch_size: 128
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: total_epochs: 100
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: lr: 0.01
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: restore: run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: output_dir: run/cifar10/prune/cifar10-global-group_sl-3.0-mobilenetv2
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: finetune: True
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: last_epochs: 100
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: reps: 1
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: method: group_sl
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: speed_up: 3.0
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: reg: 1e-05
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: delta_reg: 0.0001
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: weight_decay: 0.0005
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: seed: 1
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: global_pruning: True
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: sl_total_epochs: 100
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: sl_lr: 0.01
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: sl_reg_warmup: 0
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: sl_restore: None
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: iterative_steps: 400
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: logger: <Logger cifar10-global-group_sl-3.0-mobilenetv2 (DEBUG)>
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: device: cuda
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: num_classes: 10
[02/26 08:44:34 cifar10-global-group_sl-3.0-mobilenetv2]: Loading model from run/cifar10/pretrain/cifar10_mobilenetv2.pth
[02/26 08:44:35 cifar10-global-group_sl-3.0-mobilenetv2]: Regularizing...
[02/26 08:45:38 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 0/100, Acc=0.8530, Val Loss=0.4319, lr=0.0100
[02/26 08:46:41 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 1/100, Acc=0.8472, Val Loss=0.4596, lr=0.0100
[02/26 08:47:43 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 2/100, Acc=0.8593, Val Loss=0.4155, lr=0.0100
[02/26 08:48:45 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 3/100, Acc=0.8580, Val Loss=0.4211, lr=0.0100
[02/26 08:49:47 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 4/100, Acc=0.8601, Val Loss=0.4190, lr=0.0100
[02/26 08:50:49 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 5/100, Acc=0.8635, Val Loss=0.3988, lr=0.0100
[02/26 08:51:51 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 6/100, Acc=0.8653, Val Loss=0.3978, lr=0.0100
[02/26 08:52:53 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 7/100, Acc=0.8587, Val Loss=0.4212, lr=0.0100
[02/26 08:53:55 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 8/100, Acc=0.8626, Val Loss=0.4134, lr=0.0100
[02/26 08:54:57 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 9/100, Acc=0.8653, Val Loss=0.4005, lr=0.0100
[02/26 08:56:00 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 10/100, Acc=0.8558, Val Loss=0.4356, lr=0.0100
[02/26 08:57:03 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 11/100, Acc=0.8611, Val Loss=0.4135, lr=0.0100
[02/26 08:58:05 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 12/100, Acc=0.8737, Val Loss=0.3859, lr=0.0100
[02/26 08:59:08 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 13/100, Acc=0.8660, Val Loss=0.4042, lr=0.0100
[02/26 09:00:11 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 14/100, Acc=0.8664, Val Loss=0.4073, lr=0.0100
[02/26 09:01:13 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 15/100, Acc=0.8678, Val Loss=0.3940, lr=0.0100
[02/26 09:02:16 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 16/100, Acc=0.8476, Val Loss=0.4608, lr=0.0100
[02/26 09:03:19 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 17/100, Acc=0.8642, Val Loss=0.4037, lr=0.0100
[02/26 09:04:22 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 18/100, Acc=0.8756, Val Loss=0.3757, lr=0.0100
[02/26 09:05:25 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 19/100, Acc=0.8774, Val Loss=0.3728, lr=0.0100
[02/26 09:06:28 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 20/100, Acc=0.8738, Val Loss=0.3770, lr=0.0100
[02/26 09:07:31 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 21/100, Acc=0.8677, Val Loss=0.3962, lr=0.0100
[02/26 09:08:34 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 22/100, Acc=0.8710, Val Loss=0.3831, lr=0.0100
[02/26 09:09:36 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 23/100, Acc=0.8711, Val Loss=0.3837, lr=0.0100
[02/26 09:10:39 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 24/100, Acc=0.8663, Val Loss=0.3949, lr=0.0100
[02/26 09:11:41 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 25/100, Acc=0.8645, Val Loss=0.3957, lr=0.0100
[02/26 09:12:44 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 26/100, Acc=0.8660, Val Loss=0.4005, lr=0.0100
[02/26 09:13:46 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 27/100, Acc=0.8679, Val Loss=0.3905, lr=0.0100
[02/26 09:14:48 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 28/100, Acc=0.8659, Val Loss=0.4134, lr=0.0100
[02/26 09:15:51 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 29/100, Acc=0.8726, Val Loss=0.3861, lr=0.0100
[02/26 09:16:53 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 30/100, Acc=0.8762, Val Loss=0.3655, lr=0.0100
[02/26 09:17:55 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 31/100, Acc=0.8793, Val Loss=0.3727, lr=0.0100
[02/26 09:18:58 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 32/100, Acc=0.8758, Val Loss=0.3729, lr=0.0100
[02/26 09:20:00 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 33/100, Acc=0.8753, Val Loss=0.3681, lr=0.0100
[02/26 09:21:02 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 34/100, Acc=0.8806, Val Loss=0.3662, lr=0.0100
[02/26 09:22:04 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 35/100, Acc=0.8746, Val Loss=0.3657, lr=0.0100
[02/26 09:23:07 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 36/100, Acc=0.8779, Val Loss=0.3629, lr=0.0100
[02/26 09:24:09 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 37/100, Acc=0.8695, Val Loss=0.3812, lr=0.0100
[02/26 09:25:11 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 38/100, Acc=0.8778, Val Loss=0.3679, lr=0.0100
[02/26 09:26:13 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 39/100, Acc=0.8682, Val Loss=0.3863, lr=0.0100
[02/26 09:27:16 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 40/100, Acc=0.8701, Val Loss=0.3926, lr=0.0100
[02/26 09:28:18 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 41/100, Acc=0.8692, Val Loss=0.3856, lr=0.0100
[02/26 09:29:20 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 42/100, Acc=0.8740, Val Loss=0.3747, lr=0.0100
[02/26 09:30:22 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 43/100, Acc=0.8772, Val Loss=0.3607, lr=0.0100
[02/26 09:31:24 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 44/100, Acc=0.8833, Val Loss=0.3569, lr=0.0100
[02/26 09:32:26 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 45/100, Acc=0.8822, Val Loss=0.3519, lr=0.0100
[02/26 09:33:29 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 46/100, Acc=0.8759, Val Loss=0.3727, lr=0.0100
[02/26 09:34:32 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 47/100, Acc=0.8829, Val Loss=0.3629, lr=0.0100
[02/26 09:35:35 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 48/100, Acc=0.8839, Val Loss=0.3494, lr=0.0100
[02/26 09:36:38 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 49/100, Acc=0.8815, Val Loss=0.3735, lr=0.0100
[02/26 09:37:42 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 50/100, Acc=0.8829, Val Loss=0.3565, lr=0.0100
[02/26 09:38:46 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 51/100, Acc=0.8874, Val Loss=0.3542, lr=0.0100
[02/26 09:39:49 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 52/100, Acc=0.8780, Val Loss=0.3647, lr=0.0100
[02/26 09:40:52 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 53/100, Acc=0.8781, Val Loss=0.3660, lr=0.0100
[02/26 09:41:55 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 54/100, Acc=0.8772, Val Loss=0.3776, lr=0.0100
[02/26 09:42:58 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 55/100, Acc=0.8827, Val Loss=0.3512, lr=0.0100
[02/26 09:44:01 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 56/100, Acc=0.8860, Val Loss=0.3526, lr=0.0100
[02/26 09:45:04 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 57/100, Acc=0.8795, Val Loss=0.3677, lr=0.0100
[02/26 09:46:06 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 58/100, Acc=0.8840, Val Loss=0.3557, lr=0.0100
[02/26 09:47:09 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 59/100, Acc=0.8821, Val Loss=0.3494, lr=0.0100
[02/26 09:48:11 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 60/100, Acc=0.8928, Val Loss=0.3129, lr=0.0010
[02/26 09:49:13 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 61/100, Acc=0.8965, Val Loss=0.3086, lr=0.0010
[02/26 09:50:16 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 62/100, Acc=0.8975, Val Loss=0.3068, lr=0.0010
[02/26 09:51:18 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 63/100, Acc=0.8969, Val Loss=0.3099, lr=0.0010
[02/26 09:52:21 cifar10-global-group_sl-3.0-mobilenetv2]: Epoch 64/100, Acc=0.8977, Val Loss=0.3106, lr=0.0010
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 1
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 2
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 3
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 4
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 5
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() fAiled: Temporary failure in name resolution
slurmstepd-cf-prod-node-031: error: slurm_set_addr: Unable to resolve "cf-slurmctld01-prod"
slurmstepd-cf-prod-node-031: error: Unable to establish control machine address
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 1
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 2
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 3
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 4
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 5
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() fAiled: Temporary failure in name resolution
slurmstepd-cf-prod-node-031: error: slurm_set_addr: Unable to resolve "cf-slurmctld01-prod"
slurmstepd-cf-prod-node-031: error: Unable to establish control machine address
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 1
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 2
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 3
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 4
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 5
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() fAiled: Temporary failure in name resolution
slurmstepd-cf-prod-node-031: error: slurm_set_addr: Unable to resolve "cf-slurmctld01-prod"
slurmstepd-cf-prod-node-031: error: Unable to establish control machine address
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 1
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 2
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 3
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 4
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 5
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() fAiled: Temporary failure in name resolution
slurmstepd-cf-prod-node-031: error: slurm_set_addr: Unable to resolve "cf-slurmctld01-prod"
slurmstepd-cf-prod-node-031: error: Unable to establish control machine address
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 1
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 2
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 3
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 4
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() failed: Temporary failure in name resolution: Resource temporarily unavailable, attempt number 5
slurmstepd-cf-prod-node-031: error: get_addr_info: getaddrinfo() fAiled: Temporary failure in name resolution
slurmstepd-cf-prod-node-031: error: slurm_set_addr: Unable to resolve "cf-slurmctld01-prod"
slurmstepd-cf-prod-node-031: error: Unable to establish control machine address
