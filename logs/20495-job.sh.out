SLURM WORKLOAD START: Sun Feb 23 22:34:24 CET 2025
Sun Feb 23 22:34:24 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:0B:00.0 Off |                    0 |
| N/A   29C    P0             24W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: mode: prune
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: model: mobilenetv2
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: verbose: False
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: dataset: cifar100
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: dataroot: data
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: batch_size: 128
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: total_epochs: 100
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: lr: 0.01
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: restore: run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: output_dir: run/cifar100/prune/cifar100-global-random-2.0-mobilenetv2
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: finetune: True
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: last_epochs: 100
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: reps: 1
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: method: random
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: speed_up: 2.0
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: reg: 1e-05
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: delta_reg: 0.0001
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: weight_decay: 0.0005
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: seed: 1
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: global_pruning: True
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: sl_total_epochs: 100
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: sl_lr: 0.01
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: sl_reg_warmup: 0
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: sl_restore: None
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: iterative_steps: 400
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: logger: <Logger cifar100-global-random-2.0-mobilenetv2 (DEBUG)>
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: device: cuda
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: num_classes: 100
[02/23 22:34:31 cifar100-global-random-2.0-mobilenetv2]: Loading model from run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/23 22:34:35 cifar100-global-random-2.0-mobilenetv2]: Pruning...
[02/23 22:34:43 cifar100-global-random-2.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 1, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 38, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(38, 38, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=38)
        (4): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(38, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 86, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(86, 86, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=86)
        (4): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(86, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 86, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(86, 86, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=86)
        (4): BatchNorm2d(86, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(86, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 134, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(134, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(134, 134, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=134)
        (4): BatchNorm2d(134, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(134, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 134, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(134, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(134, 134, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=134)
        (4): BatchNorm2d(134, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(134, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 134, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(134, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(134, 134, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=134)
        (4): BatchNorm2d(134, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(134, 6, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(6, 326, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(326, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(326, 326, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=326)
        (4): BatchNorm2d(326, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(326, 6, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(6, 326, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(326, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(326, 326, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=326)
        (4): BatchNorm2d(326, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(326, 6, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(6, 326, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(326, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(326, 326, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=326)
        (4): BatchNorm2d(326, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(326, 6, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(6, 326, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(326, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(326, 326, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=326)
        (4): BatchNorm2d(326, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(326, 38, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(38, 518, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(518, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(518, 518, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=518)
        (4): BatchNorm2d(518, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(518, 38, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(38, 518, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(518, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(518, 518, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=518)
        (4): BatchNorm2d(518, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(518, 38, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(38, 518, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(518, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(518, 518, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=518)
        (4): BatchNorm2d(518, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(518, 102, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(102, 902, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(902, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(902, 902, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=902)
        (4): BatchNorm2d(902, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(902, 102, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(102, 902, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(902, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(902, 902, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=902)
        (4): BatchNorm2d(902, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(902, 102, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(102, 902, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(902, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(902, 902, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=902)
      (4): BatchNorm2d(902, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(902, 262, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(262, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(262, 1222, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1222, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1222, 100, kernel_size=(1, 1), stride=(1, 1))
)
[02/23 22:34:46 cifar100-global-random-2.0-mobilenetv2]: Params: 2.37 M => 1.42 M (59.78%)
[02/23 22:34:46 cifar100-global-random-2.0-mobilenetv2]: FLOPs: 68.40 M => 34.14 M (49.91%, 2.00X )
[02/23 22:34:46 cifar100-global-random-2.0-mobilenetv2]: Acc: 0.6699 => 0.0100
[02/23 22:34:46 cifar100-global-random-2.0-mobilenetv2]: Val Loss: 1.1637 => 4.8679
[02/23 22:34:46 cifar100-global-random-2.0-mobilenetv2]: Finetuning...
[02/23 22:35:17 cifar100-global-random-2.0-mobilenetv2]: Epoch 0/100, Acc=0.0100, Val Loss=26.6272, lr=0.0100
[02/23 22:35:49 cifar100-global-random-2.0-mobilenetv2]: Epoch 1/100, Acc=0.0100, Val Loss=15.6887, lr=0.0100
[02/23 22:36:21 cifar100-global-random-2.0-mobilenetv2]: Epoch 2/100, Acc=0.0100, Val Loss=25.4952, lr=0.0100
[02/23 22:36:52 cifar100-global-random-2.0-mobilenetv2]: Epoch 3/100, Acc=0.0100, Val Loss=7.2733, lr=0.0100
[02/23 22:37:24 cifar100-global-random-2.0-mobilenetv2]: Epoch 4/100, Acc=0.0100, Val Loss=6.9671, lr=0.0100
[02/23 22:37:56 cifar100-global-random-2.0-mobilenetv2]: Epoch 5/100, Acc=0.0100, Val Loss=9.3223, lr=0.0100
[02/23 22:38:27 cifar100-global-random-2.0-mobilenetv2]: Epoch 6/100, Acc=0.0100, Val Loss=16.0378, lr=0.0100
[02/23 22:38:59 cifar100-global-random-2.0-mobilenetv2]: Epoch 7/100, Acc=0.0100, Val Loss=11.0140, lr=0.0100
[02/23 22:39:31 cifar100-global-random-2.0-mobilenetv2]: Epoch 8/100, Acc=0.0100, Val Loss=12.8694, lr=0.0100
[02/23 22:40:03 cifar100-global-random-2.0-mobilenetv2]: Epoch 9/100, Acc=0.0100, Val Loss=9.5428, lr=0.0100
[02/23 22:40:34 cifar100-global-random-2.0-mobilenetv2]: Epoch 10/100, Acc=0.0100, Val Loss=8.0224, lr=0.0100
[02/23 22:41:05 cifar100-global-random-2.0-mobilenetv2]: Epoch 11/100, Acc=0.0100, Val Loss=6.3132, lr=0.0100
[02/23 22:41:36 cifar100-global-random-2.0-mobilenetv2]: Epoch 12/100, Acc=0.0100, Val Loss=5.5212, lr=0.0100
[02/23 22:42:08 cifar100-global-random-2.0-mobilenetv2]: Epoch 13/100, Acc=0.0100, Val Loss=4.6957, lr=0.0100
[02/23 22:42:39 cifar100-global-random-2.0-mobilenetv2]: Epoch 14/100, Acc=0.0100, Val Loss=5.2119, lr=0.0100
[02/23 22:43:10 cifar100-global-random-2.0-mobilenetv2]: Epoch 15/100, Acc=0.0100, Val Loss=4.6093, lr=0.0100
[02/23 22:43:42 cifar100-global-random-2.0-mobilenetv2]: Epoch 16/100, Acc=0.0100, Val Loss=4.6785, lr=0.0100
[02/23 22:44:13 cifar100-global-random-2.0-mobilenetv2]: Epoch 17/100, Acc=0.0100, Val Loss=4.9438, lr=0.0100
[02/23 22:44:45 cifar100-global-random-2.0-mobilenetv2]: Epoch 18/100, Acc=0.0100, Val Loss=4.8660, lr=0.0100
[02/23 22:45:16 cifar100-global-random-2.0-mobilenetv2]: Epoch 19/100, Acc=0.0100, Val Loss=4.7883, lr=0.0100
[02/23 22:45:48 cifar100-global-random-2.0-mobilenetv2]: Epoch 20/100, Acc=0.0100, Val Loss=4.6220, lr=0.0100
[02/23 22:46:19 cifar100-global-random-2.0-mobilenetv2]: Epoch 21/100, Acc=0.0100, Val Loss=4.7153, lr=0.0100
[02/23 22:46:50 cifar100-global-random-2.0-mobilenetv2]: Epoch 22/100, Acc=0.0100, Val Loss=4.6979, lr=0.0100
[02/23 22:47:21 cifar100-global-random-2.0-mobilenetv2]: Epoch 23/100, Acc=0.0100, Val Loss=4.6143, lr=0.0100
[02/23 22:47:52 cifar100-global-random-2.0-mobilenetv2]: Epoch 24/100, Acc=0.0100, Val Loss=4.6799, lr=0.0100
[02/23 22:48:24 cifar100-global-random-2.0-mobilenetv2]: Epoch 25/100, Acc=0.0100, Val Loss=4.6740, lr=0.0100
[02/23 22:48:55 cifar100-global-random-2.0-mobilenetv2]: Epoch 26/100, Acc=0.0100, Val Loss=4.6595, lr=0.0100
[02/23 22:49:27 cifar100-global-random-2.0-mobilenetv2]: Epoch 27/100, Acc=0.0100, Val Loss=4.6639, lr=0.0100
[02/23 22:49:58 cifar100-global-random-2.0-mobilenetv2]: Epoch 28/100, Acc=0.0100, Val Loss=4.6310, lr=0.0100
[02/23 22:50:29 cifar100-global-random-2.0-mobilenetv2]: Epoch 29/100, Acc=0.0100, Val Loss=4.6299, lr=0.0100
[02/23 22:51:00 cifar100-global-random-2.0-mobilenetv2]: Epoch 30/100, Acc=0.0100, Val Loss=4.6254, lr=0.0100
[02/23 22:51:32 cifar100-global-random-2.0-mobilenetv2]: Epoch 31/100, Acc=0.0100, Val Loss=4.6086, lr=0.0100
[02/23 22:52:03 cifar100-global-random-2.0-mobilenetv2]: Epoch 32/100, Acc=0.0100, Val Loss=4.6090, lr=0.0100
[02/23 22:52:35 cifar100-global-random-2.0-mobilenetv2]: Epoch 33/100, Acc=0.0100, Val Loss=4.6111, lr=0.0100
[02/23 22:53:06 cifar100-global-random-2.0-mobilenetv2]: Epoch 34/100, Acc=0.0100, Val Loss=4.6082, lr=0.0100
[02/23 22:53:37 cifar100-global-random-2.0-mobilenetv2]: Epoch 35/100, Acc=0.0100, Val Loss=4.6076, lr=0.0100
[02/23 22:54:09 cifar100-global-random-2.0-mobilenetv2]: Epoch 36/100, Acc=0.0100, Val Loss=4.6067, lr=0.0100
[02/23 22:54:40 cifar100-global-random-2.0-mobilenetv2]: Epoch 37/100, Acc=0.0100, Val Loss=4.6069, lr=0.0100
[02/23 22:55:11 cifar100-global-random-2.0-mobilenetv2]: Epoch 38/100, Acc=0.0100, Val Loss=4.6064, lr=0.0100
[02/23 22:55:43 cifar100-global-random-2.0-mobilenetv2]: Epoch 39/100, Acc=0.0100, Val Loss=4.6060, lr=0.0100
[02/23 22:56:15 cifar100-global-random-2.0-mobilenetv2]: Epoch 40/100, Acc=0.0100, Val Loss=4.6058, lr=0.0100
[02/23 22:56:46 cifar100-global-random-2.0-mobilenetv2]: Epoch 41/100, Acc=0.0100, Val Loss=4.6057, lr=0.0100
[02/23 22:57:18 cifar100-global-random-2.0-mobilenetv2]: Epoch 42/100, Acc=0.0100, Val Loss=4.6056, lr=0.0100
[02/23 22:57:50 cifar100-global-random-2.0-mobilenetv2]: Epoch 43/100, Acc=0.0100, Val Loss=4.6056, lr=0.0100
[02/23 22:58:21 cifar100-global-random-2.0-mobilenetv2]: Epoch 44/100, Acc=0.0100, Val Loss=4.6055, lr=0.0100
[02/23 22:58:52 cifar100-global-random-2.0-mobilenetv2]: Epoch 45/100, Acc=0.0100, Val Loss=4.6054, lr=0.0100
[02/23 22:59:23 cifar100-global-random-2.0-mobilenetv2]: Epoch 46/100, Acc=0.0100, Val Loss=4.6054, lr=0.0100
[02/23 22:59:54 cifar100-global-random-2.0-mobilenetv2]: Epoch 47/100, Acc=0.0100, Val Loss=4.6054, lr=0.0100
[02/23 23:00:26 cifar100-global-random-2.0-mobilenetv2]: Epoch 48/100, Acc=0.0100, Val Loss=4.6054, lr=0.0100
[02/23 23:00:57 cifar100-global-random-2.0-mobilenetv2]: Epoch 49/100, Acc=0.0100, Val Loss=4.6053, lr=0.0100
[02/23 23:01:28 cifar100-global-random-2.0-mobilenetv2]: Epoch 50/100, Acc=0.0100, Val Loss=4.6053, lr=0.0100
[02/23 23:02:00 cifar100-global-random-2.0-mobilenetv2]: Epoch 51/100, Acc=0.0100, Val Loss=4.6053, lr=0.0100
[02/23 23:02:31 cifar100-global-random-2.0-mobilenetv2]: Epoch 52/100, Acc=0.0100, Val Loss=4.6053, lr=0.0100
[02/23 23:03:02 cifar100-global-random-2.0-mobilenetv2]: Epoch 53/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/23 23:03:34 cifar100-global-random-2.0-mobilenetv2]: Epoch 54/100, Acc=0.0100, Val Loss=4.6053, lr=0.0100
[02/23 23:04:05 cifar100-global-random-2.0-mobilenetv2]: Epoch 55/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/23 23:04:36 cifar100-global-random-2.0-mobilenetv2]: Epoch 56/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/23 23:05:07 cifar100-global-random-2.0-mobilenetv2]: Epoch 57/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/23 23:05:38 cifar100-global-random-2.0-mobilenetv2]: Epoch 58/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/23 23:06:09 cifar100-global-random-2.0-mobilenetv2]: Epoch 59/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/23 23:06:40 cifar100-global-random-2.0-mobilenetv2]: Epoch 60/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/23 23:07:11 cifar100-global-random-2.0-mobilenetv2]: Epoch 61/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/23 23:07:42 cifar100-global-random-2.0-mobilenetv2]: Epoch 62/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/23 23:08:13 cifar100-global-random-2.0-mobilenetv2]: Epoch 63/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/23 23:08:45 cifar100-global-random-2.0-mobilenetv2]: Epoch 64/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/23 23:09:16 cifar100-global-random-2.0-mobilenetv2]: Epoch 65/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/23 23:09:48 cifar100-global-random-2.0-mobilenetv2]: Epoch 66/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/23 23:10:19 cifar100-global-random-2.0-mobilenetv2]: Epoch 67/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/23 23:10:50 cifar100-global-random-2.0-mobilenetv2]: Epoch 68/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/23 23:11:22 cifar100-global-random-2.0-mobilenetv2]: Epoch 69/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/23 23:11:53 cifar100-global-random-2.0-mobilenetv2]: Epoch 70/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/23 23:12:24 cifar100-global-random-2.0-mobilenetv2]: Epoch 71/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/23 23:12:55 cifar100-global-random-2.0-mobilenetv2]: Epoch 72/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/23 23:13:26 cifar100-global-random-2.0-mobilenetv2]: Epoch 73/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/23 23:13:58 cifar100-global-random-2.0-mobilenetv2]: Epoch 74/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/23 23:14:29 cifar100-global-random-2.0-mobilenetv2]: Epoch 75/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/23 23:15:00 cifar100-global-random-2.0-mobilenetv2]: Epoch 76/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/23 23:15:32 cifar100-global-random-2.0-mobilenetv2]: Epoch 77/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/23 23:16:03 cifar100-global-random-2.0-mobilenetv2]: Epoch 78/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/23 23:16:34 cifar100-global-random-2.0-mobilenetv2]: Epoch 79/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/23 23:17:05 cifar100-global-random-2.0-mobilenetv2]: Epoch 80/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/23 23:17:37 cifar100-global-random-2.0-mobilenetv2]: Epoch 81/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/23 23:18:08 cifar100-global-random-2.0-mobilenetv2]: Epoch 82/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/23 23:18:39 cifar100-global-random-2.0-mobilenetv2]: Epoch 83/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/23 23:19:11 cifar100-global-random-2.0-mobilenetv2]: Epoch 84/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/23 23:19:42 cifar100-global-random-2.0-mobilenetv2]: Epoch 85/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/23 23:20:13 cifar100-global-random-2.0-mobilenetv2]: Epoch 86/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/23 23:20:45 cifar100-global-random-2.0-mobilenetv2]: Epoch 87/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/23 23:21:16 cifar100-global-random-2.0-mobilenetv2]: Epoch 88/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/23 23:21:47 cifar100-global-random-2.0-mobilenetv2]: Epoch 89/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/23 23:22:19 cifar100-global-random-2.0-mobilenetv2]: Epoch 90/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/23 23:22:50 cifar100-global-random-2.0-mobilenetv2]: Epoch 91/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/23 23:23:21 cifar100-global-random-2.0-mobilenetv2]: Epoch 92/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/23 23:23:52 cifar100-global-random-2.0-mobilenetv2]: Epoch 93/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/23 23:24:24 cifar100-global-random-2.0-mobilenetv2]: Epoch 94/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/23 23:24:55 cifar100-global-random-2.0-mobilenetv2]: Epoch 95/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/23 23:25:27 cifar100-global-random-2.0-mobilenetv2]: Epoch 96/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/23 23:25:58 cifar100-global-random-2.0-mobilenetv2]: Epoch 97/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/23 23:26:30 cifar100-global-random-2.0-mobilenetv2]: Epoch 98/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/23 23:27:01 cifar100-global-random-2.0-mobilenetv2]: Epoch 99/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/23 23:27:01 cifar100-global-random-2.0-mobilenetv2]: Best Acc=0.0100
[02/23 23:27:01 cifar100-global-random-2.0-mobilenetv2]: Params: 1.42 M
[02/23 23:27:01 cifar100-global-random-2.0-mobilenetv2]: ops: 34.14 M
[02/23 23:27:05 cifar100-global-random-2.0-mobilenetv2]: Acc: 0.0100 Val Loss: 4.6052

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: mode: prune
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: model: mobilenetv2
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: verbose: False
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: dataset: cifar100
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: dataroot: data
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: batch_size: 128
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: total_epochs: 100
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: lr: 0.01
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: restore: run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: output_dir: run/cifar100/prune/cifar100-global-l2-2.0-mobilenetv2
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: finetune: True
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: last_epochs: 100
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: reps: 1
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: method: l2
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: speed_up: 2.0
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: reg: 1e-05
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: delta_reg: 0.0001
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: weight_decay: 0.0005
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: seed: 1
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: global_pruning: True
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: sl_total_epochs: 100
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: sl_lr: 0.01
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: sl_reg_warmup: 0
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: sl_restore: None
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: iterative_steps: 400
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: logger: <Logger cifar100-global-l2-2.0-mobilenetv2 (DEBUG)>
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: device: cuda
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: num_classes: 100
[02/23 23:27:13 cifar100-global-l2-2.0-mobilenetv2]: Loading model from run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/23 23:27:18 cifar100-global-l2-2.0-mobilenetv2]: Pruning...
[02/23 23:27:43 cifar100-global-l2-2.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 13, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(13, 15, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=15)
      (4): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(15, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 81, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(81, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(81, 81, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=81)
        (4): BatchNorm2d(81, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(81, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 115, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(115, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(115, 115, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=115)
        (4): BatchNorm2d(115, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(115, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 89, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(89, 89, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=89)
        (4): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(89, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 116, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(116, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=116)
        (4): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(116, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 89, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=89)
        (4): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(89, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 107, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(107, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(107, 107, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=107)
        (4): BatchNorm2d(107, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(107, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 139, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(139, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(139, 139, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=139)
        (4): BatchNorm2d(139, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(139, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 132, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(132, 132, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=132)
        (4): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(132, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 102, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(102, 102, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=102)
        (4): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(102, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 238, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(238, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(238, 238, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=238)
        (4): BatchNorm2d(238, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(238, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 176, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(176, 176, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=176)
        (4): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(176, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 115, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(115, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(115, 115, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=115)
        (4): BatchNorm2d(115, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(115, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 270, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(270, 270, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=270)
        (4): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(270, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 376, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(376, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(376, 376, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=376)
        (4): BatchNorm2d(376, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(376, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 418, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(418, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(418, 418, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=418)
        (4): BatchNorm2d(418, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(418, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 180, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=180)
      (4): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(180, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 100, kernel_size=(1, 1), stride=(1, 1))
)
[02/23 23:27:46 cifar100-global-l2-2.0-mobilenetv2]: Params: 2.37 M => 1.18 M (49.65%)
[02/23 23:27:46 cifar100-global-l2-2.0-mobilenetv2]: FLOPs: 68.40 M => 34.12 M (49.88%, 2.00X )
[02/23 23:27:46 cifar100-global-l2-2.0-mobilenetv2]: Acc: 0.6699 => 0.6699
[02/23 23:27:46 cifar100-global-l2-2.0-mobilenetv2]: Val Loss: 1.1637 => 1.1637
[02/23 23:27:46 cifar100-global-l2-2.0-mobilenetv2]: Finetuning...
[02/23 23:28:17 cifar100-global-l2-2.0-mobilenetv2]: Epoch 0/100, Acc=0.5938, Val Loss=1.4718, lr=0.0100
[02/23 23:28:47 cifar100-global-l2-2.0-mobilenetv2]: Epoch 1/100, Acc=0.6046, Val Loss=1.4012, lr=0.0100
[02/23 23:29:18 cifar100-global-l2-2.0-mobilenetv2]: Epoch 2/100, Acc=0.6020, Val Loss=1.4353, lr=0.0100
[02/23 23:29:49 cifar100-global-l2-2.0-mobilenetv2]: Epoch 3/100, Acc=0.5807, Val Loss=1.4912, lr=0.0100
[02/23 23:30:20 cifar100-global-l2-2.0-mobilenetv2]: Epoch 4/100, Acc=0.6099, Val Loss=1.3944, lr=0.0100
[02/23 23:30:51 cifar100-global-l2-2.0-mobilenetv2]: Epoch 5/100, Acc=0.6081, Val Loss=1.3759, lr=0.0100
[02/23 23:31:22 cifar100-global-l2-2.0-mobilenetv2]: Epoch 6/100, Acc=0.5941, Val Loss=1.4412, lr=0.0100
[02/23 23:31:53 cifar100-global-l2-2.0-mobilenetv2]: Epoch 7/100, Acc=0.6050, Val Loss=1.3985, lr=0.0100
[02/23 23:32:24 cifar100-global-l2-2.0-mobilenetv2]: Epoch 8/100, Acc=0.6031, Val Loss=1.4195, lr=0.0100
[02/23 23:32:55 cifar100-global-l2-2.0-mobilenetv2]: Epoch 9/100, Acc=0.6136, Val Loss=1.3591, lr=0.0100
[02/23 23:33:33 cifar100-global-l2-2.0-mobilenetv2]: Epoch 10/100, Acc=0.6134, Val Loss=1.3669, lr=0.0100
[02/23 23:34:11 cifar100-global-l2-2.0-mobilenetv2]: Epoch 11/100, Acc=0.6115, Val Loss=1.3758, lr=0.0100
[02/23 23:34:48 cifar100-global-l2-2.0-mobilenetv2]: Epoch 12/100, Acc=0.5676, Val Loss=1.5663, lr=0.0100
[02/23 23:35:26 cifar100-global-l2-2.0-mobilenetv2]: Epoch 13/100, Acc=0.5831, Val Loss=1.4988, lr=0.0100
[02/23 23:36:03 cifar100-global-l2-2.0-mobilenetv2]: Epoch 14/100, Acc=0.5964, Val Loss=1.4224, lr=0.0100
[02/23 23:36:37 cifar100-global-l2-2.0-mobilenetv2]: Epoch 15/100, Acc=0.6114, Val Loss=1.3732, lr=0.0100
[02/23 23:37:08 cifar100-global-l2-2.0-mobilenetv2]: Epoch 16/100, Acc=0.5828, Val Loss=1.5100, lr=0.0100
[02/23 23:37:39 cifar100-global-l2-2.0-mobilenetv2]: Epoch 17/100, Acc=0.5933, Val Loss=1.4606, lr=0.0100
[02/23 23:38:10 cifar100-global-l2-2.0-mobilenetv2]: Epoch 18/100, Acc=0.6143, Val Loss=1.3652, lr=0.0100
[02/23 23:38:41 cifar100-global-l2-2.0-mobilenetv2]: Epoch 19/100, Acc=0.5976, Val Loss=1.4586, lr=0.0100
[02/23 23:39:12 cifar100-global-l2-2.0-mobilenetv2]: Epoch 20/100, Acc=0.6055, Val Loss=1.3867, lr=0.0100
[02/23 23:39:43 cifar100-global-l2-2.0-mobilenetv2]: Epoch 21/100, Acc=0.5946, Val Loss=1.4664, lr=0.0100
[02/23 23:40:14 cifar100-global-l2-2.0-mobilenetv2]: Epoch 22/100, Acc=0.6086, Val Loss=1.3966, lr=0.0100
[02/23 23:40:44 cifar100-global-l2-2.0-mobilenetv2]: Epoch 23/100, Acc=0.6035, Val Loss=1.4119, lr=0.0100
[02/23 23:41:15 cifar100-global-l2-2.0-mobilenetv2]: Epoch 24/100, Acc=0.6153, Val Loss=1.4159, lr=0.0100
[02/23 23:41:46 cifar100-global-l2-2.0-mobilenetv2]: Epoch 25/100, Acc=0.5989, Val Loss=1.4487, lr=0.0100
[02/23 23:42:17 cifar100-global-l2-2.0-mobilenetv2]: Epoch 26/100, Acc=0.6127, Val Loss=1.3649, lr=0.0100
[02/23 23:42:48 cifar100-global-l2-2.0-mobilenetv2]: Epoch 27/100, Acc=0.5949, Val Loss=1.4515, lr=0.0100
[02/23 23:43:19 cifar100-global-l2-2.0-mobilenetv2]: Epoch 28/100, Acc=0.6129, Val Loss=1.3643, lr=0.0100
[02/23 23:43:50 cifar100-global-l2-2.0-mobilenetv2]: Epoch 29/100, Acc=0.6167, Val Loss=1.3648, lr=0.0100
[02/23 23:44:21 cifar100-global-l2-2.0-mobilenetv2]: Epoch 30/100, Acc=0.6083, Val Loss=1.3979, lr=0.0100
[02/23 23:44:52 cifar100-global-l2-2.0-mobilenetv2]: Epoch 31/100, Acc=0.6072, Val Loss=1.4032, lr=0.0100
[02/23 23:45:24 cifar100-global-l2-2.0-mobilenetv2]: Epoch 32/100, Acc=0.6031, Val Loss=1.4314, lr=0.0100
[02/23 23:45:54 cifar100-global-l2-2.0-mobilenetv2]: Epoch 33/100, Acc=0.6176, Val Loss=1.3860, lr=0.0100
[02/23 23:46:25 cifar100-global-l2-2.0-mobilenetv2]: Epoch 34/100, Acc=0.6065, Val Loss=1.4155, lr=0.0100
[02/23 23:46:56 cifar100-global-l2-2.0-mobilenetv2]: Epoch 35/100, Acc=0.6122, Val Loss=1.4058, lr=0.0100
[02/23 23:47:26 cifar100-global-l2-2.0-mobilenetv2]: Epoch 36/100, Acc=0.6108, Val Loss=1.4067, lr=0.0100
[02/23 23:47:58 cifar100-global-l2-2.0-mobilenetv2]: Epoch 37/100, Acc=0.6048, Val Loss=1.4075, lr=0.0100
[02/23 23:48:28 cifar100-global-l2-2.0-mobilenetv2]: Epoch 38/100, Acc=0.6166, Val Loss=1.3538, lr=0.0100
[02/23 23:48:59 cifar100-global-l2-2.0-mobilenetv2]: Epoch 39/100, Acc=0.6047, Val Loss=1.4155, lr=0.0100
[02/23 23:49:29 cifar100-global-l2-2.0-mobilenetv2]: Epoch 40/100, Acc=0.6177, Val Loss=1.3790, lr=0.0100
[02/23 23:50:00 cifar100-global-l2-2.0-mobilenetv2]: Epoch 41/100, Acc=0.6064, Val Loss=1.4099, lr=0.0100
[02/23 23:50:31 cifar100-global-l2-2.0-mobilenetv2]: Epoch 42/100, Acc=0.6053, Val Loss=1.4322, lr=0.0100
[02/23 23:51:01 cifar100-global-l2-2.0-mobilenetv2]: Epoch 43/100, Acc=0.6234, Val Loss=1.3440, lr=0.0100
[02/23 23:51:33 cifar100-global-l2-2.0-mobilenetv2]: Epoch 44/100, Acc=0.6096, Val Loss=1.4114, lr=0.0100
[02/23 23:52:03 cifar100-global-l2-2.0-mobilenetv2]: Epoch 45/100, Acc=0.6162, Val Loss=1.3614, lr=0.0100
[02/23 23:52:34 cifar100-global-l2-2.0-mobilenetv2]: Epoch 46/100, Acc=0.6097, Val Loss=1.3609, lr=0.0100
[02/23 23:53:04 cifar100-global-l2-2.0-mobilenetv2]: Epoch 47/100, Acc=0.6074, Val Loss=1.3874, lr=0.0100
[02/23 23:53:35 cifar100-global-l2-2.0-mobilenetv2]: Epoch 48/100, Acc=0.6303, Val Loss=1.2968, lr=0.0100
[02/23 23:54:06 cifar100-global-l2-2.0-mobilenetv2]: Epoch 49/100, Acc=0.6100, Val Loss=1.3953, lr=0.0100
[02/23 23:54:36 cifar100-global-l2-2.0-mobilenetv2]: Epoch 50/100, Acc=0.6268, Val Loss=1.3266, lr=0.0100
[02/23 23:55:07 cifar100-global-l2-2.0-mobilenetv2]: Epoch 51/100, Acc=0.6035, Val Loss=1.3878, lr=0.0100
[02/23 23:55:37 cifar100-global-l2-2.0-mobilenetv2]: Epoch 52/100, Acc=0.6081, Val Loss=1.4047, lr=0.0100
[02/23 23:56:08 cifar100-global-l2-2.0-mobilenetv2]: Epoch 53/100, Acc=0.6138, Val Loss=1.3865, lr=0.0100
[02/23 23:56:38 cifar100-global-l2-2.0-mobilenetv2]: Epoch 54/100, Acc=0.6020, Val Loss=1.4074, lr=0.0100
[02/23 23:57:09 cifar100-global-l2-2.0-mobilenetv2]: Epoch 55/100, Acc=0.6126, Val Loss=1.3693, lr=0.0100
[02/23 23:57:39 cifar100-global-l2-2.0-mobilenetv2]: Epoch 56/100, Acc=0.6099, Val Loss=1.3874, lr=0.0100
[02/23 23:58:10 cifar100-global-l2-2.0-mobilenetv2]: Epoch 57/100, Acc=0.6211, Val Loss=1.3438, lr=0.0100
[02/23 23:58:40 cifar100-global-l2-2.0-mobilenetv2]: Epoch 58/100, Acc=0.6025, Val Loss=1.4216, lr=0.0100
[02/23 23:59:11 cifar100-global-l2-2.0-mobilenetv2]: Epoch 59/100, Acc=0.6140, Val Loss=1.3851, lr=0.0100
[02/23 23:59:42 cifar100-global-l2-2.0-mobilenetv2]: Epoch 60/100, Acc=0.6720, Val Loss=1.1431, lr=0.0010
[02/24 00:00:14 cifar100-global-l2-2.0-mobilenetv2]: Epoch 61/100, Acc=0.6739, Val Loss=1.1359, lr=0.0010
[02/24 00:00:46 cifar100-global-l2-2.0-mobilenetv2]: Epoch 62/100, Acc=0.6792, Val Loss=1.1230, lr=0.0010
[02/24 00:01:18 cifar100-global-l2-2.0-mobilenetv2]: Epoch 63/100, Acc=0.6773, Val Loss=1.1245, lr=0.0010
[02/24 00:01:49 cifar100-global-l2-2.0-mobilenetv2]: Epoch 64/100, Acc=0.6796, Val Loss=1.1231, lr=0.0010
[02/24 00:02:20 cifar100-global-l2-2.0-mobilenetv2]: Epoch 65/100, Acc=0.6786, Val Loss=1.1252, lr=0.0010
[02/24 00:02:51 cifar100-global-l2-2.0-mobilenetv2]: Epoch 66/100, Acc=0.6788, Val Loss=1.1204, lr=0.0010
[02/24 00:03:22 cifar100-global-l2-2.0-mobilenetv2]: Epoch 67/100, Acc=0.6805, Val Loss=1.1217, lr=0.0010
[02/24 00:03:53 cifar100-global-l2-2.0-mobilenetv2]: Epoch 68/100, Acc=0.6775, Val Loss=1.1260, lr=0.0010
[02/24 00:04:25 cifar100-global-l2-2.0-mobilenetv2]: Epoch 69/100, Acc=0.6803, Val Loss=1.1267, lr=0.0010
[02/24 00:04:58 cifar100-global-l2-2.0-mobilenetv2]: Epoch 70/100, Acc=0.6805, Val Loss=1.1251, lr=0.0010
[02/24 00:05:29 cifar100-global-l2-2.0-mobilenetv2]: Epoch 71/100, Acc=0.6798, Val Loss=1.1268, lr=0.0010
[02/24 00:06:00 cifar100-global-l2-2.0-mobilenetv2]: Epoch 72/100, Acc=0.6786, Val Loss=1.1270, lr=0.0010
[02/24 00:06:32 cifar100-global-l2-2.0-mobilenetv2]: Epoch 73/100, Acc=0.6791, Val Loss=1.1282, lr=0.0010
[02/24 00:07:03 cifar100-global-l2-2.0-mobilenetv2]: Epoch 74/100, Acc=0.6760, Val Loss=1.1355, lr=0.0010
[02/24 00:07:34 cifar100-global-l2-2.0-mobilenetv2]: Epoch 75/100, Acc=0.6792, Val Loss=1.1333, lr=0.0010
[02/24 00:08:06 cifar100-global-l2-2.0-mobilenetv2]: Epoch 76/100, Acc=0.6763, Val Loss=1.1299, lr=0.0010
[02/24 00:08:38 cifar100-global-l2-2.0-mobilenetv2]: Epoch 77/100, Acc=0.6776, Val Loss=1.1317, lr=0.0010
[02/24 00:09:10 cifar100-global-l2-2.0-mobilenetv2]: Epoch 78/100, Acc=0.6772, Val Loss=1.1346, lr=0.0010
[02/24 00:09:42 cifar100-global-l2-2.0-mobilenetv2]: Epoch 79/100, Acc=0.6766, Val Loss=1.1337, lr=0.0010
[02/24 00:10:14 cifar100-global-l2-2.0-mobilenetv2]: Epoch 80/100, Acc=0.6806, Val Loss=1.1253, lr=0.0001
[02/24 00:10:46 cifar100-global-l2-2.0-mobilenetv2]: Epoch 81/100, Acc=0.6792, Val Loss=1.1249, lr=0.0001
[02/24 00:11:18 cifar100-global-l2-2.0-mobilenetv2]: Epoch 82/100, Acc=0.6819, Val Loss=1.1256, lr=0.0001
[02/24 00:11:50 cifar100-global-l2-2.0-mobilenetv2]: Epoch 83/100, Acc=0.6826, Val Loss=1.1236, lr=0.0001
[02/24 00:12:23 cifar100-global-l2-2.0-mobilenetv2]: Epoch 84/100, Acc=0.6832, Val Loss=1.1220, lr=0.0001
[02/24 00:12:55 cifar100-global-l2-2.0-mobilenetv2]: Epoch 85/100, Acc=0.6830, Val Loss=1.1236, lr=0.0001
[02/24 00:13:27 cifar100-global-l2-2.0-mobilenetv2]: Epoch 86/100, Acc=0.6841, Val Loss=1.1216, lr=0.0001
[02/24 00:13:59 cifar100-global-l2-2.0-mobilenetv2]: Epoch 87/100, Acc=0.6829, Val Loss=1.1219, lr=0.0001
[02/24 00:14:31 cifar100-global-l2-2.0-mobilenetv2]: Epoch 88/100, Acc=0.6839, Val Loss=1.1228, lr=0.0001
[02/24 00:15:03 cifar100-global-l2-2.0-mobilenetv2]: Epoch 89/100, Acc=0.6817, Val Loss=1.1246, lr=0.0001
[02/24 00:15:36 cifar100-global-l2-2.0-mobilenetv2]: Epoch 90/100, Acc=0.6843, Val Loss=1.1260, lr=0.0001
[02/24 00:16:10 cifar100-global-l2-2.0-mobilenetv2]: Epoch 91/100, Acc=0.6818, Val Loss=1.1250, lr=0.0001
[02/24 00:16:43 cifar100-global-l2-2.0-mobilenetv2]: Epoch 92/100, Acc=0.6812, Val Loss=1.1253, lr=0.0001
[02/24 00:17:15 cifar100-global-l2-2.0-mobilenetv2]: Epoch 93/100, Acc=0.6811, Val Loss=1.1252, lr=0.0001
[02/24 00:17:47 cifar100-global-l2-2.0-mobilenetv2]: Epoch 94/100, Acc=0.6834, Val Loss=1.1252, lr=0.0001
[02/24 00:18:18 cifar100-global-l2-2.0-mobilenetv2]: Epoch 95/100, Acc=0.6829, Val Loss=1.1249, lr=0.0001
[02/24 00:18:49 cifar100-global-l2-2.0-mobilenetv2]: Epoch 96/100, Acc=0.6812, Val Loss=1.1242, lr=0.0001
[02/24 00:19:20 cifar100-global-l2-2.0-mobilenetv2]: Epoch 97/100, Acc=0.6829, Val Loss=1.1260, lr=0.0001
[02/24 00:19:50 cifar100-global-l2-2.0-mobilenetv2]: Epoch 98/100, Acc=0.6841, Val Loss=1.1261, lr=0.0001
[02/24 00:20:22 cifar100-global-l2-2.0-mobilenetv2]: Epoch 99/100, Acc=0.6840, Val Loss=1.1271, lr=0.0001
[02/24 00:20:22 cifar100-global-l2-2.0-mobilenetv2]: Best Acc=0.6843
[02/24 00:20:22 cifar100-global-l2-2.0-mobilenetv2]: Params: 1.18 M
[02/24 00:20:22 cifar100-global-l2-2.0-mobilenetv2]: ops: 34.12 M
[02/24 00:20:25 cifar100-global-l2-2.0-mobilenetv2]: Acc: 0.6840 Val Loss: 1.1271

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: mode: prune
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: model: mobilenetv2
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: verbose: False
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: dataset: cifar100
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: dataroot: data
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: batch_size: 128
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: total_epochs: 100
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: lr: 0.01
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: restore: run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: output_dir: run/cifar100/prune/cifar100-global-fpgm-2.0-mobilenetv2
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: finetune: True
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: last_epochs: 100
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: reps: 1
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: method: fpgm
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: speed_up: 2.0
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: reg: 1e-05
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: delta_reg: 0.0001
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: weight_decay: 0.0005
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: seed: 1
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: global_pruning: True
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: sl_total_epochs: 100
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: sl_lr: 0.01
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: sl_reg_warmup: 0
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: sl_restore: None
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: iterative_steps: 400
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: logger: <Logger cifar100-global-fpgm-2.0-mobilenetv2 (DEBUG)>
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: device: cuda
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: num_classes: 100
[02/24 00:20:37 cifar100-global-fpgm-2.0-mobilenetv2]: Loading model from run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/24 00:20:41 cifar100-global-fpgm-2.0-mobilenetv2]: Pruning...
[02/24 00:21:02 cifar100-global-fpgm-2.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
      (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)
        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144)
        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 61, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=61)
        (4): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(61, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192)
        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 58, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(58, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=58)
        (4): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(58, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)
        (4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(24, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 37, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(37, 37, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=37)
        (4): BatchNorm2d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(37, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)
        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 70, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(70, 70, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=70)
        (4): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(70, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 50, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=50)
        (4): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(50, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)
        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 130, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(130, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(130, 130, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=130)
        (4): BatchNorm2d(130, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(130, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 114, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(114, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(114, 114, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=114)
        (4): BatchNorm2d(114, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(114, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 110, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(110, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(110, 110, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=110)
      (4): BatchNorm2d(110, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(110, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 100, kernel_size=(1, 1), stride=(1, 1))
)
[02/24 00:21:05 cifar100-global-fpgm-2.0-mobilenetv2]: Params: 2.37 M => 1.02 M (42.86%)
[02/24 00:21:05 cifar100-global-fpgm-2.0-mobilenetv2]: FLOPs: 68.40 M => 33.72 M (49.30%, 2.03X )
[02/24 00:21:05 cifar100-global-fpgm-2.0-mobilenetv2]: Acc: 0.6699 => 0.6699
[02/24 00:21:05 cifar100-global-fpgm-2.0-mobilenetv2]: Val Loss: 1.1637 => 1.1637
[02/24 00:21:05 cifar100-global-fpgm-2.0-mobilenetv2]: Finetuning...
[02/24 00:21:37 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 0/100, Acc=0.5899, Val Loss=1.4738, lr=0.0100
[02/24 00:22:09 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 1/100, Acc=0.6034, Val Loss=1.3921, lr=0.0100
[02/24 00:22:42 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 2/100, Acc=0.5556, Val Loss=1.6395, lr=0.0100
[02/24 00:23:15 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 3/100, Acc=0.5980, Val Loss=1.4161, lr=0.0100
[02/24 00:23:50 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 4/100, Acc=0.5955, Val Loss=1.4421, lr=0.0100
[02/24 00:24:24 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 5/100, Acc=0.6107, Val Loss=1.3793, lr=0.0100
[02/24 00:24:55 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 6/100, Acc=0.5962, Val Loss=1.4445, lr=0.0100
[02/24 00:25:27 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 7/100, Acc=0.6087, Val Loss=1.3916, lr=0.0100
[02/24 00:25:58 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 8/100, Acc=0.5920, Val Loss=1.4385, lr=0.0100
[02/24 00:26:31 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 9/100, Acc=0.6102, Val Loss=1.3809, lr=0.0100
[02/24 00:27:02 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 10/100, Acc=0.6063, Val Loss=1.3924, lr=0.0100
[02/24 00:27:34 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 11/100, Acc=0.6173, Val Loss=1.3601, lr=0.0100
[02/24 00:28:05 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 12/100, Acc=0.6018, Val Loss=1.4123, lr=0.0100
[02/24 00:28:36 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 13/100, Acc=0.6088, Val Loss=1.3949, lr=0.0100
[02/24 00:29:07 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 14/100, Acc=0.5778, Val Loss=1.5296, lr=0.0100
[02/24 00:29:38 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 15/100, Acc=0.5985, Val Loss=1.4235, lr=0.0100
[02/24 00:30:09 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 16/100, Acc=0.5995, Val Loss=1.4345, lr=0.0100
[02/24 00:30:40 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 17/100, Acc=0.5934, Val Loss=1.4376, lr=0.0100
[02/24 00:31:11 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 18/100, Acc=0.6190, Val Loss=1.3565, lr=0.0100
[02/24 00:31:43 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 19/100, Acc=0.6018, Val Loss=1.4040, lr=0.0100
[02/24 00:32:14 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 20/100, Acc=0.6143, Val Loss=1.3561, lr=0.0100
[02/24 00:32:45 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 21/100, Acc=0.6026, Val Loss=1.4127, lr=0.0100
[02/24 00:33:17 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 22/100, Acc=0.6014, Val Loss=1.4301, lr=0.0100
[02/24 00:33:49 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 23/100, Acc=0.6035, Val Loss=1.3950, lr=0.0100
[02/24 00:34:20 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 24/100, Acc=0.6087, Val Loss=1.3979, lr=0.0100
[02/24 00:34:51 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 25/100, Acc=0.5855, Val Loss=1.4978, lr=0.0100
[02/24 00:35:24 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 26/100, Acc=0.6034, Val Loss=1.4175, lr=0.0100
[02/24 00:35:58 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 27/100, Acc=0.6081, Val Loss=1.3970, lr=0.0100
[02/24 00:36:29 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 28/100, Acc=0.6116, Val Loss=1.3635, lr=0.0100
[02/24 00:37:02 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 29/100, Acc=0.6026, Val Loss=1.4373, lr=0.0100
[02/24 00:37:33 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 30/100, Acc=0.6021, Val Loss=1.4247, lr=0.0100
[02/24 00:38:08 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 31/100, Acc=0.6109, Val Loss=1.4039, lr=0.0100
[02/24 00:38:38 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 32/100, Acc=0.6114, Val Loss=1.3846, lr=0.0100
[02/24 00:39:09 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 33/100, Acc=0.5781, Val Loss=1.5176, lr=0.0100
[02/24 00:39:42 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 34/100, Acc=0.6091, Val Loss=1.3917, lr=0.0100
[02/24 00:40:15 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 35/100, Acc=0.6195, Val Loss=1.3682, lr=0.0100
[02/24 00:40:46 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 36/100, Acc=0.5792, Val Loss=1.5233, lr=0.0100
[02/24 00:41:16 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 37/100, Acc=0.6061, Val Loss=1.4031, lr=0.0100
[02/24 00:41:47 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 38/100, Acc=0.6141, Val Loss=1.3525, lr=0.0100
[02/24 00:42:22 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 39/100, Acc=0.6131, Val Loss=1.3674, lr=0.0100
[02/24 00:42:54 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 40/100, Acc=0.6200, Val Loss=1.3478, lr=0.0100
[02/24 00:43:26 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 41/100, Acc=0.6157, Val Loss=1.3609, lr=0.0100
[02/24 00:43:57 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 42/100, Acc=0.6283, Val Loss=1.3292, lr=0.0100
[02/24 00:44:29 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 43/100, Acc=0.6059, Val Loss=1.4209, lr=0.0100
[02/24 00:45:00 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 44/100, Acc=0.5944, Val Loss=1.4696, lr=0.0100
[02/24 00:45:32 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 45/100, Acc=0.6123, Val Loss=1.3762, lr=0.0100
[02/24 00:46:04 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 46/100, Acc=0.5889, Val Loss=1.4749, lr=0.0100
[02/24 00:46:36 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 47/100, Acc=0.6278, Val Loss=1.3147, lr=0.0100
[02/24 00:47:08 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 48/100, Acc=0.6050, Val Loss=1.4004, lr=0.0100
[02/24 00:47:40 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 49/100, Acc=0.6184, Val Loss=1.3482, lr=0.0100
[02/24 00:48:11 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 50/100, Acc=0.6075, Val Loss=1.4094, lr=0.0100
[02/24 00:48:43 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 51/100, Acc=0.6098, Val Loss=1.3931, lr=0.0100
[02/24 00:49:15 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 52/100, Acc=0.6108, Val Loss=1.3882, lr=0.0100
[02/24 00:49:47 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 53/100, Acc=0.6040, Val Loss=1.4466, lr=0.0100
[02/24 00:50:19 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 54/100, Acc=0.5667, Val Loss=1.5970, lr=0.0100
[02/24 00:50:50 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 55/100, Acc=0.6136, Val Loss=1.3820, lr=0.0100
[02/24 00:51:21 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 56/100, Acc=0.6109, Val Loss=1.3835, lr=0.0100
[02/24 00:51:53 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 57/100, Acc=0.6320, Val Loss=1.3071, lr=0.0100
[02/24 00:52:28 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 58/100, Acc=0.6154, Val Loss=1.3876, lr=0.0100
[02/24 00:53:00 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 59/100, Acc=0.6061, Val Loss=1.4203, lr=0.0100
[02/24 00:53:33 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 60/100, Acc=0.6702, Val Loss=1.1332, lr=0.0010
[02/24 00:54:07 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 61/100, Acc=0.6768, Val Loss=1.1258, lr=0.0010
[02/24 00:54:41 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 62/100, Acc=0.6791, Val Loss=1.1142, lr=0.0010
[02/24 00:55:14 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 63/100, Acc=0.6800, Val Loss=1.1190, lr=0.0010
[02/24 00:55:50 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 64/100, Acc=0.6787, Val Loss=1.1218, lr=0.0010
[02/24 00:56:24 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 65/100, Acc=0.6791, Val Loss=1.1204, lr=0.0010
[02/24 00:57:03 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 66/100, Acc=0.6813, Val Loss=1.1115, lr=0.0010
[02/24 00:57:36 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 67/100, Acc=0.6797, Val Loss=1.1214, lr=0.0010
[02/24 00:58:09 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 68/100, Acc=0.6811, Val Loss=1.1217, lr=0.0010
[02/24 00:58:42 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 69/100, Acc=0.6787, Val Loss=1.1196, lr=0.0010
[02/24 00:59:14 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 70/100, Acc=0.6798, Val Loss=1.1229, lr=0.0010
[02/24 00:59:49 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 71/100, Acc=0.6811, Val Loss=1.1219, lr=0.0010
[02/24 01:00:25 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 72/100, Acc=0.6813, Val Loss=1.1229, lr=0.0010
[02/24 01:00:57 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 73/100, Acc=0.6813, Val Loss=1.1230, lr=0.0010
[02/24 01:01:31 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 74/100, Acc=0.6792, Val Loss=1.1288, lr=0.0010
[02/24 01:02:04 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 75/100, Acc=0.6766, Val Loss=1.1305, lr=0.0010
[02/24 01:02:41 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 76/100, Acc=0.6851, Val Loss=1.1201, lr=0.0010
[02/24 01:03:16 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 77/100, Acc=0.6808, Val Loss=1.1269, lr=0.0010
[02/24 01:03:49 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 78/100, Acc=0.6825, Val Loss=1.1286, lr=0.0010
[02/24 01:04:27 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 79/100, Acc=0.6787, Val Loss=1.1304, lr=0.0010
[02/24 01:05:02 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 80/100, Acc=0.6830, Val Loss=1.1203, lr=0.0001
[02/24 01:05:39 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 81/100, Acc=0.6852, Val Loss=1.1180, lr=0.0001
[02/24 01:06:14 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 82/100, Acc=0.6848, Val Loss=1.1195, lr=0.0001
[02/24 01:06:47 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 83/100, Acc=0.6818, Val Loss=1.1178, lr=0.0001
[02/24 01:07:18 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 84/100, Acc=0.6833, Val Loss=1.1167, lr=0.0001
[02/24 01:07:50 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 85/100, Acc=0.6823, Val Loss=1.1179, lr=0.0001
[02/24 01:08:24 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 86/100, Acc=0.6856, Val Loss=1.1162, lr=0.0001
[02/24 01:08:57 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 87/100, Acc=0.6814, Val Loss=1.1173, lr=0.0001
[02/24 01:09:29 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 88/100, Acc=0.6831, Val Loss=1.1185, lr=0.0001
[02/24 01:10:00 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 89/100, Acc=0.6835, Val Loss=1.1188, lr=0.0001
[02/24 01:10:32 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 90/100, Acc=0.6820, Val Loss=1.1185, lr=0.0001
[02/24 01:11:05 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 91/100, Acc=0.6826, Val Loss=1.1194, lr=0.0001
[02/24 01:11:37 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 92/100, Acc=0.6826, Val Loss=1.1197, lr=0.0001
[02/24 01:12:10 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 93/100, Acc=0.6822, Val Loss=1.1192, lr=0.0001
[02/24 01:12:42 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 94/100, Acc=0.6843, Val Loss=1.1203, lr=0.0001
[02/24 01:13:13 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 95/100, Acc=0.6820, Val Loss=1.1200, lr=0.0001
[02/24 01:13:44 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 96/100, Acc=0.6810, Val Loss=1.1172, lr=0.0001
[02/24 01:14:15 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 97/100, Acc=0.6827, Val Loss=1.1218, lr=0.0001
[02/24 01:14:46 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 98/100, Acc=0.6824, Val Loss=1.1183, lr=0.0001
[02/24 01:15:17 cifar100-global-fpgm-2.0-mobilenetv2]: Epoch 99/100, Acc=0.6832, Val Loss=1.1204, lr=0.0001
[02/24 01:15:17 cifar100-global-fpgm-2.0-mobilenetv2]: Best Acc=0.6856
[02/24 01:15:17 cifar100-global-fpgm-2.0-mobilenetv2]: Params: 1.02 M
[02/24 01:15:17 cifar100-global-fpgm-2.0-mobilenetv2]: ops: 33.72 M
[02/24 01:15:20 cifar100-global-fpgm-2.0-mobilenetv2]: Acc: 0.6832 Val Loss: 1.1204

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: mode: prune
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: model: mobilenetv2
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: verbose: False
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: dataset: cifar100
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: dataroot: data
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: batch_size: 128
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: total_epochs: 100
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: lr: 0.01
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: restore: run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: output_dir: run/cifar100/prune/cifar100-global-obdc-2.0-mobilenetv2
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: finetune: True
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: last_epochs: 100
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: reps: 1
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: method: obdc
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: speed_up: 2.0
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: reg: 1e-05
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: delta_reg: 0.0001
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: weight_decay: 0.0005
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: seed: 1
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: global_pruning: True
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: sl_total_epochs: 100
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: sl_lr: 0.01
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: sl_reg_warmup: 0
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: sl_restore: None
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: iterative_steps: 400
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: logger: <Logger cifar100-global-obdc-2.0-mobilenetv2 (DEBUG)>
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: device: cuda
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: num_classes: 100
[02/24 01:15:28 cifar100-global-obdc-2.0-mobilenetv2]: Loading model from run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/24 01:15:31 cifar100-global-obdc-2.0-mobilenetv2]: Pruning...
Traceback (most recent call last):
  File "/home/REDACTED/paper1/main.py", line 429, in <module>
    main()
  File "/home/REDACTED/paper1/main.py", line 369, in main
    progressive_pruning(
  File "/home/REDACTED/paper1/main.py", line 65, in progressive_pruning
    imp._prepare_model(model, pruner)
  File "/home/REDACTED/paper1/torch_pruning/pruner/importance.py", line 872, in _prepare_model
    for i, (dep, idxs) in enumerate(group):
TypeError: 'NoneType' object is not iterable
/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: mode: prune
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: model: mobilenetv2
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: verbose: False
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: dataset: cifar100
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: dataroot: data
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: batch_size: 128
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: total_epochs: 100
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: lr: 0.01
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: restore: run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: output_dir: run/cifar100/prune/cifar100-global-lamp-2.0-mobilenetv2
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: finetune: True
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: last_epochs: 100
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: reps: 1
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: method: lamp
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: speed_up: 2.0
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: reg: 1e-05
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: delta_reg: 0.0001
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: weight_decay: 0.0005
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: seed: 1
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: global_pruning: True
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: sl_total_epochs: 100
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: sl_lr: 0.01
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: sl_reg_warmup: 0
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: sl_restore: None
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: iterative_steps: 400
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: logger: <Logger cifar100-global-lamp-2.0-mobilenetv2 (DEBUG)>
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: device: cuda
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: num_classes: 100
[02/24 01:15:39 cifar100-global-lamp-2.0-mobilenetv2]: Loading model from run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/24 01:15:43 cifar100-global-lamp-2.0-mobilenetv2]: Pruning...
[02/24 01:16:11 cifar100-global-lamp-2.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 13, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(13, 15, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=15)
      (4): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(15, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 82, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(82, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(82, 82, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=82)
        (4): BatchNorm2d(82, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(82, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 116, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(116, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=116)
        (4): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(116, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 91, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(91, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(91, 91, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=91)
        (4): BatchNorm2d(91, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(91, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 118, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(118, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(118, 118, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=118)
        (4): BatchNorm2d(118, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(118, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 95, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(95, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(95, 95, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=95)
        (4): BatchNorm2d(95, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(95, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 111, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(111, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(111, 111, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=111)
        (4): BatchNorm2d(111, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(111, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 147, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(147, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(147, 147, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=147)
        (4): BatchNorm2d(147, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(147, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 139, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(139, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(139, 139, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=139)
        (4): BatchNorm2d(139, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(139, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 113, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(113, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(113, 113, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=113)
        (4): BatchNorm2d(113, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(113, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 238, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(238, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(238, 238, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=238)
        (4): BatchNorm2d(238, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(238, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 183, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(183, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(183, 183, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=183)
        (4): BatchNorm2d(183, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(183, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 122, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(122, 122, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=122)
        (4): BatchNorm2d(122, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(122, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 269, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(269, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(269, 269, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=269)
        (4): BatchNorm2d(269, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(269, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 356, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(356, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(356, 356, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=356)
        (4): BatchNorm2d(356, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(356, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 400, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(400, 400, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=400)
        (4): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(400, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 186, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(186, 186, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=186)
      (4): BatchNorm2d(186, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(186, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 100, kernel_size=(1, 1), stride=(1, 1))
)
[02/24 01:16:14 cifar100-global-lamp-2.0-mobilenetv2]: Params: 2.37 M => 1.17 M (49.56%)
[02/24 01:16:14 cifar100-global-lamp-2.0-mobilenetv2]: FLOPs: 68.40 M => 34.18 M (49.98%, 2.00X )
[02/24 01:16:14 cifar100-global-lamp-2.0-mobilenetv2]: Acc: 0.6699 => 0.6699
[02/24 01:16:14 cifar100-global-lamp-2.0-mobilenetv2]: Val Loss: 1.1637 => 1.1637
[02/24 01:16:14 cifar100-global-lamp-2.0-mobilenetv2]: Finetuning...
[02/24 01:16:44 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 0/100, Acc=0.5896, Val Loss=1.4551, lr=0.0100
[02/24 01:17:16 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 1/100, Acc=0.6063, Val Loss=1.3932, lr=0.0100
[02/24 01:17:47 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 2/100, Acc=0.6023, Val Loss=1.4034, lr=0.0100
[02/24 01:18:22 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 3/100, Acc=0.5985, Val Loss=1.4197, lr=0.0100
[02/24 01:18:53 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 4/100, Acc=0.5956, Val Loss=1.4512, lr=0.0100
[02/24 01:19:25 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 5/100, Acc=0.6103, Val Loss=1.3768, lr=0.0100
[02/24 01:19:57 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 6/100, Acc=0.5915, Val Loss=1.4709, lr=0.0100
[02/24 01:20:35 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 7/100, Acc=0.6044, Val Loss=1.3922, lr=0.0100
[02/24 01:21:07 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 8/100, Acc=0.6070, Val Loss=1.3779, lr=0.0100
[02/24 01:21:39 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 9/100, Acc=0.6021, Val Loss=1.4339, lr=0.0100
[02/24 01:22:10 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 10/100, Acc=0.6080, Val Loss=1.3753, lr=0.0100
[02/24 01:22:43 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 11/100, Acc=0.6088, Val Loss=1.3931, lr=0.0100
[02/24 01:23:15 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 12/100, Acc=0.5995, Val Loss=1.4210, lr=0.0100
[02/24 01:23:46 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 13/100, Acc=0.6006, Val Loss=1.4272, lr=0.0100
[02/24 01:24:18 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 14/100, Acc=0.6056, Val Loss=1.4024, lr=0.0100
[02/24 01:24:50 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 15/100, Acc=0.6149, Val Loss=1.3765, lr=0.0100
[02/24 01:25:26 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 16/100, Acc=0.6049, Val Loss=1.4345, lr=0.0100
[02/24 01:26:02 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 17/100, Acc=0.5948, Val Loss=1.4483, lr=0.0100
[02/24 01:26:37 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 18/100, Acc=0.6187, Val Loss=1.3525, lr=0.0100
[02/24 01:27:08 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 19/100, Acc=0.5962, Val Loss=1.4515, lr=0.0100
[02/24 01:27:40 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 20/100, Acc=0.6067, Val Loss=1.3931, lr=0.0100
[02/24 01:28:12 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 21/100, Acc=0.5960, Val Loss=1.4355, lr=0.0100
[02/24 01:28:43 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 22/100, Acc=0.5961, Val Loss=1.4510, lr=0.0100
[02/24 01:29:15 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 23/100, Acc=0.6155, Val Loss=1.3741, lr=0.0100
[02/24 01:29:47 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 24/100, Acc=0.6042, Val Loss=1.4221, lr=0.0100
[02/24 01:30:19 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 25/100, Acc=0.5912, Val Loss=1.4554, lr=0.0100
[02/24 01:30:50 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 26/100, Acc=0.6087, Val Loss=1.3663, lr=0.0100
[02/24 01:31:22 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 27/100, Acc=0.5859, Val Loss=1.4696, lr=0.0100
[02/24 01:31:53 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 28/100, Acc=0.6179, Val Loss=1.3349, lr=0.0100
[02/24 01:32:24 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 29/100, Acc=0.6067, Val Loss=1.4018, lr=0.0100
[02/24 01:32:55 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 30/100, Acc=0.5982, Val Loss=1.4440, lr=0.0100
[02/24 01:33:26 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 31/100, Acc=0.6098, Val Loss=1.3749, lr=0.0100
[02/24 01:33:57 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 32/100, Acc=0.6122, Val Loss=1.3675, lr=0.0100
[02/24 01:34:29 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 33/100, Acc=0.6053, Val Loss=1.3925, lr=0.0100
[02/24 01:35:04 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 34/100, Acc=0.6104, Val Loss=1.3991, lr=0.0100
[02/24 01:35:42 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 35/100, Acc=0.6235, Val Loss=1.3599, lr=0.0100
[02/24 01:36:13 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 36/100, Acc=0.5990, Val Loss=1.4286, lr=0.0100
[02/24 01:36:44 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 37/100, Acc=0.5940, Val Loss=1.4599, lr=0.0100
[02/24 01:37:17 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 38/100, Acc=0.6199, Val Loss=1.3390, lr=0.0100
[02/24 01:37:48 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 39/100, Acc=0.6131, Val Loss=1.3896, lr=0.0100
[02/24 01:38:19 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 40/100, Acc=0.6067, Val Loss=1.4147, lr=0.0100
[02/24 01:38:51 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 41/100, Acc=0.6259, Val Loss=1.3334, lr=0.0100
[02/24 01:39:22 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 42/100, Acc=0.6133, Val Loss=1.3832, lr=0.0100
[02/24 01:39:53 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 43/100, Acc=0.6198, Val Loss=1.3316, lr=0.0100
[02/24 01:40:24 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 44/100, Acc=0.6139, Val Loss=1.3723, lr=0.0100
[02/24 01:40:55 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 45/100, Acc=0.6236, Val Loss=1.3494, lr=0.0100
[02/24 01:41:27 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 46/100, Acc=0.5972, Val Loss=1.4519, lr=0.0100
[02/24 01:41:58 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 47/100, Acc=0.5997, Val Loss=1.4614, lr=0.0100
[02/24 01:42:30 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 48/100, Acc=0.6188, Val Loss=1.3471, lr=0.0100
[02/24 01:43:02 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 49/100, Acc=0.6084, Val Loss=1.3910, lr=0.0100
[02/24 01:43:34 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 50/100, Acc=0.6180, Val Loss=1.3553, lr=0.0100
[02/24 01:44:05 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 51/100, Acc=0.6221, Val Loss=1.3189, lr=0.0100
[02/24 01:44:37 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 52/100, Acc=0.6147, Val Loss=1.3717, lr=0.0100
[02/24 01:45:09 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 53/100, Acc=0.6125, Val Loss=1.4015, lr=0.0100
[02/24 01:45:41 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 54/100, Acc=0.6071, Val Loss=1.3989, lr=0.0100
[02/24 01:46:13 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 55/100, Acc=0.6049, Val Loss=1.4197, lr=0.0100
[02/24 01:46:46 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 56/100, Acc=0.6227, Val Loss=1.3347, lr=0.0100
[02/24 01:47:18 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 57/100, Acc=0.6221, Val Loss=1.3274, lr=0.0100
[02/24 01:47:57 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 58/100, Acc=0.5996, Val Loss=1.4309, lr=0.0100
[02/24 01:48:35 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 59/100, Acc=0.6079, Val Loss=1.3887, lr=0.0100
[02/24 01:49:13 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 60/100, Acc=0.6683, Val Loss=1.1471, lr=0.0010
[02/24 01:49:46 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 61/100, Acc=0.6695, Val Loss=1.1453, lr=0.0010
[02/24 01:50:18 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 62/100, Acc=0.6759, Val Loss=1.1320, lr=0.0010
[02/24 01:50:50 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 63/100, Acc=0.6773, Val Loss=1.1324, lr=0.0010
[02/24 01:51:22 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 64/100, Acc=0.6794, Val Loss=1.1332, lr=0.0010
[02/24 01:51:54 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 65/100, Acc=0.6767, Val Loss=1.1345, lr=0.0010
[02/24 01:52:32 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 66/100, Acc=0.6771, Val Loss=1.1271, lr=0.0010
[02/24 01:53:10 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 67/100, Acc=0.6792, Val Loss=1.1304, lr=0.0010
[02/24 01:53:48 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 68/100, Acc=0.6768, Val Loss=1.1349, lr=0.0010
[02/24 01:54:26 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 69/100, Acc=0.6765, Val Loss=1.1329, lr=0.0010
[02/24 01:55:04 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 70/100, Acc=0.6783, Val Loss=1.1324, lr=0.0010
[02/24 01:55:42 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 71/100, Acc=0.6787, Val Loss=1.1347, lr=0.0010
[02/24 01:56:21 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 72/100, Acc=0.6800, Val Loss=1.1375, lr=0.0010
[02/24 01:57:01 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 73/100, Acc=0.6789, Val Loss=1.1382, lr=0.0010
[02/24 01:57:42 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 74/100, Acc=0.6763, Val Loss=1.1409, lr=0.0010
[02/24 01:58:22 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 75/100, Acc=0.6773, Val Loss=1.1400, lr=0.0010
[02/24 01:59:02 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 76/100, Acc=0.6795, Val Loss=1.1378, lr=0.0010
[02/24 01:59:43 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 77/100, Acc=0.6752, Val Loss=1.1414, lr=0.0010
[02/24 02:00:24 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 78/100, Acc=0.6765, Val Loss=1.1414, lr=0.0010
[02/24 02:00:58 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 79/100, Acc=0.6756, Val Loss=1.1410, lr=0.0010
[02/24 02:01:31 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 80/100, Acc=0.6797, Val Loss=1.1349, lr=0.0001
[02/24 02:02:04 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 81/100, Acc=0.6809, Val Loss=1.1318, lr=0.0001
[02/24 02:02:37 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 82/100, Acc=0.6788, Val Loss=1.1334, lr=0.0001
[02/24 02:03:09 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 83/100, Acc=0.6789, Val Loss=1.1330, lr=0.0001
[02/24 02:03:42 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 84/100, Acc=0.6795, Val Loss=1.1316, lr=0.0001
[02/24 02:04:15 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 85/100, Acc=0.6807, Val Loss=1.1327, lr=0.0001
[02/24 02:04:49 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 86/100, Acc=0.6786, Val Loss=1.1323, lr=0.0001
[02/24 02:05:23 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 87/100, Acc=0.6795, Val Loss=1.1322, lr=0.0001
[02/24 02:05:56 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 88/100, Acc=0.6787, Val Loss=1.1339, lr=0.0001
[02/24 02:06:30 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 89/100, Acc=0.6800, Val Loss=1.1336, lr=0.0001
[02/24 02:07:02 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 90/100, Acc=0.6823, Val Loss=1.1338, lr=0.0001
[02/24 02:07:34 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 91/100, Acc=0.6810, Val Loss=1.1338, lr=0.0001
[02/24 02:08:06 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 92/100, Acc=0.6820, Val Loss=1.1349, lr=0.0001
[02/24 02:08:41 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 93/100, Acc=0.6824, Val Loss=1.1340, lr=0.0001
[02/24 02:09:13 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 94/100, Acc=0.6819, Val Loss=1.1352, lr=0.0001
[02/24 02:09:45 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 95/100, Acc=0.6815, Val Loss=1.1344, lr=0.0001
[02/24 02:10:16 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 96/100, Acc=0.6816, Val Loss=1.1338, lr=0.0001
[02/24 02:10:48 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 97/100, Acc=0.6796, Val Loss=1.1357, lr=0.0001
[02/24 02:11:20 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 98/100, Acc=0.6819, Val Loss=1.1341, lr=0.0001
[02/24 02:11:52 cifar100-global-lamp-2.0-mobilenetv2]: Epoch 99/100, Acc=0.6800, Val Loss=1.1357, lr=0.0001
[02/24 02:11:52 cifar100-global-lamp-2.0-mobilenetv2]: Best Acc=0.6824
[02/24 02:11:52 cifar100-global-lamp-2.0-mobilenetv2]: Params: 1.17 M
[02/24 02:11:52 cifar100-global-lamp-2.0-mobilenetv2]: ops: 34.18 M
[02/24 02:11:55 cifar100-global-lamp-2.0-mobilenetv2]: Acc: 0.6800 Val Loss: 1.1357

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: mode: prune
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: model: mobilenetv2
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: verbose: False
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: dataset: cifar100
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: dataroot: data
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: batch_size: 128
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: total_epochs: 100
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: lr: 0.01
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: restore: run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: output_dir: run/cifar100/prune/cifar100-global-slim-2.0-mobilenetv2
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: finetune: True
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: last_epochs: 100
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: reps: 1
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: method: slim
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: speed_up: 2.0
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: reg: 1e-05
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: delta_reg: 0.0001
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: weight_decay: 0.0005
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: seed: 1
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: global_pruning: True
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: sl_total_epochs: 100
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: sl_lr: 0.01
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: sl_reg_warmup: 0
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: sl_restore: None
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: iterative_steps: 400
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: logger: <Logger cifar100-global-slim-2.0-mobilenetv2 (DEBUG)>
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: device: cuda
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: num_classes: 100
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: Loading model from run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/24 02:12:04 cifar100-global-slim-2.0-mobilenetv2]: Regularizing...
[02/24 02:12:39 cifar100-global-slim-2.0-mobilenetv2]: Epoch 0/100, Acc=0.5902, Val Loss=1.5017, lr=0.0100
[02/24 02:13:14 cifar100-global-slim-2.0-mobilenetv2]: Epoch 1/100, Acc=0.6025, Val Loss=1.4100, lr=0.0100
[02/24 02:13:49 cifar100-global-slim-2.0-mobilenetv2]: Epoch 2/100, Acc=0.6021, Val Loss=1.4389, lr=0.0100
[02/24 02:14:25 cifar100-global-slim-2.0-mobilenetv2]: Epoch 3/100, Acc=0.6064, Val Loss=1.4493, lr=0.0100
[02/24 02:15:00 cifar100-global-slim-2.0-mobilenetv2]: Epoch 4/100, Acc=0.6042, Val Loss=1.4167, lr=0.0100
[02/24 02:15:35 cifar100-global-slim-2.0-mobilenetv2]: Epoch 5/100, Acc=0.6135, Val Loss=1.3909, lr=0.0100
[02/24 02:16:11 cifar100-global-slim-2.0-mobilenetv2]: Epoch 6/100, Acc=0.6003, Val Loss=1.4232, lr=0.0100
[02/24 02:16:46 cifar100-global-slim-2.0-mobilenetv2]: Epoch 7/100, Acc=0.6176, Val Loss=1.3747, lr=0.0100
[02/24 02:17:22 cifar100-global-slim-2.0-mobilenetv2]: Epoch 8/100, Acc=0.6094, Val Loss=1.4002, lr=0.0100
[02/24 02:17:57 cifar100-global-slim-2.0-mobilenetv2]: Epoch 9/100, Acc=0.6189, Val Loss=1.3849, lr=0.0100
[02/24 02:18:33 cifar100-global-slim-2.0-mobilenetv2]: Epoch 10/100, Acc=0.6212, Val Loss=1.3640, lr=0.0100
[02/24 02:19:08 cifar100-global-slim-2.0-mobilenetv2]: Epoch 11/100, Acc=0.6103, Val Loss=1.4041, lr=0.0100
[02/24 02:19:42 cifar100-global-slim-2.0-mobilenetv2]: Epoch 12/100, Acc=0.6141, Val Loss=1.4143, lr=0.0100
[02/24 02:20:18 cifar100-global-slim-2.0-mobilenetv2]: Epoch 13/100, Acc=0.6276, Val Loss=1.3480, lr=0.0100
[02/24 02:20:53 cifar100-global-slim-2.0-mobilenetv2]: Epoch 14/100, Acc=0.6172, Val Loss=1.3681, lr=0.0100
[02/24 02:21:29 cifar100-global-slim-2.0-mobilenetv2]: Epoch 15/100, Acc=0.6213, Val Loss=1.3817, lr=0.0100
[02/24 02:22:04 cifar100-global-slim-2.0-mobilenetv2]: Epoch 16/100, Acc=0.6302, Val Loss=1.3532, lr=0.0100
[02/24 02:22:39 cifar100-global-slim-2.0-mobilenetv2]: Epoch 17/100, Acc=0.6281, Val Loss=1.3506, lr=0.0100
[02/24 02:23:15 cifar100-global-slim-2.0-mobilenetv2]: Epoch 18/100, Acc=0.6347, Val Loss=1.3191, lr=0.0100
[02/24 02:23:50 cifar100-global-slim-2.0-mobilenetv2]: Epoch 19/100, Acc=0.6339, Val Loss=1.3267, lr=0.0100
[02/24 02:24:26 cifar100-global-slim-2.0-mobilenetv2]: Epoch 20/100, Acc=0.6446, Val Loss=1.2963, lr=0.0100
[02/24 02:25:02 cifar100-global-slim-2.0-mobilenetv2]: Epoch 21/100, Acc=0.6347, Val Loss=1.3261, lr=0.0100
[02/24 02:25:37 cifar100-global-slim-2.0-mobilenetv2]: Epoch 22/100, Acc=0.6310, Val Loss=1.3395, lr=0.0100
[02/24 02:26:13 cifar100-global-slim-2.0-mobilenetv2]: Epoch 23/100, Acc=0.6270, Val Loss=1.3509, lr=0.0100
[02/24 02:26:49 cifar100-global-slim-2.0-mobilenetv2]: Epoch 24/100, Acc=0.6376, Val Loss=1.3300, lr=0.0100
[02/24 02:27:25 cifar100-global-slim-2.0-mobilenetv2]: Epoch 25/100, Acc=0.6429, Val Loss=1.3006, lr=0.0100
[02/24 02:28:00 cifar100-global-slim-2.0-mobilenetv2]: Epoch 26/100, Acc=0.6368, Val Loss=1.3350, lr=0.0100
[02/24 02:28:35 cifar100-global-slim-2.0-mobilenetv2]: Epoch 27/100, Acc=0.6380, Val Loss=1.3337, lr=0.0100
[02/24 02:29:10 cifar100-global-slim-2.0-mobilenetv2]: Epoch 28/100, Acc=0.6470, Val Loss=1.3011, lr=0.0100
[02/24 02:29:45 cifar100-global-slim-2.0-mobilenetv2]: Epoch 29/100, Acc=0.6431, Val Loss=1.3305, lr=0.0100
[02/24 02:30:21 cifar100-global-slim-2.0-mobilenetv2]: Epoch 30/100, Acc=0.6442, Val Loss=1.3109, lr=0.0100
[02/24 02:30:58 cifar100-global-slim-2.0-mobilenetv2]: Epoch 31/100, Acc=0.6405, Val Loss=1.3066, lr=0.0100
[02/24 02:31:34 cifar100-global-slim-2.0-mobilenetv2]: Epoch 32/100, Acc=0.6479, Val Loss=1.2961, lr=0.0100
[02/24 02:32:10 cifar100-global-slim-2.0-mobilenetv2]: Epoch 33/100, Acc=0.6432, Val Loss=1.3291, lr=0.0100
[02/24 02:32:45 cifar100-global-slim-2.0-mobilenetv2]: Epoch 34/100, Acc=0.6465, Val Loss=1.3211, lr=0.0100
[02/24 02:33:20 cifar100-global-slim-2.0-mobilenetv2]: Epoch 35/100, Acc=0.6329, Val Loss=1.3488, lr=0.0100
[02/24 02:33:56 cifar100-global-slim-2.0-mobilenetv2]: Epoch 36/100, Acc=0.6439, Val Loss=1.3235, lr=0.0100
[02/24 02:34:31 cifar100-global-slim-2.0-mobilenetv2]: Epoch 37/100, Acc=0.6403, Val Loss=1.3349, lr=0.0100
[02/24 02:35:06 cifar100-global-slim-2.0-mobilenetv2]: Epoch 38/100, Acc=0.6432, Val Loss=1.3278, lr=0.0100
[02/24 02:35:41 cifar100-global-slim-2.0-mobilenetv2]: Epoch 39/100, Acc=0.6467, Val Loss=1.3134, lr=0.0100
[02/24 02:36:16 cifar100-global-slim-2.0-mobilenetv2]: Epoch 40/100, Acc=0.6454, Val Loss=1.3191, lr=0.0100
[02/24 02:36:52 cifar100-global-slim-2.0-mobilenetv2]: Epoch 41/100, Acc=0.6411, Val Loss=1.3374, lr=0.0100
[02/24 02:37:27 cifar100-global-slim-2.0-mobilenetv2]: Epoch 42/100, Acc=0.6482, Val Loss=1.3133, lr=0.0100
[02/24 02:38:02 cifar100-global-slim-2.0-mobilenetv2]: Epoch 43/100, Acc=0.6470, Val Loss=1.3167, lr=0.0100
[02/24 02:38:37 cifar100-global-slim-2.0-mobilenetv2]: Epoch 44/100, Acc=0.6495, Val Loss=1.3072, lr=0.0100
[02/24 02:39:13 cifar100-global-slim-2.0-mobilenetv2]: Epoch 45/100, Acc=0.6446, Val Loss=1.3252, lr=0.0100
[02/24 02:39:48 cifar100-global-slim-2.0-mobilenetv2]: Epoch 46/100, Acc=0.6435, Val Loss=1.3565, lr=0.0100
[02/24 02:40:24 cifar100-global-slim-2.0-mobilenetv2]: Epoch 47/100, Acc=0.6404, Val Loss=1.3571, lr=0.0100
[02/24 02:40:59 cifar100-global-slim-2.0-mobilenetv2]: Epoch 48/100, Acc=0.6460, Val Loss=1.3300, lr=0.0100
[02/24 02:41:34 cifar100-global-slim-2.0-mobilenetv2]: Epoch 49/100, Acc=0.6528, Val Loss=1.3306, lr=0.0100
[02/24 02:42:10 cifar100-global-slim-2.0-mobilenetv2]: Epoch 50/100, Acc=0.6521, Val Loss=1.3280, lr=0.0100
[02/24 02:42:45 cifar100-global-slim-2.0-mobilenetv2]: Epoch 51/100, Acc=0.6512, Val Loss=1.3146, lr=0.0100
[02/24 02:43:20 cifar100-global-slim-2.0-mobilenetv2]: Epoch 52/100, Acc=0.6465, Val Loss=1.3393, lr=0.0100
[02/24 02:43:55 cifar100-global-slim-2.0-mobilenetv2]: Epoch 53/100, Acc=0.6550, Val Loss=1.2904, lr=0.0100
[02/24 02:44:30 cifar100-global-slim-2.0-mobilenetv2]: Epoch 54/100, Acc=0.6522, Val Loss=1.3315, lr=0.0100
[02/24 02:45:05 cifar100-global-slim-2.0-mobilenetv2]: Epoch 55/100, Acc=0.6594, Val Loss=1.3182, lr=0.0100
[02/24 02:45:41 cifar100-global-slim-2.0-mobilenetv2]: Epoch 56/100, Acc=0.6539, Val Loss=1.3127, lr=0.0100
[02/24 02:46:17 cifar100-global-slim-2.0-mobilenetv2]: Epoch 57/100, Acc=0.6535, Val Loss=1.3215, lr=0.0100
[02/24 02:46:52 cifar100-global-slim-2.0-mobilenetv2]: Epoch 58/100, Acc=0.6490, Val Loss=1.3522, lr=0.0100
[02/24 02:47:27 cifar100-global-slim-2.0-mobilenetv2]: Epoch 59/100, Acc=0.6543, Val Loss=1.3536, lr=0.0100
[02/24 02:48:01 cifar100-global-slim-2.0-mobilenetv2]: Epoch 60/100, Acc=0.6799, Val Loss=1.2209, lr=0.0010
[02/24 02:48:37 cifar100-global-slim-2.0-mobilenetv2]: Epoch 61/100, Acc=0.6795, Val Loss=1.2165, lr=0.0010
[02/24 02:49:13 cifar100-global-slim-2.0-mobilenetv2]: Epoch 62/100, Acc=0.6834, Val Loss=1.2135, lr=0.0010
[02/24 02:49:48 cifar100-global-slim-2.0-mobilenetv2]: Epoch 63/100, Acc=0.6811, Val Loss=1.2134, lr=0.0010
[02/24 02:50:24 cifar100-global-slim-2.0-mobilenetv2]: Epoch 64/100, Acc=0.6836, Val Loss=1.2223, lr=0.0010
[02/24 02:51:06 cifar100-global-slim-2.0-mobilenetv2]: Epoch 65/100, Acc=0.6817, Val Loss=1.2181, lr=0.0010
[02/24 02:51:47 cifar100-global-slim-2.0-mobilenetv2]: Epoch 66/100, Acc=0.6841, Val Loss=1.2178, lr=0.0010
[02/24 02:52:23 cifar100-global-slim-2.0-mobilenetv2]: Epoch 67/100, Acc=0.6829, Val Loss=1.2244, lr=0.0010
[02/24 02:52:58 cifar100-global-slim-2.0-mobilenetv2]: Epoch 68/100, Acc=0.6835, Val Loss=1.2220, lr=0.0010
[02/24 02:53:33 cifar100-global-slim-2.0-mobilenetv2]: Epoch 69/100, Acc=0.6842, Val Loss=1.2241, lr=0.0010
[02/24 02:54:07 cifar100-global-slim-2.0-mobilenetv2]: Epoch 70/100, Acc=0.6833, Val Loss=1.2303, lr=0.0010
[02/24 02:54:43 cifar100-global-slim-2.0-mobilenetv2]: Epoch 71/100, Acc=0.6823, Val Loss=1.2281, lr=0.0010
[02/24 02:55:19 cifar100-global-slim-2.0-mobilenetv2]: Epoch 72/100, Acc=0.6829, Val Loss=1.2336, lr=0.0010
[02/24 02:55:54 cifar100-global-slim-2.0-mobilenetv2]: Epoch 73/100, Acc=0.6815, Val Loss=1.2368, lr=0.0010
[02/24 02:56:30 cifar100-global-slim-2.0-mobilenetv2]: Epoch 74/100, Acc=0.6812, Val Loss=1.2359, lr=0.0010
[02/24 02:57:05 cifar100-global-slim-2.0-mobilenetv2]: Epoch 75/100, Acc=0.6843, Val Loss=1.2317, lr=0.0010
[02/24 02:57:40 cifar100-global-slim-2.0-mobilenetv2]: Epoch 76/100, Acc=0.6806, Val Loss=1.2400, lr=0.0010
[02/24 02:58:16 cifar100-global-slim-2.0-mobilenetv2]: Epoch 77/100, Acc=0.6822, Val Loss=1.2399, lr=0.0010
[02/24 02:58:52 cifar100-global-slim-2.0-mobilenetv2]: Epoch 78/100, Acc=0.6812, Val Loss=1.2428, lr=0.0010
[02/24 02:59:27 cifar100-global-slim-2.0-mobilenetv2]: Epoch 79/100, Acc=0.6842, Val Loss=1.2447, lr=0.0010
[02/24 03:00:03 cifar100-global-slim-2.0-mobilenetv2]: Epoch 80/100, Acc=0.6833, Val Loss=1.2434, lr=0.0001
[02/24 03:00:39 cifar100-global-slim-2.0-mobilenetv2]: Epoch 81/100, Acc=0.6843, Val Loss=1.2437, lr=0.0001
[02/24 03:01:14 cifar100-global-slim-2.0-mobilenetv2]: Epoch 82/100, Acc=0.6831, Val Loss=1.2430, lr=0.0001
[02/24 03:01:50 cifar100-global-slim-2.0-mobilenetv2]: Epoch 83/100, Acc=0.6836, Val Loss=1.2387, lr=0.0001
[02/24 03:02:26 cifar100-global-slim-2.0-mobilenetv2]: Epoch 84/100, Acc=0.6849, Val Loss=1.2403, lr=0.0001
[02/24 03:03:02 cifar100-global-slim-2.0-mobilenetv2]: Epoch 85/100, Acc=0.6835, Val Loss=1.2418, lr=0.0001
[02/24 03:03:38 cifar100-global-slim-2.0-mobilenetv2]: Epoch 86/100, Acc=0.6842, Val Loss=1.2430, lr=0.0001
[02/24 03:04:14 cifar100-global-slim-2.0-mobilenetv2]: Epoch 87/100, Acc=0.6847, Val Loss=1.2399, lr=0.0001
[02/24 03:04:49 cifar100-global-slim-2.0-mobilenetv2]: Epoch 88/100, Acc=0.6828, Val Loss=1.2416, lr=0.0001
[02/24 03:05:25 cifar100-global-slim-2.0-mobilenetv2]: Epoch 89/100, Acc=0.6844, Val Loss=1.2444, lr=0.0001
[02/24 03:06:01 cifar100-global-slim-2.0-mobilenetv2]: Epoch 90/100, Acc=0.6847, Val Loss=1.2430, lr=0.0001
[02/24 03:06:37 cifar100-global-slim-2.0-mobilenetv2]: Epoch 91/100, Acc=0.6839, Val Loss=1.2438, lr=0.0001
[02/24 03:07:14 cifar100-global-slim-2.0-mobilenetv2]: Epoch 92/100, Acc=0.6837, Val Loss=1.2419, lr=0.0001
[02/24 03:07:50 cifar100-global-slim-2.0-mobilenetv2]: Epoch 93/100, Acc=0.6837, Val Loss=1.2400, lr=0.0001
[02/24 03:08:27 cifar100-global-slim-2.0-mobilenetv2]: Epoch 94/100, Acc=0.6838, Val Loss=1.2423, lr=0.0001
[02/24 03:09:03 cifar100-global-slim-2.0-mobilenetv2]: Epoch 95/100, Acc=0.6829, Val Loss=1.2460, lr=0.0001
[02/24 03:09:40 cifar100-global-slim-2.0-mobilenetv2]: Epoch 96/100, Acc=0.6833, Val Loss=1.2438, lr=0.0001
[02/24 03:10:16 cifar100-global-slim-2.0-mobilenetv2]: Epoch 97/100, Acc=0.6829, Val Loss=1.2414, lr=0.0001
[02/24 03:10:52 cifar100-global-slim-2.0-mobilenetv2]: Epoch 98/100, Acc=0.6835, Val Loss=1.2433, lr=0.0001
[02/24 03:11:28 cifar100-global-slim-2.0-mobilenetv2]: Epoch 99/100, Acc=0.6846, Val Loss=1.2457, lr=0.0001
[02/24 03:11:28 cifar100-global-slim-2.0-mobilenetv2]: Best Acc=0.6849
[02/24 03:11:28 cifar100-global-slim-2.0-mobilenetv2]: Loading the sparse model from run/cifar100/prune/cifar100-global-slim-2.0-mobilenetv2/reg_cifar100_mobilenetv2_slim_1e-05.pth...
[02/24 03:11:32 cifar100-global-slim-2.0-mobilenetv2]: Pruning...
[02/24 03:11:53 cifar100-global-slim-2.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 9, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(9, 15, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=15)
      (4): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(15, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72)
        (4): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 69, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(69, 69, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=69)
        (4): BatchNorm2d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(69, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 64, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 78, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(78, 78, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=78)
        (4): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(78, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 47, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(47, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=47)
        (4): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(47, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 78, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(78, 78, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=78)
        (4): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(78, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 121, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(121, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(121, 121, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=121)
        (4): BatchNorm2d(121, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(121, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 372, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(372, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(372, 372, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=372)
        (4): BatchNorm2d(372, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(372, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 353, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(353, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(353, 353, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=353)
        (4): BatchNorm2d(353, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(353, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 164, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(164, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(164, 164, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=164)
        (4): BatchNorm2d(164, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(164, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 458, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(458, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(458, 458, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=458)
        (4): BatchNorm2d(458, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(458, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 530, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(530, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(530, 530, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=530)
        (4): BatchNorm2d(530, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(530, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 151, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(151, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(151, 151, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=151)
        (4): BatchNorm2d(151, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(151, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 74, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(74, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(74, 74, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=74)
        (4): BatchNorm2d(74, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(74, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 613, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(613, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(613, 613, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=613)
        (4): BatchNorm2d(613, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(613, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 98, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(98, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(98, 98, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=98)
      (4): BatchNorm2d(98, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(98, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 100, kernel_size=(1, 1), stride=(1, 1))
)
[02/24 03:11:56 cifar100-global-slim-2.0-mobilenetv2]: Params: 2.37 M => 1.25 M (52.83%)
[02/24 03:11:56 cifar100-global-slim-2.0-mobilenetv2]: FLOPs: 68.40 M => 34.19 M (49.98%, 2.00X )
[02/24 03:11:56 cifar100-global-slim-2.0-mobilenetv2]: Acc: 0.6849 => 0.6843
[02/24 03:11:56 cifar100-global-slim-2.0-mobilenetv2]: Val Loss: 1.2403 => 1.2403
[02/24 03:11:56 cifar100-global-slim-2.0-mobilenetv2]: Finetuning...
[02/24 03:12:30 cifar100-global-slim-2.0-mobilenetv2]: Epoch 0/100, Acc=0.6513, Val Loss=1.3581, lr=0.0100
[02/24 03:13:04 cifar100-global-slim-2.0-mobilenetv2]: Epoch 1/100, Acc=0.6528, Val Loss=1.3109, lr=0.0100
[02/24 03:13:45 cifar100-global-slim-2.0-mobilenetv2]: Epoch 2/100, Acc=0.6494, Val Loss=1.2938, lr=0.0100
[02/24 03:14:21 cifar100-global-slim-2.0-mobilenetv2]: Epoch 3/100, Acc=0.6587, Val Loss=1.2864, lr=0.0100
[02/24 03:14:54 cifar100-global-slim-2.0-mobilenetv2]: Epoch 4/100, Acc=0.6509, Val Loss=1.2969, lr=0.0100
[02/24 03:15:28 cifar100-global-slim-2.0-mobilenetv2]: Epoch 5/100, Acc=0.6471, Val Loss=1.3114, lr=0.0100
[02/24 03:16:02 cifar100-global-slim-2.0-mobilenetv2]: Epoch 6/100, Acc=0.6457, Val Loss=1.3200, lr=0.0100
[02/24 03:16:36 cifar100-global-slim-2.0-mobilenetv2]: Epoch 7/100, Acc=0.6489, Val Loss=1.2862, lr=0.0100
[02/24 03:17:09 cifar100-global-slim-2.0-mobilenetv2]: Epoch 8/100, Acc=0.6472, Val Loss=1.2770, lr=0.0100
[02/24 03:17:41 cifar100-global-slim-2.0-mobilenetv2]: Epoch 9/100, Acc=0.6497, Val Loss=1.2860, lr=0.0100
[02/24 03:18:14 cifar100-global-slim-2.0-mobilenetv2]: Epoch 10/100, Acc=0.6437, Val Loss=1.3115, lr=0.0100
[02/24 03:18:47 cifar100-global-slim-2.0-mobilenetv2]: Epoch 11/100, Acc=0.6456, Val Loss=1.2858, lr=0.0100
[02/24 03:19:20 cifar100-global-slim-2.0-mobilenetv2]: Epoch 12/100, Acc=0.6452, Val Loss=1.3018, lr=0.0100
[02/24 03:19:52 cifar100-global-slim-2.0-mobilenetv2]: Epoch 13/100, Acc=0.6442, Val Loss=1.2983, lr=0.0100
[02/24 03:20:25 cifar100-global-slim-2.0-mobilenetv2]: Epoch 14/100, Acc=0.6442, Val Loss=1.3112, lr=0.0100
[02/24 03:20:58 cifar100-global-slim-2.0-mobilenetv2]: Epoch 15/100, Acc=0.6309, Val Loss=1.3363, lr=0.0100
[02/24 03:21:31 cifar100-global-slim-2.0-mobilenetv2]: Epoch 16/100, Acc=0.6450, Val Loss=1.2767, lr=0.0100
[02/24 03:22:03 cifar100-global-slim-2.0-mobilenetv2]: Epoch 17/100, Acc=0.6298, Val Loss=1.3645, lr=0.0100
[02/24 03:22:36 cifar100-global-slim-2.0-mobilenetv2]: Epoch 18/100, Acc=0.6333, Val Loss=1.3274, lr=0.0100
[02/24 03:23:08 cifar100-global-slim-2.0-mobilenetv2]: Epoch 19/100, Acc=0.6425, Val Loss=1.2897, lr=0.0100
[02/24 03:23:41 cifar100-global-slim-2.0-mobilenetv2]: Epoch 20/100, Acc=0.6353, Val Loss=1.3236, lr=0.0100
[02/24 03:24:13 cifar100-global-slim-2.0-mobilenetv2]: Epoch 21/100, Acc=0.6414, Val Loss=1.2924, lr=0.0100
[02/24 03:24:45 cifar100-global-slim-2.0-mobilenetv2]: Epoch 22/100, Acc=0.6367, Val Loss=1.3064, lr=0.0100
[02/24 03:25:18 cifar100-global-slim-2.0-mobilenetv2]: Epoch 23/100, Acc=0.6344, Val Loss=1.3366, lr=0.0100
[02/24 03:25:51 cifar100-global-slim-2.0-mobilenetv2]: Epoch 24/100, Acc=0.6424, Val Loss=1.2952, lr=0.0100
[02/24 03:26:23 cifar100-global-slim-2.0-mobilenetv2]: Epoch 25/100, Acc=0.6270, Val Loss=1.3621, lr=0.0100
[02/24 03:26:56 cifar100-global-slim-2.0-mobilenetv2]: Epoch 26/100, Acc=0.6364, Val Loss=1.3163, lr=0.0100
[02/24 03:27:28 cifar100-global-slim-2.0-mobilenetv2]: Epoch 27/100, Acc=0.6329, Val Loss=1.3296, lr=0.0100
[02/24 03:28:01 cifar100-global-slim-2.0-mobilenetv2]: Epoch 28/100, Acc=0.6278, Val Loss=1.3722, lr=0.0100
[02/24 03:28:33 cifar100-global-slim-2.0-mobilenetv2]: Epoch 29/100, Acc=0.6174, Val Loss=1.3807, lr=0.0100
[02/24 03:29:05 cifar100-global-slim-2.0-mobilenetv2]: Epoch 30/100, Acc=0.6127, Val Loss=1.3874, lr=0.0100
[02/24 03:29:36 cifar100-global-slim-2.0-mobilenetv2]: Epoch 31/100, Acc=0.6333, Val Loss=1.3359, lr=0.0100
[02/24 03:30:08 cifar100-global-slim-2.0-mobilenetv2]: Epoch 32/100, Acc=0.6285, Val Loss=1.3500, lr=0.0100
[02/24 03:30:40 cifar100-global-slim-2.0-mobilenetv2]: Epoch 33/100, Acc=0.6342, Val Loss=1.3167, lr=0.0100
[02/24 03:31:13 cifar100-global-slim-2.0-mobilenetv2]: Epoch 34/100, Acc=0.6292, Val Loss=1.3344, lr=0.0100
[02/24 03:31:47 cifar100-global-slim-2.0-mobilenetv2]: Epoch 35/100, Acc=0.6272, Val Loss=1.3372, lr=0.0100
[02/24 03:32:20 cifar100-global-slim-2.0-mobilenetv2]: Epoch 36/100, Acc=0.6101, Val Loss=1.3983, lr=0.0100
[02/24 03:32:58 cifar100-global-slim-2.0-mobilenetv2]: Epoch 37/100, Acc=0.6376, Val Loss=1.3098, lr=0.0100
[02/24 03:33:38 cifar100-global-slim-2.0-mobilenetv2]: Epoch 38/100, Acc=0.6153, Val Loss=1.4105, lr=0.0100
[02/24 03:34:14 cifar100-global-slim-2.0-mobilenetv2]: Epoch 39/100, Acc=0.6319, Val Loss=1.3074, lr=0.0100
[02/24 03:34:47 cifar100-global-slim-2.0-mobilenetv2]: Epoch 40/100, Acc=0.6237, Val Loss=1.3771, lr=0.0100
[02/24 03:35:20 cifar100-global-slim-2.0-mobilenetv2]: Epoch 41/100, Acc=0.6217, Val Loss=1.3917, lr=0.0100
[02/24 03:35:53 cifar100-global-slim-2.0-mobilenetv2]: Epoch 42/100, Acc=0.6278, Val Loss=1.3472, lr=0.0100
[02/24 03:36:26 cifar100-global-slim-2.0-mobilenetv2]: Epoch 43/100, Acc=0.6232, Val Loss=1.3553, lr=0.0100
[02/24 03:36:59 cifar100-global-slim-2.0-mobilenetv2]: Epoch 44/100, Acc=0.6223, Val Loss=1.3537, lr=0.0100
[02/24 03:37:37 cifar100-global-slim-2.0-mobilenetv2]: Epoch 45/100, Acc=0.6006, Val Loss=1.4590, lr=0.0100
[02/24 03:38:17 cifar100-global-slim-2.0-mobilenetv2]: Epoch 46/100, Acc=0.6138, Val Loss=1.3780, lr=0.0100
[02/24 03:38:57 cifar100-global-slim-2.0-mobilenetv2]: Epoch 47/100, Acc=0.6283, Val Loss=1.3328, lr=0.0100
[02/24 03:39:38 cifar100-global-slim-2.0-mobilenetv2]: Epoch 48/100, Acc=0.5981, Val Loss=1.4442, lr=0.0100
[02/24 03:40:18 cifar100-global-slim-2.0-mobilenetv2]: Epoch 49/100, Acc=0.6164, Val Loss=1.3769, lr=0.0100
[02/24 03:40:59 cifar100-global-slim-2.0-mobilenetv2]: Epoch 50/100, Acc=0.6029, Val Loss=1.4338, lr=0.0100
[02/24 03:41:35 cifar100-global-slim-2.0-mobilenetv2]: Epoch 51/100, Acc=0.6279, Val Loss=1.3363, lr=0.0100
[02/24 03:42:08 cifar100-global-slim-2.0-mobilenetv2]: Epoch 52/100, Acc=0.6159, Val Loss=1.3774, lr=0.0100
[02/24 03:42:40 cifar100-global-slim-2.0-mobilenetv2]: Epoch 53/100, Acc=0.6211, Val Loss=1.3805, lr=0.0100
[02/24 03:43:13 cifar100-global-slim-2.0-mobilenetv2]: Epoch 54/100, Acc=0.6195, Val Loss=1.3426, lr=0.0100
[02/24 03:43:46 cifar100-global-slim-2.0-mobilenetv2]: Epoch 55/100, Acc=0.6127, Val Loss=1.3994, lr=0.0100
[02/24 03:44:19 cifar100-global-slim-2.0-mobilenetv2]: Epoch 56/100, Acc=0.6211, Val Loss=1.3662, lr=0.0100
[02/24 03:44:52 cifar100-global-slim-2.0-mobilenetv2]: Epoch 57/100, Acc=0.6206, Val Loss=1.3613, lr=0.0100
[02/24 03:45:25 cifar100-global-slim-2.0-mobilenetv2]: Epoch 58/100, Acc=0.6354, Val Loss=1.3160, lr=0.0100
[02/24 03:45:58 cifar100-global-slim-2.0-mobilenetv2]: Epoch 59/100, Acc=0.6249, Val Loss=1.3500, lr=0.0100
[02/24 03:46:32 cifar100-global-slim-2.0-mobilenetv2]: Epoch 60/100, Acc=0.6798, Val Loss=1.1314, lr=0.0010
[02/24 03:47:07 cifar100-global-slim-2.0-mobilenetv2]: Epoch 61/100, Acc=0.6808, Val Loss=1.1212, lr=0.0010
[02/24 03:47:40 cifar100-global-slim-2.0-mobilenetv2]: Epoch 62/100, Acc=0.6843, Val Loss=1.1111, lr=0.0010
[02/24 03:48:14 cifar100-global-slim-2.0-mobilenetv2]: Epoch 63/100, Acc=0.6827, Val Loss=1.1110, lr=0.0010
[02/24 03:48:48 cifar100-global-slim-2.0-mobilenetv2]: Epoch 64/100, Acc=0.6832, Val Loss=1.1153, lr=0.0010
[02/24 03:49:21 cifar100-global-slim-2.0-mobilenetv2]: Epoch 65/100, Acc=0.6874, Val Loss=1.1123, lr=0.0010
[02/24 03:50:02 cifar100-global-slim-2.0-mobilenetv2]: Epoch 66/100, Acc=0.6859, Val Loss=1.1147, lr=0.0010
[02/24 03:50:39 cifar100-global-slim-2.0-mobilenetv2]: Epoch 67/100, Acc=0.6888, Val Loss=1.1122, lr=0.0010
[02/24 03:51:13 cifar100-global-slim-2.0-mobilenetv2]: Epoch 68/100, Acc=0.6863, Val Loss=1.1148, lr=0.0010
[02/24 03:51:46 cifar100-global-slim-2.0-mobilenetv2]: Epoch 69/100, Acc=0.6849, Val Loss=1.1181, lr=0.0010
[02/24 03:52:20 cifar100-global-slim-2.0-mobilenetv2]: Epoch 70/100, Acc=0.6889, Val Loss=1.1184, lr=0.0010
[02/24 03:52:53 cifar100-global-slim-2.0-mobilenetv2]: Epoch 71/100, Acc=0.6892, Val Loss=1.1146, lr=0.0010
[02/24 03:53:27 cifar100-global-slim-2.0-mobilenetv2]: Epoch 72/100, Acc=0.6861, Val Loss=1.1200, lr=0.0010
[02/24 03:54:01 cifar100-global-slim-2.0-mobilenetv2]: Epoch 73/100, Acc=0.6831, Val Loss=1.1188, lr=0.0010
[02/24 03:54:36 cifar100-global-slim-2.0-mobilenetv2]: Epoch 74/100, Acc=0.6857, Val Loss=1.1213, lr=0.0010
[02/24 03:55:09 cifar100-global-slim-2.0-mobilenetv2]: Epoch 75/100, Acc=0.6839, Val Loss=1.1278, lr=0.0010
[02/24 03:55:43 cifar100-global-slim-2.0-mobilenetv2]: Epoch 76/100, Acc=0.6851, Val Loss=1.1272, lr=0.0010
[02/24 03:56:16 cifar100-global-slim-2.0-mobilenetv2]: Epoch 77/100, Acc=0.6857, Val Loss=1.1240, lr=0.0010
[02/24 03:56:50 cifar100-global-slim-2.0-mobilenetv2]: Epoch 78/100, Acc=0.6853, Val Loss=1.1311, lr=0.0010
[02/24 03:57:23 cifar100-global-slim-2.0-mobilenetv2]: Epoch 79/100, Acc=0.6861, Val Loss=1.1296, lr=0.0010
[02/24 03:57:56 cifar100-global-slim-2.0-mobilenetv2]: Epoch 80/100, Acc=0.6866, Val Loss=1.1258, lr=0.0001
[02/24 03:58:28 cifar100-global-slim-2.0-mobilenetv2]: Epoch 81/100, Acc=0.6850, Val Loss=1.1245, lr=0.0001
[02/24 03:59:01 cifar100-global-slim-2.0-mobilenetv2]: Epoch 82/100, Acc=0.6867, Val Loss=1.1229, lr=0.0001
[02/24 03:59:33 cifar100-global-slim-2.0-mobilenetv2]: Epoch 83/100, Acc=0.6843, Val Loss=1.1209, lr=0.0001
[02/24 04:00:05 cifar100-global-slim-2.0-mobilenetv2]: Epoch 84/100, Acc=0.6847, Val Loss=1.1250, lr=0.0001
[02/24 04:00:37 cifar100-global-slim-2.0-mobilenetv2]: Epoch 85/100, Acc=0.6849, Val Loss=1.1215, lr=0.0001
[02/24 04:01:10 cifar100-global-slim-2.0-mobilenetv2]: Epoch 86/100, Acc=0.6852, Val Loss=1.1211, lr=0.0001
[02/24 04:01:42 cifar100-global-slim-2.0-mobilenetv2]: Epoch 87/100, Acc=0.6844, Val Loss=1.1210, lr=0.0001
[02/24 04:02:14 cifar100-global-slim-2.0-mobilenetv2]: Epoch 88/100, Acc=0.6855, Val Loss=1.1231, lr=0.0001
[02/24 04:02:47 cifar100-global-slim-2.0-mobilenetv2]: Epoch 89/100, Acc=0.6853, Val Loss=1.1216, lr=0.0001
[02/24 04:03:19 cifar100-global-slim-2.0-mobilenetv2]: Epoch 90/100, Acc=0.6864, Val Loss=1.1225, lr=0.0001
[02/24 04:03:52 cifar100-global-slim-2.0-mobilenetv2]: Epoch 91/100, Acc=0.6867, Val Loss=1.1202, lr=0.0001
[02/24 04:04:25 cifar100-global-slim-2.0-mobilenetv2]: Epoch 92/100, Acc=0.6854, Val Loss=1.1229, lr=0.0001
[02/24 04:04:59 cifar100-global-slim-2.0-mobilenetv2]: Epoch 93/100, Acc=0.6862, Val Loss=1.1228, lr=0.0001
[02/24 04:05:31 cifar100-global-slim-2.0-mobilenetv2]: Epoch 94/100, Acc=0.6842, Val Loss=1.1256, lr=0.0001
[02/24 04:06:03 cifar100-global-slim-2.0-mobilenetv2]: Epoch 95/100, Acc=0.6846, Val Loss=1.1212, lr=0.0001
[02/24 04:06:36 cifar100-global-slim-2.0-mobilenetv2]: Epoch 96/100, Acc=0.6842, Val Loss=1.1257, lr=0.0001
[02/24 04:07:09 cifar100-global-slim-2.0-mobilenetv2]: Epoch 97/100, Acc=0.6852, Val Loss=1.1233, lr=0.0001
[02/24 04:07:42 cifar100-global-slim-2.0-mobilenetv2]: Epoch 98/100, Acc=0.6871, Val Loss=1.1245, lr=0.0001
[02/24 04:08:15 cifar100-global-slim-2.0-mobilenetv2]: Epoch 99/100, Acc=0.6852, Val Loss=1.1243, lr=0.0001
[02/24 04:08:15 cifar100-global-slim-2.0-mobilenetv2]: Best Acc=0.6892
[02/24 04:08:15 cifar100-global-slim-2.0-mobilenetv2]: Params: 1.25 M
[02/24 04:08:15 cifar100-global-slim-2.0-mobilenetv2]: ops: 34.19 M
[02/24 04:08:19 cifar100-global-slim-2.0-mobilenetv2]: Acc: 0.6852 Val Loss: 1.1243

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: mode: prune
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: model: mobilenetv2
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: verbose: False
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: dataset: cifar100
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: dataroot: data
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: batch_size: 128
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: total_epochs: 100
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: lr: 0.01
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: restore: run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: output_dir: run/cifar100/prune/cifar100-global-group_norm-2.0-mobilenetv2
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: finetune: True
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: last_epochs: 100
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: reps: 1
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: method: group_norm
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: speed_up: 2.0
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: reg: 1e-05
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: delta_reg: 0.0001
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: weight_decay: 0.0005
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: seed: 1
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: global_pruning: True
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: sl_total_epochs: 100
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: sl_lr: 0.01
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: sl_reg_warmup: 0
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: sl_restore: None
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: iterative_steps: 400
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: logger: <Logger cifar100-global-group_norm-2.0-mobilenetv2 (DEBUG)>
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: device: cuda
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: num_classes: 100
[02/24 04:08:27 cifar100-global-group_norm-2.0-mobilenetv2]: Loading model from run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/24 04:08:31 cifar100-global-group_norm-2.0-mobilenetv2]: Pruning...
[02/24 04:08:57 cifar100-global-group_norm-2.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 13, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(13, 15, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=15)
      (4): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(15, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 81, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(81, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(81, 81, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=81)
        (4): BatchNorm2d(81, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(81, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 115, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(115, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(115, 115, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=115)
        (4): BatchNorm2d(115, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(115, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 89, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(89, 89, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=89)
        (4): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(89, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 116, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(116, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=116)
        (4): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(116, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 89, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(89, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=89)
        (4): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(89, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 107, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(107, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(107, 107, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=107)
        (4): BatchNorm2d(107, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(107, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 139, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(139, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(139, 139, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=139)
        (4): BatchNorm2d(139, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(139, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 132, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(132, 132, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=132)
        (4): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(132, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 102, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(102, 102, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=102)
        (4): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(102, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 238, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(238, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(238, 238, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=238)
        (4): BatchNorm2d(238, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(238, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 176, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(176, 176, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=176)
        (4): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(176, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 115, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(115, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(115, 115, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=115)
        (4): BatchNorm2d(115, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(115, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 270, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(270, 270, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=270)
        (4): BatchNorm2d(270, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(270, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 376, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(376, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(376, 376, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=376)
        (4): BatchNorm2d(376, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(376, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 418, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(418, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(418, 418, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=418)
        (4): BatchNorm2d(418, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(418, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 180, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=180)
      (4): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(180, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 100, kernel_size=(1, 1), stride=(1, 1))
)
[02/24 04:09:00 cifar100-global-group_norm-2.0-mobilenetv2]: Params: 2.37 M => 1.18 M (49.65%)
[02/24 04:09:00 cifar100-global-group_norm-2.0-mobilenetv2]: FLOPs: 68.40 M => 34.12 M (49.88%, 2.00X )
[02/24 04:09:00 cifar100-global-group_norm-2.0-mobilenetv2]: Acc: 0.6699 => 0.6699
[02/24 04:09:00 cifar100-global-group_norm-2.0-mobilenetv2]: Val Loss: 1.1637 => 1.1637
[02/24 04:09:00 cifar100-global-group_norm-2.0-mobilenetv2]: Finetuning...
[02/24 04:09:32 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 0/100, Acc=0.5982, Val Loss=1.4418, lr=0.0100
[02/24 04:10:05 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 1/100, Acc=0.6009, Val Loss=1.4087, lr=0.0100
[02/24 04:10:37 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 2/100, Acc=0.5928, Val Loss=1.4416, lr=0.0100
[02/24 04:11:10 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 3/100, Acc=0.6001, Val Loss=1.4264, lr=0.0100
[02/24 04:11:42 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 4/100, Acc=0.5962, Val Loss=1.4225, lr=0.0100
[02/24 04:12:14 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 5/100, Acc=0.5900, Val Loss=1.4406, lr=0.0100
[02/24 04:12:47 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 6/100, Acc=0.6093, Val Loss=1.3989, lr=0.0100
[02/24 04:13:20 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 7/100, Acc=0.6094, Val Loss=1.3936, lr=0.0100
[02/24 04:13:53 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 8/100, Acc=0.6084, Val Loss=1.3931, lr=0.0100
[02/24 04:14:25 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 9/100, Acc=0.6065, Val Loss=1.3993, lr=0.0100
[02/24 04:14:57 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 10/100, Acc=0.6051, Val Loss=1.4005, lr=0.0100
[02/24 04:15:29 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 11/100, Acc=0.6234, Val Loss=1.3399, lr=0.0100
[02/24 04:16:00 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 12/100, Acc=0.6037, Val Loss=1.4041, lr=0.0100
[02/24 04:16:31 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 13/100, Acc=0.5989, Val Loss=1.4350, lr=0.0100
[02/24 04:17:02 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 14/100, Acc=0.6079, Val Loss=1.3965, lr=0.0100
[02/24 04:17:33 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 15/100, Acc=0.6130, Val Loss=1.3605, lr=0.0100
[02/24 04:18:05 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 16/100, Acc=0.5955, Val Loss=1.4402, lr=0.0100
[02/24 04:18:37 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 17/100, Acc=0.5990, Val Loss=1.4462, lr=0.0100
[02/24 04:19:09 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 18/100, Acc=0.6022, Val Loss=1.4137, lr=0.0100
[02/24 04:19:40 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 19/100, Acc=0.6036, Val Loss=1.4109, lr=0.0100
[02/24 04:20:12 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 20/100, Acc=0.6095, Val Loss=1.3897, lr=0.0100
[02/24 04:20:44 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 21/100, Acc=0.6035, Val Loss=1.4054, lr=0.0100
[02/24 04:21:15 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 22/100, Acc=0.5836, Val Loss=1.4816, lr=0.0100
[02/24 04:21:46 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 23/100, Acc=0.6121, Val Loss=1.3708, lr=0.0100
[02/24 04:22:17 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 24/100, Acc=0.6093, Val Loss=1.4063, lr=0.0100
[02/24 04:22:48 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 25/100, Acc=0.5952, Val Loss=1.4602, lr=0.0100
[02/24 04:23:19 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 26/100, Acc=0.6094, Val Loss=1.3932, lr=0.0100
[02/24 04:23:51 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 27/100, Acc=0.6013, Val Loss=1.4172, lr=0.0100
[02/24 04:24:22 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 28/100, Acc=0.6145, Val Loss=1.3800, lr=0.0100
[02/24 04:24:54 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 29/100, Acc=0.6130, Val Loss=1.3903, lr=0.0100
[02/24 04:25:25 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 30/100, Acc=0.6097, Val Loss=1.4070, lr=0.0100
[02/24 04:25:57 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 31/100, Acc=0.5732, Val Loss=1.5540, lr=0.0100
[02/24 04:26:28 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 32/100, Acc=0.5849, Val Loss=1.5036, lr=0.0100
[02/24 04:26:59 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 33/100, Acc=0.6020, Val Loss=1.4090, lr=0.0100
[02/24 04:27:30 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 34/100, Acc=0.6306, Val Loss=1.3170, lr=0.0100
[02/24 04:28:02 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 35/100, Acc=0.6195, Val Loss=1.3502, lr=0.0100
[02/24 04:28:35 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 36/100, Acc=0.6065, Val Loss=1.3803, lr=0.0100
[02/24 04:29:07 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 37/100, Acc=0.5983, Val Loss=1.4243, lr=0.0100
[02/24 04:29:39 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 38/100, Acc=0.6117, Val Loss=1.3844, lr=0.0100
[02/24 04:30:11 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 39/100, Acc=0.6044, Val Loss=1.4292, lr=0.0100
[02/24 04:30:44 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 40/100, Acc=0.6048, Val Loss=1.4163, lr=0.0100
[02/24 04:31:16 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 41/100, Acc=0.6183, Val Loss=1.3610, lr=0.0100
[02/24 04:31:48 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 42/100, Acc=0.6132, Val Loss=1.3727, lr=0.0100
[02/24 04:32:20 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 43/100, Acc=0.6140, Val Loss=1.4005, lr=0.0100
[02/24 04:32:52 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 44/100, Acc=0.5826, Val Loss=1.4885, lr=0.0100
[02/24 04:33:25 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 45/100, Acc=0.6070, Val Loss=1.3820, lr=0.0100
[02/24 04:33:59 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 46/100, Acc=0.6137, Val Loss=1.3789, lr=0.0100
[02/24 04:34:33 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 47/100, Acc=0.6183, Val Loss=1.3649, lr=0.0100
[02/24 04:35:06 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 48/100, Acc=0.6073, Val Loss=1.3850, lr=0.0100
[02/24 04:35:39 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 49/100, Acc=0.6065, Val Loss=1.4186, lr=0.0100
[02/24 04:36:11 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 50/100, Acc=0.6152, Val Loss=1.3757, lr=0.0100
[02/24 04:36:44 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 51/100, Acc=0.6165, Val Loss=1.3783, lr=0.0100
[02/24 04:37:17 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 52/100, Acc=0.6171, Val Loss=1.3641, lr=0.0100
[02/24 04:37:56 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 53/100, Acc=0.6085, Val Loss=1.3877, lr=0.0100
[02/24 04:38:34 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 54/100, Acc=0.6018, Val Loss=1.4543, lr=0.0100
[02/24 04:39:13 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 55/100, Acc=0.6146, Val Loss=1.3703, lr=0.0100
[02/24 04:39:52 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 56/100, Acc=0.6185, Val Loss=1.3650, lr=0.0100
[02/24 04:40:30 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 57/100, Acc=0.6125, Val Loss=1.3802, lr=0.0100
[02/24 04:41:04 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 58/100, Acc=0.6057, Val Loss=1.4065, lr=0.0100
[02/24 04:41:37 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 59/100, Acc=0.6013, Val Loss=1.4352, lr=0.0100
[02/24 04:42:10 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 60/100, Acc=0.6754, Val Loss=1.1429, lr=0.0010
[02/24 04:42:43 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 61/100, Acc=0.6772, Val Loss=1.1348, lr=0.0010
[02/24 04:43:17 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 62/100, Acc=0.6834, Val Loss=1.1262, lr=0.0010
[02/24 04:43:51 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 63/100, Acc=0.6820, Val Loss=1.1232, lr=0.0010
[02/24 04:44:24 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 64/100, Acc=0.6797, Val Loss=1.1252, lr=0.0010
[02/24 04:44:57 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 65/100, Acc=0.6814, Val Loss=1.1278, lr=0.0010
[02/24 04:45:30 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 66/100, Acc=0.6814, Val Loss=1.1225, lr=0.0010
[02/24 04:46:03 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 67/100, Acc=0.6792, Val Loss=1.1252, lr=0.0010
[02/24 04:46:36 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 68/100, Acc=0.6788, Val Loss=1.1253, lr=0.0010
[02/24 04:47:09 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 69/100, Acc=0.6789, Val Loss=1.1259, lr=0.0010
[02/24 04:47:43 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 70/100, Acc=0.6799, Val Loss=1.1262, lr=0.0010
[02/24 04:48:16 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 71/100, Acc=0.6823, Val Loss=1.1260, lr=0.0010
[02/24 04:48:50 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 72/100, Acc=0.6795, Val Loss=1.1276, lr=0.0010
[02/24 04:49:25 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 73/100, Acc=0.6785, Val Loss=1.1303, lr=0.0010
[02/24 04:49:59 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 74/100, Acc=0.6810, Val Loss=1.1346, lr=0.0010
[02/24 04:50:33 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 75/100, Acc=0.6799, Val Loss=1.1325, lr=0.0010
[02/24 04:51:06 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 76/100, Acc=0.6801, Val Loss=1.1307, lr=0.0010
[02/24 04:51:40 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 77/100, Acc=0.6788, Val Loss=1.1360, lr=0.0010
[02/24 04:52:13 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 78/100, Acc=0.6820, Val Loss=1.1317, lr=0.0010
[02/24 04:52:48 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 79/100, Acc=0.6777, Val Loss=1.1352, lr=0.0010
[02/24 04:53:22 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 80/100, Acc=0.6808, Val Loss=1.1261, lr=0.0001
[02/24 04:54:03 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 81/100, Acc=0.6812, Val Loss=1.1245, lr=0.0001
[02/24 04:54:44 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 82/100, Acc=0.6834, Val Loss=1.1252, lr=0.0001
[02/24 04:55:25 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 83/100, Acc=0.6827, Val Loss=1.1243, lr=0.0001
[02/24 04:56:06 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 84/100, Acc=0.6829, Val Loss=1.1233, lr=0.0001
[02/24 04:56:41 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 85/100, Acc=0.6839, Val Loss=1.1255, lr=0.0001
[02/24 04:57:21 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 86/100, Acc=0.6811, Val Loss=1.1232, lr=0.0001
[02/24 04:58:00 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 87/100, Acc=0.6827, Val Loss=1.1232, lr=0.0001
[02/24 04:58:39 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 88/100, Acc=0.6835, Val Loss=1.1232, lr=0.0001
[02/24 04:59:18 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 89/100, Acc=0.6837, Val Loss=1.1244, lr=0.0001
[02/24 04:59:57 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 90/100, Acc=0.6841, Val Loss=1.1240, lr=0.0001
[02/24 05:00:37 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 91/100, Acc=0.6815, Val Loss=1.1262, lr=0.0001
[02/24 05:01:09 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 92/100, Acc=0.6833, Val Loss=1.1258, lr=0.0001
[02/24 05:01:40 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 93/100, Acc=0.6816, Val Loss=1.1246, lr=0.0001
[02/24 05:02:11 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 94/100, Acc=0.6847, Val Loss=1.1258, lr=0.0001
[02/24 05:02:42 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 95/100, Acc=0.6811, Val Loss=1.1266, lr=0.0001
[02/24 05:03:14 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 96/100, Acc=0.6816, Val Loss=1.1239, lr=0.0001
[02/24 05:03:45 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 97/100, Acc=0.6833, Val Loss=1.1252, lr=0.0001
[02/24 05:04:17 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 98/100, Acc=0.6832, Val Loss=1.1235, lr=0.0001
[02/24 05:04:49 cifar100-global-group_norm-2.0-mobilenetv2]: Epoch 99/100, Acc=0.6828, Val Loss=1.1260, lr=0.0001
[02/24 05:04:49 cifar100-global-group_norm-2.0-mobilenetv2]: Best Acc=0.6847
[02/24 05:04:49 cifar100-global-group_norm-2.0-mobilenetv2]: Params: 1.18 M
[02/24 05:04:49 cifar100-global-group_norm-2.0-mobilenetv2]: ops: 34.12 M
[02/24 05:04:52 cifar100-global-group_norm-2.0-mobilenetv2]: Acc: 0.6828 Val Loss: 1.1260

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: mode: prune
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: model: mobilenetv2
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: verbose: False
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: dataset: cifar100
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: dataroot: data
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: batch_size: 128
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: total_epochs: 100
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: lr: 0.01
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: restore: run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: output_dir: run/cifar100/prune/cifar100-global-group_sl-2.0-mobilenetv2
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: finetune: True
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: last_epochs: 100
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: reps: 1
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: method: group_sl
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: speed_up: 2.0
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: reg: 1e-05
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: delta_reg: 0.0001
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: weight_decay: 0.0005
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: seed: 1
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: global_pruning: True
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: sl_total_epochs: 100
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: sl_lr: 0.01
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: sl_reg_warmup: 0
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: sl_restore: None
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: iterative_steps: 400
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: logger: <Logger cifar100-global-group_sl-2.0-mobilenetv2 (DEBUG)>
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: device: cuda
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: num_classes: 100
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: Loading model from run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/24 05:05:01 cifar100-global-group_sl-2.0-mobilenetv2]: Regularizing...
[02/24 05:06:13 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 0/100, Acc=0.6018, Val Loss=1.4246, lr=0.0100
[02/24 05:07:27 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 1/100, Acc=0.5941, Val Loss=1.4640, lr=0.0100
[02/24 05:08:40 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 2/100, Acc=0.5965, Val Loss=1.4465, lr=0.0100
[02/24 05:09:53 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 3/100, Acc=0.6047, Val Loss=1.4183, lr=0.0100
[02/24 05:11:05 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 4/100, Acc=0.6160, Val Loss=1.3820, lr=0.0100
[02/24 05:12:23 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 5/100, Acc=0.6078, Val Loss=1.4133, lr=0.0100
[02/24 05:13:35 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 6/100, Acc=0.6116, Val Loss=1.3955, lr=0.0100
[02/24 05:14:48 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 7/100, Acc=0.6133, Val Loss=1.3699, lr=0.0100
[02/24 05:16:02 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 8/100, Acc=0.6202, Val Loss=1.3452, lr=0.0100
[02/24 05:17:15 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 9/100, Acc=0.6157, Val Loss=1.3766, lr=0.0100
[02/24 05:18:30 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 10/100, Acc=0.6237, Val Loss=1.3495, lr=0.0100
[02/24 05:19:45 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 11/100, Acc=0.6140, Val Loss=1.3633, lr=0.0100
[02/24 05:20:59 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 12/100, Acc=0.6255, Val Loss=1.3328, lr=0.0100
[02/24 05:22:13 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 13/100, Acc=0.6241, Val Loss=1.3568, lr=0.0100
[02/24 05:23:27 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 14/100, Acc=0.6111, Val Loss=1.4258, lr=0.0100
[02/24 05:24:41 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 15/100, Acc=0.6243, Val Loss=1.3727, lr=0.0100
[02/24 05:25:55 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 16/100, Acc=0.6197, Val Loss=1.3895, lr=0.0100
[02/24 05:27:10 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 17/100, Acc=0.6224, Val Loss=1.3443, lr=0.0100
[02/24 05:28:23 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 18/100, Acc=0.6223, Val Loss=1.3870, lr=0.0100
[02/24 05:29:37 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 19/100, Acc=0.6330, Val Loss=1.3372, lr=0.0100
[02/24 05:30:50 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 20/100, Acc=0.6339, Val Loss=1.3245, lr=0.0100
[02/24 05:32:16 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 21/100, Acc=0.6390, Val Loss=1.3026, lr=0.0100
[02/24 05:33:34 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 22/100, Acc=0.6272, Val Loss=1.3563, lr=0.0100
[02/24 05:34:48 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 23/100, Acc=0.6296, Val Loss=1.3238, lr=0.0100
[02/24 05:36:01 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 24/100, Acc=0.6337, Val Loss=1.3292, lr=0.0100
[02/24 05:37:15 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 25/100, Acc=0.6418, Val Loss=1.3047, lr=0.0100
[02/24 05:38:31 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 26/100, Acc=0.6320, Val Loss=1.3352, lr=0.0100
[02/24 05:39:50 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 27/100, Acc=0.6411, Val Loss=1.3072, lr=0.0100
[02/24 05:41:09 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 28/100, Acc=0.6386, Val Loss=1.3123, lr=0.0100
[02/24 05:42:25 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 29/100, Acc=0.6358, Val Loss=1.3343, lr=0.0100
[02/24 05:43:41 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 30/100, Acc=0.6344, Val Loss=1.3336, lr=0.0100
[02/24 05:44:57 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 31/100, Acc=0.6369, Val Loss=1.3090, lr=0.0100
[02/24 05:46:12 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 32/100, Acc=0.6380, Val Loss=1.3088, lr=0.0100
[02/24 05:47:27 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 33/100, Acc=0.6460, Val Loss=1.3028, lr=0.0100
[02/24 05:48:44 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 34/100, Acc=0.6364, Val Loss=1.3400, lr=0.0100
[02/24 05:50:00 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 35/100, Acc=0.6309, Val Loss=1.3514, lr=0.0100
[02/24 05:51:15 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 36/100, Acc=0.6463, Val Loss=1.2867, lr=0.0100
[02/24 05:52:32 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 37/100, Acc=0.6345, Val Loss=1.3411, lr=0.0100
[02/24 05:53:49 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 38/100, Acc=0.6479, Val Loss=1.2827, lr=0.0100
[02/24 05:55:12 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 39/100, Acc=0.6420, Val Loss=1.3075, lr=0.0100
[02/24 05:56:27 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 40/100, Acc=0.6386, Val Loss=1.3169, lr=0.0100
[02/24 05:57:41 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 41/100, Acc=0.6438, Val Loss=1.3182, lr=0.0100
[02/24 05:58:56 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 42/100, Acc=0.6403, Val Loss=1.3297, lr=0.0100
[02/24 06:00:13 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 43/100, Acc=0.6407, Val Loss=1.3139, lr=0.0100
[02/24 06:01:30 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 44/100, Acc=0.6488, Val Loss=1.3117, lr=0.0100
[02/24 06:02:46 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 45/100, Acc=0.6490, Val Loss=1.3090, lr=0.0100
[02/24 06:04:03 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 46/100, Acc=0.6387, Val Loss=1.3444, lr=0.0100
[02/24 06:05:21 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 47/100, Acc=0.6449, Val Loss=1.3185, lr=0.0100
[02/24 06:06:51 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 48/100, Acc=0.6502, Val Loss=1.3077, lr=0.0100
[02/24 06:08:13 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 49/100, Acc=0.6559, Val Loss=1.3099, lr=0.0100
[02/24 06:09:31 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 50/100, Acc=0.6464, Val Loss=1.3287, lr=0.0100
[02/24 06:10:50 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 51/100, Acc=0.6477, Val Loss=1.3060, lr=0.0100
[02/24 06:12:09 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 52/100, Acc=0.6401, Val Loss=1.3528, lr=0.0100
[02/24 06:13:27 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 53/100, Acc=0.6510, Val Loss=1.2970, lr=0.0100
[02/24 06:14:45 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 54/100, Acc=0.6493, Val Loss=1.3123, lr=0.0100
[02/24 06:16:01 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 55/100, Acc=0.6544, Val Loss=1.3028, lr=0.0100
[02/24 06:17:19 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 56/100, Acc=0.6537, Val Loss=1.2816, lr=0.0100
[02/24 06:18:37 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 57/100, Acc=0.6465, Val Loss=1.3338, lr=0.0100
[02/24 06:19:54 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 58/100, Acc=0.6479, Val Loss=1.3303, lr=0.0100
[02/24 06:21:17 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 59/100, Acc=0.6407, Val Loss=1.3597, lr=0.0100
[02/24 06:22:46 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 60/100, Acc=0.6797, Val Loss=1.2056, lr=0.0010
[02/24 06:24:04 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 61/100, Acc=0.6803, Val Loss=1.1985, lr=0.0010
[02/24 06:25:22 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 62/100, Acc=0.6804, Val Loss=1.2004, lr=0.0010
[02/24 06:26:39 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 63/100, Acc=0.6792, Val Loss=1.2000, lr=0.0010
[02/24 06:27:57 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 64/100, Acc=0.6806, Val Loss=1.2056, lr=0.0010
[02/24 06:29:15 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 65/100, Acc=0.6799, Val Loss=1.1996, lr=0.0010
[02/24 06:30:32 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 66/100, Acc=0.6834, Val Loss=1.1975, lr=0.0010
[02/24 06:31:48 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 67/100, Acc=0.6799, Val Loss=1.2029, lr=0.0010
[02/24 06:33:02 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 68/100, Acc=0.6801, Val Loss=1.2033, lr=0.0010
[02/24 06:34:18 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 69/100, Acc=0.6798, Val Loss=1.2052, lr=0.0010
[02/24 06:35:34 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 70/100, Acc=0.6810, Val Loss=1.2094, lr=0.0010
[02/24 06:36:51 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 71/100, Acc=0.6794, Val Loss=1.2088, lr=0.0010
[02/24 06:38:06 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 72/100, Acc=0.6810, Val Loss=1.2111, lr=0.0010
[02/24 06:39:22 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 73/100, Acc=0.6797, Val Loss=1.2163, lr=0.0010
[02/24 06:40:37 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 74/100, Acc=0.6813, Val Loss=1.2134, lr=0.0010
[02/24 06:41:54 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 75/100, Acc=0.6810, Val Loss=1.2087, lr=0.0010
[02/24 06:43:12 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 76/100, Acc=0.6775, Val Loss=1.2180, lr=0.0010
[02/24 06:44:32 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 77/100, Acc=0.6798, Val Loss=1.2161, lr=0.0010
[02/24 06:45:51 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 78/100, Acc=0.6780, Val Loss=1.2228, lr=0.0010
[02/24 06:47:10 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 79/100, Acc=0.6798, Val Loss=1.2259, lr=0.0010
[02/24 06:48:27 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 80/100, Acc=0.6798, Val Loss=1.2222, lr=0.0001
[02/24 06:49:44 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 81/100, Acc=0.6799, Val Loss=1.2223, lr=0.0001
[02/24 06:50:58 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 82/100, Acc=0.6777, Val Loss=1.2224, lr=0.0001
[02/24 06:52:13 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 83/100, Acc=0.6791, Val Loss=1.2173, lr=0.0001
[02/24 06:53:29 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 84/100, Acc=0.6795, Val Loss=1.2193, lr=0.0001
[02/24 06:54:45 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 85/100, Acc=0.6813, Val Loss=1.2200, lr=0.0001
[02/24 06:56:02 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 86/100, Acc=0.6812, Val Loss=1.2202, lr=0.0001
[02/24 06:57:19 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 87/100, Acc=0.6786, Val Loss=1.2176, lr=0.0001
[02/24 06:58:37 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 88/100, Acc=0.6806, Val Loss=1.2189, lr=0.0001
[02/24 07:00:01 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 89/100, Acc=0.6810, Val Loss=1.2222, lr=0.0001
[02/24 07:01:29 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 90/100, Acc=0.6801, Val Loss=1.2226, lr=0.0001
[02/24 07:02:54 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 91/100, Acc=0.6790, Val Loss=1.2211, lr=0.0001
[02/24 07:04:13 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 92/100, Acc=0.6788, Val Loss=1.2213, lr=0.0001
[02/24 07:05:32 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 93/100, Acc=0.6791, Val Loss=1.2172, lr=0.0001
[02/24 07:06:49 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 94/100, Acc=0.6803, Val Loss=1.2203, lr=0.0001
[02/24 07:08:07 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 95/100, Acc=0.6783, Val Loss=1.2242, lr=0.0001
[02/24 07:09:25 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 96/100, Acc=0.6784, Val Loss=1.2204, lr=0.0001
[02/24 07:10:42 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 97/100, Acc=0.6799, Val Loss=1.2177, lr=0.0001
[02/24 07:11:59 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 98/100, Acc=0.6798, Val Loss=1.2185, lr=0.0001
[02/24 07:13:17 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 99/100, Acc=0.6776, Val Loss=1.2243, lr=0.0001
[02/24 07:13:17 cifar100-global-group_sl-2.0-mobilenetv2]: Best Acc=0.6834
[02/24 07:13:17 cifar100-global-group_sl-2.0-mobilenetv2]: Loading the sparse model from run/cifar100/prune/cifar100-global-group_sl-2.0-mobilenetv2/reg_cifar100_mobilenetv2_group_sl_1e-05.pth...
[02/24 07:13:21 cifar100-global-group_sl-2.0-mobilenetv2]: Pruning...
[02/24 07:13:52 cifar100-global-group_sl-2.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 13, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(13, 15, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=15)
      (4): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(15, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 81, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(81, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(81, 81, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=81)
        (4): BatchNorm2d(81, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(81, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 116, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(116, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=116)
        (4): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(116, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 92, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(92, 92, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=92)
        (4): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(92, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 119, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(119, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(119, 119, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=119)
        (4): BatchNorm2d(119, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(119, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 93, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(93, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(93, 93, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=93)
        (4): BatchNorm2d(93, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(93, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 112, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(112, 112, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=112)
        (4): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(112, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 138, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(138, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(138, 138, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=138)
        (4): BatchNorm2d(138, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(138, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 119, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(119, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(119, 119, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=119)
        (4): BatchNorm2d(119, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(119, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 92, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(92, 92, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=92)
        (4): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(92, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 239, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(239, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(239, 239, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=239)
        (4): BatchNorm2d(239, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(239, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 170, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(170, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(170, 170, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=170)
        (4): BatchNorm2d(170, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(170, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 112, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(112, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=112)
        (4): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(112, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 284, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(284, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(284, 284, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=284)
        (4): BatchNorm2d(284, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(284, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 366, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(366, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(366, 366, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=366)
        (4): BatchNorm2d(366, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(366, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 389, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(389, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(389, 389, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=389)
        (4): BatchNorm2d(389, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(389, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 198, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(198, 198, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=198)
      (4): BatchNorm2d(198, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(198, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 100, kernel_size=(1, 1), stride=(1, 1))
)
[02/24 07:13:56 cifar100-global-group_sl-2.0-mobilenetv2]: Params: 2.37 M => 1.17 M (49.48%)
[02/24 07:13:56 cifar100-global-group_sl-2.0-mobilenetv2]: FLOPs: 68.40 M => 34.11 M (49.86%, 2.01X )
[02/24 07:13:56 cifar100-global-group_sl-2.0-mobilenetv2]: Acc: 0.6834 => 0.6834
[02/24 07:13:56 cifar100-global-group_sl-2.0-mobilenetv2]: Val Loss: 1.1975 => 1.1975
[02/24 07:13:56 cifar100-global-group_sl-2.0-mobilenetv2]: Finetuning...
[02/24 07:14:31 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 0/100, Acc=0.6481, Val Loss=1.3174, lr=0.0100
[02/24 07:15:13 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 1/100, Acc=0.6474, Val Loss=1.3156, lr=0.0100
[02/24 07:15:53 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 2/100, Acc=0.6511, Val Loss=1.2812, lr=0.0100
[02/24 07:16:27 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 3/100, Acc=0.6543, Val Loss=1.2747, lr=0.0100
[02/24 07:17:08 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 4/100, Acc=0.6494, Val Loss=1.2766, lr=0.0100
[02/24 07:17:48 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 5/100, Acc=0.6541, Val Loss=1.2684, lr=0.0100
[02/24 07:18:30 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 6/100, Acc=0.6399, Val Loss=1.3064, lr=0.0100
[02/24 07:19:12 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 7/100, Acc=0.6480, Val Loss=1.2747, lr=0.0100
[02/24 07:19:53 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 8/100, Acc=0.6396, Val Loss=1.2871, lr=0.0100
[02/24 07:20:35 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 9/100, Acc=0.6495, Val Loss=1.3042, lr=0.0100
[02/24 07:21:16 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 10/100, Acc=0.6421, Val Loss=1.2811, lr=0.0100
[02/24 07:21:57 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 11/100, Acc=0.6402, Val Loss=1.3291, lr=0.0100
[02/24 07:22:39 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 12/100, Acc=0.6367, Val Loss=1.3304, lr=0.0100
[02/24 07:23:20 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 13/100, Acc=0.6392, Val Loss=1.2995, lr=0.0100
[02/24 07:24:01 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 14/100, Acc=0.6447, Val Loss=1.2889, lr=0.0100
[02/24 07:24:41 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 15/100, Acc=0.6456, Val Loss=1.2820, lr=0.0100
[02/24 07:25:22 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 16/100, Acc=0.6394, Val Loss=1.3017, lr=0.0100
[02/24 07:26:03 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 17/100, Acc=0.6383, Val Loss=1.3085, lr=0.0100
[02/24 07:26:43 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 18/100, Acc=0.6318, Val Loss=1.3318, lr=0.0100
[02/24 07:27:23 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 19/100, Acc=0.6353, Val Loss=1.3177, lr=0.0100
[02/24 07:28:03 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 20/100, Acc=0.6312, Val Loss=1.3133, lr=0.0100
[02/24 07:28:43 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 21/100, Acc=0.6413, Val Loss=1.2817, lr=0.0100
[02/24 07:29:22 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 22/100, Acc=0.6384, Val Loss=1.3059, lr=0.0100
[02/24 07:29:57 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 23/100, Acc=0.6219, Val Loss=1.3781, lr=0.0100
[02/24 07:30:32 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 24/100, Acc=0.6377, Val Loss=1.3019, lr=0.0100
[02/24 07:31:07 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 25/100, Acc=0.6351, Val Loss=1.3224, lr=0.0100
[02/24 07:31:42 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 26/100, Acc=0.6322, Val Loss=1.3323, lr=0.0100
[02/24 07:32:19 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 27/100, Acc=0.6323, Val Loss=1.3247, lr=0.0100
[02/24 07:33:00 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 28/100, Acc=0.6388, Val Loss=1.2893, lr=0.0100
[02/24 07:33:40 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 29/100, Acc=0.6343, Val Loss=1.3257, lr=0.0100
[02/24 07:34:20 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 30/100, Acc=0.6247, Val Loss=1.3381, lr=0.0100
[02/24 07:35:01 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 31/100, Acc=0.6357, Val Loss=1.3176, lr=0.0100
[02/24 07:35:42 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 32/100, Acc=0.6326, Val Loss=1.3418, lr=0.0100
[02/24 07:36:23 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 33/100, Acc=0.6274, Val Loss=1.3329, lr=0.0100
[02/24 07:37:04 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 34/100, Acc=0.6237, Val Loss=1.3410, lr=0.0100
[02/24 07:37:45 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 35/100, Acc=0.6257, Val Loss=1.3308, lr=0.0100
[02/24 07:38:25 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 36/100, Acc=0.6123, Val Loss=1.3757, lr=0.0100
[02/24 07:39:06 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 37/100, Acc=0.6339, Val Loss=1.3148, lr=0.0100
[02/24 07:39:48 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 38/100, Acc=0.6297, Val Loss=1.3298, lr=0.0100
[02/24 07:40:30 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 39/100, Acc=0.6298, Val Loss=1.3156, lr=0.0100
[02/24 07:41:05 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 40/100, Acc=0.6077, Val Loss=1.4580, lr=0.0100
[02/24 07:41:40 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 41/100, Acc=0.6318, Val Loss=1.3405, lr=0.0100
[02/24 07:42:16 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 42/100, Acc=0.6200, Val Loss=1.3839, lr=0.0100
[02/24 07:42:51 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 43/100, Acc=0.6150, Val Loss=1.3958, lr=0.0100
[02/24 07:43:27 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 44/100, Acc=0.6206, Val Loss=1.3543, lr=0.0100
[02/24 07:44:03 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 45/100, Acc=0.6215, Val Loss=1.3843, lr=0.0100
[02/24 07:44:39 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 46/100, Acc=0.6306, Val Loss=1.2996, lr=0.0100
[02/24 07:45:15 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 47/100, Acc=0.6244, Val Loss=1.3356, lr=0.0100
[02/24 07:45:51 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 48/100, Acc=0.6049, Val Loss=1.4374, lr=0.0100
[02/24 07:46:27 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 49/100, Acc=0.6158, Val Loss=1.3768, lr=0.0100
[02/24 07:47:04 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 50/100, Acc=0.6049, Val Loss=1.4109, lr=0.0100
[02/24 07:47:41 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 51/100, Acc=0.6139, Val Loss=1.3688, lr=0.0100
[02/24 07:48:18 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 52/100, Acc=0.6217, Val Loss=1.3628, lr=0.0100
[02/24 07:48:54 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 53/100, Acc=0.6373, Val Loss=1.3060, lr=0.0100
[02/24 07:49:30 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 54/100, Acc=0.6255, Val Loss=1.3430, lr=0.0100
[02/24 07:50:06 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 55/100, Acc=0.6225, Val Loss=1.3470, lr=0.0100
[02/24 07:50:42 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 56/100, Acc=0.6056, Val Loss=1.4236, lr=0.0100
[02/24 07:51:18 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 57/100, Acc=0.6332, Val Loss=1.3118, lr=0.0100
[02/24 07:51:55 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 58/100, Acc=0.6261, Val Loss=1.3412, lr=0.0100
[02/24 07:52:31 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 59/100, Acc=0.6241, Val Loss=1.3420, lr=0.0100
[02/24 07:53:07 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 60/100, Acc=0.6757, Val Loss=1.1190, lr=0.0010
[02/24 07:53:44 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 61/100, Acc=0.6782, Val Loss=1.1121, lr=0.0010
[02/24 07:54:20 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 62/100, Acc=0.6817, Val Loss=1.1057, lr=0.0010
[02/24 07:54:55 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 63/100, Acc=0.6827, Val Loss=1.1100, lr=0.0010
[02/24 07:55:30 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 64/100, Acc=0.6857, Val Loss=1.1093, lr=0.0010
[02/24 07:56:05 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 65/100, Acc=0.6833, Val Loss=1.1088, lr=0.0010
[02/24 07:56:40 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 66/100, Acc=0.6852, Val Loss=1.1075, lr=0.0010
[02/24 07:57:15 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 67/100, Acc=0.6858, Val Loss=1.1140, lr=0.0010
[02/24 07:57:56 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 68/100, Acc=0.6861, Val Loss=1.1118, lr=0.0010
[02/24 07:58:30 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 69/100, Acc=0.6873, Val Loss=1.1110, lr=0.0010
[02/24 07:59:05 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 70/100, Acc=0.6860, Val Loss=1.1144, lr=0.0010
[02/24 07:59:39 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 71/100, Acc=0.6861, Val Loss=1.1136, lr=0.0010
[02/24 08:00:14 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 72/100, Acc=0.6854, Val Loss=1.1183, lr=0.0010
[02/24 08:00:49 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 73/100, Acc=0.6853, Val Loss=1.1167, lr=0.0010
[02/24 08:01:25 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 74/100, Acc=0.6842, Val Loss=1.1211, lr=0.0010
[02/24 08:02:00 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 75/100, Acc=0.6846, Val Loss=1.1247, lr=0.0010
[02/24 08:02:34 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 76/100, Acc=0.6850, Val Loss=1.1252, lr=0.0010
[02/24 08:03:08 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 77/100, Acc=0.6852, Val Loss=1.1287, lr=0.0010
[02/24 08:03:42 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 78/100, Acc=0.6839, Val Loss=1.1318, lr=0.0010
[02/24 08:04:16 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 79/100, Acc=0.6819, Val Loss=1.1297, lr=0.0010
[02/24 08:04:50 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 80/100, Acc=0.6831, Val Loss=1.1251, lr=0.0001
[02/24 08:05:24 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 81/100, Acc=0.6830, Val Loss=1.1237, lr=0.0001
[02/24 08:05:58 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 82/100, Acc=0.6839, Val Loss=1.1219, lr=0.0001
[02/24 08:06:32 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 83/100, Acc=0.6852, Val Loss=1.1201, lr=0.0001
[02/24 08:07:06 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 84/100, Acc=0.6851, Val Loss=1.1244, lr=0.0001
[02/24 08:07:40 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 85/100, Acc=0.6840, Val Loss=1.1220, lr=0.0001
[02/24 08:08:14 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 86/100, Acc=0.6854, Val Loss=1.1193, lr=0.0001
[02/24 08:08:48 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 87/100, Acc=0.6853, Val Loss=1.1198, lr=0.0001
[02/24 08:09:22 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 88/100, Acc=0.6839, Val Loss=1.1204, lr=0.0001
[02/24 08:09:57 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 89/100, Acc=0.6851, Val Loss=1.1206, lr=0.0001
[02/24 08:10:31 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 90/100, Acc=0.6870, Val Loss=1.1212, lr=0.0001
[02/24 08:11:05 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 91/100, Acc=0.6853, Val Loss=1.1189, lr=0.0001
[02/24 08:11:39 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 92/100, Acc=0.6858, Val Loss=1.1216, lr=0.0001
[02/24 08:12:14 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 93/100, Acc=0.6862, Val Loss=1.1214, lr=0.0001
[02/24 08:12:48 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 94/100, Acc=0.6856, Val Loss=1.1240, lr=0.0001
[02/24 08:13:22 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 95/100, Acc=0.6860, Val Loss=1.1197, lr=0.0001
[02/24 08:13:56 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 96/100, Acc=0.6869, Val Loss=1.1216, lr=0.0001
[02/24 08:14:29 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 97/100, Acc=0.6852, Val Loss=1.1223, lr=0.0001
[02/24 08:15:03 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 98/100, Acc=0.6838, Val Loss=1.1220, lr=0.0001
[02/24 08:15:36 cifar100-global-group_sl-2.0-mobilenetv2]: Epoch 99/100, Acc=0.6833, Val Loss=1.1229, lr=0.0001
[02/24 08:15:36 cifar100-global-group_sl-2.0-mobilenetv2]: Best Acc=0.6873
[02/24 08:15:36 cifar100-global-group_sl-2.0-mobilenetv2]: Params: 1.17 M
[02/24 08:15:36 cifar100-global-group_sl-2.0-mobilenetv2]: ops: 34.11 M
[02/24 08:15:40 cifar100-global-group_sl-2.0-mobilenetv2]: Acc: 0.6833 Val Loss: 1.1229

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: mode: prune
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: model: mobilenetv2
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: verbose: False
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: dataset: cifar100
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: dataroot: data
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: batch_size: 128
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: total_epochs: 100
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: lr: 0.01
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: restore: run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: output_dir: run/cifar100/prune/cifar100-global-proj-2.0-mobilenetv2
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: finetune: True
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: last_epochs: 100
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: reps: 1
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: method: proj
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: speed_up: 2.0
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: reg: 1e-05
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: delta_reg: 0.0001
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: weight_decay: 0.0005
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: seed: 1
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: global_pruning: True
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: sl_total_epochs: 100
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: sl_lr: 0.01
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: sl_reg_warmup: 0
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: sl_restore: None
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: iterative_steps: 400
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: logger: <Logger cifar100-global-proj-2.0-mobilenetv2 (DEBUG)>
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: device: cuda
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: num_classes: 100
[02/24 08:15:48 cifar100-global-proj-2.0-mobilenetv2]: Loading model from run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/24 08:15:52 cifar100-global-proj-2.0-mobilenetv2]: Pruning...
[02/24 08:28:31 cifar100-global-proj-2.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 13, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(13, 15, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=15)
      (4): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(15, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 78, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(78, 78, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=78)
        (4): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(78, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 107, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(107, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(107, 107, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=107)
        (4): BatchNorm2d(107, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(107, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 87, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(87, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(87, 87, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=87)
        (4): BatchNorm2d(87, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(87, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 107, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(107, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(107, 107, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=107)
        (4): BatchNorm2d(107, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(107, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 85, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(85, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(85, 85, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=85)
        (4): BatchNorm2d(85, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(85, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 106, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(106, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(106, 106, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=106)
        (4): BatchNorm2d(106, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(106, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 146, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(146, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(146, 146, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=146)
        (4): BatchNorm2d(146, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(146, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 153, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(153, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(153, 153, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=153)
        (4): BatchNorm2d(153, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(153, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 114, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(114, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(114, 114, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=114)
        (4): BatchNorm2d(114, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(114, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 229, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(229, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(229, 229, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=229)
        (4): BatchNorm2d(229, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(229, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 190, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(190, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(190, 190, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=190)
        (4): BatchNorm2d(190, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(190, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 123, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(123, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(123, 123, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=123)
        (4): BatchNorm2d(123, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(123, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 260, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(260, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(260, 260, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=260)
        (4): BatchNorm2d(260, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(260, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 383, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(383, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(383, 383, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=383)
        (4): BatchNorm2d(383, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(383, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 448, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
        (4): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(448, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 173, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(173, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(173, 173, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=173)
      (4): BatchNorm2d(173, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(173, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 100, kernel_size=(1, 1), stride=(1, 1))
)
[02/24 08:28:35 cifar100-global-proj-2.0-mobilenetv2]: Params: 2.37 M => 1.19 M (50.20%)
[02/24 08:28:35 cifar100-global-proj-2.0-mobilenetv2]: FLOPs: 68.40 M => 34.13 M (49.90%, 2.00X )
[02/24 08:28:35 cifar100-global-proj-2.0-mobilenetv2]: Acc: 0.6699 => 0.6699
[02/24 08:28:35 cifar100-global-proj-2.0-mobilenetv2]: Val Loss: 1.1637 => 1.1637
[02/24 08:28:35 cifar100-global-proj-2.0-mobilenetv2]: Finetuning...
[02/24 08:29:09 cifar100-global-proj-2.0-mobilenetv2]: Epoch 0/100, Acc=0.5937, Val Loss=1.4502, lr=0.0100
[02/24 08:29:49 cifar100-global-proj-2.0-mobilenetv2]: Epoch 1/100, Acc=0.6096, Val Loss=1.3907, lr=0.0100
[02/24 08:30:23 cifar100-global-proj-2.0-mobilenetv2]: Epoch 2/100, Acc=0.5916, Val Loss=1.4606, lr=0.0100
[02/24 08:30:57 cifar100-global-proj-2.0-mobilenetv2]: Epoch 3/100, Acc=0.5914, Val Loss=1.4481, lr=0.0100
[02/24 08:31:37 cifar100-global-proj-2.0-mobilenetv2]: Epoch 4/100, Acc=0.6039, Val Loss=1.4085, lr=0.0100
[02/24 08:32:18 cifar100-global-proj-2.0-mobilenetv2]: Epoch 5/100, Acc=0.5829, Val Loss=1.4923, lr=0.0100
[02/24 08:32:59 cifar100-global-proj-2.0-mobilenetv2]: Epoch 6/100, Acc=0.5877, Val Loss=1.4834, lr=0.0100
[02/24 08:33:39 cifar100-global-proj-2.0-mobilenetv2]: Epoch 7/100, Acc=0.6045, Val Loss=1.3850, lr=0.0100
[02/24 08:34:21 cifar100-global-proj-2.0-mobilenetv2]: Epoch 8/100, Acc=0.6075, Val Loss=1.3873, lr=0.0100
[02/24 08:34:57 cifar100-global-proj-2.0-mobilenetv2]: Epoch 9/100, Acc=0.6114, Val Loss=1.3773, lr=0.0100
[02/24 08:35:32 cifar100-global-proj-2.0-mobilenetv2]: Epoch 10/100, Acc=0.6011, Val Loss=1.3910, lr=0.0100
[02/24 08:36:08 cifar100-global-proj-2.0-mobilenetv2]: Epoch 11/100, Acc=0.6142, Val Loss=1.3853, lr=0.0100
[02/24 08:36:44 cifar100-global-proj-2.0-mobilenetv2]: Epoch 12/100, Acc=0.5945, Val Loss=1.4456, lr=0.0100
[02/24 08:37:20 cifar100-global-proj-2.0-mobilenetv2]: Epoch 13/100, Acc=0.5977, Val Loss=1.4478, lr=0.0100
[02/24 08:37:56 cifar100-global-proj-2.0-mobilenetv2]: Epoch 14/100, Acc=0.5876, Val Loss=1.4821, lr=0.0100
[02/24 08:38:31 cifar100-global-proj-2.0-mobilenetv2]: Epoch 15/100, Acc=0.6088, Val Loss=1.3916, lr=0.0100
[02/24 08:39:08 cifar100-global-proj-2.0-mobilenetv2]: Epoch 16/100, Acc=0.6126, Val Loss=1.3854, lr=0.0100
[02/24 08:39:46 cifar100-global-proj-2.0-mobilenetv2]: Epoch 17/100, Acc=0.6061, Val Loss=1.3988, lr=0.0100
[02/24 08:40:30 cifar100-global-proj-2.0-mobilenetv2]: Epoch 18/100, Acc=0.6223, Val Loss=1.3338, lr=0.0100
[02/24 08:41:07 cifar100-global-proj-2.0-mobilenetv2]: Epoch 19/100, Acc=0.6014, Val Loss=1.4138, lr=0.0100
[02/24 08:41:44 cifar100-global-proj-2.0-mobilenetv2]: Epoch 20/100, Acc=0.6002, Val Loss=1.4356, lr=0.0100
[02/24 08:42:20 cifar100-global-proj-2.0-mobilenetv2]: Epoch 21/100, Acc=0.5985, Val Loss=1.4276, lr=0.0100
[02/24 08:42:56 cifar100-global-proj-2.0-mobilenetv2]: Epoch 22/100, Acc=0.5987, Val Loss=1.4390, lr=0.0100
[02/24 08:43:33 cifar100-global-proj-2.0-mobilenetv2]: Epoch 23/100, Acc=0.6044, Val Loss=1.4061, lr=0.0100
[02/24 08:44:10 cifar100-global-proj-2.0-mobilenetv2]: Epoch 24/100, Acc=0.6153, Val Loss=1.3796, lr=0.0100
[02/24 08:44:45 cifar100-global-proj-2.0-mobilenetv2]: Epoch 25/100, Acc=0.6136, Val Loss=1.3724, lr=0.0100
[02/24 08:45:20 cifar100-global-proj-2.0-mobilenetv2]: Epoch 26/100, Acc=0.6050, Val Loss=1.4090, lr=0.0100
[02/24 08:45:55 cifar100-global-proj-2.0-mobilenetv2]: Epoch 27/100, Acc=0.5938, Val Loss=1.4590, lr=0.0100
[02/24 08:46:30 cifar100-global-proj-2.0-mobilenetv2]: Epoch 28/100, Acc=0.6246, Val Loss=1.3351, lr=0.0100
[02/24 08:47:13 cifar100-global-proj-2.0-mobilenetv2]: Epoch 29/100, Acc=0.6083, Val Loss=1.3870, lr=0.0100
[02/24 08:47:56 cifar100-global-proj-2.0-mobilenetv2]: Epoch 30/100, Acc=0.5881, Val Loss=1.4992, lr=0.0100
[02/24 08:48:38 cifar100-global-proj-2.0-mobilenetv2]: Epoch 31/100, Acc=0.6081, Val Loss=1.3961, lr=0.0100
[02/24 08:49:21 cifar100-global-proj-2.0-mobilenetv2]: Epoch 32/100, Acc=0.6136, Val Loss=1.3865, lr=0.0100
[02/24 08:50:04 cifar100-global-proj-2.0-mobilenetv2]: Epoch 33/100, Acc=0.5872, Val Loss=1.4769, lr=0.0100
[02/24 08:50:48 cifar100-global-proj-2.0-mobilenetv2]: Epoch 34/100, Acc=0.6188, Val Loss=1.3598, lr=0.0100
[02/24 08:51:31 cifar100-global-proj-2.0-mobilenetv2]: Epoch 35/100, Acc=0.6220, Val Loss=1.3481, lr=0.0100
[02/24 08:52:13 cifar100-global-proj-2.0-mobilenetv2]: Epoch 36/100, Acc=0.6050, Val Loss=1.4045, lr=0.0100
[02/24 08:52:58 cifar100-global-proj-2.0-mobilenetv2]: Epoch 37/100, Acc=0.6056, Val Loss=1.3900, lr=0.0100
[02/24 08:53:42 cifar100-global-proj-2.0-mobilenetv2]: Epoch 38/100, Acc=0.6204, Val Loss=1.3443, lr=0.0100
[02/24 08:54:25 cifar100-global-proj-2.0-mobilenetv2]: Epoch 39/100, Acc=0.6003, Val Loss=1.4403, lr=0.0100
[02/24 08:55:08 cifar100-global-proj-2.0-mobilenetv2]: Epoch 40/100, Acc=0.6099, Val Loss=1.3986, lr=0.0100
[02/24 08:55:45 cifar100-global-proj-2.0-mobilenetv2]: Epoch 41/100, Acc=0.6182, Val Loss=1.3603, lr=0.0100
[02/24 08:56:21 cifar100-global-proj-2.0-mobilenetv2]: Epoch 42/100, Acc=0.5953, Val Loss=1.4723, lr=0.0100
[02/24 08:56:57 cifar100-global-proj-2.0-mobilenetv2]: Epoch 43/100, Acc=0.6202, Val Loss=1.3648, lr=0.0100
[02/24 08:57:32 cifar100-global-proj-2.0-mobilenetv2]: Epoch 44/100, Acc=0.6003, Val Loss=1.4274, lr=0.0100
[02/24 08:58:09 cifar100-global-proj-2.0-mobilenetv2]: Epoch 45/100, Acc=0.5985, Val Loss=1.4403, lr=0.0100
[02/24 08:58:44 cifar100-global-proj-2.0-mobilenetv2]: Epoch 46/100, Acc=0.5839, Val Loss=1.5058, lr=0.0100
[02/24 08:59:21 cifar100-global-proj-2.0-mobilenetv2]: Epoch 47/100, Acc=0.6080, Val Loss=1.4082, lr=0.0100
[02/24 08:59:56 cifar100-global-proj-2.0-mobilenetv2]: Epoch 48/100, Acc=0.6106, Val Loss=1.3863, lr=0.0100
[02/24 09:00:33 cifar100-global-proj-2.0-mobilenetv2]: Epoch 49/100, Acc=0.6054, Val Loss=1.4067, lr=0.0100
[02/24 09:01:10 cifar100-global-proj-2.0-mobilenetv2]: Epoch 50/100, Acc=0.6317, Val Loss=1.3053, lr=0.0100
[02/24 09:01:46 cifar100-global-proj-2.0-mobilenetv2]: Epoch 51/100, Acc=0.5977, Val Loss=1.4304, lr=0.0100
[02/24 09:02:22 cifar100-global-proj-2.0-mobilenetv2]: Epoch 52/100, Acc=0.5972, Val Loss=1.4445, lr=0.0100
[02/24 09:02:58 cifar100-global-proj-2.0-mobilenetv2]: Epoch 53/100, Acc=0.5968, Val Loss=1.4738, lr=0.0100
[02/24 09:03:34 cifar100-global-proj-2.0-mobilenetv2]: Epoch 54/100, Acc=0.6087, Val Loss=1.4078, lr=0.0100
[02/24 09:04:09 cifar100-global-proj-2.0-mobilenetv2]: Epoch 55/100, Acc=0.6031, Val Loss=1.3993, lr=0.0100
[02/24 09:04:45 cifar100-global-proj-2.0-mobilenetv2]: Epoch 56/100, Acc=0.6207, Val Loss=1.3484, lr=0.0100
[02/24 09:05:21 cifar100-global-proj-2.0-mobilenetv2]: Epoch 57/100, Acc=0.6196, Val Loss=1.3487, lr=0.0100
[02/24 09:05:57 cifar100-global-proj-2.0-mobilenetv2]: Epoch 58/100, Acc=0.6019, Val Loss=1.4186, lr=0.0100
[02/24 09:06:34 cifar100-global-proj-2.0-mobilenetv2]: Epoch 59/100, Acc=0.6089, Val Loss=1.4221, lr=0.0100
[02/24 09:07:11 cifar100-global-proj-2.0-mobilenetv2]: Epoch 60/100, Acc=0.6744, Val Loss=1.1368, lr=0.0010
[02/24 09:07:48 cifar100-global-proj-2.0-mobilenetv2]: Epoch 61/100, Acc=0.6778, Val Loss=1.1284, lr=0.0010
[02/24 09:08:32 cifar100-global-proj-2.0-mobilenetv2]: Epoch 62/100, Acc=0.6780, Val Loss=1.1214, lr=0.0010
[02/24 09:09:09 cifar100-global-proj-2.0-mobilenetv2]: Epoch 63/100, Acc=0.6785, Val Loss=1.1186, lr=0.0010
[02/24 09:09:46 cifar100-global-proj-2.0-mobilenetv2]: Epoch 64/100, Acc=0.6799, Val Loss=1.1212, lr=0.0010
[02/24 09:10:23 cifar100-global-proj-2.0-mobilenetv2]: Epoch 65/100, Acc=0.6816, Val Loss=1.1166, lr=0.0010
[02/24 09:10:59 cifar100-global-proj-2.0-mobilenetv2]: Epoch 66/100, Acc=0.6811, Val Loss=1.1196, lr=0.0010
[02/24 09:11:35 cifar100-global-proj-2.0-mobilenetv2]: Epoch 67/100, Acc=0.6782, Val Loss=1.1208, lr=0.0010
[02/24 09:12:11 cifar100-global-proj-2.0-mobilenetv2]: Epoch 68/100, Acc=0.6831, Val Loss=1.1225, lr=0.0010
[02/24 09:12:48 cifar100-global-proj-2.0-mobilenetv2]: Epoch 69/100, Acc=0.6773, Val Loss=1.1218, lr=0.0010
[02/24 09:13:25 cifar100-global-proj-2.0-mobilenetv2]: Epoch 70/100, Acc=0.6802, Val Loss=1.1222, lr=0.0010
[02/24 09:14:02 cifar100-global-proj-2.0-mobilenetv2]: Epoch 71/100, Acc=0.6819, Val Loss=1.1218, lr=0.0010
[02/24 09:14:39 cifar100-global-proj-2.0-mobilenetv2]: Epoch 72/100, Acc=0.6810, Val Loss=1.1284, lr=0.0010
[02/24 09:15:16 cifar100-global-proj-2.0-mobilenetv2]: Epoch 73/100, Acc=0.6793, Val Loss=1.1312, lr=0.0010
[02/24 09:15:54 cifar100-global-proj-2.0-mobilenetv2]: Epoch 74/100, Acc=0.6787, Val Loss=1.1295, lr=0.0010
[02/24 09:16:31 cifar100-global-proj-2.0-mobilenetv2]: Epoch 75/100, Acc=0.6806, Val Loss=1.1369, lr=0.0010
[02/24 09:17:09 cifar100-global-proj-2.0-mobilenetv2]: Epoch 76/100, Acc=0.6793, Val Loss=1.1301, lr=0.0010
[02/24 09:17:46 cifar100-global-proj-2.0-mobilenetv2]: Epoch 77/100, Acc=0.6802, Val Loss=1.1322, lr=0.0010
[02/24 09:18:23 cifar100-global-proj-2.0-mobilenetv2]: Epoch 78/100, Acc=0.6800, Val Loss=1.1323, lr=0.0010
[02/24 09:19:00 cifar100-global-proj-2.0-mobilenetv2]: Epoch 79/100, Acc=0.6771, Val Loss=1.1368, lr=0.0010
[02/24 09:19:36 cifar100-global-proj-2.0-mobilenetv2]: Epoch 80/100, Acc=0.6801, Val Loss=1.1268, lr=0.0001
[02/24 09:20:13 cifar100-global-proj-2.0-mobilenetv2]: Epoch 81/100, Acc=0.6816, Val Loss=1.1265, lr=0.0001
[02/24 09:20:50 cifar100-global-proj-2.0-mobilenetv2]: Epoch 82/100, Acc=0.6806, Val Loss=1.1271, lr=0.0001
[02/24 09:21:26 cifar100-global-proj-2.0-mobilenetv2]: Epoch 83/100, Acc=0.6807, Val Loss=1.1268, lr=0.0001
[02/24 09:22:02 cifar100-global-proj-2.0-mobilenetv2]: Epoch 84/100, Acc=0.6822, Val Loss=1.1236, lr=0.0001
[02/24 09:22:41 cifar100-global-proj-2.0-mobilenetv2]: Epoch 85/100, Acc=0.6820, Val Loss=1.1249, lr=0.0001
[02/24 09:23:26 cifar100-global-proj-2.0-mobilenetv2]: Epoch 86/100, Acc=0.6826, Val Loss=1.1231, lr=0.0001
[02/24 09:24:03 cifar100-global-proj-2.0-mobilenetv2]: Epoch 87/100, Acc=0.6816, Val Loss=1.1232, lr=0.0001
[02/24 09:24:39 cifar100-global-proj-2.0-mobilenetv2]: Epoch 88/100, Acc=0.6839, Val Loss=1.1244, lr=0.0001
[02/24 09:25:15 cifar100-global-proj-2.0-mobilenetv2]: Epoch 89/100, Acc=0.6824, Val Loss=1.1249, lr=0.0001
[02/24 09:25:51 cifar100-global-proj-2.0-mobilenetv2]: Epoch 90/100, Acc=0.6834, Val Loss=1.1256, lr=0.0001
[02/24 09:26:26 cifar100-global-proj-2.0-mobilenetv2]: Epoch 91/100, Acc=0.6817, Val Loss=1.1264, lr=0.0001
[02/24 09:27:02 cifar100-global-proj-2.0-mobilenetv2]: Epoch 92/100, Acc=0.6847, Val Loss=1.1265, lr=0.0001
[02/24 09:27:47 cifar100-global-proj-2.0-mobilenetv2]: Epoch 93/100, Acc=0.6831, Val Loss=1.1262, lr=0.0001
[02/24 09:28:26 cifar100-global-proj-2.0-mobilenetv2]: Epoch 94/100, Acc=0.6837, Val Loss=1.1258, lr=0.0001
[02/24 09:29:03 cifar100-global-proj-2.0-mobilenetv2]: Epoch 95/100, Acc=0.6824, Val Loss=1.1264, lr=0.0001
[02/24 09:29:40 cifar100-global-proj-2.0-mobilenetv2]: Epoch 96/100, Acc=0.6829, Val Loss=1.1233, lr=0.0001
[02/24 09:30:17 cifar100-global-proj-2.0-mobilenetv2]: Epoch 97/100, Acc=0.6832, Val Loss=1.1257, lr=0.0001
[02/24 09:30:54 cifar100-global-proj-2.0-mobilenetv2]: Epoch 98/100, Acc=0.6841, Val Loss=1.1246, lr=0.0001
[02/24 09:31:30 cifar100-global-proj-2.0-mobilenetv2]: Epoch 99/100, Acc=0.6825, Val Loss=1.1253, lr=0.0001
[02/24 09:31:30 cifar100-global-proj-2.0-mobilenetv2]: Best Acc=0.6847
[02/24 09:31:30 cifar100-global-proj-2.0-mobilenetv2]: Params: 1.19 M
[02/24 09:31:30 cifar100-global-proj-2.0-mobilenetv2]: ops: 34.13 M
[02/24 09:31:34 cifar100-global-proj-2.0-mobilenetv2]: Acc: 0.6825 Val Loss: 1.1253

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: mode: prune
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: model: mobilenetv2
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: verbose: False
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: dataset: cifar100
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: dataroot: data
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: batch_size: 128
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: total_epochs: 100
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: lr: 0.01
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: restore: run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: output_dir: run/cifar100/prune/cifar100-global-proj_sl-2.0-mobilenetv2
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: finetune: True
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: last_epochs: 100
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: reps: 1
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: method: proj_sl
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: speed_up: 2.0
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: reg: 1e-05
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: delta_reg: 0.0001
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: weight_decay: 0.0005
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: seed: 1
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: global_pruning: True
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: sl_total_epochs: 100
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: sl_lr: 0.01
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: sl_reg_warmup: 0
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: sl_restore: None
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: iterative_steps: 400
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: logger: <Logger cifar100-global-proj_sl-2.0-mobilenetv2 (DEBUG)>
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: device: cuda
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: num_classes: 100
[02/24 09:31:43 cifar100-global-proj_sl-2.0-mobilenetv2]: Loading model from run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/24 09:31:44 cifar100-global-proj_sl-2.0-mobilenetv2]: Regularizing...
[02/24 10:06:42 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 0/100, Acc=0.5924, Val Loss=1.4596, lr=0.0100
[02/24 10:40:53 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 1/100, Acc=0.6051, Val Loss=1.4049, lr=0.0100
[02/24 11:14:39 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 2/100, Acc=0.5949, Val Loss=1.4607, lr=0.0100
[02/24 11:46:21 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 3/100, Acc=0.5996, Val Loss=1.4545, lr=0.0100
[02/24 12:19:36 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 4/100, Acc=0.6100, Val Loss=1.3828, lr=0.0100
[02/24 12:53:35 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 5/100, Acc=0.6052, Val Loss=1.4188, lr=0.0100
[02/24 13:29:43 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 6/100, Acc=0.6120, Val Loss=1.4065, lr=0.0100
[02/24 14:00:35 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 7/100, Acc=0.6163, Val Loss=1.3797, lr=0.0100
[02/24 14:34:19 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 8/100, Acc=0.6260, Val Loss=1.3291, lr=0.0100
[02/24 15:06:14 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 9/100, Acc=0.6212, Val Loss=1.3510, lr=0.0100
[02/24 15:38:10 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 10/100, Acc=0.6214, Val Loss=1.3634, lr=0.0100
[02/24 16:08:50 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 11/100, Acc=0.6037, Val Loss=1.4459, lr=0.0100
[02/24 16:39:25 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 12/100, Acc=0.6146, Val Loss=1.3944, lr=0.0100
[02/24 17:10:19 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 13/100, Acc=0.6324, Val Loss=1.3239, lr=0.0100
[02/24 17:41:05 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 14/100, Acc=0.6140, Val Loss=1.3902, lr=0.0100
[02/24 18:11:57 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 15/100, Acc=0.6297, Val Loss=1.3422, lr=0.0100
[02/24 18:43:01 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 16/100, Acc=0.6236, Val Loss=1.3583, lr=0.0100
[02/24 19:13:31 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 17/100, Acc=0.6181, Val Loss=1.3769, lr=0.0100
[02/24 19:43:52 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 18/100, Acc=0.6314, Val Loss=1.3274, lr=0.0100
[02/24 20:17:08 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 19/100, Acc=0.6344, Val Loss=1.3229, lr=0.0100
[02/24 20:51:20 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 20/100, Acc=0.6377, Val Loss=1.3186, lr=0.0100
[02/24 21:25:07 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 21/100, Acc=0.6365, Val Loss=1.3143, lr=0.0100
[02/24 21:58:12 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 22/100, Acc=0.6277, Val Loss=1.3754, lr=0.0100
[02/24 22:32:37 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 23/100, Acc=0.6268, Val Loss=1.3298, lr=0.0100
[02/24 23:06:27 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 24/100, Acc=0.6393, Val Loss=1.3043, lr=0.0100
[02/24 23:39:34 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 25/100, Acc=0.6462, Val Loss=1.2939, lr=0.0100
[02/25 00:13:04 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 26/100, Acc=0.6423, Val Loss=1.2895, lr=0.0100
[02/25 00:46:16 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 27/100, Acc=0.6389, Val Loss=1.3208, lr=0.0100
[02/25 01:18:20 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 28/100, Acc=0.6412, Val Loss=1.2992, lr=0.0100
[02/25 01:52:04 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 29/100, Acc=0.6468, Val Loss=1.3156, lr=0.0100
[02/25 02:22:59 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 30/100, Acc=0.6393, Val Loss=1.3163, lr=0.0100
[02/25 02:53:52 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 31/100, Acc=0.6357, Val Loss=1.3162, lr=0.0100
[02/25 03:25:48 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 32/100, Acc=0.6539, Val Loss=1.2720, lr=0.0100
[02/25 03:57:25 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 33/100, Acc=0.6423, Val Loss=1.3257, lr=0.0100
[02/25 04:30:19 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 34/100, Acc=0.6386, Val Loss=1.3371, lr=0.0100
[02/25 05:02:59 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 35/100, Acc=0.6245, Val Loss=1.3880, lr=0.0100
[02/25 05:37:17 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 36/100, Acc=0.6510, Val Loss=1.2912, lr=0.0100
[02/25 06:11:21 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 37/100, Acc=0.6435, Val Loss=1.3232, lr=0.0100
[02/25 06:46:17 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 38/100, Acc=0.6406, Val Loss=1.3303, lr=0.0100
[02/25 07:19:14 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 39/100, Acc=0.6451, Val Loss=1.3103, lr=0.0100
[02/25 07:50:22 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 40/100, Acc=0.6425, Val Loss=1.3173, lr=0.0100
[02/25 08:20:59 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 41/100, Acc=0.6391, Val Loss=1.3298, lr=0.0100
[02/25 08:52:23 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 42/100, Acc=0.6472, Val Loss=1.3016, lr=0.0100
[02/25 09:23:05 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 43/100, Acc=0.6405, Val Loss=1.3305, lr=0.0100
[02/25 09:53:27 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 44/100, Acc=0.6436, Val Loss=1.3113, lr=0.0100
[02/25 10:24:51 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 45/100, Acc=0.6516, Val Loss=1.2785, lr=0.0100
[02/25 10:55:57 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 46/100, Acc=0.6351, Val Loss=1.3521, lr=0.0100
[02/25 11:27:46 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 47/100, Acc=0.6480, Val Loss=1.3153, lr=0.0100
[02/25 11:58:06 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 48/100, Acc=0.6480, Val Loss=1.3156, lr=0.0100
[02/25 12:29:24 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 49/100, Acc=0.6491, Val Loss=1.3292, lr=0.0100
[02/25 13:01:38 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 50/100, Acc=0.6457, Val Loss=1.3285, lr=0.0100
[02/25 13:32:28 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 51/100, Acc=0.6419, Val Loss=1.3368, lr=0.0100
[02/25 14:03:52 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 52/100, Acc=0.6460, Val Loss=1.3174, lr=0.0100
[02/25 14:35:46 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 53/100, Acc=0.6525, Val Loss=1.2848, lr=0.0100
[02/25 15:06:56 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 54/100, Acc=0.6543, Val Loss=1.3032, lr=0.0100
[02/25 15:38:34 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 55/100, Acc=0.6539, Val Loss=1.3111, lr=0.0100
[02/25 16:09:26 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 56/100, Acc=0.6480, Val Loss=1.3126, lr=0.0100
[02/25 16:41:17 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 57/100, Acc=0.6519, Val Loss=1.3204, lr=0.0100
[02/25 17:12:18 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 58/100, Acc=0.6509, Val Loss=1.3386, lr=0.0100
[02/25 17:43:53 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 59/100, Acc=0.6461, Val Loss=1.3360, lr=0.0100
[02/25 18:14:42 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 60/100, Acc=0.6779, Val Loss=1.2031, lr=0.0010
[02/25 18:47:11 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 61/100, Acc=0.6787, Val Loss=1.1970, lr=0.0010
[02/25 19:17:50 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 62/100, Acc=0.6791, Val Loss=1.2007, lr=0.0010
[02/25 19:49:33 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 63/100, Acc=0.6799, Val Loss=1.1976, lr=0.0010
[02/25 20:20:38 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 64/100, Acc=0.6787, Val Loss=1.2026, lr=0.0010
[02/25 20:51:06 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 65/100, Acc=0.6791, Val Loss=1.2038, lr=0.0010
[02/25 21:21:51 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 66/100, Acc=0.6819, Val Loss=1.2014, lr=0.0010
[02/25 21:54:11 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 67/100, Acc=0.6810, Val Loss=1.2052, lr=0.0010
[02/25 22:25:06 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 68/100, Acc=0.6817, Val Loss=1.2037, lr=0.0010
[02/25 22:55:50 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 69/100, Acc=0.6801, Val Loss=1.2136, lr=0.0010
[02/25 23:27:35 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 70/100, Acc=0.6781, Val Loss=1.2137, lr=0.0010
[02/25 23:59:11 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 71/100, Acc=0.6786, Val Loss=1.2147, lr=0.0010
[02/26 00:30:47 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 72/100, Acc=0.6796, Val Loss=1.2218, lr=0.0010
[02/26 01:01:46 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 73/100, Acc=0.6791, Val Loss=1.2211, lr=0.0010
[02/26 01:32:23 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 74/100, Acc=0.6820, Val Loss=1.2203, lr=0.0010
[02/26 02:05:07 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 75/100, Acc=0.6831, Val Loss=1.2153, lr=0.0010
[02/26 02:36:19 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 76/100, Acc=0.6805, Val Loss=1.2252, lr=0.0010
[02/26 03:07:18 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 77/100, Acc=0.6806, Val Loss=1.2233, lr=0.0010
[02/26 03:39:49 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 78/100, Acc=0.6779, Val Loss=1.2277, lr=0.0010
[02/26 04:10:47 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 79/100, Acc=0.6797, Val Loss=1.2305, lr=0.0010
[02/26 04:41:36 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 80/100, Acc=0.6791, Val Loss=1.2282, lr=0.0001
[02/26 05:13:00 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 81/100, Acc=0.6788, Val Loss=1.2277, lr=0.0001
[02/26 05:44:20 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 82/100, Acc=0.6793, Val Loss=1.2267, lr=0.0001
[02/26 06:15:25 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 83/100, Acc=0.6803, Val Loss=1.2218, lr=0.0001
[02/26 06:46:32 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 84/100, Acc=0.6793, Val Loss=1.2235, lr=0.0001
[02/26 07:17:13 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 85/100, Acc=0.6802, Val Loss=1.2240, lr=0.0001
[02/26 07:49:39 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 86/100, Acc=0.6798, Val Loss=1.2251, lr=0.0001
[02/26 08:21:28 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 87/100, Acc=0.6817, Val Loss=1.2230, lr=0.0001
[02/26 08:52:19 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 88/100, Acc=0.6806, Val Loss=1.2242, lr=0.0001
[02/26 09:23:02 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 89/100, Acc=0.6806, Val Loss=1.2285, lr=0.0001
[02/26 09:54:57 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 90/100, Acc=0.6801, Val Loss=1.2266, lr=0.0001
[02/26 10:25:34 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 91/100, Acc=0.6814, Val Loss=1.2259, lr=0.0001
[02/26 10:57:12 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 92/100, Acc=0.6811, Val Loss=1.2258, lr=0.0001
[02/26 11:27:40 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 93/100, Acc=0.6814, Val Loss=1.2234, lr=0.0001
[02/26 11:57:57 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 94/100, Acc=0.6801, Val Loss=1.2265, lr=0.0001
[02/26 12:28:01 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 95/100, Acc=0.6821, Val Loss=1.2290, lr=0.0001
[02/26 12:58:38 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 96/100, Acc=0.6796, Val Loss=1.2266, lr=0.0001
[02/26 13:28:54 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 97/100, Acc=0.6790, Val Loss=1.2238, lr=0.0001
[02/26 13:59:13 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 98/100, Acc=0.6817, Val Loss=1.2252, lr=0.0001
[02/26 14:29:30 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 99/100, Acc=0.6799, Val Loss=1.2282, lr=0.0001
[02/26 14:29:30 cifar100-global-proj_sl-2.0-mobilenetv2]: Best Acc=0.6831
[02/26 14:29:30 cifar100-global-proj_sl-2.0-mobilenetv2]: Loading the sparse model from run/cifar100/prune/cifar100-global-proj_sl-2.0-mobilenetv2/reg_cifar100_mobilenetv2_proj_sl_1e-05.pth...
[02/26 14:29:33 cifar100-global-proj_sl-2.0-mobilenetv2]: Pruning...
[02/26 14:41:10 cifar100-global-proj_sl-2.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 13, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(13, 15, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=15)
      (4): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(15, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 81, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(81, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(81, 81, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=81)
        (4): BatchNorm2d(81, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(81, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 116, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(116, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=116)
        (4): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(116, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 89, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(89, 89, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=89)
        (4): BatchNorm2d(89, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(89, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 116, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(116, 116, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=116)
        (4): BatchNorm2d(116, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(116, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 91, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(91, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(91, 91, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=91)
        (4): BatchNorm2d(91, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(91, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 111, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(111, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(111, 111, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=111)
        (4): BatchNorm2d(111, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(111, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 141, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(141, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(141, 141, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=141)
        (4): BatchNorm2d(141, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(141, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 123, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(123, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(123, 123, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=123)
        (4): BatchNorm2d(123, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(123, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 93, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(93, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(93, 93, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=93)
        (4): BatchNorm2d(93, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(93, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 239, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(239, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(239, 239, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=239)
        (4): BatchNorm2d(239, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(239, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 171, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(171, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(171, 171, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=171)
        (4): BatchNorm2d(171, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(171, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 114, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(114, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(114, 114, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=114)
        (4): BatchNorm2d(114, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(114, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 276, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(276, 276, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=276)
        (4): BatchNorm2d(276, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(276, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 372, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(372, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(372, 372, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=372)
        (4): BatchNorm2d(372, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(372, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 387, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(387, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(387, 387, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=387)
        (4): BatchNorm2d(387, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(387, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 200, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200)
      (4): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(200, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 100, kernel_size=(1, 1), stride=(1, 1))
)
[02/26 14:41:13 cifar100-global-proj_sl-2.0-mobilenetv2]: Params: 2.37 M => 1.17 M (49.53%)
[02/26 14:41:13 cifar100-global-proj_sl-2.0-mobilenetv2]: FLOPs: 68.40 M => 34.08 M (49.83%, 2.01X )
[02/26 14:41:13 cifar100-global-proj_sl-2.0-mobilenetv2]: Acc: 0.6831 => 0.6831
[02/26 14:41:13 cifar100-global-proj_sl-2.0-mobilenetv2]: Val Loss: 1.2153 => 1.2153
[02/26 14:41:13 cifar100-global-proj_sl-2.0-mobilenetv2]: Finetuning...
[02/26 14:41:45 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 0/100, Acc=0.6455, Val Loss=1.3454, lr=0.0100
[02/26 14:42:16 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 1/100, Acc=0.6480, Val Loss=1.3189, lr=0.0100
[02/26 14:42:48 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 2/100, Acc=0.6527, Val Loss=1.3034, lr=0.0100
[02/26 14:43:20 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 3/100, Acc=0.6526, Val Loss=1.2928, lr=0.0100
[02/26 14:43:52 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 4/100, Acc=0.6467, Val Loss=1.2797, lr=0.0100
[02/26 14:44:24 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 5/100, Acc=0.6494, Val Loss=1.2827, lr=0.0100
[02/26 14:44:59 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 6/100, Acc=0.6398, Val Loss=1.3291, lr=0.0100
[02/26 14:45:36 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 7/100, Acc=0.6430, Val Loss=1.3286, lr=0.0100
[02/26 14:46:13 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 8/100, Acc=0.6376, Val Loss=1.3338, lr=0.0100
[02/26 14:46:51 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 9/100, Acc=0.6498, Val Loss=1.2852, lr=0.0100
[02/26 14:47:28 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 10/100, Acc=0.6500, Val Loss=1.2712, lr=0.0100
[02/26 14:48:05 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 11/100, Acc=0.6377, Val Loss=1.3136, lr=0.0100
[02/26 14:48:42 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 12/100, Acc=0.6389, Val Loss=1.3071, lr=0.0100
[02/26 14:49:14 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 13/100, Acc=0.6407, Val Loss=1.3104, lr=0.0100
[02/26 14:49:45 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 14/100, Acc=0.6465, Val Loss=1.2835, lr=0.0100
[02/26 14:50:17 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 15/100, Acc=0.6434, Val Loss=1.2780, lr=0.0100
[02/26 14:50:49 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 16/100, Acc=0.6480, Val Loss=1.2599, lr=0.0100
[02/26 14:51:20 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 17/100, Acc=0.6394, Val Loss=1.3289, lr=0.0100
[02/26 14:51:53 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 18/100, Acc=0.6320, Val Loss=1.3349, lr=0.0100
[02/26 14:52:24 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 19/100, Acc=0.6375, Val Loss=1.3107, lr=0.0100
[02/26 14:52:57 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 20/100, Acc=0.6284, Val Loss=1.3385, lr=0.0100
[02/26 14:53:29 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 21/100, Acc=0.6360, Val Loss=1.3221, lr=0.0100
[02/26 14:54:02 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 22/100, Acc=0.6415, Val Loss=1.2790, lr=0.0100
[02/26 14:54:34 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 23/100, Acc=0.6286, Val Loss=1.3289, lr=0.0100
[02/26 14:55:08 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 24/100, Acc=0.6374, Val Loss=1.3341, lr=0.0100
[02/26 14:55:45 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 25/100, Acc=0.6269, Val Loss=1.3449, lr=0.0100
[02/26 14:56:23 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 26/100, Acc=0.6302, Val Loss=1.3413, lr=0.0100
[02/26 14:57:01 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 27/100, Acc=0.6290, Val Loss=1.3382, lr=0.0100
[02/26 14:57:39 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 28/100, Acc=0.6341, Val Loss=1.3285, lr=0.0100
[02/26 14:58:17 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 29/100, Acc=0.6172, Val Loss=1.3937, lr=0.0100
[02/26 14:58:55 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 30/100, Acc=0.6180, Val Loss=1.3784, lr=0.0100
[02/26 14:59:33 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 31/100, Acc=0.6272, Val Loss=1.3609, lr=0.0100
[02/26 15:00:11 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 32/100, Acc=0.6338, Val Loss=1.3369, lr=0.0100
[02/26 15:00:48 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 33/100, Acc=0.6361, Val Loss=1.3007, lr=0.0100
[02/26 15:01:27 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 34/100, Acc=0.6298, Val Loss=1.3229, lr=0.0100
[02/26 15:02:06 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 35/100, Acc=0.6252, Val Loss=1.3645, lr=0.0100
[02/26 15:02:44 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 36/100, Acc=0.6250, Val Loss=1.3634, lr=0.0100
[02/26 15:03:23 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 37/100, Acc=0.6309, Val Loss=1.3345, lr=0.0100
[02/26 15:04:01 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 38/100, Acc=0.6357, Val Loss=1.3130, lr=0.0100
[02/26 15:04:36 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 39/100, Acc=0.6280, Val Loss=1.3379, lr=0.0100
[02/26 15:05:08 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 40/100, Acc=0.6283, Val Loss=1.3412, lr=0.0100
[02/26 15:05:40 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 41/100, Acc=0.6287, Val Loss=1.3413, lr=0.0100
[02/26 15:06:13 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 42/100, Acc=0.6237, Val Loss=1.3909, lr=0.0100
[02/26 15:06:45 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 43/100, Acc=0.6286, Val Loss=1.3482, lr=0.0100
[02/26 15:07:17 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 44/100, Acc=0.6312, Val Loss=1.3104, lr=0.0100
[02/26 15:07:48 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 45/100, Acc=0.6267, Val Loss=1.3292, lr=0.0100
[02/26 15:08:20 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 46/100, Acc=0.6241, Val Loss=1.3438, lr=0.0100
[02/26 15:08:52 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 47/100, Acc=0.6220, Val Loss=1.3628, lr=0.0100
[02/26 15:09:24 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 48/100, Acc=0.6313, Val Loss=1.3258, lr=0.0100
[02/26 15:09:56 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 49/100, Acc=0.5969, Val Loss=1.4520, lr=0.0100
[02/26 15:10:28 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 50/100, Acc=0.5916, Val Loss=1.4655, lr=0.0100
[02/26 15:11:00 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 51/100, Acc=0.6268, Val Loss=1.3364, lr=0.0100
[02/26 15:11:32 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 52/100, Acc=0.6295, Val Loss=1.3343, lr=0.0100
[02/26 15:12:04 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 53/100, Acc=0.6234, Val Loss=1.3705, lr=0.0100
[02/26 15:12:36 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 54/100, Acc=0.6266, Val Loss=1.3493, lr=0.0100
[02/26 15:13:08 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 55/100, Acc=0.6064, Val Loss=1.4163, lr=0.0100
[02/26 15:13:40 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 56/100, Acc=0.6082, Val Loss=1.4196, lr=0.0100
[02/26 15:14:13 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 57/100, Acc=0.6150, Val Loss=1.3844, lr=0.0100
[02/26 15:14:45 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 58/100, Acc=0.6249, Val Loss=1.3604, lr=0.0100
[02/26 15:15:18 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 59/100, Acc=0.6283, Val Loss=1.3187, lr=0.0100
[02/26 15:15:50 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 60/100, Acc=0.6787, Val Loss=1.1207, lr=0.0010
[02/26 15:16:23 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 61/100, Acc=0.6812, Val Loss=1.1169, lr=0.0010
[02/26 15:16:55 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 62/100, Acc=0.6834, Val Loss=1.1102, lr=0.0010
[02/26 15:17:27 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 63/100, Acc=0.6844, Val Loss=1.1138, lr=0.0010
[02/26 15:17:59 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 64/100, Acc=0.6828, Val Loss=1.1159, lr=0.0010
[02/26 15:18:31 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 65/100, Acc=0.6853, Val Loss=1.1155, lr=0.0010
[02/26 15:19:03 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 66/100, Acc=0.6827, Val Loss=1.1196, lr=0.0010
[02/26 15:19:36 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 67/100, Acc=0.6840, Val Loss=1.1180, lr=0.0010
[02/26 15:20:08 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 68/100, Acc=0.6837, Val Loss=1.1232, lr=0.0010
[02/26 15:20:40 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 69/100, Acc=0.6847, Val Loss=1.1184, lr=0.0010
[02/26 15:21:12 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 70/100, Acc=0.6832, Val Loss=1.1203, lr=0.0010
[02/26 15:21:44 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 71/100, Acc=0.6831, Val Loss=1.1216, lr=0.0010
[02/26 15:22:16 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 72/100, Acc=0.6823, Val Loss=1.1264, lr=0.0010
[02/26 15:22:48 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 73/100, Acc=0.6817, Val Loss=1.1289, lr=0.0010
[02/26 15:23:21 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 74/100, Acc=0.6830, Val Loss=1.1294, lr=0.0010
[02/26 15:23:53 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 75/100, Acc=0.6822, Val Loss=1.1339, lr=0.0010
[02/26 15:24:26 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 76/100, Acc=0.6814, Val Loss=1.1337, lr=0.0010
[02/26 15:24:58 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 77/100, Acc=0.6796, Val Loss=1.1347, lr=0.0010
[02/26 15:25:30 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 78/100, Acc=0.6780, Val Loss=1.1450, lr=0.0010
[02/26 15:26:02 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 79/100, Acc=0.6826, Val Loss=1.1408, lr=0.0010
[02/26 15:26:34 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 80/100, Acc=0.6828, Val Loss=1.1359, lr=0.0001
[02/26 15:27:06 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 81/100, Acc=0.6819, Val Loss=1.1329, lr=0.0001
[02/26 15:27:38 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 82/100, Acc=0.6842, Val Loss=1.1319, lr=0.0001
[02/26 15:28:10 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 83/100, Acc=0.6835, Val Loss=1.1286, lr=0.0001
[02/26 15:28:42 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 84/100, Acc=0.6824, Val Loss=1.1335, lr=0.0001
[02/26 15:29:13 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 85/100, Acc=0.6834, Val Loss=1.1300, lr=0.0001
[02/26 15:29:45 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 86/100, Acc=0.6851, Val Loss=1.1292, lr=0.0001
[02/26 15:30:16 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 87/100, Acc=0.6855, Val Loss=1.1291, lr=0.0001
[02/26 15:30:47 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 88/100, Acc=0.6846, Val Loss=1.1302, lr=0.0001
[02/26 15:31:19 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 89/100, Acc=0.6845, Val Loss=1.1296, lr=0.0001
[02/26 15:31:51 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 90/100, Acc=0.6841, Val Loss=1.1321, lr=0.0001
[02/26 15:32:22 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 91/100, Acc=0.6847, Val Loss=1.1294, lr=0.0001
[02/26 15:32:54 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 92/100, Acc=0.6826, Val Loss=1.1323, lr=0.0001
[02/26 15:33:26 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 93/100, Acc=0.6844, Val Loss=1.1313, lr=0.0001
[02/26 15:33:58 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 94/100, Acc=0.6831, Val Loss=1.1347, lr=0.0001
[02/26 15:34:29 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 95/100, Acc=0.6831, Val Loss=1.1304, lr=0.0001
[02/26 15:35:01 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 96/100, Acc=0.6830, Val Loss=1.1337, lr=0.0001
[02/26 15:35:33 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 97/100, Acc=0.6834, Val Loss=1.1330, lr=0.0001
[02/26 15:36:04 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 98/100, Acc=0.6846, Val Loss=1.1327, lr=0.0001
[02/26 15:36:36 cifar100-global-proj_sl-2.0-mobilenetv2]: Epoch 99/100, Acc=0.6838, Val Loss=1.1332, lr=0.0001
[02/26 15:36:36 cifar100-global-proj_sl-2.0-mobilenetv2]: Best Acc=0.6855
[02/26 15:36:36 cifar100-global-proj_sl-2.0-mobilenetv2]: Params: 1.17 M
[02/26 15:36:36 cifar100-global-proj_sl-2.0-mobilenetv2]: ops: 34.08 M
[02/26 15:36:39 cifar100-global-proj_sl-2.0-mobilenetv2]: Acc: 0.6838 Val Loss: 1.1332

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: mode: prune
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: model: mobilenetv2
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: verbose: False
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: dataset: cifar100
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: dataroot: data
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: batch_size: 128
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: total_epochs: 100
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: lr: 0.01
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: restore: run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: output_dir: run/cifar100/prune/cifar100-global-random-3.0-mobilenetv2
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: finetune: True
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: last_epochs: 100
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: reps: 1
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: method: random
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: speed_up: 3.0
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: reg: 1e-05
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: delta_reg: 0.0001
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: weight_decay: 0.0005
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: seed: 1
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: global_pruning: True
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: sl_total_epochs: 100
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: sl_lr: 0.01
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: sl_reg_warmup: 0
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: sl_restore: None
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: iterative_steps: 400
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: logger: <Logger cifar100-global-random-3.0-mobilenetv2 (DEBUG)>
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: device: cuda
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: num_classes: 100
[02/26 15:36:47 cifar100-global-random-3.0-mobilenetv2]: Loading model from run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/26 15:36:51 cifar100-global-random-3.0-mobilenetv2]: Pruning...
[02/26 15:37:02 cifar100-global-random-3.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 1, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (4): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(1, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (4): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(1, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 49, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(49, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=49)
        (4): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(49, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 49, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(49, 49, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=49)
        (4): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(49, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 97, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(97, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=97)
        (4): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(97, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 97, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(97, 97, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=97)
        (4): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(97, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 97, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(97, 97, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=97)
        (4): BatchNorm2d(97, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(97, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 289, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(289, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(289, 289, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=289)
        (4): BatchNorm2d(289, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(289, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 289, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(289, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(289, 289, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=289)
        (4): BatchNorm2d(289, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(289, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 289, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(289, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(289, 289, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=289)
        (4): BatchNorm2d(289, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(289, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 289, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(289, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(289, 289, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=289)
        (4): BatchNorm2d(289, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(289, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 481, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(481, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(481, 481, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=481)
        (4): BatchNorm2d(481, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(481, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 481, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(481, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(481, 481, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=481)
        (4): BatchNorm2d(481, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(481, 1, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(1, 481, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(481, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(481, 481, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=481)
        (4): BatchNorm2d(481, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(481, 65, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(65, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(65, 865, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(865, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(865, 865, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=865)
        (4): BatchNorm2d(865, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(865, 65, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(65, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(65, 865, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(865, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(865, 865, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=865)
        (4): BatchNorm2d(865, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(865, 65, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(65, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(65, 865, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(865, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(865, 865, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=865)
      (4): BatchNorm2d(865, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(865, 225, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(225, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(225, 1185, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1185, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1185, 100, kernel_size=(1, 1), stride=(1, 1))
)
[02/26 15:37:05 cifar100-global-random-3.0-mobilenetv2]: Params: 2.37 M => 0.99 M (41.63%)
[02/26 15:37:05 cifar100-global-random-3.0-mobilenetv2]: FLOPs: 68.40 M => 22.75 M (33.26%, 3.01X )
[02/26 15:37:05 cifar100-global-random-3.0-mobilenetv2]: Acc: 0.6699 => 0.0100
[02/26 15:37:05 cifar100-global-random-3.0-mobilenetv2]: Val Loss: 1.1637 => 5.6273
[02/26 15:37:05 cifar100-global-random-3.0-mobilenetv2]: Finetuning...
[02/26 15:37:36 cifar100-global-random-3.0-mobilenetv2]: Epoch 0/100, Acc=0.0100, Val Loss=6.5763, lr=0.0100
[02/26 15:38:07 cifar100-global-random-3.0-mobilenetv2]: Epoch 1/100, Acc=0.0100, Val Loss=7.4771, lr=0.0100
[02/26 15:38:38 cifar100-global-random-3.0-mobilenetv2]: Epoch 2/100, Acc=0.0100, Val Loss=15.7860, lr=0.0100
[02/26 15:39:09 cifar100-global-random-3.0-mobilenetv2]: Epoch 3/100, Acc=0.0100, Val Loss=5.9605, lr=0.0100
[02/26 15:39:41 cifar100-global-random-3.0-mobilenetv2]: Epoch 4/100, Acc=0.0100, Val Loss=7.6046, lr=0.0100
[02/26 15:40:12 cifar100-global-random-3.0-mobilenetv2]: Epoch 5/100, Acc=0.0100, Val Loss=4.6681, lr=0.0100
[02/26 15:40:43 cifar100-global-random-3.0-mobilenetv2]: Epoch 6/100, Acc=0.0100, Val Loss=4.6449, lr=0.0100
[02/26 15:41:15 cifar100-global-random-3.0-mobilenetv2]: Epoch 7/100, Acc=0.0100, Val Loss=10.1187, lr=0.0100
[02/26 15:41:46 cifar100-global-random-3.0-mobilenetv2]: Epoch 8/100, Acc=0.0100, Val Loss=4.8786, lr=0.0100
[02/26 15:42:17 cifar100-global-random-3.0-mobilenetv2]: Epoch 9/100, Acc=0.0100, Val Loss=4.6156, lr=0.0100
[02/26 15:42:48 cifar100-global-random-3.0-mobilenetv2]: Epoch 10/100, Acc=0.0100, Val Loss=4.7033, lr=0.0100
[02/26 15:43:19 cifar100-global-random-3.0-mobilenetv2]: Epoch 11/100, Acc=0.0100, Val Loss=4.6133, lr=0.0100
[02/26 15:43:50 cifar100-global-random-3.0-mobilenetv2]: Epoch 12/100, Acc=0.0100, Val Loss=4.6630, lr=0.0100
[02/26 15:44:21 cifar100-global-random-3.0-mobilenetv2]: Epoch 13/100, Acc=0.0100, Val Loss=4.6090, lr=0.0100
[02/26 15:44:51 cifar100-global-random-3.0-mobilenetv2]: Epoch 14/100, Acc=0.0100, Val Loss=4.6084, lr=0.0100
[02/26 15:45:22 cifar100-global-random-3.0-mobilenetv2]: Epoch 15/100, Acc=0.0100, Val Loss=4.6073, lr=0.0100
[02/26 15:45:53 cifar100-global-random-3.0-mobilenetv2]: Epoch 16/100, Acc=0.0100, Val Loss=4.6067, lr=0.0100
[02/26 15:46:24 cifar100-global-random-3.0-mobilenetv2]: Epoch 17/100, Acc=0.0100, Val Loss=4.6076, lr=0.0100
[02/26 15:46:54 cifar100-global-random-3.0-mobilenetv2]: Epoch 18/100, Acc=0.0100, Val Loss=4.6064, lr=0.0100
[02/26 15:47:25 cifar100-global-random-3.0-mobilenetv2]: Epoch 19/100, Acc=0.0100, Val Loss=4.6064, lr=0.0100
[02/26 15:47:55 cifar100-global-random-3.0-mobilenetv2]: Epoch 20/100, Acc=0.0100, Val Loss=4.6070, lr=0.0100
[02/26 15:48:26 cifar100-global-random-3.0-mobilenetv2]: Epoch 21/100, Acc=0.0100, Val Loss=4.6062, lr=0.0100
[02/26 15:48:57 cifar100-global-random-3.0-mobilenetv2]: Epoch 22/100, Acc=0.0100, Val Loss=4.6060, lr=0.0100
[02/26 15:49:28 cifar100-global-random-3.0-mobilenetv2]: Epoch 23/100, Acc=0.0100, Val Loss=4.6066, lr=0.0100
[02/26 15:49:59 cifar100-global-random-3.0-mobilenetv2]: Epoch 24/100, Acc=0.0100, Val Loss=4.6058, lr=0.0100
[02/26 15:50:30 cifar100-global-random-3.0-mobilenetv2]: Epoch 25/100, Acc=0.0100, Val Loss=4.6059, lr=0.0100
[02/26 15:51:00 cifar100-global-random-3.0-mobilenetv2]: Epoch 26/100, Acc=0.0100, Val Loss=4.6058, lr=0.0100
[02/26 15:51:31 cifar100-global-random-3.0-mobilenetv2]: Epoch 27/100, Acc=0.0100, Val Loss=4.6056, lr=0.0100
[02/26 15:52:02 cifar100-global-random-3.0-mobilenetv2]: Epoch 28/100, Acc=0.0100, Val Loss=4.6056, lr=0.0100
[02/26 15:52:32 cifar100-global-random-3.0-mobilenetv2]: Epoch 29/100, Acc=0.0100, Val Loss=4.6055, lr=0.0100
[02/26 15:53:03 cifar100-global-random-3.0-mobilenetv2]: Epoch 30/100, Acc=0.0100, Val Loss=4.6055, lr=0.0100
[02/26 15:53:34 cifar100-global-random-3.0-mobilenetv2]: Epoch 31/100, Acc=0.0100, Val Loss=4.6055, lr=0.0100
[02/26 15:54:05 cifar100-global-random-3.0-mobilenetv2]: Epoch 32/100, Acc=0.0100, Val Loss=4.6055, lr=0.0100
[02/26 15:54:36 cifar100-global-random-3.0-mobilenetv2]: Epoch 33/100, Acc=0.0100, Val Loss=4.6054, lr=0.0100
[02/26 15:55:07 cifar100-global-random-3.0-mobilenetv2]: Epoch 34/100, Acc=0.0100, Val Loss=4.6053, lr=0.0100
[02/26 15:55:38 cifar100-global-random-3.0-mobilenetv2]: Epoch 35/100, Acc=0.0100, Val Loss=4.6054, lr=0.0100
[02/26 15:56:09 cifar100-global-random-3.0-mobilenetv2]: Epoch 36/100, Acc=0.0100, Val Loss=4.6053, lr=0.0100
[02/26 15:56:41 cifar100-global-random-3.0-mobilenetv2]: Epoch 37/100, Acc=0.0100, Val Loss=4.6053, lr=0.0100
[02/26 15:57:12 cifar100-global-random-3.0-mobilenetv2]: Epoch 38/100, Acc=0.0100, Val Loss=4.6053, lr=0.0100
[02/26 15:57:43 cifar100-global-random-3.0-mobilenetv2]: Epoch 39/100, Acc=0.0100, Val Loss=4.6053, lr=0.0100
[02/26 15:58:14 cifar100-global-random-3.0-mobilenetv2]: Epoch 40/100, Acc=0.0100, Val Loss=4.6053, lr=0.0100
[02/26 15:58:46 cifar100-global-random-3.0-mobilenetv2]: Epoch 41/100, Acc=0.0100, Val Loss=4.6053, lr=0.0100
[02/26 15:59:17 cifar100-global-random-3.0-mobilenetv2]: Epoch 42/100, Acc=0.0100, Val Loss=4.6053, lr=0.0100
[02/26 15:59:48 cifar100-global-random-3.0-mobilenetv2]: Epoch 43/100, Acc=0.0100, Val Loss=4.6053, lr=0.0100
[02/26 16:00:20 cifar100-global-random-3.0-mobilenetv2]: Epoch 44/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/26 16:00:51 cifar100-global-random-3.0-mobilenetv2]: Epoch 45/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/26 16:01:23 cifar100-global-random-3.0-mobilenetv2]: Epoch 46/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/26 16:01:55 cifar100-global-random-3.0-mobilenetv2]: Epoch 47/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/26 16:02:27 cifar100-global-random-3.0-mobilenetv2]: Epoch 48/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/26 16:02:58 cifar100-global-random-3.0-mobilenetv2]: Epoch 49/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/26 16:03:29 cifar100-global-random-3.0-mobilenetv2]: Epoch 50/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/26 16:04:01 cifar100-global-random-3.0-mobilenetv2]: Epoch 51/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/26 16:04:33 cifar100-global-random-3.0-mobilenetv2]: Epoch 52/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/26 16:05:05 cifar100-global-random-3.0-mobilenetv2]: Epoch 53/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/26 16:05:37 cifar100-global-random-3.0-mobilenetv2]: Epoch 54/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/26 16:06:08 cifar100-global-random-3.0-mobilenetv2]: Epoch 55/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/26 16:06:40 cifar100-global-random-3.0-mobilenetv2]: Epoch 56/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/26 16:07:12 cifar100-global-random-3.0-mobilenetv2]: Epoch 57/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/26 16:07:44 cifar100-global-random-3.0-mobilenetv2]: Epoch 58/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/26 16:08:17 cifar100-global-random-3.0-mobilenetv2]: Epoch 59/100, Acc=0.0100, Val Loss=4.6052, lr=0.0100
[02/26 16:08:48 cifar100-global-random-3.0-mobilenetv2]: Epoch 60/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/26 16:09:20 cifar100-global-random-3.0-mobilenetv2]: Epoch 61/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/26 16:09:52 cifar100-global-random-3.0-mobilenetv2]: Epoch 62/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/26 16:10:24 cifar100-global-random-3.0-mobilenetv2]: Epoch 63/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/26 16:10:56 cifar100-global-random-3.0-mobilenetv2]: Epoch 64/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/26 16:11:28 cifar100-global-random-3.0-mobilenetv2]: Epoch 65/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/26 16:12:00 cifar100-global-random-3.0-mobilenetv2]: Epoch 66/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/26 16:12:32 cifar100-global-random-3.0-mobilenetv2]: Epoch 67/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/26 16:13:04 cifar100-global-random-3.0-mobilenetv2]: Epoch 68/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/26 16:13:35 cifar100-global-random-3.0-mobilenetv2]: Epoch 69/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/26 16:14:07 cifar100-global-random-3.0-mobilenetv2]: Epoch 70/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/26 16:14:38 cifar100-global-random-3.0-mobilenetv2]: Epoch 71/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/26 16:15:10 cifar100-global-random-3.0-mobilenetv2]: Epoch 72/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/26 16:15:40 cifar100-global-random-3.0-mobilenetv2]: Epoch 73/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/26 16:16:11 cifar100-global-random-3.0-mobilenetv2]: Epoch 74/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/26 16:16:43 cifar100-global-random-3.0-mobilenetv2]: Epoch 75/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/26 16:17:14 cifar100-global-random-3.0-mobilenetv2]: Epoch 76/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/26 16:17:45 cifar100-global-random-3.0-mobilenetv2]: Epoch 77/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/26 16:18:16 cifar100-global-random-3.0-mobilenetv2]: Epoch 78/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/26 16:18:47 cifar100-global-random-3.0-mobilenetv2]: Epoch 79/100, Acc=0.0100, Val Loss=4.6052, lr=0.0010
[02/26 16:19:19 cifar100-global-random-3.0-mobilenetv2]: Epoch 80/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/26 16:19:50 cifar100-global-random-3.0-mobilenetv2]: Epoch 81/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/26 16:20:21 cifar100-global-random-3.0-mobilenetv2]: Epoch 82/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/26 16:20:52 cifar100-global-random-3.0-mobilenetv2]: Epoch 83/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/26 16:21:23 cifar100-global-random-3.0-mobilenetv2]: Epoch 84/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/26 16:21:54 cifar100-global-random-3.0-mobilenetv2]: Epoch 85/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/26 16:22:25 cifar100-global-random-3.0-mobilenetv2]: Epoch 86/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/26 16:22:56 cifar100-global-random-3.0-mobilenetv2]: Epoch 87/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/26 16:23:27 cifar100-global-random-3.0-mobilenetv2]: Epoch 88/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/26 16:23:58 cifar100-global-random-3.0-mobilenetv2]: Epoch 89/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/26 16:24:29 cifar100-global-random-3.0-mobilenetv2]: Epoch 90/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/26 16:25:00 cifar100-global-random-3.0-mobilenetv2]: Epoch 91/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/26 16:25:30 cifar100-global-random-3.0-mobilenetv2]: Epoch 92/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/26 16:26:01 cifar100-global-random-3.0-mobilenetv2]: Epoch 93/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/26 16:26:32 cifar100-global-random-3.0-mobilenetv2]: Epoch 94/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/26 16:27:03 cifar100-global-random-3.0-mobilenetv2]: Epoch 95/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/26 16:27:34 cifar100-global-random-3.0-mobilenetv2]: Epoch 96/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/26 16:28:05 cifar100-global-random-3.0-mobilenetv2]: Epoch 97/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/26 16:28:36 cifar100-global-random-3.0-mobilenetv2]: Epoch 98/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/26 16:29:07 cifar100-global-random-3.0-mobilenetv2]: Epoch 99/100, Acc=0.0100, Val Loss=4.6052, lr=0.0001
[02/26 16:29:07 cifar100-global-random-3.0-mobilenetv2]: Best Acc=0.0100
[02/26 16:29:07 cifar100-global-random-3.0-mobilenetv2]: Params: 0.99 M
[02/26 16:29:07 cifar100-global-random-3.0-mobilenetv2]: ops: 22.75 M
[02/26 16:29:10 cifar100-global-random-3.0-mobilenetv2]: Acc: 0.0100 Val Loss: 4.6052

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: mode: prune
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: model: mobilenetv2
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: verbose: False
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: dataset: cifar100
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: dataroot: data
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: batch_size: 128
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: total_epochs: 100
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: lr: 0.01
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: restore: run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: output_dir: run/cifar100/prune/cifar100-global-l2-3.0-mobilenetv2
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: finetune: True
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: last_epochs: 100
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: reps: 1
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: method: l2
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: speed_up: 3.0
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: reg: 1e-05
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: delta_reg: 0.0001
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: weight_decay: 0.0005
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: seed: 1
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: global_pruning: True
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: sl_total_epochs: 100
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: sl_lr: 0.01
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: sl_reg_warmup: 0
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: sl_restore: None
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: iterative_steps: 400
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: logger: <Logger cifar100-global-l2-3.0-mobilenetv2 (DEBUG)>
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: device: cuda
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: num_classes: 100
[02/26 16:29:18 cifar100-global-l2-3.0-mobilenetv2]: Loading model from run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/26 16:29:21 cifar100-global-l2-3.0-mobilenetv2]: Pruning...
[02/26 16:29:54 cifar100-global-l2-3.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 9, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(9, 15, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=15)
      (4): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(15, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72)
        (4): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 78, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(78, 78, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=78)
        (4): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(78, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 71, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(71, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(71, 71, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=71)
        (4): BatchNorm2d(71, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(71, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 82, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(82, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(82, 82, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=82)
        (4): BatchNorm2d(82, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(82, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 55, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=55)
        (4): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(55, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 81, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(81, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(81, 81, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=81)
        (4): BatchNorm2d(81, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(81, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 33, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(33, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=33)
        (4): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(33, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 14, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(14, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14)
        (4): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(14, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 20, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20)
        (4): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(20, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 175, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(175, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(175, 175, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=175)
        (4): BatchNorm2d(175, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(175, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 43, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(43, 43, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=43)
        (4): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(43, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 29, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(29, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=29)
        (4): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(29, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 172, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(172, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(172, 172, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=172)
        (4): BatchNorm2d(172, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(172, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 94, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(94, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(94, 94, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=94)
        (4): BatchNorm2d(94, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(94, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 69, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(69, 69, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=69)
        (4): BatchNorm2d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(69, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 102, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(102, 102, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=102)
      (4): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(102, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 100, kernel_size=(1, 1), stride=(1, 1))
)
[02/26 16:29:58 cifar100-global-l2-3.0-mobilenetv2]: Params: 2.37 M => 0.79 M (33.23%)
[02/26 16:29:58 cifar100-global-l2-3.0-mobilenetv2]: FLOPs: 68.40 M => 22.75 M (33.26%, 3.01X )
[02/26 16:29:58 cifar100-global-l2-3.0-mobilenetv2]: Acc: 0.6699 => 0.6699
[02/26 16:29:58 cifar100-global-l2-3.0-mobilenetv2]: Val Loss: 1.1637 => 1.1637
[02/26 16:29:58 cifar100-global-l2-3.0-mobilenetv2]: Finetuning...
[02/26 16:30:27 cifar100-global-l2-3.0-mobilenetv2]: Epoch 0/100, Acc=0.5891, Val Loss=1.4707, lr=0.0100
[02/26 16:30:57 cifar100-global-l2-3.0-mobilenetv2]: Epoch 1/100, Acc=0.6099, Val Loss=1.3783, lr=0.0100
[02/26 16:31:27 cifar100-global-l2-3.0-mobilenetv2]: Epoch 2/100, Acc=0.5934, Val Loss=1.4568, lr=0.0100
[02/26 16:31:57 cifar100-global-l2-3.0-mobilenetv2]: Epoch 3/100, Acc=0.5992, Val Loss=1.4294, lr=0.0100
[02/26 16:32:27 cifar100-global-l2-3.0-mobilenetv2]: Epoch 4/100, Acc=0.5913, Val Loss=1.4621, lr=0.0100
[02/26 16:32:57 cifar100-global-l2-3.0-mobilenetv2]: Epoch 5/100, Acc=0.6065, Val Loss=1.3917, lr=0.0100
[02/26 16:33:27 cifar100-global-l2-3.0-mobilenetv2]: Epoch 6/100, Acc=0.6000, Val Loss=1.4379, lr=0.0100
[02/26 16:33:57 cifar100-global-l2-3.0-mobilenetv2]: Epoch 7/100, Acc=0.6062, Val Loss=1.4106, lr=0.0100
[02/26 16:34:27 cifar100-global-l2-3.0-mobilenetv2]: Epoch 8/100, Acc=0.6078, Val Loss=1.3803, lr=0.0100
[02/26 16:34:57 cifar100-global-l2-3.0-mobilenetv2]: Epoch 9/100, Acc=0.5993, Val Loss=1.4347, lr=0.0100
[02/26 16:35:28 cifar100-global-l2-3.0-mobilenetv2]: Epoch 10/100, Acc=0.5958, Val Loss=1.4139, lr=0.0100
[02/26 16:35:58 cifar100-global-l2-3.0-mobilenetv2]: Epoch 11/100, Acc=0.6152, Val Loss=1.3753, lr=0.0100
[02/26 16:36:29 cifar100-global-l2-3.0-mobilenetv2]: Epoch 12/100, Acc=0.5958, Val Loss=1.4236, lr=0.0100
[02/26 16:36:59 cifar100-global-l2-3.0-mobilenetv2]: Epoch 13/100, Acc=0.5960, Val Loss=1.4573, lr=0.0100
[02/26 16:37:30 cifar100-global-l2-3.0-mobilenetv2]: Epoch 14/100, Acc=0.5970, Val Loss=1.4455, lr=0.0100
[02/26 16:38:01 cifar100-global-l2-3.0-mobilenetv2]: Epoch 15/100, Acc=0.6080, Val Loss=1.3895, lr=0.0100
[02/26 16:38:31 cifar100-global-l2-3.0-mobilenetv2]: Epoch 16/100, Acc=0.6003, Val Loss=1.4369, lr=0.0100
[02/26 16:39:02 cifar100-global-l2-3.0-mobilenetv2]: Epoch 17/100, Acc=0.5977, Val Loss=1.4366, lr=0.0100
[02/26 16:39:33 cifar100-global-l2-3.0-mobilenetv2]: Epoch 18/100, Acc=0.6172, Val Loss=1.3609, lr=0.0100
[02/26 16:40:04 cifar100-global-l2-3.0-mobilenetv2]: Epoch 19/100, Acc=0.6047, Val Loss=1.4122, lr=0.0100
[02/26 16:40:34 cifar100-global-l2-3.0-mobilenetv2]: Epoch 20/100, Acc=0.5993, Val Loss=1.4199, lr=0.0100
[02/26 16:41:05 cifar100-global-l2-3.0-mobilenetv2]: Epoch 21/100, Acc=0.6086, Val Loss=1.3649, lr=0.0100
[02/26 16:41:35 cifar100-global-l2-3.0-mobilenetv2]: Epoch 22/100, Acc=0.6039, Val Loss=1.4297, lr=0.0100
[02/26 16:42:06 cifar100-global-l2-3.0-mobilenetv2]: Epoch 23/100, Acc=0.5962, Val Loss=1.4322, lr=0.0100
[02/26 16:42:37 cifar100-global-l2-3.0-mobilenetv2]: Epoch 24/100, Acc=0.6107, Val Loss=1.3779, lr=0.0100
[02/26 16:43:08 cifar100-global-l2-3.0-mobilenetv2]: Epoch 25/100, Acc=0.5879, Val Loss=1.4771, lr=0.0100
[02/26 16:43:38 cifar100-global-l2-3.0-mobilenetv2]: Epoch 26/100, Acc=0.6065, Val Loss=1.3930, lr=0.0100
[02/26 16:44:09 cifar100-global-l2-3.0-mobilenetv2]: Epoch 27/100, Acc=0.5918, Val Loss=1.4760, lr=0.0100
[02/26 16:44:40 cifar100-global-l2-3.0-mobilenetv2]: Epoch 28/100, Acc=0.6046, Val Loss=1.3880, lr=0.0100
[02/26 16:45:10 cifar100-global-l2-3.0-mobilenetv2]: Epoch 29/100, Acc=0.6026, Val Loss=1.4288, lr=0.0100
[02/26 16:45:41 cifar100-global-l2-3.0-mobilenetv2]: Epoch 30/100, Acc=0.6048, Val Loss=1.4136, lr=0.0100
[02/26 16:46:12 cifar100-global-l2-3.0-mobilenetv2]: Epoch 31/100, Acc=0.6034, Val Loss=1.4241, lr=0.0100
[02/26 16:46:43 cifar100-global-l2-3.0-mobilenetv2]: Epoch 32/100, Acc=0.6061, Val Loss=1.4026, lr=0.0100
[02/26 16:47:13 cifar100-global-l2-3.0-mobilenetv2]: Epoch 33/100, Acc=0.6148, Val Loss=1.3507, lr=0.0100
[02/26 16:47:44 cifar100-global-l2-3.0-mobilenetv2]: Epoch 34/100, Acc=0.6083, Val Loss=1.3830, lr=0.0100
[02/26 16:48:15 cifar100-global-l2-3.0-mobilenetv2]: Epoch 35/100, Acc=0.6122, Val Loss=1.3899, lr=0.0100
[02/26 16:48:46 cifar100-global-l2-3.0-mobilenetv2]: Epoch 36/100, Acc=0.6144, Val Loss=1.3749, lr=0.0100
[02/26 16:49:17 cifar100-global-l2-3.0-mobilenetv2]: Epoch 37/100, Acc=0.6110, Val Loss=1.3561, lr=0.0100
[02/26 16:49:48 cifar100-global-l2-3.0-mobilenetv2]: Epoch 38/100, Acc=0.6023, Val Loss=1.4312, lr=0.0100
[02/26 16:50:18 cifar100-global-l2-3.0-mobilenetv2]: Epoch 39/100, Acc=0.5988, Val Loss=1.4481, lr=0.0100
[02/26 16:50:49 cifar100-global-l2-3.0-mobilenetv2]: Epoch 40/100, Acc=0.6140, Val Loss=1.3641, lr=0.0100
[02/26 16:51:20 cifar100-global-l2-3.0-mobilenetv2]: Epoch 41/100, Acc=0.6139, Val Loss=1.3759, lr=0.0100
[02/26 16:51:51 cifar100-global-l2-3.0-mobilenetv2]: Epoch 42/100, Acc=0.6110, Val Loss=1.3862, lr=0.0100
[02/26 16:52:22 cifar100-global-l2-3.0-mobilenetv2]: Epoch 43/100, Acc=0.6179, Val Loss=1.3801, lr=0.0100
[02/26 16:52:53 cifar100-global-l2-3.0-mobilenetv2]: Epoch 44/100, Acc=0.5660, Val Loss=1.5949, lr=0.0100
[02/26 16:53:24 cifar100-global-l2-3.0-mobilenetv2]: Epoch 45/100, Acc=0.6101, Val Loss=1.3999, lr=0.0100
[02/26 16:53:56 cifar100-global-l2-3.0-mobilenetv2]: Epoch 46/100, Acc=0.6038, Val Loss=1.4053, lr=0.0100
[02/26 16:54:27 cifar100-global-l2-3.0-mobilenetv2]: Epoch 47/100, Acc=0.6030, Val Loss=1.4193, lr=0.0100
[02/26 16:54:59 cifar100-global-l2-3.0-mobilenetv2]: Epoch 48/100, Acc=0.6131, Val Loss=1.3789, lr=0.0100
[02/26 16:55:31 cifar100-global-l2-3.0-mobilenetv2]: Epoch 49/100, Acc=0.6186, Val Loss=1.3676, lr=0.0100
[02/26 16:56:03 cifar100-global-l2-3.0-mobilenetv2]: Epoch 50/100, Acc=0.6066, Val Loss=1.3945, lr=0.0100
[02/26 16:56:34 cifar100-global-l2-3.0-mobilenetv2]: Epoch 51/100, Acc=0.6162, Val Loss=1.3505, lr=0.0100
[02/26 16:57:06 cifar100-global-l2-3.0-mobilenetv2]: Epoch 52/100, Acc=0.6146, Val Loss=1.3988, lr=0.0100
[02/26 16:57:38 cifar100-global-l2-3.0-mobilenetv2]: Epoch 53/100, Acc=0.6153, Val Loss=1.3627, lr=0.0100
[02/26 16:58:10 cifar100-global-l2-3.0-mobilenetv2]: Epoch 54/100, Acc=0.6069, Val Loss=1.4206, lr=0.0100
[02/26 16:58:41 cifar100-global-l2-3.0-mobilenetv2]: Epoch 55/100, Acc=0.6133, Val Loss=1.3651, lr=0.0100
[02/26 16:59:12 cifar100-global-l2-3.0-mobilenetv2]: Epoch 56/100, Acc=0.6074, Val Loss=1.3836, lr=0.0100
[02/26 16:59:44 cifar100-global-l2-3.0-mobilenetv2]: Epoch 57/100, Acc=0.6286, Val Loss=1.3323, lr=0.0100
[02/26 17:00:15 cifar100-global-l2-3.0-mobilenetv2]: Epoch 58/100, Acc=0.6076, Val Loss=1.4150, lr=0.0100
[02/26 17:00:46 cifar100-global-l2-3.0-mobilenetv2]: Epoch 59/100, Acc=0.6019, Val Loss=1.4361, lr=0.0100
[02/26 17:01:17 cifar100-global-l2-3.0-mobilenetv2]: Epoch 60/100, Acc=0.6700, Val Loss=1.1405, lr=0.0010
[02/26 17:01:48 cifar100-global-l2-3.0-mobilenetv2]: Epoch 61/100, Acc=0.6728, Val Loss=1.1396, lr=0.0010
[02/26 17:02:19 cifar100-global-l2-3.0-mobilenetv2]: Epoch 62/100, Acc=0.6771, Val Loss=1.1285, lr=0.0010
[02/26 17:02:50 cifar100-global-l2-3.0-mobilenetv2]: Epoch 63/100, Acc=0.6781, Val Loss=1.1315, lr=0.0010
[02/26 17:03:21 cifar100-global-l2-3.0-mobilenetv2]: Epoch 64/100, Acc=0.6750, Val Loss=1.1336, lr=0.0010
[02/26 17:03:52 cifar100-global-l2-3.0-mobilenetv2]: Epoch 65/100, Acc=0.6771, Val Loss=1.1341, lr=0.0010
[02/26 17:04:23 cifar100-global-l2-3.0-mobilenetv2]: Epoch 66/100, Acc=0.6792, Val Loss=1.1287, lr=0.0010
[02/26 17:04:54 cifar100-global-l2-3.0-mobilenetv2]: Epoch 67/100, Acc=0.6780, Val Loss=1.1338, lr=0.0010
[02/26 17:05:25 cifar100-global-l2-3.0-mobilenetv2]: Epoch 68/100, Acc=0.6759, Val Loss=1.1389, lr=0.0010
[02/26 17:05:56 cifar100-global-l2-3.0-mobilenetv2]: Epoch 69/100, Acc=0.6777, Val Loss=1.1355, lr=0.0010
[02/26 17:06:27 cifar100-global-l2-3.0-mobilenetv2]: Epoch 70/100, Acc=0.6781, Val Loss=1.1368, lr=0.0010
[02/26 17:06:58 cifar100-global-l2-3.0-mobilenetv2]: Epoch 71/100, Acc=0.6796, Val Loss=1.1380, lr=0.0010
[02/26 17:07:29 cifar100-global-l2-3.0-mobilenetv2]: Epoch 72/100, Acc=0.6769, Val Loss=1.1421, lr=0.0010
[02/26 17:07:59 cifar100-global-l2-3.0-mobilenetv2]: Epoch 73/100, Acc=0.6747, Val Loss=1.1427, lr=0.0010
[02/26 17:08:30 cifar100-global-l2-3.0-mobilenetv2]: Epoch 74/100, Acc=0.6774, Val Loss=1.1483, lr=0.0010
[02/26 17:09:01 cifar100-global-l2-3.0-mobilenetv2]: Epoch 75/100, Acc=0.6766, Val Loss=1.1463, lr=0.0010
[02/26 17:09:31 cifar100-global-l2-3.0-mobilenetv2]: Epoch 76/100, Acc=0.6780, Val Loss=1.1433, lr=0.0010
[02/26 17:10:02 cifar100-global-l2-3.0-mobilenetv2]: Epoch 77/100, Acc=0.6748, Val Loss=1.1496, lr=0.0010
[02/26 17:10:33 cifar100-global-l2-3.0-mobilenetv2]: Epoch 78/100, Acc=0.6811, Val Loss=1.1453, lr=0.0010
[02/26 17:11:04 cifar100-global-l2-3.0-mobilenetv2]: Epoch 79/100, Acc=0.6741, Val Loss=1.1543, lr=0.0010
[02/26 17:11:35 cifar100-global-l2-3.0-mobilenetv2]: Epoch 80/100, Acc=0.6771, Val Loss=1.1430, lr=0.0001
[02/26 17:12:06 cifar100-global-l2-3.0-mobilenetv2]: Epoch 81/100, Acc=0.6803, Val Loss=1.1390, lr=0.0001
[02/26 17:12:37 cifar100-global-l2-3.0-mobilenetv2]: Epoch 82/100, Acc=0.6791, Val Loss=1.1408, lr=0.0001
[02/26 17:13:08 cifar100-global-l2-3.0-mobilenetv2]: Epoch 83/100, Acc=0.6790, Val Loss=1.1386, lr=0.0001
[02/26 17:13:39 cifar100-global-l2-3.0-mobilenetv2]: Epoch 84/100, Acc=0.6808, Val Loss=1.1377, lr=0.0001
[02/26 17:14:09 cifar100-global-l2-3.0-mobilenetv2]: Epoch 85/100, Acc=0.6803, Val Loss=1.1404, lr=0.0001
[02/26 17:14:40 cifar100-global-l2-3.0-mobilenetv2]: Epoch 86/100, Acc=0.6805, Val Loss=1.1387, lr=0.0001
[02/26 17:15:11 cifar100-global-l2-3.0-mobilenetv2]: Epoch 87/100, Acc=0.6793, Val Loss=1.1392, lr=0.0001
[02/26 17:15:41 cifar100-global-l2-3.0-mobilenetv2]: Epoch 88/100, Acc=0.6798, Val Loss=1.1390, lr=0.0001
[02/26 17:16:12 cifar100-global-l2-3.0-mobilenetv2]: Epoch 89/100, Acc=0.6823, Val Loss=1.1382, lr=0.0001
[02/26 17:16:43 cifar100-global-l2-3.0-mobilenetv2]: Epoch 90/100, Acc=0.6789, Val Loss=1.1400, lr=0.0001
[02/26 17:17:13 cifar100-global-l2-3.0-mobilenetv2]: Epoch 91/100, Acc=0.6801, Val Loss=1.1410, lr=0.0001
[02/26 17:17:44 cifar100-global-l2-3.0-mobilenetv2]: Epoch 92/100, Acc=0.6799, Val Loss=1.1418, lr=0.0001
[02/26 17:18:15 cifar100-global-l2-3.0-mobilenetv2]: Epoch 93/100, Acc=0.6800, Val Loss=1.1402, lr=0.0001
[02/26 17:18:45 cifar100-global-l2-3.0-mobilenetv2]: Epoch 94/100, Acc=0.6808, Val Loss=1.1406, lr=0.0001
[02/26 17:19:16 cifar100-global-l2-3.0-mobilenetv2]: Epoch 95/100, Acc=0.6792, Val Loss=1.1410, lr=0.0001
[02/26 17:19:46 cifar100-global-l2-3.0-mobilenetv2]: Epoch 96/100, Acc=0.6772, Val Loss=1.1384, lr=0.0001
[02/26 17:20:17 cifar100-global-l2-3.0-mobilenetv2]: Epoch 97/100, Acc=0.6794, Val Loss=1.1428, lr=0.0001
[02/26 17:20:48 cifar100-global-l2-3.0-mobilenetv2]: Epoch 98/100, Acc=0.6794, Val Loss=1.1416, lr=0.0001
[02/26 17:21:18 cifar100-global-l2-3.0-mobilenetv2]: Epoch 99/100, Acc=0.6786, Val Loss=1.1411, lr=0.0001
[02/26 17:21:18 cifar100-global-l2-3.0-mobilenetv2]: Best Acc=0.6823
[02/26 17:21:18 cifar100-global-l2-3.0-mobilenetv2]: Params: 0.79 M
[02/26 17:21:18 cifar100-global-l2-3.0-mobilenetv2]: ops: 22.75 M
[02/26 17:21:22 cifar100-global-l2-3.0-mobilenetv2]: Acc: 0.6786 Val Loss: 1.1411

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: mode: prune
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: model: mobilenetv2
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: verbose: False
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: dataset: cifar100
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: dataroot: data
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: batch_size: 128
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: total_epochs: 100
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: lr: 0.01
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: restore: run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: output_dir: run/cifar100/prune/cifar100-global-fpgm-3.0-mobilenetv2
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: finetune: True
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: last_epochs: 100
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: reps: 1
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: method: fpgm
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: speed_up: 3.0
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: reg: 1e-05
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: delta_reg: 0.0001
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: weight_decay: 0.0005
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: seed: 1
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: global_pruning: True
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: sl_total_epochs: 100
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: sl_lr: 0.01
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: sl_reg_warmup: 0
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: sl_restore: None
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: iterative_steps: 400
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: logger: <Logger cifar100-global-fpgm-3.0-mobilenetv2 (DEBUG)>
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: device: cuda
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: num_classes: 100
[02/26 17:21:30 cifar100-global-fpgm-3.0-mobilenetv2]: Loading model from run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/26 17:21:34 cifar100-global-fpgm-3.0-mobilenetv2]: Pruning...
[02/26 17:21:59 cifar100-global-fpgm-3.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 9, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(9, 15, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=15)
      (4): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(15, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 73, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(73, 73, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=73)
        (4): BatchNorm2d(73, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(73, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 84, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(84, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=84)
        (4): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(84, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 76, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(76, 76, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=76)
        (4): BatchNorm2d(76, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(76, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 85, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(85, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(85, 85, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=85)
        (4): BatchNorm2d(85, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(85, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 61, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(61, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=61)
        (4): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(61, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 88, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(88, 88, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=88)
        (4): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(88, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 38, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(38, 38, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=38)
        (4): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(38, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 9, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=9)
        (4): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(9, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 13, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(13, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=13)
        (4): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(13, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 188, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(188, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(188, 188, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=188)
        (4): BatchNorm2d(188, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(188, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 33, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(33, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=33)
        (4): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(33, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 15, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=15)
        (4): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(15, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 189, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(189, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(189, 189, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=189)
        (4): BatchNorm2d(189, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(189, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 102, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(102, 102, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=102)
        (4): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(102, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 45, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(45, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=45)
        (4): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(45, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 110, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(110, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(110, 110, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=110)
      (4): BatchNorm2d(110, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(110, 283, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(283, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(283, 1278, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1278, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1278, 100, kernel_size=(1, 1), stride=(1, 1))
)
[02/26 17:22:03 cifar100-global-fpgm-3.0-mobilenetv2]: Params: 2.37 M => 0.74 M (31.09%)
[02/26 17:22:03 cifar100-global-fpgm-3.0-mobilenetv2]: FLOPs: 68.40 M => 21.73 M (31.77%, 3.15X )
[02/26 17:22:03 cifar100-global-fpgm-3.0-mobilenetv2]: Acc: 0.6699 => 0.6671
[02/26 17:22:03 cifar100-global-fpgm-3.0-mobilenetv2]: Val Loss: 1.1637 => 1.1706
[02/26 17:22:03 cifar100-global-fpgm-3.0-mobilenetv2]: Finetuning...
[02/26 17:22:33 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 0/100, Acc=0.5948, Val Loss=1.4458, lr=0.0100
[02/26 17:23:03 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 1/100, Acc=0.5989, Val Loss=1.4451, lr=0.0100
[02/26 17:23:33 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 2/100, Acc=0.5927, Val Loss=1.4607, lr=0.0100
[02/26 17:24:04 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 3/100, Acc=0.6015, Val Loss=1.4087, lr=0.0100
[02/26 17:24:35 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 4/100, Acc=0.5831, Val Loss=1.5054, lr=0.0100
[02/26 17:25:06 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 5/100, Acc=0.5917, Val Loss=1.4236, lr=0.0100
[02/26 17:25:36 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 6/100, Acc=0.5992, Val Loss=1.4206, lr=0.0100
[02/26 17:26:07 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 7/100, Acc=0.5969, Val Loss=1.4277, lr=0.0100
[02/26 17:26:37 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 8/100, Acc=0.6004, Val Loss=1.4093, lr=0.0100
[02/26 17:27:08 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 9/100, Acc=0.6092, Val Loss=1.3817, lr=0.0100
[02/26 17:27:39 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 10/100, Acc=0.6100, Val Loss=1.3818, lr=0.0100
[02/26 17:28:09 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 11/100, Acc=0.6178, Val Loss=1.3778, lr=0.0100
[02/26 17:28:40 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 12/100, Acc=0.6036, Val Loss=1.4219, lr=0.0100
[02/26 17:29:10 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 13/100, Acc=0.5790, Val Loss=1.5157, lr=0.0100
[02/26 17:29:41 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 14/100, Acc=0.5886, Val Loss=1.4660, lr=0.0100
[02/26 17:30:11 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 15/100, Acc=0.6077, Val Loss=1.3858, lr=0.0100
[02/26 17:30:42 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 16/100, Acc=0.6001, Val Loss=1.4268, lr=0.0100
[02/26 17:31:12 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 17/100, Acc=0.5909, Val Loss=1.4486, lr=0.0100
[02/26 17:31:42 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 18/100, Acc=0.6102, Val Loss=1.3840, lr=0.0100
[02/26 17:32:13 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 19/100, Acc=0.5983, Val Loss=1.4347, lr=0.0100
[02/26 17:32:44 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 20/100, Acc=0.6143, Val Loss=1.3615, lr=0.0100
[02/26 17:33:14 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 21/100, Acc=0.5979, Val Loss=1.4419, lr=0.0100
[02/26 17:33:45 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 22/100, Acc=0.6170, Val Loss=1.3535, lr=0.0100
[02/26 17:34:15 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 23/100, Acc=0.6020, Val Loss=1.4195, lr=0.0100
[02/26 17:34:46 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 24/100, Acc=0.5956, Val Loss=1.4611, lr=0.0100
[02/26 17:35:17 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 25/100, Acc=0.5931, Val Loss=1.4439, lr=0.0100
[02/26 17:35:47 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 26/100, Acc=0.5843, Val Loss=1.5370, lr=0.0100
[02/26 17:36:18 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 27/100, Acc=0.6047, Val Loss=1.4019, lr=0.0100
[02/26 17:36:49 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 28/100, Acc=0.6243, Val Loss=1.3347, lr=0.0100
[02/26 17:37:20 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 29/100, Acc=0.6113, Val Loss=1.3804, lr=0.0100
[02/26 17:37:50 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 30/100, Acc=0.6077, Val Loss=1.4062, lr=0.0100
[02/26 17:38:20 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 31/100, Acc=0.6182, Val Loss=1.3592, lr=0.0100
[02/26 17:38:51 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 32/100, Acc=0.5965, Val Loss=1.4455, lr=0.0100
[02/26 17:39:22 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 33/100, Acc=0.6047, Val Loss=1.4204, lr=0.0100
[02/26 17:39:52 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 34/100, Acc=0.6165, Val Loss=1.3705, lr=0.0100
[02/26 17:40:23 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 35/100, Acc=0.6213, Val Loss=1.3554, lr=0.0100
[02/26 17:40:53 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 36/100, Acc=0.6066, Val Loss=1.3996, lr=0.0100
[02/26 17:41:24 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 37/100, Acc=0.6089, Val Loss=1.3873, lr=0.0100
[02/26 17:41:54 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 38/100, Acc=0.6203, Val Loss=1.3304, lr=0.0100
[02/26 17:42:25 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 39/100, Acc=0.6029, Val Loss=1.4286, lr=0.0100
[02/26 17:42:56 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 40/100, Acc=0.6157, Val Loss=1.3778, lr=0.0100
[02/26 17:43:26 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 41/100, Acc=0.6254, Val Loss=1.3172, lr=0.0100
[02/26 17:43:57 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 42/100, Acc=0.6033, Val Loss=1.4167, lr=0.0100
[02/26 17:44:27 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 43/100, Acc=0.6201, Val Loss=1.3438, lr=0.0100
[02/26 17:44:58 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 44/100, Acc=0.5908, Val Loss=1.4927, lr=0.0100
[02/26 17:45:28 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 45/100, Acc=0.6143, Val Loss=1.3611, lr=0.0100
[02/26 17:45:59 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 46/100, Acc=0.5942, Val Loss=1.4568, lr=0.0100
[02/26 17:46:29 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 47/100, Acc=0.6095, Val Loss=1.4041, lr=0.0100
[02/26 17:46:59 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 48/100, Acc=0.6186, Val Loss=1.3383, lr=0.0100
[02/26 17:47:30 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 49/100, Acc=0.6103, Val Loss=1.3990, lr=0.0100
[02/26 17:48:00 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 50/100, Acc=0.6286, Val Loss=1.3290, lr=0.0100
[02/26 17:48:31 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 51/100, Acc=0.6068, Val Loss=1.3829, lr=0.0100
[02/26 17:49:02 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 52/100, Acc=0.6005, Val Loss=1.4550, lr=0.0100
[02/26 17:49:32 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 53/100, Acc=0.6189, Val Loss=1.3818, lr=0.0100
[02/26 17:50:03 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 54/100, Acc=0.6204, Val Loss=1.3362, lr=0.0100
[02/26 17:50:34 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 55/100, Acc=0.5986, Val Loss=1.4292, lr=0.0100
[02/26 17:51:05 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 56/100, Acc=0.6168, Val Loss=1.3462, lr=0.0100
[02/26 17:51:35 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 57/100, Acc=0.6347, Val Loss=1.3133, lr=0.0100
[02/26 17:52:07 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 58/100, Acc=0.6083, Val Loss=1.4115, lr=0.0100
[02/26 17:52:38 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 59/100, Acc=0.6037, Val Loss=1.3988, lr=0.0100
[02/26 17:53:09 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 60/100, Acc=0.6776, Val Loss=1.1386, lr=0.0010
[02/26 17:53:40 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 61/100, Acc=0.6768, Val Loss=1.1337, lr=0.0010
[02/26 17:54:11 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 62/100, Acc=0.6812, Val Loss=1.1208, lr=0.0010
[02/26 17:54:42 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 63/100, Acc=0.6797, Val Loss=1.1231, lr=0.0010
[02/26 17:55:13 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 64/100, Acc=0.6821, Val Loss=1.1209, lr=0.0010
[02/26 17:55:45 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 65/100, Acc=0.6814, Val Loss=1.1216, lr=0.0010
[02/26 17:56:16 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 66/100, Acc=0.6830, Val Loss=1.1160, lr=0.0010
[02/26 17:56:52 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 67/100, Acc=0.6797, Val Loss=1.1207, lr=0.0010
[02/26 17:57:29 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 68/100, Acc=0.6822, Val Loss=1.1218, lr=0.0010
[02/26 17:58:02 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 69/100, Acc=0.6818, Val Loss=1.1211, lr=0.0010
[02/26 17:58:33 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 70/100, Acc=0.6813, Val Loss=1.1238, lr=0.0010
[02/26 17:59:04 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 71/100, Acc=0.6826, Val Loss=1.1241, lr=0.0010
[02/26 17:59:36 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 72/100, Acc=0.6785, Val Loss=1.1249, lr=0.0010
[02/26 18:00:07 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 73/100, Acc=0.6800, Val Loss=1.1266, lr=0.0010
[02/26 18:00:38 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 74/100, Acc=0.6789, Val Loss=1.1341, lr=0.0010
[02/26 18:01:09 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 75/100, Acc=0.6801, Val Loss=1.1329, lr=0.0010
[02/26 18:01:40 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 76/100, Acc=0.6802, Val Loss=1.1263, lr=0.0010
[02/26 18:02:11 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 77/100, Acc=0.6783, Val Loss=1.1296, lr=0.0010
[02/26 18:02:43 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 78/100, Acc=0.6808, Val Loss=1.1311, lr=0.0010
[02/26 18:03:14 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 79/100, Acc=0.6762, Val Loss=1.1349, lr=0.0010
[02/26 18:03:45 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 80/100, Acc=0.6826, Val Loss=1.1236, lr=0.0001
[02/26 18:04:16 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 81/100, Acc=0.6827, Val Loss=1.1230, lr=0.0001
[02/26 18:04:47 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 82/100, Acc=0.6840, Val Loss=1.1236, lr=0.0001
[02/26 18:05:18 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 83/100, Acc=0.6834, Val Loss=1.1236, lr=0.0001
[02/26 18:05:49 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 84/100, Acc=0.6848, Val Loss=1.1210, lr=0.0001
[02/26 18:06:20 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 85/100, Acc=0.6839, Val Loss=1.1238, lr=0.0001
[02/26 18:06:51 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 86/100, Acc=0.6818, Val Loss=1.1221, lr=0.0001
[02/26 18:07:22 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 87/100, Acc=0.6844, Val Loss=1.1211, lr=0.0001
[02/26 18:07:53 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 88/100, Acc=0.6819, Val Loss=1.1232, lr=0.0001
[02/26 18:08:24 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 89/100, Acc=0.6837, Val Loss=1.1237, lr=0.0001
[02/26 18:08:55 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 90/100, Acc=0.6825, Val Loss=1.1236, lr=0.0001
[02/26 18:09:26 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 91/100, Acc=0.6819, Val Loss=1.1249, lr=0.0001
[02/26 18:09:57 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 92/100, Acc=0.6828, Val Loss=1.1244, lr=0.0001
[02/26 18:10:28 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 93/100, Acc=0.6834, Val Loss=1.1245, lr=0.0001
[02/26 18:11:00 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 94/100, Acc=0.6836, Val Loss=1.1244, lr=0.0001
[02/26 18:11:31 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 95/100, Acc=0.6831, Val Loss=1.1246, lr=0.0001
[02/26 18:12:03 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 96/100, Acc=0.6830, Val Loss=1.1242, lr=0.0001
[02/26 18:12:35 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 97/100, Acc=0.6838, Val Loss=1.1262, lr=0.0001
[02/26 18:13:06 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 98/100, Acc=0.6831, Val Loss=1.1249, lr=0.0001
[02/26 18:13:38 cifar100-global-fpgm-3.0-mobilenetv2]: Epoch 99/100, Acc=0.6818, Val Loss=1.1261, lr=0.0001
[02/26 18:13:38 cifar100-global-fpgm-3.0-mobilenetv2]: Best Acc=0.6848
[02/26 18:13:38 cifar100-global-fpgm-3.0-mobilenetv2]: Params: 0.74 M
[02/26 18:13:38 cifar100-global-fpgm-3.0-mobilenetv2]: ops: 21.73 M
[02/26 18:13:41 cifar100-global-fpgm-3.0-mobilenetv2]: Acc: 0.6818 Val Loss: 1.1261

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: mode: prune
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: model: mobilenetv2
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: verbose: False
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: dataset: cifar100
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: dataroot: data
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: batch_size: 128
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: total_epochs: 100
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: lr: 0.01
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: restore: run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: output_dir: run/cifar100/prune/cifar100-global-obdc-3.0-mobilenetv2
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: finetune: True
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: last_epochs: 100
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: reps: 1
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: method: obdc
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: speed_up: 3.0
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: reg: 1e-05
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: delta_reg: 0.0001
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: weight_decay: 0.0005
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: seed: 1
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: global_pruning: True
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: sl_total_epochs: 100
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: sl_lr: 0.01
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: sl_reg_warmup: 0
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: sl_restore: None
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: iterative_steps: 400
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: logger: <Logger cifar100-global-obdc-3.0-mobilenetv2 (DEBUG)>
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: device: cuda
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: num_classes: 100
[02/26 18:13:49 cifar100-global-obdc-3.0-mobilenetv2]: Loading model from run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/26 18:13:53 cifar100-global-obdc-3.0-mobilenetv2]: Pruning...
Traceback (most recent call last):
  File "/home/REDACTED/paper1/main.py", line 429, in <module>
    main()
  File "/home/REDACTED/paper1/main.py", line 369, in main
    progressive_pruning(
  File "/home/REDACTED/paper1/main.py", line 65, in progressive_pruning
    imp._prepare_model(model, pruner)
  File "/home/REDACTED/paper1/torch_pruning/pruner/importance.py", line 872, in _prepare_model
    for i, (dep, idxs) in enumerate(group):
TypeError: 'NoneType' object is not iterable
/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: mode: prune
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: model: mobilenetv2
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: verbose: False
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: dataset: cifar100
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: dataroot: data
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: batch_size: 128
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: total_epochs: 100
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: lr: 0.01
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: restore: run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: output_dir: run/cifar100/prune/cifar100-global-lamp-3.0-mobilenetv2
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: finetune: True
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: last_epochs: 100
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: reps: 1
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: method: lamp
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: speed_up: 3.0
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: reg: 1e-05
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: delta_reg: 0.0001
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: weight_decay: 0.0005
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: seed: 1
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: global_pruning: True
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: sl_total_epochs: 100
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: sl_lr: 0.01
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: sl_reg_warmup: 0
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: sl_restore: None
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: iterative_steps: 400
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: logger: <Logger cifar100-global-lamp-3.0-mobilenetv2 (DEBUG)>
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: device: cuda
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: num_classes: 100
[02/26 18:14:00 cifar100-global-lamp-3.0-mobilenetv2]: Loading model from run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/26 18:14:04 cifar100-global-lamp-3.0-mobilenetv2]: Pruning...
[02/26 18:14:39 cifar100-global-lamp-3.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 9, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(9, 15, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=15)
      (4): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(15, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72)
        (4): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 78, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(78, 78, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=78)
        (4): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(78, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 71, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(71, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(71, 71, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=71)
        (4): BatchNorm2d(71, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(71, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 82, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(82, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(82, 82, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=82)
        (4): BatchNorm2d(82, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(82, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 55, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=55)
        (4): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(55, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 81, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(81, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(81, 81, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=81)
        (4): BatchNorm2d(81, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(81, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 35, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(35, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=35)
        (4): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(35, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 21, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(21, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=21)
        (4): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(21, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 22, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=22)
        (4): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(22, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 174, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(174, 174, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=174)
        (4): BatchNorm2d(174, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(174, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 43, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(43, 43, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=43)
        (4): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(43, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 30, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(30, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=30)
        (4): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(30, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 167, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(167, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(167, 167, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=167)
        (4): BatchNorm2d(167, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(167, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 90, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(90, 90, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=90)
        (4): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(90, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 67, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(67, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(67, 67, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=67)
        (4): BatchNorm2d(67, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(67, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 102, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(102, 102, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=102)
      (4): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(102, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 100, kernel_size=(1, 1), stride=(1, 1))
)
[02/26 18:14:42 cifar100-global-lamp-3.0-mobilenetv2]: Params: 2.37 M => 0.79 M (33.15%)
[02/26 18:14:42 cifar100-global-lamp-3.0-mobilenetv2]: FLOPs: 68.40 M => 22.71 M (33.20%, 3.01X )
[02/26 18:14:42 cifar100-global-lamp-3.0-mobilenetv2]: Acc: 0.6699 => 0.6699
[02/26 18:14:42 cifar100-global-lamp-3.0-mobilenetv2]: Val Loss: 1.1637 => 1.1637
[02/26 18:14:42 cifar100-global-lamp-3.0-mobilenetv2]: Finetuning...
[02/26 18:15:13 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 0/100, Acc=0.5937, Val Loss=1.4408, lr=0.0100
[02/26 18:15:44 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 1/100, Acc=0.6050, Val Loss=1.4046, lr=0.0100
[02/26 18:16:15 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 2/100, Acc=0.5944, Val Loss=1.4573, lr=0.0100
[02/26 18:16:46 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 3/100, Acc=0.5952, Val Loss=1.4286, lr=0.0100
[02/26 18:17:16 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 4/100, Acc=0.5979, Val Loss=1.4177, lr=0.0100
[02/26 18:17:47 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 5/100, Acc=0.6036, Val Loss=1.4096, lr=0.0100
[02/26 18:18:18 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 6/100, Acc=0.6002, Val Loss=1.4352, lr=0.0100
[02/26 18:18:49 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 7/100, Acc=0.6096, Val Loss=1.3704, lr=0.0100
[02/26 18:19:20 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 8/100, Acc=0.6090, Val Loss=1.3863, lr=0.0100
[02/26 18:19:50 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 9/100, Acc=0.6118, Val Loss=1.3635, lr=0.0100
[02/26 18:20:21 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 10/100, Acc=0.5903, Val Loss=1.4356, lr=0.0100
[02/26 18:20:52 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 11/100, Acc=0.6152, Val Loss=1.3694, lr=0.0100
[02/26 18:21:22 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 12/100, Acc=0.6051, Val Loss=1.4016, lr=0.0100
[02/26 18:21:52 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 13/100, Acc=0.5981, Val Loss=1.4512, lr=0.0100
[02/26 18:22:23 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 14/100, Acc=0.6069, Val Loss=1.3880, lr=0.0100
[02/26 18:22:53 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 15/100, Acc=0.6233, Val Loss=1.3220, lr=0.0100
[02/26 18:23:23 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 16/100, Acc=0.6011, Val Loss=1.4137, lr=0.0100
[02/26 18:23:54 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 17/100, Acc=0.5995, Val Loss=1.4222, lr=0.0100
[02/26 18:24:24 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 18/100, Acc=0.6092, Val Loss=1.3921, lr=0.0100
[02/26 18:24:55 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 19/100, Acc=0.6035, Val Loss=1.4276, lr=0.0100
[02/26 18:25:25 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 20/100, Acc=0.6208, Val Loss=1.3440, lr=0.0100
[02/26 18:25:55 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 21/100, Acc=0.6113, Val Loss=1.3895, lr=0.0100
[02/26 18:26:25 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 22/100, Acc=0.6010, Val Loss=1.4327, lr=0.0100
[02/26 18:26:55 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 23/100, Acc=0.6053, Val Loss=1.4028, lr=0.0100
[02/26 18:27:25 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 24/100, Acc=0.6128, Val Loss=1.3863, lr=0.0100
[02/26 18:27:55 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 25/100, Acc=0.5799, Val Loss=1.5076, lr=0.0100
[02/26 18:28:25 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 26/100, Acc=0.6008, Val Loss=1.4145, lr=0.0100
[02/26 18:28:55 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 27/100, Acc=0.6018, Val Loss=1.4267, lr=0.0100
[02/26 18:29:25 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 28/100, Acc=0.6026, Val Loss=1.4276, lr=0.0100
[02/26 18:29:55 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 29/100, Acc=0.6200, Val Loss=1.3583, lr=0.0100
[02/26 18:30:26 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 30/100, Acc=0.6070, Val Loss=1.3909, lr=0.0100
[02/26 18:30:56 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 31/100, Acc=0.6001, Val Loss=1.4365, lr=0.0100
[02/26 18:31:27 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 32/100, Acc=0.5950, Val Loss=1.4511, lr=0.0100
[02/26 18:31:58 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 33/100, Acc=0.5873, Val Loss=1.4701, lr=0.0100
[02/26 18:32:28 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 34/100, Acc=0.6231, Val Loss=1.3439, lr=0.0100
[02/26 18:32:59 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 35/100, Acc=0.6273, Val Loss=1.3301, lr=0.0100
[02/26 18:33:36 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 36/100, Acc=0.6135, Val Loss=1.3955, lr=0.0100
[02/26 18:34:12 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 37/100, Acc=0.5811, Val Loss=1.4913, lr=0.0100
[02/26 18:34:48 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 38/100, Acc=0.6169, Val Loss=1.3535, lr=0.0100
[02/26 18:35:25 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 39/100, Acc=0.6146, Val Loss=1.3720, lr=0.0100
[02/26 18:36:01 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 40/100, Acc=0.6178, Val Loss=1.3658, lr=0.0100
[02/26 18:36:37 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 41/100, Acc=0.6276, Val Loss=1.3262, lr=0.0100
[02/26 18:37:13 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 42/100, Acc=0.6039, Val Loss=1.4084, lr=0.0100
[02/26 18:37:49 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 43/100, Acc=0.6232, Val Loss=1.3680, lr=0.0100
[02/26 18:38:25 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 44/100, Acc=0.6061, Val Loss=1.4324, lr=0.0100
[02/26 18:39:01 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 45/100, Acc=0.6091, Val Loss=1.3766, lr=0.0100
[02/26 18:39:38 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 46/100, Acc=0.6109, Val Loss=1.3679, lr=0.0100
[02/26 18:40:14 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 47/100, Acc=0.5999, Val Loss=1.4378, lr=0.0100
[02/26 18:40:50 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 48/100, Acc=0.5988, Val Loss=1.4252, lr=0.0100
[02/26 18:41:27 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 49/100, Acc=0.6003, Val Loss=1.4419, lr=0.0100
[02/26 18:42:03 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 50/100, Acc=0.6176, Val Loss=1.3633, lr=0.0100
[02/26 18:42:37 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 51/100, Acc=0.6137, Val Loss=1.3671, lr=0.0100
[02/26 18:43:08 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 52/100, Acc=0.6127, Val Loss=1.3914, lr=0.0100
[02/26 18:43:39 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 53/100, Acc=0.6208, Val Loss=1.3337, lr=0.0100
[02/26 18:44:09 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 54/100, Acc=0.6119, Val Loss=1.4005, lr=0.0100
[02/26 18:44:40 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 55/100, Acc=0.5920, Val Loss=1.4531, lr=0.0100
[02/26 18:45:11 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 56/100, Acc=0.6168, Val Loss=1.3592, lr=0.0100
[02/26 18:45:43 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 57/100, Acc=0.6132, Val Loss=1.3761, lr=0.0100
[02/26 18:46:19 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 58/100, Acc=0.6142, Val Loss=1.3734, lr=0.0100
[02/26 18:46:56 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 59/100, Acc=0.6093, Val Loss=1.4077, lr=0.0100
[02/26 18:47:32 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 60/100, Acc=0.6799, Val Loss=1.1360, lr=0.0010
[02/26 18:48:04 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 61/100, Acc=0.6814, Val Loss=1.1283, lr=0.0010
[02/26 18:48:35 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 62/100, Acc=0.6835, Val Loss=1.1197, lr=0.0010
[02/26 18:49:06 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 63/100, Acc=0.6854, Val Loss=1.1176, lr=0.0010
[02/26 18:49:37 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 64/100, Acc=0.6853, Val Loss=1.1177, lr=0.0010
[02/26 18:50:09 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 65/100, Acc=0.6806, Val Loss=1.1196, lr=0.0010
[02/26 18:50:40 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 66/100, Acc=0.6844, Val Loss=1.1144, lr=0.0010
[02/26 18:51:11 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 67/100, Acc=0.6837, Val Loss=1.1205, lr=0.0010
[02/26 18:51:43 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 68/100, Acc=0.6789, Val Loss=1.1205, lr=0.0010
[02/26 18:52:14 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 69/100, Acc=0.6818, Val Loss=1.1230, lr=0.0010
[02/26 18:52:45 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 70/100, Acc=0.6826, Val Loss=1.1245, lr=0.0010
[02/26 18:53:16 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 71/100, Acc=0.6839, Val Loss=1.1223, lr=0.0010
[02/26 18:53:47 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 72/100, Acc=0.6828, Val Loss=1.1210, lr=0.0010
[02/26 18:54:20 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 73/100, Acc=0.6812, Val Loss=1.1247, lr=0.0010
[02/26 18:54:52 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 74/100, Acc=0.6830, Val Loss=1.1270, lr=0.0010
[02/26 18:55:24 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 75/100, Acc=0.6812, Val Loss=1.1318, lr=0.0010
[02/26 18:55:55 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 76/100, Acc=0.6842, Val Loss=1.1237, lr=0.0010
[02/26 18:56:27 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 77/100, Acc=0.6803, Val Loss=1.1308, lr=0.0010
[02/26 18:56:59 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 78/100, Acc=0.6831, Val Loss=1.1270, lr=0.0010
[02/26 18:57:30 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 79/100, Acc=0.6797, Val Loss=1.1340, lr=0.0010
[02/26 18:58:01 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 80/100, Acc=0.6833, Val Loss=1.1242, lr=0.0001
[02/26 18:58:32 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 81/100, Acc=0.6850, Val Loss=1.1224, lr=0.0001
[02/26 18:59:04 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 82/100, Acc=0.6848, Val Loss=1.1210, lr=0.0001
[02/26 18:59:34 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 83/100, Acc=0.6850, Val Loss=1.1201, lr=0.0001
[02/26 19:00:06 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 84/100, Acc=0.6843, Val Loss=1.1187, lr=0.0001
[02/26 19:00:36 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 85/100, Acc=0.6847, Val Loss=1.1213, lr=0.0001
[02/26 19:01:07 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 86/100, Acc=0.6862, Val Loss=1.1202, lr=0.0001
[02/26 19:01:38 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 87/100, Acc=0.6837, Val Loss=1.1178, lr=0.0001
[02/26 19:02:08 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 88/100, Acc=0.6837, Val Loss=1.1208, lr=0.0001
[02/26 19:02:39 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 89/100, Acc=0.6835, Val Loss=1.1209, lr=0.0001
[02/26 19:03:09 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 90/100, Acc=0.6844, Val Loss=1.1209, lr=0.0001
[02/26 19:03:40 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 91/100, Acc=0.6849, Val Loss=1.1209, lr=0.0001
[02/26 19:04:11 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 92/100, Acc=0.6847, Val Loss=1.1209, lr=0.0001
[02/26 19:04:42 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 93/100, Acc=0.6852, Val Loss=1.1208, lr=0.0001
[02/26 19:05:13 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 94/100, Acc=0.6848, Val Loss=1.1207, lr=0.0001
[02/26 19:05:44 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 95/100, Acc=0.6840, Val Loss=1.1210, lr=0.0001
[02/26 19:06:14 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 96/100, Acc=0.6836, Val Loss=1.1181, lr=0.0001
[02/26 19:06:45 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 97/100, Acc=0.6855, Val Loss=1.1214, lr=0.0001
[02/26 19:07:16 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 98/100, Acc=0.6836, Val Loss=1.1202, lr=0.0001
[02/26 19:07:46 cifar100-global-lamp-3.0-mobilenetv2]: Epoch 99/100, Acc=0.6851, Val Loss=1.1224, lr=0.0001
[02/26 19:07:46 cifar100-global-lamp-3.0-mobilenetv2]: Best Acc=0.6862
[02/26 19:07:46 cifar100-global-lamp-3.0-mobilenetv2]: Params: 0.79 M
[02/26 19:07:46 cifar100-global-lamp-3.0-mobilenetv2]: ops: 22.71 M
[02/26 19:07:49 cifar100-global-lamp-3.0-mobilenetv2]: Acc: 0.6851 Val Loss: 1.1224

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: mode: prune
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: model: mobilenetv2
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: verbose: False
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: dataset: cifar100
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: dataroot: data
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: batch_size: 128
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: total_epochs: 100
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: lr: 0.01
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: restore: run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: output_dir: run/cifar100/prune/cifar100-global-slim-3.0-mobilenetv2
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: finetune: True
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: last_epochs: 100
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: reps: 1
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: method: slim
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: speed_up: 3.0
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: reg: 1e-05
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: delta_reg: 0.0001
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: weight_decay: 0.0005
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: seed: 1
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: global_pruning: True
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: sl_total_epochs: 100
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: sl_lr: 0.01
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: sl_reg_warmup: 0
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: sl_restore: None
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: iterative_steps: 400
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: logger: <Logger cifar100-global-slim-3.0-mobilenetv2 (DEBUG)>
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: device: cuda
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: num_classes: 100
[02/26 19:07:57 cifar100-global-slim-3.0-mobilenetv2]: Loading model from run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/26 19:07:58 cifar100-global-slim-3.0-mobilenetv2]: Regularizing...
[02/26 19:08:31 cifar100-global-slim-3.0-mobilenetv2]: Epoch 0/100, Acc=0.5945, Val Loss=1.4834, lr=0.0100
[02/26 19:09:05 cifar100-global-slim-3.0-mobilenetv2]: Epoch 1/100, Acc=0.5727, Val Loss=1.5707, lr=0.0100
[02/26 19:09:38 cifar100-global-slim-3.0-mobilenetv2]: Epoch 2/100, Acc=0.6128, Val Loss=1.3799, lr=0.0100
[02/26 19:10:12 cifar100-global-slim-3.0-mobilenetv2]: Epoch 3/100, Acc=0.5938, Val Loss=1.4503, lr=0.0100
[02/26 19:10:46 cifar100-global-slim-3.0-mobilenetv2]: Epoch 4/100, Acc=0.6065, Val Loss=1.4170, lr=0.0100
[02/26 19:11:19 cifar100-global-slim-3.0-mobilenetv2]: Epoch 5/100, Acc=0.6162, Val Loss=1.3892, lr=0.0100
[02/26 19:11:54 cifar100-global-slim-3.0-mobilenetv2]: Epoch 6/100, Acc=0.6120, Val Loss=1.3748, lr=0.0100
[02/26 19:12:28 cifar100-global-slim-3.0-mobilenetv2]: Epoch 7/100, Acc=0.6210, Val Loss=1.3490, lr=0.0100
[02/26 19:13:02 cifar100-global-slim-3.0-mobilenetv2]: Epoch 8/100, Acc=0.6113, Val Loss=1.3973, lr=0.0100
[02/26 19:13:37 cifar100-global-slim-3.0-mobilenetv2]: Epoch 9/100, Acc=0.6140, Val Loss=1.3887, lr=0.0100
[02/26 19:14:12 cifar100-global-slim-3.0-mobilenetv2]: Epoch 10/100, Acc=0.6072, Val Loss=1.4294, lr=0.0100
[02/26 19:14:46 cifar100-global-slim-3.0-mobilenetv2]: Epoch 11/100, Acc=0.6164, Val Loss=1.3654, lr=0.0100
[02/26 19:15:21 cifar100-global-slim-3.0-mobilenetv2]: Epoch 12/100, Acc=0.6300, Val Loss=1.3174, lr=0.0100
[02/26 19:15:55 cifar100-global-slim-3.0-mobilenetv2]: Epoch 13/100, Acc=0.6216, Val Loss=1.3611, lr=0.0100
[02/26 19:16:29 cifar100-global-slim-3.0-mobilenetv2]: Epoch 14/100, Acc=0.6190, Val Loss=1.3877, lr=0.0100
[02/26 19:17:03 cifar100-global-slim-3.0-mobilenetv2]: Epoch 15/100, Acc=0.6285, Val Loss=1.3656, lr=0.0100
[02/26 19:17:38 cifar100-global-slim-3.0-mobilenetv2]: Epoch 16/100, Acc=0.6186, Val Loss=1.3861, lr=0.0100
[02/26 19:18:12 cifar100-global-slim-3.0-mobilenetv2]: Epoch 17/100, Acc=0.6327, Val Loss=1.3274, lr=0.0100
[02/26 19:18:46 cifar100-global-slim-3.0-mobilenetv2]: Epoch 18/100, Acc=0.6304, Val Loss=1.3330, lr=0.0100
[02/26 19:19:20 cifar100-global-slim-3.0-mobilenetv2]: Epoch 19/100, Acc=0.6329, Val Loss=1.3271, lr=0.0100
[02/26 19:19:59 cifar100-global-slim-3.0-mobilenetv2]: Epoch 20/100, Acc=0.6431, Val Loss=1.3011, lr=0.0100
[02/26 19:20:33 cifar100-global-slim-3.0-mobilenetv2]: Epoch 21/100, Acc=0.6393, Val Loss=1.3103, lr=0.0100
[02/26 19:21:08 cifar100-global-slim-3.0-mobilenetv2]: Epoch 22/100, Acc=0.6236, Val Loss=1.3737, lr=0.0100
[02/26 19:21:42 cifar100-global-slim-3.0-mobilenetv2]: Epoch 23/100, Acc=0.6366, Val Loss=1.3029, lr=0.0100
[02/26 19:22:16 cifar100-global-slim-3.0-mobilenetv2]: Epoch 24/100, Acc=0.6394, Val Loss=1.3087, lr=0.0100
[02/26 19:22:51 cifar100-global-slim-3.0-mobilenetv2]: Epoch 25/100, Acc=0.6440, Val Loss=1.2875, lr=0.0100
[02/26 19:23:25 cifar100-global-slim-3.0-mobilenetv2]: Epoch 26/100, Acc=0.6345, Val Loss=1.3213, lr=0.0100
[02/26 19:24:00 cifar100-global-slim-3.0-mobilenetv2]: Epoch 27/100, Acc=0.6357, Val Loss=1.3276, lr=0.0100
[02/26 19:24:34 cifar100-global-slim-3.0-mobilenetv2]: Epoch 28/100, Acc=0.6446, Val Loss=1.2969, lr=0.0100
[02/26 19:25:08 cifar100-global-slim-3.0-mobilenetv2]: Epoch 29/100, Acc=0.6485, Val Loss=1.3088, lr=0.0100
[02/26 19:25:47 cifar100-global-slim-3.0-mobilenetv2]: Epoch 30/100, Acc=0.6417, Val Loss=1.3124, lr=0.0100
[02/26 19:26:25 cifar100-global-slim-3.0-mobilenetv2]: Epoch 31/100, Acc=0.6442, Val Loss=1.3129, lr=0.0100
[02/26 19:27:04 cifar100-global-slim-3.0-mobilenetv2]: Epoch 32/100, Acc=0.6441, Val Loss=1.2978, lr=0.0100
[02/26 19:27:42 cifar100-global-slim-3.0-mobilenetv2]: Epoch 33/100, Acc=0.6416, Val Loss=1.3198, lr=0.0100
[02/26 19:28:20 cifar100-global-slim-3.0-mobilenetv2]: Epoch 34/100, Acc=0.6412, Val Loss=1.3261, lr=0.0100
[02/26 19:28:59 cifar100-global-slim-3.0-mobilenetv2]: Epoch 35/100, Acc=0.6378, Val Loss=1.3261, lr=0.0100
[02/26 19:29:37 cifar100-global-slim-3.0-mobilenetv2]: Epoch 36/100, Acc=0.6455, Val Loss=1.3172, lr=0.0100
[02/26 19:30:16 cifar100-global-slim-3.0-mobilenetv2]: Epoch 37/100, Acc=0.6398, Val Loss=1.3494, lr=0.0100
[02/26 19:30:52 cifar100-global-slim-3.0-mobilenetv2]: Epoch 38/100, Acc=0.6453, Val Loss=1.3119, lr=0.0100
[02/26 19:31:26 cifar100-global-slim-3.0-mobilenetv2]: Epoch 39/100, Acc=0.6428, Val Loss=1.3091, lr=0.0100
[02/26 19:32:01 cifar100-global-slim-3.0-mobilenetv2]: Epoch 40/100, Acc=0.6415, Val Loss=1.3263, lr=0.0100
[02/26 19:32:34 cifar100-global-slim-3.0-mobilenetv2]: Epoch 41/100, Acc=0.6440, Val Loss=1.3238, lr=0.0100
[02/26 19:33:08 cifar100-global-slim-3.0-mobilenetv2]: Epoch 42/100, Acc=0.6476, Val Loss=1.3170, lr=0.0100
[02/26 19:33:42 cifar100-global-slim-3.0-mobilenetv2]: Epoch 43/100, Acc=0.6450, Val Loss=1.3440, lr=0.0100
[02/26 19:34:16 cifar100-global-slim-3.0-mobilenetv2]: Epoch 44/100, Acc=0.6471, Val Loss=1.3097, lr=0.0100
[02/26 19:34:50 cifar100-global-slim-3.0-mobilenetv2]: Epoch 45/100, Acc=0.6514, Val Loss=1.3117, lr=0.0100
[02/26 19:35:29 cifar100-global-slim-3.0-mobilenetv2]: Epoch 46/100, Acc=0.6423, Val Loss=1.3370, lr=0.0100
[02/26 19:36:09 cifar100-global-slim-3.0-mobilenetv2]: Epoch 47/100, Acc=0.6461, Val Loss=1.3306, lr=0.0100
[02/26 19:36:49 cifar100-global-slim-3.0-mobilenetv2]: Epoch 48/100, Acc=0.6497, Val Loss=1.3332, lr=0.0100
[02/26 19:37:25 cifar100-global-slim-3.0-mobilenetv2]: Epoch 49/100, Acc=0.6426, Val Loss=1.3513, lr=0.0100
[02/26 19:38:01 cifar100-global-slim-3.0-mobilenetv2]: Epoch 50/100, Acc=0.6476, Val Loss=1.3244, lr=0.0100
[02/26 19:38:36 cifar100-global-slim-3.0-mobilenetv2]: Epoch 51/100, Acc=0.6459, Val Loss=1.3362, lr=0.0100
[02/26 19:39:11 cifar100-global-slim-3.0-mobilenetv2]: Epoch 52/100, Acc=0.6472, Val Loss=1.3438, lr=0.0100
[02/26 19:39:47 cifar100-global-slim-3.0-mobilenetv2]: Epoch 53/100, Acc=0.6530, Val Loss=1.3091, lr=0.0100
[02/26 19:40:22 cifar100-global-slim-3.0-mobilenetv2]: Epoch 54/100, Acc=0.6522, Val Loss=1.3219, lr=0.0100
[02/26 19:40:56 cifar100-global-slim-3.0-mobilenetv2]: Epoch 55/100, Acc=0.6506, Val Loss=1.3348, lr=0.0100
[02/26 19:41:30 cifar100-global-slim-3.0-mobilenetv2]: Epoch 56/100, Acc=0.6564, Val Loss=1.2889, lr=0.0100
[02/26 19:42:04 cifar100-global-slim-3.0-mobilenetv2]: Epoch 57/100, Acc=0.6488, Val Loss=1.3204, lr=0.0100
[02/26 19:42:39 cifar100-global-slim-3.0-mobilenetv2]: Epoch 58/100, Acc=0.6471, Val Loss=1.3389, lr=0.0100
[02/26 19:43:13 cifar100-global-slim-3.0-mobilenetv2]: Epoch 59/100, Acc=0.6448, Val Loss=1.3622, lr=0.0100
[02/26 19:43:47 cifar100-global-slim-3.0-mobilenetv2]: Epoch 60/100, Acc=0.6754, Val Loss=1.2234, lr=0.0010
[02/26 19:44:21 cifar100-global-slim-3.0-mobilenetv2]: Epoch 61/100, Acc=0.6764, Val Loss=1.2170, lr=0.0010
[02/26 19:44:55 cifar100-global-slim-3.0-mobilenetv2]: Epoch 62/100, Acc=0.6783, Val Loss=1.2138, lr=0.0010
[02/26 19:45:29 cifar100-global-slim-3.0-mobilenetv2]: Epoch 63/100, Acc=0.6785, Val Loss=1.2147, lr=0.0010
[02/26 19:46:06 cifar100-global-slim-3.0-mobilenetv2]: Epoch 64/100, Acc=0.6777, Val Loss=1.2200, lr=0.0010
[02/26 19:46:40 cifar100-global-slim-3.0-mobilenetv2]: Epoch 65/100, Acc=0.6790, Val Loss=1.2165, lr=0.0010
[02/26 19:47:14 cifar100-global-slim-3.0-mobilenetv2]: Epoch 66/100, Acc=0.6833, Val Loss=1.2136, lr=0.0010
[02/26 19:47:48 cifar100-global-slim-3.0-mobilenetv2]: Epoch 67/100, Acc=0.6799, Val Loss=1.2224, lr=0.0010
[02/26 19:48:22 cifar100-global-slim-3.0-mobilenetv2]: Epoch 68/100, Acc=0.6782, Val Loss=1.2219, lr=0.0010
[02/26 19:48:56 cifar100-global-slim-3.0-mobilenetv2]: Epoch 69/100, Acc=0.6800, Val Loss=1.2247, lr=0.0010
[02/26 19:49:30 cifar100-global-slim-3.0-mobilenetv2]: Epoch 70/100, Acc=0.6783, Val Loss=1.2286, lr=0.0010
[02/26 19:50:04 cifar100-global-slim-3.0-mobilenetv2]: Epoch 71/100, Acc=0.6789, Val Loss=1.2279, lr=0.0010
[02/26 19:50:37 cifar100-global-slim-3.0-mobilenetv2]: Epoch 72/100, Acc=0.6796, Val Loss=1.2321, lr=0.0010
[02/26 19:51:12 cifar100-global-slim-3.0-mobilenetv2]: Epoch 73/100, Acc=0.6797, Val Loss=1.2343, lr=0.0010
[02/26 19:51:45 cifar100-global-slim-3.0-mobilenetv2]: Epoch 74/100, Acc=0.6807, Val Loss=1.2339, lr=0.0010
[02/26 19:52:20 cifar100-global-slim-3.0-mobilenetv2]: Epoch 75/100, Acc=0.6810, Val Loss=1.2294, lr=0.0010
[02/26 19:52:54 cifar100-global-slim-3.0-mobilenetv2]: Epoch 76/100, Acc=0.6787, Val Loss=1.2427, lr=0.0010
[02/26 19:53:28 cifar100-global-slim-3.0-mobilenetv2]: Epoch 77/100, Acc=0.6785, Val Loss=1.2390, lr=0.0010
[02/26 19:54:03 cifar100-global-slim-3.0-mobilenetv2]: Epoch 78/100, Acc=0.6755, Val Loss=1.2439, lr=0.0010
[02/26 19:54:37 cifar100-global-slim-3.0-mobilenetv2]: Epoch 79/100, Acc=0.6788, Val Loss=1.2425, lr=0.0010
[02/26 19:55:11 cifar100-global-slim-3.0-mobilenetv2]: Epoch 80/100, Acc=0.6790, Val Loss=1.2425, lr=0.0001
[02/26 19:55:45 cifar100-global-slim-3.0-mobilenetv2]: Epoch 81/100, Acc=0.6797, Val Loss=1.2414, lr=0.0001
[02/26 19:56:19 cifar100-global-slim-3.0-mobilenetv2]: Epoch 82/100, Acc=0.6787, Val Loss=1.2406, lr=0.0001
[02/26 19:56:53 cifar100-global-slim-3.0-mobilenetv2]: Epoch 83/100, Acc=0.6775, Val Loss=1.2379, lr=0.0001
[02/26 19:57:28 cifar100-global-slim-3.0-mobilenetv2]: Epoch 84/100, Acc=0.6786, Val Loss=1.2390, lr=0.0001
[02/26 19:58:02 cifar100-global-slim-3.0-mobilenetv2]: Epoch 85/100, Acc=0.6796, Val Loss=1.2405, lr=0.0001
[02/26 19:58:36 cifar100-global-slim-3.0-mobilenetv2]: Epoch 86/100, Acc=0.6791, Val Loss=1.2418, lr=0.0001
[02/26 19:59:11 cifar100-global-slim-3.0-mobilenetv2]: Epoch 87/100, Acc=0.6776, Val Loss=1.2388, lr=0.0001
[02/26 19:59:45 cifar100-global-slim-3.0-mobilenetv2]: Epoch 88/100, Acc=0.6788, Val Loss=1.2408, lr=0.0001
[02/26 20:00:20 cifar100-global-slim-3.0-mobilenetv2]: Epoch 89/100, Acc=0.6786, Val Loss=1.2424, lr=0.0001
[02/26 20:00:55 cifar100-global-slim-3.0-mobilenetv2]: Epoch 90/100, Acc=0.6791, Val Loss=1.2420, lr=0.0001
[02/26 20:01:30 cifar100-global-slim-3.0-mobilenetv2]: Epoch 91/100, Acc=0.6785, Val Loss=1.2416, lr=0.0001
[02/26 20:02:04 cifar100-global-slim-3.0-mobilenetv2]: Epoch 92/100, Acc=0.6776, Val Loss=1.2417, lr=0.0001
[02/26 20:02:38 cifar100-global-slim-3.0-mobilenetv2]: Epoch 93/100, Acc=0.6800, Val Loss=1.2382, lr=0.0001
[02/26 20:03:12 cifar100-global-slim-3.0-mobilenetv2]: Epoch 94/100, Acc=0.6789, Val Loss=1.2403, lr=0.0001
[02/26 20:03:46 cifar100-global-slim-3.0-mobilenetv2]: Epoch 95/100, Acc=0.6797, Val Loss=1.2441, lr=0.0001
[02/26 20:04:20 cifar100-global-slim-3.0-mobilenetv2]: Epoch 96/100, Acc=0.6786, Val Loss=1.2415, lr=0.0001
[02/26 20:04:54 cifar100-global-slim-3.0-mobilenetv2]: Epoch 97/100, Acc=0.6766, Val Loss=1.2388, lr=0.0001
[02/26 20:05:28 cifar100-global-slim-3.0-mobilenetv2]: Epoch 98/100, Acc=0.6767, Val Loss=1.2420, lr=0.0001
[02/26 20:06:02 cifar100-global-slim-3.0-mobilenetv2]: Epoch 99/100, Acc=0.6790, Val Loss=1.2446, lr=0.0001
[02/26 20:06:02 cifar100-global-slim-3.0-mobilenetv2]: Best Acc=0.6833
[02/26 20:06:02 cifar100-global-slim-3.0-mobilenetv2]: Loading the sparse model from run/cifar100/prune/cifar100-global-slim-3.0-mobilenetv2/reg_cifar100_mobilenetv2_slim_1e-05.pth...
[02/26 20:06:05 cifar100-global-slim-3.0-mobilenetv2]: Pruning...
[02/26 20:06:33 cifar100-global-slim-3.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 9, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(9, 15, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=15)
      (4): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(15, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72)
        (4): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 69, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(69, 69, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=69)
        (4): BatchNorm2d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(69, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 64, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 78, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(78, 78, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=78)
        (4): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(78, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 47, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(47, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=47)
        (4): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(47, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 78, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(78, 78, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=78)
        (4): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(78, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 30, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(30, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=30)
        (4): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(30, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 232, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(232, 232, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=232)
        (4): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(232, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 12, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=12)
        (4): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(12, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 164, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(164, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(164, 164, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=164)
        (4): BatchNorm2d(164, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(164, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 26, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(26, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=26)
        (4): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(26, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 87, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(87, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(87, 87, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=87)
        (4): BatchNorm2d(87, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(87, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 151, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(151, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(151, 151, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=151)
        (4): BatchNorm2d(151, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(151, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 74, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(74, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(74, 74, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=74)
        (4): BatchNorm2d(74, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(74, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 45, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(45, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=45)
        (4): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(45, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 98, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(98, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(98, 98, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=98)
      (4): BatchNorm2d(98, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(98, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 100, kernel_size=(1, 1), stride=(1, 1))
)
[02/26 20:06:36 cifar100-global-slim-3.0-mobilenetv2]: Params: 2.37 M => 0.80 M (33.71%)
[02/26 20:06:36 cifar100-global-slim-3.0-mobilenetv2]: FLOPs: 68.40 M => 22.76 M (33.27%, 3.01X )
[02/26 20:06:36 cifar100-global-slim-3.0-mobilenetv2]: Acc: 0.6833 => 0.6832
[02/26 20:06:36 cifar100-global-slim-3.0-mobilenetv2]: Val Loss: 1.2136 => 1.2137
[02/26 20:06:36 cifar100-global-slim-3.0-mobilenetv2]: Finetuning...
[02/26 20:07:07 cifar100-global-slim-3.0-mobilenetv2]: Epoch 0/100, Acc=0.6529, Val Loss=1.3195, lr=0.0100
[02/26 20:07:37 cifar100-global-slim-3.0-mobilenetv2]: Epoch 1/100, Acc=0.6478, Val Loss=1.3205, lr=0.0100
[02/26 20:08:08 cifar100-global-slim-3.0-mobilenetv2]: Epoch 2/100, Acc=0.6507, Val Loss=1.2781, lr=0.0100
[02/26 20:08:39 cifar100-global-slim-3.0-mobilenetv2]: Epoch 3/100, Acc=0.6548, Val Loss=1.2804, lr=0.0100
[02/26 20:09:11 cifar100-global-slim-3.0-mobilenetv2]: Epoch 4/100, Acc=0.6500, Val Loss=1.2806, lr=0.0100
[02/26 20:09:42 cifar100-global-slim-3.0-mobilenetv2]: Epoch 5/100, Acc=0.6525, Val Loss=1.2899, lr=0.0100
[02/26 20:10:13 cifar100-global-slim-3.0-mobilenetv2]: Epoch 6/100, Acc=0.6389, Val Loss=1.3182, lr=0.0100
[02/26 20:10:45 cifar100-global-slim-3.0-mobilenetv2]: Epoch 7/100, Acc=0.6515, Val Loss=1.2750, lr=0.0100
[02/26 20:11:16 cifar100-global-slim-3.0-mobilenetv2]: Epoch 8/100, Acc=0.6419, Val Loss=1.2982, lr=0.0100
[02/26 20:11:48 cifar100-global-slim-3.0-mobilenetv2]: Epoch 9/100, Acc=0.6470, Val Loss=1.2856, lr=0.0100
[02/26 20:12:20 cifar100-global-slim-3.0-mobilenetv2]: Epoch 10/100, Acc=0.6453, Val Loss=1.2977, lr=0.0100
[02/26 20:12:51 cifar100-global-slim-3.0-mobilenetv2]: Epoch 11/100, Acc=0.6438, Val Loss=1.3144, lr=0.0100
[02/26 20:13:22 cifar100-global-slim-3.0-mobilenetv2]: Epoch 12/100, Acc=0.6454, Val Loss=1.2936, lr=0.0100
[02/26 20:13:53 cifar100-global-slim-3.0-mobilenetv2]: Epoch 13/100, Acc=0.6471, Val Loss=1.2979, lr=0.0100
[02/26 20:14:25 cifar100-global-slim-3.0-mobilenetv2]: Epoch 14/100, Acc=0.6402, Val Loss=1.3145, lr=0.0100
[02/26 20:14:56 cifar100-global-slim-3.0-mobilenetv2]: Epoch 15/100, Acc=0.6332, Val Loss=1.3282, lr=0.0100
[02/26 20:15:28 cifar100-global-slim-3.0-mobilenetv2]: Epoch 16/100, Acc=0.6410, Val Loss=1.3005, lr=0.0100
[02/26 20:15:59 cifar100-global-slim-3.0-mobilenetv2]: Epoch 17/100, Acc=0.6404, Val Loss=1.3120, lr=0.0100
[02/26 20:16:30 cifar100-global-slim-3.0-mobilenetv2]: Epoch 18/100, Acc=0.6454, Val Loss=1.2933, lr=0.0100
[02/26 20:17:00 cifar100-global-slim-3.0-mobilenetv2]: Epoch 19/100, Acc=0.6386, Val Loss=1.3075, lr=0.0100
[02/26 20:17:31 cifar100-global-slim-3.0-mobilenetv2]: Epoch 20/100, Acc=0.6425, Val Loss=1.2937, lr=0.0100
[02/26 20:18:01 cifar100-global-slim-3.0-mobilenetv2]: Epoch 21/100, Acc=0.6450, Val Loss=1.2927, lr=0.0100
[02/26 20:18:32 cifar100-global-slim-3.0-mobilenetv2]: Epoch 22/100, Acc=0.6329, Val Loss=1.3273, lr=0.0100
[02/26 20:19:03 cifar100-global-slim-3.0-mobilenetv2]: Epoch 23/100, Acc=0.6371, Val Loss=1.3077, lr=0.0100
[02/26 20:19:33 cifar100-global-slim-3.0-mobilenetv2]: Epoch 24/100, Acc=0.6406, Val Loss=1.2883, lr=0.0100
[02/26 20:20:04 cifar100-global-slim-3.0-mobilenetv2]: Epoch 25/100, Acc=0.6280, Val Loss=1.3406, lr=0.0100
[02/26 20:20:35 cifar100-global-slim-3.0-mobilenetv2]: Epoch 26/100, Acc=0.6346, Val Loss=1.3149, lr=0.0100
[02/26 20:21:06 cifar100-global-slim-3.0-mobilenetv2]: Epoch 27/100, Acc=0.6379, Val Loss=1.3100, lr=0.0100
[02/26 20:21:38 cifar100-global-slim-3.0-mobilenetv2]: Epoch 28/100, Acc=0.6320, Val Loss=1.3473, lr=0.0100
[02/26 20:22:09 cifar100-global-slim-3.0-mobilenetv2]: Epoch 29/100, Acc=0.6225, Val Loss=1.3579, lr=0.0100
[02/26 20:22:40 cifar100-global-slim-3.0-mobilenetv2]: Epoch 30/100, Acc=0.6219, Val Loss=1.3661, lr=0.0100
[02/26 20:23:11 cifar100-global-slim-3.0-mobilenetv2]: Epoch 31/100, Acc=0.6361, Val Loss=1.3171, lr=0.0100
[02/26 20:23:42 cifar100-global-slim-3.0-mobilenetv2]: Epoch 32/100, Acc=0.6319, Val Loss=1.3304, lr=0.0100
[02/26 20:24:14 cifar100-global-slim-3.0-mobilenetv2]: Epoch 33/100, Acc=0.6333, Val Loss=1.3248, lr=0.0100
[02/26 20:24:44 cifar100-global-slim-3.0-mobilenetv2]: Epoch 34/100, Acc=0.6265, Val Loss=1.3231, lr=0.0100
[02/26 20:25:15 cifar100-global-slim-3.0-mobilenetv2]: Epoch 35/100, Acc=0.6305, Val Loss=1.3286, lr=0.0100
[02/26 20:25:46 cifar100-global-slim-3.0-mobilenetv2]: Epoch 36/100, Acc=0.6254, Val Loss=1.3409, lr=0.0100
[02/26 20:26:17 cifar100-global-slim-3.0-mobilenetv2]: Epoch 37/100, Acc=0.6362, Val Loss=1.3014, lr=0.0100
[02/26 20:26:49 cifar100-global-slim-3.0-mobilenetv2]: Epoch 38/100, Acc=0.6316, Val Loss=1.3261, lr=0.0100
[02/26 20:27:20 cifar100-global-slim-3.0-mobilenetv2]: Epoch 39/100, Acc=0.6269, Val Loss=1.3258, lr=0.0100
[02/26 20:27:51 cifar100-global-slim-3.0-mobilenetv2]: Epoch 40/100, Acc=0.6260, Val Loss=1.3603, lr=0.0100
[02/26 20:28:22 cifar100-global-slim-3.0-mobilenetv2]: Epoch 41/100, Acc=0.6221, Val Loss=1.4007, lr=0.0100
[02/26 20:28:53 cifar100-global-slim-3.0-mobilenetv2]: Epoch 42/100, Acc=0.6079, Val Loss=1.4686, lr=0.0100
[02/26 20:29:23 cifar100-global-slim-3.0-mobilenetv2]: Epoch 43/100, Acc=0.6191, Val Loss=1.3885, lr=0.0100
[02/26 20:29:54 cifar100-global-slim-3.0-mobilenetv2]: Epoch 44/100, Acc=0.6165, Val Loss=1.3675, lr=0.0100
[02/26 20:30:25 cifar100-global-slim-3.0-mobilenetv2]: Epoch 45/100, Acc=0.6179, Val Loss=1.3675, lr=0.0100
[02/26 20:30:56 cifar100-global-slim-3.0-mobilenetv2]: Epoch 46/100, Acc=0.6233, Val Loss=1.3383, lr=0.0100
[02/26 20:31:27 cifar100-global-slim-3.0-mobilenetv2]: Epoch 47/100, Acc=0.6313, Val Loss=1.3089, lr=0.0100
[02/26 20:31:58 cifar100-global-slim-3.0-mobilenetv2]: Epoch 48/100, Acc=0.6129, Val Loss=1.4075, lr=0.0100
[02/26 20:32:29 cifar100-global-slim-3.0-mobilenetv2]: Epoch 49/100, Acc=0.5992, Val Loss=1.4632, lr=0.0100
[02/26 20:33:00 cifar100-global-slim-3.0-mobilenetv2]: Epoch 50/100, Acc=0.5987, Val Loss=1.4512, lr=0.0100
[02/26 20:33:32 cifar100-global-slim-3.0-mobilenetv2]: Epoch 51/100, Acc=0.6339, Val Loss=1.3207, lr=0.0100
[02/26 20:34:03 cifar100-global-slim-3.0-mobilenetv2]: Epoch 52/100, Acc=0.6020, Val Loss=1.4558, lr=0.0100
[02/26 20:34:34 cifar100-global-slim-3.0-mobilenetv2]: Epoch 53/100, Acc=0.6178, Val Loss=1.3684, lr=0.0100
[02/26 20:35:05 cifar100-global-slim-3.0-mobilenetv2]: Epoch 54/100, Acc=0.6295, Val Loss=1.3180, lr=0.0100
[02/26 20:35:37 cifar100-global-slim-3.0-mobilenetv2]: Epoch 55/100, Acc=0.6313, Val Loss=1.3321, lr=0.0100
[02/26 20:36:08 cifar100-global-slim-3.0-mobilenetv2]: Epoch 56/100, Acc=0.6104, Val Loss=1.3945, lr=0.0100
[02/26 20:36:40 cifar100-global-slim-3.0-mobilenetv2]: Epoch 57/100, Acc=0.6253, Val Loss=1.3413, lr=0.0100
[02/26 20:37:11 cifar100-global-slim-3.0-mobilenetv2]: Epoch 58/100, Acc=0.6259, Val Loss=1.3306, lr=0.0100
[02/26 20:37:43 cifar100-global-slim-3.0-mobilenetv2]: Epoch 59/100, Acc=0.6235, Val Loss=1.3417, lr=0.0100
[02/26 20:38:14 cifar100-global-slim-3.0-mobilenetv2]: Epoch 60/100, Acc=0.6820, Val Loss=1.1245, lr=0.0010
[02/26 20:38:46 cifar100-global-slim-3.0-mobilenetv2]: Epoch 61/100, Acc=0.6825, Val Loss=1.1175, lr=0.0010
[02/26 20:39:18 cifar100-global-slim-3.0-mobilenetv2]: Epoch 62/100, Acc=0.6843, Val Loss=1.1119, lr=0.0010
[02/26 20:39:50 cifar100-global-slim-3.0-mobilenetv2]: Epoch 63/100, Acc=0.6854, Val Loss=1.1113, lr=0.0010
[02/26 20:40:22 cifar100-global-slim-3.0-mobilenetv2]: Epoch 64/100, Acc=0.6844, Val Loss=1.1147, lr=0.0010
[02/26 20:40:53 cifar100-global-slim-3.0-mobilenetv2]: Epoch 65/100, Acc=0.6832, Val Loss=1.1119, lr=0.0010
[02/26 20:41:25 cifar100-global-slim-3.0-mobilenetv2]: Epoch 66/100, Acc=0.6808, Val Loss=1.1164, lr=0.0010
[02/26 20:41:57 cifar100-global-slim-3.0-mobilenetv2]: Epoch 67/100, Acc=0.6816, Val Loss=1.1181, lr=0.0010
[02/26 20:42:28 cifar100-global-slim-3.0-mobilenetv2]: Epoch 68/100, Acc=0.6860, Val Loss=1.1203, lr=0.0010
[02/26 20:43:00 cifar100-global-slim-3.0-mobilenetv2]: Epoch 69/100, Acc=0.6828, Val Loss=1.1176, lr=0.0010
[02/26 20:43:32 cifar100-global-slim-3.0-mobilenetv2]: Epoch 70/100, Acc=0.6848, Val Loss=1.1214, lr=0.0010
[02/26 20:44:04 cifar100-global-slim-3.0-mobilenetv2]: Epoch 71/100, Acc=0.6826, Val Loss=1.1228, lr=0.0010
[02/26 20:44:36 cifar100-global-slim-3.0-mobilenetv2]: Epoch 72/100, Acc=0.6820, Val Loss=1.1263, lr=0.0010
[02/26 20:45:08 cifar100-global-slim-3.0-mobilenetv2]: Epoch 73/100, Acc=0.6827, Val Loss=1.1267, lr=0.0010
[02/26 20:45:40 cifar100-global-slim-3.0-mobilenetv2]: Epoch 74/100, Acc=0.6825, Val Loss=1.1293, lr=0.0010
[02/26 20:46:12 cifar100-global-slim-3.0-mobilenetv2]: Epoch 75/100, Acc=0.6817, Val Loss=1.1296, lr=0.0010
[02/26 20:46:44 cifar100-global-slim-3.0-mobilenetv2]: Epoch 76/100, Acc=0.6805, Val Loss=1.1301, lr=0.0010
[02/26 20:47:16 cifar100-global-slim-3.0-mobilenetv2]: Epoch 77/100, Acc=0.6829, Val Loss=1.1308, lr=0.0010
[02/26 20:47:48 cifar100-global-slim-3.0-mobilenetv2]: Epoch 78/100, Acc=0.6836, Val Loss=1.1328, lr=0.0010
[02/26 20:48:19 cifar100-global-slim-3.0-mobilenetv2]: Epoch 79/100, Acc=0.6794, Val Loss=1.1374, lr=0.0010
[02/26 20:48:51 cifar100-global-slim-3.0-mobilenetv2]: Epoch 80/100, Acc=0.6838, Val Loss=1.1320, lr=0.0001
[02/26 20:49:22 cifar100-global-slim-3.0-mobilenetv2]: Epoch 81/100, Acc=0.6835, Val Loss=1.1306, lr=0.0001
[02/26 20:49:53 cifar100-global-slim-3.0-mobilenetv2]: Epoch 82/100, Acc=0.6865, Val Loss=1.1286, lr=0.0001
[02/26 20:50:24 cifar100-global-slim-3.0-mobilenetv2]: Epoch 83/100, Acc=0.6851, Val Loss=1.1257, lr=0.0001
[02/26 20:50:55 cifar100-global-slim-3.0-mobilenetv2]: Epoch 84/100, Acc=0.6860, Val Loss=1.1304, lr=0.0001
[02/26 20:51:26 cifar100-global-slim-3.0-mobilenetv2]: Epoch 85/100, Acc=0.6855, Val Loss=1.1275, lr=0.0001
[02/26 20:51:58 cifar100-global-slim-3.0-mobilenetv2]: Epoch 86/100, Acc=0.6843, Val Loss=1.1294, lr=0.0001
[02/26 20:52:29 cifar100-global-slim-3.0-mobilenetv2]: Epoch 87/100, Acc=0.6844, Val Loss=1.1274, lr=0.0001
[02/26 20:53:01 cifar100-global-slim-3.0-mobilenetv2]: Epoch 88/100, Acc=0.6844, Val Loss=1.1289, lr=0.0001
[02/26 20:53:33 cifar100-global-slim-3.0-mobilenetv2]: Epoch 89/100, Acc=0.6853, Val Loss=1.1272, lr=0.0001
[02/26 20:54:04 cifar100-global-slim-3.0-mobilenetv2]: Epoch 90/100, Acc=0.6834, Val Loss=1.1293, lr=0.0001
[02/26 20:54:35 cifar100-global-slim-3.0-mobilenetv2]: Epoch 91/100, Acc=0.6849, Val Loss=1.1267, lr=0.0001
[02/26 20:55:07 cifar100-global-slim-3.0-mobilenetv2]: Epoch 92/100, Acc=0.6841, Val Loss=1.1290, lr=0.0001
[02/26 20:55:38 cifar100-global-slim-3.0-mobilenetv2]: Epoch 93/100, Acc=0.6841, Val Loss=1.1280, lr=0.0001
[02/26 20:56:10 cifar100-global-slim-3.0-mobilenetv2]: Epoch 94/100, Acc=0.6824, Val Loss=1.1309, lr=0.0001
[02/26 20:56:41 cifar100-global-slim-3.0-mobilenetv2]: Epoch 95/100, Acc=0.6844, Val Loss=1.1266, lr=0.0001
[02/26 20:57:12 cifar100-global-slim-3.0-mobilenetv2]: Epoch 96/100, Acc=0.6830, Val Loss=1.1307, lr=0.0001
[02/26 20:57:44 cifar100-global-slim-3.0-mobilenetv2]: Epoch 97/100, Acc=0.6827, Val Loss=1.1292, lr=0.0001
[02/26 20:58:15 cifar100-global-slim-3.0-mobilenetv2]: Epoch 98/100, Acc=0.6839, Val Loss=1.1290, lr=0.0001
[02/26 20:58:47 cifar100-global-slim-3.0-mobilenetv2]: Epoch 99/100, Acc=0.6853, Val Loss=1.1300, lr=0.0001
[02/26 20:58:47 cifar100-global-slim-3.0-mobilenetv2]: Best Acc=0.6865
[02/26 20:58:47 cifar100-global-slim-3.0-mobilenetv2]: Params: 0.80 M
[02/26 20:58:47 cifar100-global-slim-3.0-mobilenetv2]: ops: 22.76 M
[02/26 20:58:50 cifar100-global-slim-3.0-mobilenetv2]: Acc: 0.6853 Val Loss: 1.1300

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: mode: prune
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: model: mobilenetv2
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: verbose: False
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: dataset: cifar100
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: dataroot: data
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: batch_size: 128
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: total_epochs: 100
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: lr: 0.01
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: restore: run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: output_dir: run/cifar100/prune/cifar100-global-group_norm-3.0-mobilenetv2
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: finetune: True
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: last_epochs: 100
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: reps: 1
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: method: group_norm
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: speed_up: 3.0
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: reg: 1e-05
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: delta_reg: 0.0001
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: weight_decay: 0.0005
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: seed: 1
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: global_pruning: True
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: sl_total_epochs: 100
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: sl_lr: 0.01
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: sl_reg_warmup: 0
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: sl_restore: None
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: iterative_steps: 400
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: logger: <Logger cifar100-global-group_norm-3.0-mobilenetv2 (DEBUG)>
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: device: cuda
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: num_classes: 100
[02/26 20:58:58 cifar100-global-group_norm-3.0-mobilenetv2]: Loading model from run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/26 20:59:01 cifar100-global-group_norm-3.0-mobilenetv2]: Pruning...
[02/26 20:59:35 cifar100-global-group_norm-3.0-mobilenetv2]: MobileNetV2(
  (pre): Sequential(
    (0): Conv2d(3, 9, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (stage1): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(9, 15, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=15)
      (4): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(15, 16, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage2): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72)
        (4): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 78, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(78, 78, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=78)
        (4): BatchNorm2d(78, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(78, 24, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage3): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(24, 71, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(71, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(71, 71, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=71)
        (4): BatchNorm2d(71, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(71, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 82, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(82, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(82, 82, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=82)
        (4): BatchNorm2d(82, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(82, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 55, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=55)
        (4): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(55, 32, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage4): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(32, 81, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(81, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(81, 81, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=81)
        (4): BatchNorm2d(81, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(81, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 33, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(33, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=33)
        (4): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(33, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 14, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(14, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=14)
        (4): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(14, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 20, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20)
        (4): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(20, 64, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage5): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(64, 175, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(175, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(175, 175, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=175)
        (4): BatchNorm2d(175, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(175, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 43, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(43, 43, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=43)
        (4): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(43, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 29, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(29, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=29)
        (4): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(29, 96, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage6): Sequential(
    (0): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(96, 172, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(172, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(172, 172, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=172)
        (4): BatchNorm2d(172, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(172, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 94, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(94, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(94, 94, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=94)
        (4): BatchNorm2d(94, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(94, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): LinearBottleNeck(
      (residual): Sequential(
        (0): Conv2d(160, 69, kernel_size=(1, 1), stride=(1, 1))
        (1): BatchNorm2d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU6(inplace=True)
        (3): Conv2d(69, 69, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=69)
        (4): BatchNorm2d(69, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU6(inplace=True)
        (6): Conv2d(69, 160, kernel_size=(1, 1), stride=(1, 1))
        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (stage7): LinearBottleNeck(
    (residual): Sequential(
      (0): Conv2d(160, 102, kernel_size=(1, 1), stride=(1, 1))
      (1): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU6(inplace=True)
      (3): Conv2d(102, 102, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=102)
      (4): BatchNorm2d(102, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (5): ReLU6(inplace=True)
      (6): Conv2d(102, 320, kernel_size=(1, 1), stride=(1, 1))
      (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (conv1): Sequential(
    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU6(inplace=True)
  )
  (conv2): Conv2d(1280, 100, kernel_size=(1, 1), stride=(1, 1))
)
[02/26 20:59:39 cifar100-global-group_norm-3.0-mobilenetv2]: Params: 2.37 M => 0.79 M (33.23%)
[02/26 20:59:39 cifar100-global-group_norm-3.0-mobilenetv2]: FLOPs: 68.40 M => 22.75 M (33.26%, 3.01X )
[02/26 20:59:39 cifar100-global-group_norm-3.0-mobilenetv2]: Acc: 0.6699 => 0.6699
[02/26 20:59:39 cifar100-global-group_norm-3.0-mobilenetv2]: Val Loss: 1.1637 => 1.1637
[02/26 20:59:39 cifar100-global-group_norm-3.0-mobilenetv2]: Finetuning...
[02/26 21:00:10 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 0/100, Acc=0.5853, Val Loss=1.4848, lr=0.0100
[02/26 21:00:40 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 1/100, Acc=0.6048, Val Loss=1.3806, lr=0.0100
[02/26 21:01:11 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 2/100, Acc=0.5913, Val Loss=1.4728, lr=0.0100
[02/26 21:01:42 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 3/100, Acc=0.5908, Val Loss=1.4680, lr=0.0100
[02/26 21:02:13 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 4/100, Acc=0.5928, Val Loss=1.4591, lr=0.0100
[02/26 21:02:44 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 5/100, Acc=0.6095, Val Loss=1.3836, lr=0.0100
[02/26 21:03:15 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 6/100, Acc=0.5931, Val Loss=1.4542, lr=0.0100
[02/26 21:03:46 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 7/100, Acc=0.6048, Val Loss=1.4005, lr=0.0100
[02/26 21:04:17 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 8/100, Acc=0.5908, Val Loss=1.4661, lr=0.0100
[02/26 21:04:48 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 9/100, Acc=0.6059, Val Loss=1.3996, lr=0.0100
[02/26 21:05:19 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 10/100, Acc=0.6093, Val Loss=1.3612, lr=0.0100
[02/26 21:05:50 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 11/100, Acc=0.6160, Val Loss=1.3713, lr=0.0100
[02/26 21:06:21 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 12/100, Acc=0.5865, Val Loss=1.5115, lr=0.0100
[02/26 21:06:52 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 13/100, Acc=0.5953, Val Loss=1.4448, lr=0.0100
[02/26 21:07:22 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 14/100, Acc=0.6047, Val Loss=1.4071, lr=0.0100
[02/26 21:07:53 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 15/100, Acc=0.5983, Val Loss=1.4224, lr=0.0100
[02/26 21:08:24 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 16/100, Acc=0.5732, Val Loss=1.5440, lr=0.0100
[02/26 21:08:57 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 17/100, Acc=0.5888, Val Loss=1.4698, lr=0.0100
[02/26 21:09:28 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 18/100, Acc=0.6143, Val Loss=1.3719, lr=0.0100
[02/26 21:10:00 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 19/100, Acc=0.5914, Val Loss=1.4753, lr=0.0100
[02/26 21:10:31 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 20/100, Acc=0.6000, Val Loss=1.4042, lr=0.0100
[02/26 21:11:02 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 21/100, Acc=0.6025, Val Loss=1.4269, lr=0.0100
[02/26 21:11:33 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 22/100, Acc=0.6043, Val Loss=1.3972, lr=0.0100
[02/26 21:12:04 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 23/100, Acc=0.6060, Val Loss=1.3998, lr=0.0100
[02/26 21:12:35 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 24/100, Acc=0.6031, Val Loss=1.4236, lr=0.0100
[02/26 21:13:07 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 25/100, Acc=0.5945, Val Loss=1.4632, lr=0.0100
[02/26 21:13:38 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 26/100, Acc=0.5902, Val Loss=1.4826, lr=0.0100
[02/26 21:14:09 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 27/100, Acc=0.5750, Val Loss=1.5243, lr=0.0100
[02/26 21:14:40 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 28/100, Acc=0.6057, Val Loss=1.4065, lr=0.0100
[02/26 21:15:12 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 29/100, Acc=0.5985, Val Loss=1.4473, lr=0.0100
[02/26 21:15:44 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 30/100, Acc=0.6047, Val Loss=1.4152, lr=0.0100
[02/26 21:16:16 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 31/100, Acc=0.5973, Val Loss=1.4490, lr=0.0100
[02/26 21:16:48 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 32/100, Acc=0.6040, Val Loss=1.4331, lr=0.0100
[02/26 21:17:20 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 33/100, Acc=0.6160, Val Loss=1.3513, lr=0.0100
[02/26 21:17:51 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 34/100, Acc=0.6165, Val Loss=1.3838, lr=0.0100
[02/26 21:18:23 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 35/100, Acc=0.6228, Val Loss=1.3430, lr=0.0100
[02/26 21:18:55 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 36/100, Acc=0.6167, Val Loss=1.3560, lr=0.0100
[02/26 21:19:28 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 37/100, Acc=0.6100, Val Loss=1.3798, lr=0.0100
[02/26 21:20:00 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 38/100, Acc=0.6179, Val Loss=1.3452, lr=0.0100
[02/26 21:20:32 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 39/100, Acc=0.5812, Val Loss=1.5354, lr=0.0100
[02/26 21:21:04 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 40/100, Acc=0.6107, Val Loss=1.3675, lr=0.0100
[02/26 21:21:36 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 41/100, Acc=0.6144, Val Loss=1.3723, lr=0.0100
[02/26 21:22:09 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 42/100, Acc=0.6082, Val Loss=1.3942, lr=0.0100
[02/26 21:22:41 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 43/100, Acc=0.6219, Val Loss=1.3554, lr=0.0100
[02/26 21:23:13 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 44/100, Acc=0.5903, Val Loss=1.4803, lr=0.0100
[02/26 21:23:45 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 45/100, Acc=0.6211, Val Loss=1.3318, lr=0.0100
[02/26 21:24:17 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 46/100, Acc=0.6094, Val Loss=1.3860, lr=0.0100
[02/26 21:24:49 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 47/100, Acc=0.6224, Val Loss=1.3350, lr=0.0100
[02/26 21:25:20 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 48/100, Acc=0.6017, Val Loss=1.4053, lr=0.0100
[02/26 21:25:52 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 49/100, Acc=0.6122, Val Loss=1.4108, lr=0.0100
[02/26 21:26:23 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 50/100, Acc=0.6232, Val Loss=1.3414, lr=0.0100
[02/26 21:26:55 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 51/100, Acc=0.6122, Val Loss=1.3824, lr=0.0100
[02/26 21:27:27 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 52/100, Acc=0.6066, Val Loss=1.4210, lr=0.0100
[02/26 21:27:58 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 53/100, Acc=0.6162, Val Loss=1.3779, lr=0.0100
[02/26 21:28:30 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 54/100, Acc=0.6133, Val Loss=1.3681, lr=0.0100
[02/26 21:29:01 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 55/100, Acc=0.6049, Val Loss=1.4063, lr=0.0100
[02/26 21:29:33 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 56/100, Acc=0.6204, Val Loss=1.3628, lr=0.0100
[02/26 21:30:05 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 57/100, Acc=0.6310, Val Loss=1.3359, lr=0.0100
[02/26 21:30:36 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 58/100, Acc=0.6106, Val Loss=1.4005, lr=0.0100
[02/26 21:31:07 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 59/100, Acc=0.6091, Val Loss=1.3823, lr=0.0100
[02/26 21:31:39 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 60/100, Acc=0.6737, Val Loss=1.1419, lr=0.0010
[02/26 21:32:11 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 61/100, Acc=0.6754, Val Loss=1.1419, lr=0.0010
[02/26 21:32:42 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 62/100, Acc=0.6760, Val Loss=1.1313, lr=0.0010
[02/26 21:33:13 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 63/100, Acc=0.6803, Val Loss=1.1289, lr=0.0010
[02/26 21:33:45 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 64/100, Acc=0.6801, Val Loss=1.1295, lr=0.0010
[02/26 21:34:17 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 65/100, Acc=0.6786, Val Loss=1.1295, lr=0.0010
[02/26 21:34:48 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 66/100, Acc=0.6789, Val Loss=1.1265, lr=0.0010
[02/26 21:35:20 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 67/100, Acc=0.6811, Val Loss=1.1279, lr=0.0010
[02/26 21:35:53 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 68/100, Acc=0.6801, Val Loss=1.1294, lr=0.0010
[02/26 21:36:25 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 69/100, Acc=0.6783, Val Loss=1.1299, lr=0.0010
[02/26 21:36:57 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 70/100, Acc=0.6822, Val Loss=1.1322, lr=0.0010
[02/26 21:37:29 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 71/100, Acc=0.6835, Val Loss=1.1304, lr=0.0010
[02/26 21:38:00 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 72/100, Acc=0.6815, Val Loss=1.1310, lr=0.0010
[02/26 21:38:32 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 73/100, Acc=0.6810, Val Loss=1.1331, lr=0.0010
[02/26 21:39:04 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 74/100, Acc=0.6815, Val Loss=1.1321, lr=0.0010
[02/26 21:39:36 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 75/100, Acc=0.6783, Val Loss=1.1390, lr=0.0010
[02/26 21:40:08 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 76/100, Acc=0.6800, Val Loss=1.1360, lr=0.0010
[02/26 21:40:43 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 77/100, Acc=0.6799, Val Loss=1.1410, lr=0.0010
[02/26 21:41:14 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 78/100, Acc=0.6803, Val Loss=1.1360, lr=0.0010
[02/26 21:41:46 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 79/100, Acc=0.6798, Val Loss=1.1413, lr=0.0010
[02/26 21:42:17 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 80/100, Acc=0.6840, Val Loss=1.1326, lr=0.0001
[02/26 21:42:49 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 81/100, Acc=0.6841, Val Loss=1.1317, lr=0.0001
[02/26 21:43:21 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 82/100, Acc=0.6843, Val Loss=1.1312, lr=0.0001
[02/26 21:43:52 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 83/100, Acc=0.6840, Val Loss=1.1308, lr=0.0001
[02/26 21:44:24 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 84/100, Acc=0.6845, Val Loss=1.1291, lr=0.0001
[02/26 21:44:56 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 85/100, Acc=0.6849, Val Loss=1.1316, lr=0.0001
[02/26 21:45:28 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 86/100, Acc=0.6835, Val Loss=1.1305, lr=0.0001
[02/26 21:46:00 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 87/100, Acc=0.6828, Val Loss=1.1297, lr=0.0001
[02/26 21:46:32 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 88/100, Acc=0.6832, Val Loss=1.1308, lr=0.0001
[02/26 21:47:03 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 89/100, Acc=0.6834, Val Loss=1.1312, lr=0.0001
[02/26 21:47:35 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 90/100, Acc=0.6834, Val Loss=1.1325, lr=0.0001
[02/26 21:48:06 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 91/100, Acc=0.6843, Val Loss=1.1313, lr=0.0001
[02/26 21:48:37 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 92/100, Acc=0.6813, Val Loss=1.1328, lr=0.0001
[02/26 21:49:08 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 93/100, Acc=0.6821, Val Loss=1.1331, lr=0.0001
[02/26 21:49:40 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 94/100, Acc=0.6814, Val Loss=1.1337, lr=0.0001
[02/26 21:50:11 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 95/100, Acc=0.6822, Val Loss=1.1324, lr=0.0001
[02/26 21:50:42 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 96/100, Acc=0.6833, Val Loss=1.1300, lr=0.0001
[02/26 21:51:13 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 97/100, Acc=0.6822, Val Loss=1.1324, lr=0.0001
[02/26 21:51:44 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 98/100, Acc=0.6832, Val Loss=1.1314, lr=0.0001
[02/26 21:52:15 cifar100-global-group_norm-3.0-mobilenetv2]: Epoch 99/100, Acc=0.6837, Val Loss=1.1323, lr=0.0001
[02/26 21:52:15 cifar100-global-group_norm-3.0-mobilenetv2]: Best Acc=0.6849
[02/26 21:52:16 cifar100-global-group_norm-3.0-mobilenetv2]: Params: 0.79 M
[02/26 21:52:16 cifar100-global-group_norm-3.0-mobilenetv2]: ops: 22.75 M
[02/26 21:52:19 cifar100-global-group_norm-3.0-mobilenetv2]: Acc: 0.6837 Val Loss: 1.1323

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: mode: prune
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: model: mobilenetv2
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: verbose: False
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: dataset: cifar100
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: dataroot: data
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: batch_size: 128
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: total_epochs: 100
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: lr_decay_milestones: 60,80
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: lr_decay_gamma: 0.1
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: lr: 0.01
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: restore: run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: output_dir: run/cifar100/prune/cifar100-global-group_sl-3.0-mobilenetv2
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: finetune: True
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: last_epochs: 100
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: reps: 1
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: method: group_sl
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: speed_up: 3.0
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: max_pruning_ratio: 1.0
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: soft_keeping_ratio: 0.0
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: reg: 1e-05
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: delta_reg: 0.0001
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: weight_decay: 0.0005
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: seed: 1
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: global_pruning: True
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: sl_total_epochs: 100
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: sl_lr: 0.01
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: sl_lr_decay_milestones: 60,80
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: sl_reg_warmup: 0
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: sl_restore: None
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: iterative_steps: 400
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: logger: <Logger cifar100-global-group_sl-3.0-mobilenetv2 (DEBUG)>
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: device: cuda
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: num_classes: 100
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: Loading model from run/cifar100/pretrain/cifar100_mobilenetv2.pth
[02/26 21:52:27 cifar100-global-group_sl-3.0-mobilenetv2]: Regularizing...
[02/26 21:53:38 cifar100-global-group_sl-3.0-mobilenetv2]: Epoch 0/100, Acc=0.6030, Val Loss=1.4118, lr=0.0100
[02/26 21:54:49 cifar100-global-group_sl-3.0-mobilenetv2]: Epoch 1/100, Acc=0.5980, Val Loss=1.4358, lr=0.0100
[02/26 21:56:01 cifar100-global-group_sl-3.0-mobilenetv2]: Epoch 2/100, Acc=0.6052, Val Loss=1.4156, lr=0.0100
[02/26 21:57:13 cifar100-global-group_sl-3.0-mobilenetv2]: Epoch 3/100, Acc=0.5992, Val Loss=1.4396, lr=0.0100
[02/26 21:58:25 cifar100-global-group_sl-3.0-mobilenetv2]: Epoch 4/100, Acc=0.6129, Val Loss=1.3701, lr=0.0100
[02/26 21:59:36 cifar100-global-group_sl-3.0-mobilenetv2]: Epoch 5/100, Acc=0.6052, Val Loss=1.3965, lr=0.0100
slurmstepd-cf-prod-node-021: error: *** JOB 20495 ON cf-prod-node-021 CANCELLED AT 2025-02-26T22:00:24 ***
