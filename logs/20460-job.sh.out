SLURM WORKLOAD START: Fri Feb 21 16:13:51 CET 2025
Fri Feb 21 16:13:51 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-PCIE-40GB          Off |   00000000:0B:00.0 Off |                    0 |
| N/A   36C    P0             36W /  250W |       1MiB /  40960MiB |     47%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: mode: prune
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: model: resnet56
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: verbose: False
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: dataset: cifar100
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: dataroot: data
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: batch_size: 128
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: total_epochs: 100
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: lr_decay_milestones: 60,80
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: lr_decay_gamma: 0.1
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: lr: 0.01
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: output_dir: run/cifar100/prune/cifar100-global-random-3.0-resnet56
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: finetune: True
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: last_epochs: 100
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: reps: 1
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: method: random
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: speed_up: 3.0
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: max_pruning_ratio: 1.0
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: reg: 1e-05
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: delta_reg: 0.0001
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: weight_decay: 0.0005
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: seed: 1
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: global_pruning: True
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: sl_total_epochs: 100
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: sl_lr: 0.01
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: sl_reg_warmup: 0
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: sl_restore: None
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: iterative_steps: 400
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: logger: <Logger cifar100-global-random-3.0-resnet56 (DEBUG)>
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: device: cuda
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: num_classes: 100
[02/21 16:13:58 cifar100-global-random-3.0-resnet56]: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 16:14:02 cifar100-global-random-3.0-resnet56]: Pruning...
[02/21 16:14:10 cifar100-global-random-3.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(2, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(2, 18, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(18, 50, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(18, 50, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=50, out_features=100, bias=True)
)
[02/21 16:14:13 cifar100-global-random-3.0-resnet56]: Params: 0.86 M => 0.45 M (52.21%)
[02/21 16:14:13 cifar100-global-random-3.0-resnet56]: FLOPs: 127.12 M => 39.03 M (30.70%, 3.26X )
[02/21 16:14:13 cifar100-global-random-3.0-resnet56]: Acc: 0.7269 => 0.0100
[02/21 16:14:13 cifar100-global-random-3.0-resnet56]: Val Loss: 1.1578 => 22.4019
[02/21 16:14:13 cifar100-global-random-3.0-resnet56]: Finetuning...
[02/21 16:14:40 cifar100-global-random-3.0-resnet56]: Epoch 0/100, Acc=0.2287, Val Loss=3.2025, lr=0.0100
[02/21 16:15:07 cifar100-global-random-3.0-resnet56]: Epoch 1/100, Acc=0.3757, Val Loss=2.4780, lr=0.0100
[02/21 16:15:34 cifar100-global-random-3.0-resnet56]: Epoch 2/100, Acc=0.4216, Val Loss=2.2664, lr=0.0100
[02/21 16:16:01 cifar100-global-random-3.0-resnet56]: Epoch 3/100, Acc=0.4531, Val Loss=2.1542, lr=0.0100
[02/21 16:16:28 cifar100-global-random-3.0-resnet56]: Epoch 4/100, Acc=0.4808, Val Loss=1.9509, lr=0.0100
[02/21 16:16:55 cifar100-global-random-3.0-resnet56]: Epoch 5/100, Acc=0.4835, Val Loss=2.0132, lr=0.0100
[02/21 16:17:22 cifar100-global-random-3.0-resnet56]: Epoch 6/100, Acc=0.5174, Val Loss=1.8325, lr=0.0100
[02/21 16:17:49 cifar100-global-random-3.0-resnet56]: Epoch 7/100, Acc=0.5281, Val Loss=1.7575, lr=0.0100
[02/21 16:18:16 cifar100-global-random-3.0-resnet56]: Epoch 8/100, Acc=0.5349, Val Loss=1.7336, lr=0.0100
[02/21 16:18:43 cifar100-global-random-3.0-resnet56]: Epoch 9/100, Acc=0.4705, Val Loss=2.1237, lr=0.0100
[02/21 16:19:10 cifar100-global-random-3.0-resnet56]: Epoch 10/100, Acc=0.5142, Val Loss=1.8513, lr=0.0100
[02/21 16:19:37 cifar100-global-random-3.0-resnet56]: Epoch 11/100, Acc=0.5364, Val Loss=1.7633, lr=0.0100
[02/21 16:20:04 cifar100-global-random-3.0-resnet56]: Epoch 12/100, Acc=0.5703, Val Loss=1.5768, lr=0.0100
[02/21 16:20:31 cifar100-global-random-3.0-resnet56]: Epoch 13/100, Acc=0.5477, Val Loss=1.7237, lr=0.0100
[02/21 16:20:58 cifar100-global-random-3.0-resnet56]: Epoch 14/100, Acc=0.5303, Val Loss=1.7852, lr=0.0100
[02/21 16:21:25 cifar100-global-random-3.0-resnet56]: Epoch 15/100, Acc=0.5563, Val Loss=1.6896, lr=0.0100
[02/21 16:21:52 cifar100-global-random-3.0-resnet56]: Epoch 16/100, Acc=0.5411, Val Loss=1.7413, lr=0.0100
[02/21 16:22:19 cifar100-global-random-3.0-resnet56]: Epoch 17/100, Acc=0.5653, Val Loss=1.6248, lr=0.0100
[02/21 16:22:46 cifar100-global-random-3.0-resnet56]: Epoch 18/100, Acc=0.5829, Val Loss=1.5550, lr=0.0100
[02/21 16:23:13 cifar100-global-random-3.0-resnet56]: Epoch 19/100, Acc=0.5712, Val Loss=1.5869, lr=0.0100
[02/21 16:23:40 cifar100-global-random-3.0-resnet56]: Epoch 20/100, Acc=0.5683, Val Loss=1.6239, lr=0.0100
[02/21 16:24:07 cifar100-global-random-3.0-resnet56]: Epoch 21/100, Acc=0.5584, Val Loss=1.6985, lr=0.0100
[02/21 16:24:34 cifar100-global-random-3.0-resnet56]: Epoch 22/100, Acc=0.5732, Val Loss=1.6040, lr=0.0100
[02/21 16:25:01 cifar100-global-random-3.0-resnet56]: Epoch 23/100, Acc=0.5706, Val Loss=1.6302, lr=0.0100
[02/21 16:25:28 cifar100-global-random-3.0-resnet56]: Epoch 24/100, Acc=0.5840, Val Loss=1.5687, lr=0.0100
[02/21 16:25:55 cifar100-global-random-3.0-resnet56]: Epoch 25/100, Acc=0.5838, Val Loss=1.5768, lr=0.0100
[02/21 16:26:22 cifar100-global-random-3.0-resnet56]: Epoch 26/100, Acc=0.6021, Val Loss=1.4972, lr=0.0100
[02/21 16:26:49 cifar100-global-random-3.0-resnet56]: Epoch 27/100, Acc=0.5887, Val Loss=1.5469, lr=0.0100
[02/21 16:27:16 cifar100-global-random-3.0-resnet56]: Epoch 28/100, Acc=0.5547, Val Loss=1.7302, lr=0.0100
[02/21 16:27:43 cifar100-global-random-3.0-resnet56]: Epoch 29/100, Acc=0.5685, Val Loss=1.7053, lr=0.0100
[02/21 16:28:10 cifar100-global-random-3.0-resnet56]: Epoch 30/100, Acc=0.5715, Val Loss=1.6799, lr=0.0100
[02/21 16:28:37 cifar100-global-random-3.0-resnet56]: Epoch 31/100, Acc=0.5561, Val Loss=1.6785, lr=0.0100
[02/21 16:29:04 cifar100-global-random-3.0-resnet56]: Epoch 32/100, Acc=0.5482, Val Loss=1.7748, lr=0.0100
[02/21 16:29:31 cifar100-global-random-3.0-resnet56]: Epoch 33/100, Acc=0.5750, Val Loss=1.6402, lr=0.0100
[02/21 16:29:58 cifar100-global-random-3.0-resnet56]: Epoch 34/100, Acc=0.5912, Val Loss=1.5652, lr=0.0100
[02/21 16:30:25 cifar100-global-random-3.0-resnet56]: Epoch 35/100, Acc=0.5793, Val Loss=1.6233, lr=0.0100
[02/21 16:30:52 cifar100-global-random-3.0-resnet56]: Epoch 36/100, Acc=0.5554, Val Loss=1.7909, lr=0.0100
[02/21 16:31:20 cifar100-global-random-3.0-resnet56]: Epoch 37/100, Acc=0.5809, Val Loss=1.6093, lr=0.0100
[02/21 16:31:47 cifar100-global-random-3.0-resnet56]: Epoch 38/100, Acc=0.5813, Val Loss=1.6087, lr=0.0100
[02/21 16:32:14 cifar100-global-random-3.0-resnet56]: Epoch 39/100, Acc=0.5984, Val Loss=1.5184, lr=0.0100
[02/21 16:32:41 cifar100-global-random-3.0-resnet56]: Epoch 40/100, Acc=0.5891, Val Loss=1.5407, lr=0.0100
[02/21 16:33:08 cifar100-global-random-3.0-resnet56]: Epoch 41/100, Acc=0.5983, Val Loss=1.5403, lr=0.0100
[02/21 16:33:35 cifar100-global-random-3.0-resnet56]: Epoch 42/100, Acc=0.5728, Val Loss=1.6555, lr=0.0100
[02/21 16:34:02 cifar100-global-random-3.0-resnet56]: Epoch 43/100, Acc=0.5988, Val Loss=1.5201, lr=0.0100
[02/21 16:34:29 cifar100-global-random-3.0-resnet56]: Epoch 44/100, Acc=0.6036, Val Loss=1.4982, lr=0.0100
[02/21 16:34:56 cifar100-global-random-3.0-resnet56]: Epoch 45/100, Acc=0.5856, Val Loss=1.5813, lr=0.0100
[02/21 16:35:23 cifar100-global-random-3.0-resnet56]: Epoch 46/100, Acc=0.5994, Val Loss=1.5295, lr=0.0100
[02/21 16:35:50 cifar100-global-random-3.0-resnet56]: Epoch 47/100, Acc=0.5947, Val Loss=1.5456, lr=0.0100
[02/21 16:36:17 cifar100-global-random-3.0-resnet56]: Epoch 48/100, Acc=0.5931, Val Loss=1.5765, lr=0.0100
[02/21 16:36:45 cifar100-global-random-3.0-resnet56]: Epoch 49/100, Acc=0.5873, Val Loss=1.5814, lr=0.0100
[02/21 16:37:12 cifar100-global-random-3.0-resnet56]: Epoch 50/100, Acc=0.5755, Val Loss=1.6957, lr=0.0100
[02/21 16:37:39 cifar100-global-random-3.0-resnet56]: Epoch 51/100, Acc=0.5615, Val Loss=1.7325, lr=0.0100
[02/21 16:38:07 cifar100-global-random-3.0-resnet56]: Epoch 52/100, Acc=0.5561, Val Loss=1.8330, lr=0.0100
[02/21 16:38:34 cifar100-global-random-3.0-resnet56]: Epoch 53/100, Acc=0.5983, Val Loss=1.5450, lr=0.0100
[02/21 16:39:02 cifar100-global-random-3.0-resnet56]: Epoch 54/100, Acc=0.5920, Val Loss=1.6316, lr=0.0100
[02/21 16:39:29 cifar100-global-random-3.0-resnet56]: Epoch 55/100, Acc=0.5949, Val Loss=1.5747, lr=0.0100
[02/21 16:39:56 cifar100-global-random-3.0-resnet56]: Epoch 56/100, Acc=0.5945, Val Loss=1.5721, lr=0.0100
[02/21 16:40:24 cifar100-global-random-3.0-resnet56]: Epoch 57/100, Acc=0.5809, Val Loss=1.7018, lr=0.0100
[02/21 16:40:51 cifar100-global-random-3.0-resnet56]: Epoch 58/100, Acc=0.6016, Val Loss=1.5554, lr=0.0100
[02/21 16:41:18 cifar100-global-random-3.0-resnet56]: Epoch 59/100, Acc=0.5912, Val Loss=1.5736, lr=0.0100
[02/21 16:41:46 cifar100-global-random-3.0-resnet56]: Epoch 60/100, Acc=0.6617, Val Loss=1.2571, lr=0.0010
[02/21 16:42:13 cifar100-global-random-3.0-resnet56]: Epoch 61/100, Acc=0.6598, Val Loss=1.2537, lr=0.0010
[02/21 16:42:40 cifar100-global-random-3.0-resnet56]: Epoch 62/100, Acc=0.6651, Val Loss=1.2500, lr=0.0010
[02/21 16:43:08 cifar100-global-random-3.0-resnet56]: Epoch 63/100, Acc=0.6668, Val Loss=1.2488, lr=0.0010
[02/21 16:43:35 cifar100-global-random-3.0-resnet56]: Epoch 64/100, Acc=0.6658, Val Loss=1.2593, lr=0.0010
[02/21 16:44:01 cifar100-global-random-3.0-resnet56]: Epoch 65/100, Acc=0.6666, Val Loss=1.2672, lr=0.0010
[02/21 16:44:28 cifar100-global-random-3.0-resnet56]: Epoch 66/100, Acc=0.6661, Val Loss=1.2756, lr=0.0010
[02/21 16:44:55 cifar100-global-random-3.0-resnet56]: Epoch 67/100, Acc=0.6647, Val Loss=1.2678, lr=0.0010
[02/21 16:45:22 cifar100-global-random-3.0-resnet56]: Epoch 68/100, Acc=0.6675, Val Loss=1.2694, lr=0.0010
[02/21 16:45:49 cifar100-global-random-3.0-resnet56]: Epoch 69/100, Acc=0.6637, Val Loss=1.2787, lr=0.0010
[02/21 16:46:16 cifar100-global-random-3.0-resnet56]: Epoch 70/100, Acc=0.6663, Val Loss=1.2742, lr=0.0010
[02/21 16:46:43 cifar100-global-random-3.0-resnet56]: Epoch 71/100, Acc=0.6651, Val Loss=1.2825, lr=0.0010
[02/21 16:47:11 cifar100-global-random-3.0-resnet56]: Epoch 72/100, Acc=0.6688, Val Loss=1.2772, lr=0.0010
[02/21 16:47:38 cifar100-global-random-3.0-resnet56]: Epoch 73/100, Acc=0.6685, Val Loss=1.2814, lr=0.0010
[02/21 16:48:05 cifar100-global-random-3.0-resnet56]: Epoch 74/100, Acc=0.6688, Val Loss=1.2818, lr=0.0010
[02/21 16:48:33 cifar100-global-random-3.0-resnet56]: Epoch 75/100, Acc=0.6672, Val Loss=1.2921, lr=0.0010
[02/21 16:49:00 cifar100-global-random-3.0-resnet56]: Epoch 76/100, Acc=0.6697, Val Loss=1.2871, lr=0.0010
[02/21 16:49:27 cifar100-global-random-3.0-resnet56]: Epoch 77/100, Acc=0.6652, Val Loss=1.3023, lr=0.0010
[02/21 16:49:54 cifar100-global-random-3.0-resnet56]: Epoch 78/100, Acc=0.6619, Val Loss=1.3072, lr=0.0010
[02/21 16:50:21 cifar100-global-random-3.0-resnet56]: Epoch 79/100, Acc=0.6628, Val Loss=1.3194, lr=0.0010
[02/21 16:50:48 cifar100-global-random-3.0-resnet56]: Epoch 80/100, Acc=0.6673, Val Loss=1.2986, lr=0.0001
[02/21 16:51:15 cifar100-global-random-3.0-resnet56]: Epoch 81/100, Acc=0.6693, Val Loss=1.2938, lr=0.0001
[02/21 16:51:42 cifar100-global-random-3.0-resnet56]: Epoch 82/100, Acc=0.6662, Val Loss=1.2941, lr=0.0001
[02/21 16:52:09 cifar100-global-random-3.0-resnet56]: Epoch 83/100, Acc=0.6684, Val Loss=1.2957, lr=0.0001
[02/21 16:52:36 cifar100-global-random-3.0-resnet56]: Epoch 84/100, Acc=0.6668, Val Loss=1.2980, lr=0.0001
[02/21 16:53:02 cifar100-global-random-3.0-resnet56]: Epoch 85/100, Acc=0.6681, Val Loss=1.2973, lr=0.0001
[02/21 16:53:29 cifar100-global-random-3.0-resnet56]: Epoch 86/100, Acc=0.6696, Val Loss=1.2992, lr=0.0001
[02/21 16:53:56 cifar100-global-random-3.0-resnet56]: Epoch 87/100, Acc=0.6677, Val Loss=1.2996, lr=0.0001
[02/21 16:54:23 cifar100-global-random-3.0-resnet56]: Epoch 88/100, Acc=0.6677, Val Loss=1.2946, lr=0.0001
[02/21 16:54:50 cifar100-global-random-3.0-resnet56]: Epoch 89/100, Acc=0.6670, Val Loss=1.2969, lr=0.0001
[02/21 16:55:16 cifar100-global-random-3.0-resnet56]: Epoch 90/100, Acc=0.6687, Val Loss=1.2950, lr=0.0001
[02/21 16:55:43 cifar100-global-random-3.0-resnet56]: Epoch 91/100, Acc=0.6669, Val Loss=1.3034, lr=0.0001
[02/21 16:56:10 cifar100-global-random-3.0-resnet56]: Epoch 92/100, Acc=0.6708, Val Loss=1.2922, lr=0.0001
[02/21 16:56:37 cifar100-global-random-3.0-resnet56]: Epoch 93/100, Acc=0.6671, Val Loss=1.3000, lr=0.0001
[02/21 16:57:04 cifar100-global-random-3.0-resnet56]: Epoch 94/100, Acc=0.6705, Val Loss=1.2988, lr=0.0001
[02/21 16:57:31 cifar100-global-random-3.0-resnet56]: Epoch 95/100, Acc=0.6679, Val Loss=1.3026, lr=0.0001
[02/21 16:57:58 cifar100-global-random-3.0-resnet56]: Epoch 96/100, Acc=0.6670, Val Loss=1.3024, lr=0.0001
[02/21 16:58:25 cifar100-global-random-3.0-resnet56]: Epoch 97/100, Acc=0.6658, Val Loss=1.3000, lr=0.0001
[02/21 16:58:51 cifar100-global-random-3.0-resnet56]: Epoch 98/100, Acc=0.6680, Val Loss=1.3008, lr=0.0001
[02/21 16:59:18 cifar100-global-random-3.0-resnet56]: Epoch 99/100, Acc=0.6671, Val Loss=1.3013, lr=0.0001
[02/21 16:59:18 cifar100-global-random-3.0-resnet56]: Best Acc=0.6708
[02/21 16:59:18 cifar100-global-random-3.0-resnet56]: Params: 0.45 M
[02/21 16:59:18 cifar100-global-random-3.0-resnet56]: ops: 39.03 M
[02/21 16:59:21 cifar100-global-random-3.0-resnet56]: Acc: 0.6671 Val Loss: 1.3013

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: mode: prune
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: model: resnet56
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: verbose: False
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: dataset: cifar100
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: dataroot: data
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: batch_size: 128
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: total_epochs: 100
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: lr_decay_milestones: 60,80
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: lr_decay_gamma: 0.1
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: lr: 0.01
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: output_dir: run/cifar100/prune/cifar100-global-l2-3.0-resnet56
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: finetune: True
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: last_epochs: 100
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: reps: 1
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: method: l2
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: speed_up: 3.0
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: max_pruning_ratio: 1.0
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: reg: 1e-05
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: delta_reg: 0.0001
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: weight_decay: 0.0005
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: seed: 1
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: global_pruning: True
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: sl_total_epochs: 100
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: sl_lr: 0.01
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: sl_reg_warmup: 0
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: sl_restore: None
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: iterative_steps: 400
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: logger: <Logger cifar100-global-l2-3.0-resnet56 (DEBUG)>
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: device: cuda
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: num_classes: 100
[02/21 16:59:28 cifar100-global-l2-3.0-resnet56]: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 16:59:32 cifar100-global-l2-3.0-resnet56]: Pruning...
[02/21 16:59:48 cifar100-global-l2-3.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(7, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(7, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(7, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(7, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(7, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(7, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(7, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(7, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(7, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(7, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(7, 9, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(9, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(9, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(9, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(9, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(9, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(9, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(9, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(9, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(40, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(9, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 43, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(43, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(64, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(64, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(64, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(64, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(58, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=100, bias=True)
)
[02/21 16:59:51 cifar100-global-l2-3.0-resnet56]: Params: 0.86 M => 0.58 M (66.81%)
[02/21 16:59:51 cifar100-global-l2-3.0-resnet56]: FLOPs: 127.12 M => 42.33 M (33.29%, 3.00X )
[02/21 16:59:51 cifar100-global-l2-3.0-resnet56]: Acc: 0.7269 => 0.0147
[02/21 16:59:51 cifar100-global-l2-3.0-resnet56]: Val Loss: 1.1578 => 15.9639
[02/21 16:59:51 cifar100-global-l2-3.0-resnet56]: Finetuning...
[02/21 17:00:18 cifar100-global-l2-3.0-resnet56]: Epoch 0/100, Acc=0.4913, Val Loss=2.0116, lr=0.0100
[02/21 17:00:46 cifar100-global-l2-3.0-resnet56]: Epoch 1/100, Acc=0.5623, Val Loss=1.6519, lr=0.0100
[02/21 17:01:14 cifar100-global-l2-3.0-resnet56]: Epoch 2/100, Acc=0.5673, Val Loss=1.6267, lr=0.0100
[02/21 17:01:42 cifar100-global-l2-3.0-resnet56]: Epoch 3/100, Acc=0.5766, Val Loss=1.5848, lr=0.0100
[02/21 17:02:09 cifar100-global-l2-3.0-resnet56]: Epoch 4/100, Acc=0.5643, Val Loss=1.6929, lr=0.0100
[02/21 17:02:36 cifar100-global-l2-3.0-resnet56]: Epoch 5/100, Acc=0.5800, Val Loss=1.6234, lr=0.0100
[02/21 17:03:03 cifar100-global-l2-3.0-resnet56]: Epoch 6/100, Acc=0.6083, Val Loss=1.4852, lr=0.0100
[02/21 17:03:31 cifar100-global-l2-3.0-resnet56]: Epoch 7/100, Acc=0.5970, Val Loss=1.4797, lr=0.0100
[02/21 17:03:59 cifar100-global-l2-3.0-resnet56]: Epoch 8/100, Acc=0.6221, Val Loss=1.4453, lr=0.0100
[02/21 17:04:26 cifar100-global-l2-3.0-resnet56]: Epoch 9/100, Acc=0.6100, Val Loss=1.4671, lr=0.0100
[02/21 17:04:54 cifar100-global-l2-3.0-resnet56]: Epoch 10/100, Acc=0.5915, Val Loss=1.5604, lr=0.0100
[02/21 17:05:21 cifar100-global-l2-3.0-resnet56]: Epoch 11/100, Acc=0.5906, Val Loss=1.5863, lr=0.0100
[02/21 17:05:49 cifar100-global-l2-3.0-resnet56]: Epoch 12/100, Acc=0.5997, Val Loss=1.5773, lr=0.0100
[02/21 17:06:17 cifar100-global-l2-3.0-resnet56]: Epoch 13/100, Acc=0.5702, Val Loss=1.7177, lr=0.0100
[02/21 17:06:45 cifar100-global-l2-3.0-resnet56]: Epoch 14/100, Acc=0.6026, Val Loss=1.5082, lr=0.0100
[02/21 17:07:12 cifar100-global-l2-3.0-resnet56]: Epoch 15/100, Acc=0.6024, Val Loss=1.5147, lr=0.0100
[02/21 17:07:40 cifar100-global-l2-3.0-resnet56]: Epoch 16/100, Acc=0.6079, Val Loss=1.4955, lr=0.0100
[02/21 17:08:08 cifar100-global-l2-3.0-resnet56]: Epoch 17/100, Acc=0.6174, Val Loss=1.4872, lr=0.0100
[02/21 17:08:35 cifar100-global-l2-3.0-resnet56]: Epoch 18/100, Acc=0.6223, Val Loss=1.4308, lr=0.0100
[02/21 17:09:03 cifar100-global-l2-3.0-resnet56]: Epoch 19/100, Acc=0.6128, Val Loss=1.4889, lr=0.0100
[02/21 17:09:31 cifar100-global-l2-3.0-resnet56]: Epoch 20/100, Acc=0.6011, Val Loss=1.5725, lr=0.0100
[02/21 17:09:59 cifar100-global-l2-3.0-resnet56]: Epoch 21/100, Acc=0.6153, Val Loss=1.4817, lr=0.0100
[02/21 17:10:27 cifar100-global-l2-3.0-resnet56]: Epoch 22/100, Acc=0.6123, Val Loss=1.4847, lr=0.0100
[02/21 17:10:55 cifar100-global-l2-3.0-resnet56]: Epoch 23/100, Acc=0.6367, Val Loss=1.3602, lr=0.0100
[02/21 17:11:22 cifar100-global-l2-3.0-resnet56]: Epoch 24/100, Acc=0.6202, Val Loss=1.4679, lr=0.0100
[02/21 17:11:50 cifar100-global-l2-3.0-resnet56]: Epoch 25/100, Acc=0.6221, Val Loss=1.4692, lr=0.0100
[02/21 17:12:18 cifar100-global-l2-3.0-resnet56]: Epoch 26/100, Acc=0.6232, Val Loss=1.4745, lr=0.0100
[02/21 17:12:45 cifar100-global-l2-3.0-resnet56]: Epoch 27/100, Acc=0.6140, Val Loss=1.5019, lr=0.0100
[02/21 17:13:13 cifar100-global-l2-3.0-resnet56]: Epoch 28/100, Acc=0.5891, Val Loss=1.6058, lr=0.0100
[02/21 17:13:40 cifar100-global-l2-3.0-resnet56]: Epoch 29/100, Acc=0.6214, Val Loss=1.4759, lr=0.0100
[02/21 17:14:08 cifar100-global-l2-3.0-resnet56]: Epoch 30/100, Acc=0.6293, Val Loss=1.4388, lr=0.0100
[02/21 17:14:35 cifar100-global-l2-3.0-resnet56]: Epoch 31/100, Acc=0.6058, Val Loss=1.5475, lr=0.0100
[02/21 17:15:03 cifar100-global-l2-3.0-resnet56]: Epoch 32/100, Acc=0.6197, Val Loss=1.4988, lr=0.0100
[02/21 17:15:30 cifar100-global-l2-3.0-resnet56]: Epoch 33/100, Acc=0.6250, Val Loss=1.4629, lr=0.0100
[02/21 17:15:58 cifar100-global-l2-3.0-resnet56]: Epoch 34/100, Acc=0.6183, Val Loss=1.4898, lr=0.0100
[02/21 17:16:25 cifar100-global-l2-3.0-resnet56]: Epoch 35/100, Acc=0.6373, Val Loss=1.3774, lr=0.0100
[02/21 17:16:53 cifar100-global-l2-3.0-resnet56]: Epoch 36/100, Acc=0.6222, Val Loss=1.5035, lr=0.0100
[02/21 17:17:20 cifar100-global-l2-3.0-resnet56]: Epoch 37/100, Acc=0.6286, Val Loss=1.4802, lr=0.0100
[02/21 17:17:48 cifar100-global-l2-3.0-resnet56]: Epoch 38/100, Acc=0.6074, Val Loss=1.5682, lr=0.0100
[02/21 17:18:15 cifar100-global-l2-3.0-resnet56]: Epoch 39/100, Acc=0.6243, Val Loss=1.4815, lr=0.0100
[02/21 17:18:43 cifar100-global-l2-3.0-resnet56]: Epoch 40/100, Acc=0.6165, Val Loss=1.5262, lr=0.0100
[02/21 17:19:11 cifar100-global-l2-3.0-resnet56]: Epoch 41/100, Acc=0.6323, Val Loss=1.4361, lr=0.0100
[02/21 17:19:38 cifar100-global-l2-3.0-resnet56]: Epoch 42/100, Acc=0.6169, Val Loss=1.5240, lr=0.0100
[02/21 17:20:06 cifar100-global-l2-3.0-resnet56]: Epoch 43/100, Acc=0.5991, Val Loss=1.6112, lr=0.0100
[02/21 17:20:34 cifar100-global-l2-3.0-resnet56]: Epoch 44/100, Acc=0.6235, Val Loss=1.4696, lr=0.0100
[02/21 17:21:02 cifar100-global-l2-3.0-resnet56]: Epoch 45/100, Acc=0.6115, Val Loss=1.5566, lr=0.0100
[02/21 17:21:30 cifar100-global-l2-3.0-resnet56]: Epoch 46/100, Acc=0.6273, Val Loss=1.4796, lr=0.0100
[02/21 17:21:57 cifar100-global-l2-3.0-resnet56]: Epoch 47/100, Acc=0.6152, Val Loss=1.5455, lr=0.0100
[02/21 17:22:25 cifar100-global-l2-3.0-resnet56]: Epoch 48/100, Acc=0.6251, Val Loss=1.4844, lr=0.0100
[02/21 17:22:52 cifar100-global-l2-3.0-resnet56]: Epoch 49/100, Acc=0.5812, Val Loss=1.7383, lr=0.0100
[02/21 17:23:19 cifar100-global-l2-3.0-resnet56]: Epoch 50/100, Acc=0.6296, Val Loss=1.4703, lr=0.0100
[02/21 17:23:46 cifar100-global-l2-3.0-resnet56]: Epoch 51/100, Acc=0.6230, Val Loss=1.4522, lr=0.0100
[02/21 17:24:14 cifar100-global-l2-3.0-resnet56]: Epoch 52/100, Acc=0.6210, Val Loss=1.4825, lr=0.0100
[02/21 17:24:41 cifar100-global-l2-3.0-resnet56]: Epoch 53/100, Acc=0.6200, Val Loss=1.5059, lr=0.0100
[02/21 17:25:08 cifar100-global-l2-3.0-resnet56]: Epoch 54/100, Acc=0.6334, Val Loss=1.4283, lr=0.0100
[02/21 17:25:36 cifar100-global-l2-3.0-resnet56]: Epoch 55/100, Acc=0.6298, Val Loss=1.4514, lr=0.0100
[02/21 17:26:03 cifar100-global-l2-3.0-resnet56]: Epoch 56/100, Acc=0.6110, Val Loss=1.5243, lr=0.0100
[02/21 17:26:30 cifar100-global-l2-3.0-resnet56]: Epoch 57/100, Acc=0.6278, Val Loss=1.4733, lr=0.0100
[02/21 17:26:57 cifar100-global-l2-3.0-resnet56]: Epoch 58/100, Acc=0.6234, Val Loss=1.5222, lr=0.0100
[02/21 17:27:25 cifar100-global-l2-3.0-resnet56]: Epoch 59/100, Acc=0.6239, Val Loss=1.4950, lr=0.0100
[02/21 17:27:52 cifar100-global-l2-3.0-resnet56]: Epoch 60/100, Acc=0.6864, Val Loss=1.1895, lr=0.0010
[02/21 17:28:19 cifar100-global-l2-3.0-resnet56]: Epoch 61/100, Acc=0.6880, Val Loss=1.1978, lr=0.0010
[02/21 17:28:47 cifar100-global-l2-3.0-resnet56]: Epoch 62/100, Acc=0.6890, Val Loss=1.1922, lr=0.0010
[02/21 17:29:14 cifar100-global-l2-3.0-resnet56]: Epoch 63/100, Acc=0.6900, Val Loss=1.2013, lr=0.0010
[02/21 17:29:42 cifar100-global-l2-3.0-resnet56]: Epoch 64/100, Acc=0.6886, Val Loss=1.2083, lr=0.0010
[02/21 17:30:09 cifar100-global-l2-3.0-resnet56]: Epoch 65/100, Acc=0.6870, Val Loss=1.2157, lr=0.0010
[02/21 17:30:36 cifar100-global-l2-3.0-resnet56]: Epoch 66/100, Acc=0.6886, Val Loss=1.2226, lr=0.0010
[02/21 17:31:04 cifar100-global-l2-3.0-resnet56]: Epoch 67/100, Acc=0.6866, Val Loss=1.2297, lr=0.0010
[02/21 17:31:31 cifar100-global-l2-3.0-resnet56]: Epoch 68/100, Acc=0.6851, Val Loss=1.2363, lr=0.0010
[02/21 17:31:59 cifar100-global-l2-3.0-resnet56]: Epoch 69/100, Acc=0.6832, Val Loss=1.2440, lr=0.0010
[02/21 17:32:26 cifar100-global-l2-3.0-resnet56]: Epoch 70/100, Acc=0.6891, Val Loss=1.2440, lr=0.0010
[02/21 17:32:54 cifar100-global-l2-3.0-resnet56]: Epoch 71/100, Acc=0.6876, Val Loss=1.2502, lr=0.0010
[02/21 17:33:21 cifar100-global-l2-3.0-resnet56]: Epoch 72/100, Acc=0.6880, Val Loss=1.2542, lr=0.0010
[02/21 17:33:49 cifar100-global-l2-3.0-resnet56]: Epoch 73/100, Acc=0.6857, Val Loss=1.2562, lr=0.0010
[02/21 17:34:16 cifar100-global-l2-3.0-resnet56]: Epoch 74/100, Acc=0.6863, Val Loss=1.2613, lr=0.0010
[02/21 17:34:44 cifar100-global-l2-3.0-resnet56]: Epoch 75/100, Acc=0.6889, Val Loss=1.2603, lr=0.0010
[02/21 17:35:11 cifar100-global-l2-3.0-resnet56]: Epoch 76/100, Acc=0.6840, Val Loss=1.2625, lr=0.0010
[02/21 17:35:39 cifar100-global-l2-3.0-resnet56]: Epoch 77/100, Acc=0.6845, Val Loss=1.2791, lr=0.0010
[02/21 17:36:06 cifar100-global-l2-3.0-resnet56]: Epoch 78/100, Acc=0.6857, Val Loss=1.2785, lr=0.0010
[02/21 17:36:34 cifar100-global-l2-3.0-resnet56]: Epoch 79/100, Acc=0.6842, Val Loss=1.2825, lr=0.0010
[02/21 17:37:01 cifar100-global-l2-3.0-resnet56]: Epoch 80/100, Acc=0.6864, Val Loss=1.2720, lr=0.0001
[02/21 17:37:29 cifar100-global-l2-3.0-resnet56]: Epoch 81/100, Acc=0.6881, Val Loss=1.2686, lr=0.0001
[02/21 17:37:56 cifar100-global-l2-3.0-resnet56]: Epoch 82/100, Acc=0.6885, Val Loss=1.2690, lr=0.0001
[02/21 17:38:24 cifar100-global-l2-3.0-resnet56]: Epoch 83/100, Acc=0.6876, Val Loss=1.2713, lr=0.0001
[02/21 17:38:51 cifar100-global-l2-3.0-resnet56]: Epoch 84/100, Acc=0.6881, Val Loss=1.2770, lr=0.0001
[02/21 17:39:19 cifar100-global-l2-3.0-resnet56]: Epoch 85/100, Acc=0.6870, Val Loss=1.2666, lr=0.0001
[02/21 17:39:47 cifar100-global-l2-3.0-resnet56]: Epoch 86/100, Acc=0.6888, Val Loss=1.2765, lr=0.0001
[02/21 17:40:14 cifar100-global-l2-3.0-resnet56]: Epoch 87/100, Acc=0.6874, Val Loss=1.2769, lr=0.0001
[02/21 17:40:41 cifar100-global-l2-3.0-resnet56]: Epoch 88/100, Acc=0.6886, Val Loss=1.2702, lr=0.0001
[02/21 17:41:09 cifar100-global-l2-3.0-resnet56]: Epoch 89/100, Acc=0.6894, Val Loss=1.2732, lr=0.0001
[02/21 17:41:37 cifar100-global-l2-3.0-resnet56]: Epoch 90/100, Acc=0.6888, Val Loss=1.2689, lr=0.0001
[02/21 17:42:04 cifar100-global-l2-3.0-resnet56]: Epoch 91/100, Acc=0.6857, Val Loss=1.2792, lr=0.0001
[02/21 17:42:32 cifar100-global-l2-3.0-resnet56]: Epoch 92/100, Acc=0.6881, Val Loss=1.2718, lr=0.0001
[02/21 17:42:59 cifar100-global-l2-3.0-resnet56]: Epoch 93/100, Acc=0.6886, Val Loss=1.2758, lr=0.0001
[02/21 17:43:27 cifar100-global-l2-3.0-resnet56]: Epoch 94/100, Acc=0.6891, Val Loss=1.2729, lr=0.0001
[02/21 17:43:55 cifar100-global-l2-3.0-resnet56]: Epoch 95/100, Acc=0.6860, Val Loss=1.2766, lr=0.0001
[02/21 17:44:22 cifar100-global-l2-3.0-resnet56]: Epoch 96/100, Acc=0.6847, Val Loss=1.2823, lr=0.0001
[02/21 17:44:50 cifar100-global-l2-3.0-resnet56]: Epoch 97/100, Acc=0.6879, Val Loss=1.2785, lr=0.0001
[02/21 17:45:18 cifar100-global-l2-3.0-resnet56]: Epoch 98/100, Acc=0.6880, Val Loss=1.2751, lr=0.0001
[02/21 17:45:46 cifar100-global-l2-3.0-resnet56]: Epoch 99/100, Acc=0.6856, Val Loss=1.2770, lr=0.0001
[02/21 17:45:46 cifar100-global-l2-3.0-resnet56]: Best Acc=0.6900
[02/21 17:45:46 cifar100-global-l2-3.0-resnet56]: Params: 0.58 M
[02/21 17:45:46 cifar100-global-l2-3.0-resnet56]: ops: 42.33 M
[02/21 17:45:49 cifar100-global-l2-3.0-resnet56]: Acc: 0.6856 Val Loss: 1.2770

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: mode: prune
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: model: resnet56
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: verbose: False
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: dataset: cifar100
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: dataroot: data
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: batch_size: 128
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: total_epochs: 100
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: lr_decay_milestones: 60,80
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: lr_decay_gamma: 0.1
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: lr: 0.01
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: output_dir: run/cifar100/prune/cifar100-global-fpgm-3.0-resnet56
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: finetune: True
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: last_epochs: 100
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: reps: 1
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: method: fpgm
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: speed_up: 3.0
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: max_pruning_ratio: 1.0
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: reg: 1e-05
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: delta_reg: 0.0001
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: weight_decay: 0.0005
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: seed: 1
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: global_pruning: True
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: sl_total_epochs: 100
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: sl_lr: 0.01
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: sl_reg_warmup: 0
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: sl_restore: None
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: iterative_steps: 400
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: logger: <Logger cifar100-global-fpgm-3.0-resnet56 (DEBUG)>
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: device: cuda
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: num_classes: 100
[02/21 17:45:56 cifar100-global-fpgm-3.0-resnet56]: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 17:46:00 cifar100-global-fpgm-3.0-resnet56]: Pruning...
[02/21 17:46:20 cifar100-global-fpgm-3.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(11, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(11, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(11, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(11, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(11, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(11, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(11, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(11, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(11, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(11, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(11, 11, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(11, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(11, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(11, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(11, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(11, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(11, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(11, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(11, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(11, 9, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(11, 57, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(57, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(47, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(57, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(58, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(57, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(57, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(57, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(57, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(57, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(57, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(46, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(57, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=57, out_features=100, bias=True)
)
[02/21 17:46:22 cifar100-global-fpgm-3.0-resnet56]: Params: 0.86 M => 0.44 M (51.21%)
[02/21 17:46:22 cifar100-global-fpgm-3.0-resnet56]: FLOPs: 127.12 M => 42.26 M (33.25%, 3.01X )
[02/21 17:46:22 cifar100-global-fpgm-3.0-resnet56]: Acc: 0.7269 => 0.0193
[02/21 17:46:22 cifar100-global-fpgm-3.0-resnet56]: Val Loss: 1.1578 => 7.9602
[02/21 17:46:22 cifar100-global-fpgm-3.0-resnet56]: Finetuning...
[02/21 17:46:49 cifar100-global-fpgm-3.0-resnet56]: Epoch 0/100, Acc=0.5029, Val Loss=1.8702, lr=0.0100
[02/21 17:47:17 cifar100-global-fpgm-3.0-resnet56]: Epoch 1/100, Acc=0.5682, Val Loss=1.5498, lr=0.0100
[02/21 17:47:44 cifar100-global-fpgm-3.0-resnet56]: Epoch 2/100, Acc=0.5768, Val Loss=1.5428, lr=0.0100
[02/21 17:48:11 cifar100-global-fpgm-3.0-resnet56]: Epoch 3/100, Acc=0.5700, Val Loss=1.6043, lr=0.0100
[02/21 17:48:38 cifar100-global-fpgm-3.0-resnet56]: Epoch 4/100, Acc=0.5861, Val Loss=1.5265, lr=0.0100
[02/21 17:49:06 cifar100-global-fpgm-3.0-resnet56]: Epoch 5/100, Acc=0.6032, Val Loss=1.4439, lr=0.0100
[02/21 17:49:33 cifar100-global-fpgm-3.0-resnet56]: Epoch 6/100, Acc=0.6076, Val Loss=1.4333, lr=0.0100
[02/21 17:50:00 cifar100-global-fpgm-3.0-resnet56]: Epoch 7/100, Acc=0.5954, Val Loss=1.5184, lr=0.0100
[02/21 17:50:27 cifar100-global-fpgm-3.0-resnet56]: Epoch 8/100, Acc=0.6026, Val Loss=1.4884, lr=0.0100
[02/21 17:50:55 cifar100-global-fpgm-3.0-resnet56]: Epoch 9/100, Acc=0.5950, Val Loss=1.5088, lr=0.0100
[02/21 17:51:22 cifar100-global-fpgm-3.0-resnet56]: Epoch 10/100, Acc=0.5372, Val Loss=1.8422, lr=0.0100
[02/21 17:51:49 cifar100-global-fpgm-3.0-resnet56]: Epoch 11/100, Acc=0.6103, Val Loss=1.4378, lr=0.0100
[02/21 17:52:17 cifar100-global-fpgm-3.0-resnet56]: Epoch 12/100, Acc=0.5982, Val Loss=1.5152, lr=0.0100
[02/21 17:52:44 cifar100-global-fpgm-3.0-resnet56]: Epoch 13/100, Acc=0.6097, Val Loss=1.4545, lr=0.0100
[02/21 17:53:12 cifar100-global-fpgm-3.0-resnet56]: Epoch 14/100, Acc=0.6187, Val Loss=1.4072, lr=0.0100
[02/21 17:53:39 cifar100-global-fpgm-3.0-resnet56]: Epoch 15/100, Acc=0.6134, Val Loss=1.4246, lr=0.0100
[02/21 17:54:07 cifar100-global-fpgm-3.0-resnet56]: Epoch 16/100, Acc=0.6035, Val Loss=1.4483, lr=0.0100
[02/21 17:54:34 cifar100-global-fpgm-3.0-resnet56]: Epoch 17/100, Acc=0.6076, Val Loss=1.4725, lr=0.0100
[02/21 17:55:02 cifar100-global-fpgm-3.0-resnet56]: Epoch 18/100, Acc=0.6132, Val Loss=1.4507, lr=0.0100
[02/21 17:55:29 cifar100-global-fpgm-3.0-resnet56]: Epoch 19/100, Acc=0.6152, Val Loss=1.4596, lr=0.0100
[02/21 17:55:57 cifar100-global-fpgm-3.0-resnet56]: Epoch 20/100, Acc=0.6197, Val Loss=1.3881, lr=0.0100
[02/21 17:56:24 cifar100-global-fpgm-3.0-resnet56]: Epoch 21/100, Acc=0.6165, Val Loss=1.4172, lr=0.0100
[02/21 17:56:52 cifar100-global-fpgm-3.0-resnet56]: Epoch 22/100, Acc=0.6173, Val Loss=1.4433, lr=0.0100
[02/21 17:57:19 cifar100-global-fpgm-3.0-resnet56]: Epoch 23/100, Acc=0.6044, Val Loss=1.5104, lr=0.0100
[02/21 17:57:47 cifar100-global-fpgm-3.0-resnet56]: Epoch 24/100, Acc=0.6215, Val Loss=1.4004, lr=0.0100
[02/21 17:58:15 cifar100-global-fpgm-3.0-resnet56]: Epoch 25/100, Acc=0.6141, Val Loss=1.4644, lr=0.0100
[02/21 17:58:42 cifar100-global-fpgm-3.0-resnet56]: Epoch 26/100, Acc=0.5965, Val Loss=1.5339, lr=0.0100
[02/21 17:59:10 cifar100-global-fpgm-3.0-resnet56]: Epoch 27/100, Acc=0.6097, Val Loss=1.5093, lr=0.0100
[02/21 17:59:38 cifar100-global-fpgm-3.0-resnet56]: Epoch 28/100, Acc=0.6142, Val Loss=1.4499, lr=0.0100
[02/21 18:00:05 cifar100-global-fpgm-3.0-resnet56]: Epoch 29/100, Acc=0.6134, Val Loss=1.4782, lr=0.0100
[02/21 18:00:33 cifar100-global-fpgm-3.0-resnet56]: Epoch 30/100, Acc=0.6085, Val Loss=1.5155, lr=0.0100
[02/21 18:01:00 cifar100-global-fpgm-3.0-resnet56]: Epoch 31/100, Acc=0.6041, Val Loss=1.5326, lr=0.0100
[02/21 18:01:28 cifar100-global-fpgm-3.0-resnet56]: Epoch 32/100, Acc=0.6336, Val Loss=1.3782, lr=0.0100
[02/21 18:01:56 cifar100-global-fpgm-3.0-resnet56]: Epoch 33/100, Acc=0.6389, Val Loss=1.3489, lr=0.0100
[02/21 18:02:24 cifar100-global-fpgm-3.0-resnet56]: Epoch 34/100, Acc=0.6244, Val Loss=1.4232, lr=0.0100
[02/21 18:02:51 cifar100-global-fpgm-3.0-resnet56]: Epoch 35/100, Acc=0.6340, Val Loss=1.3732, lr=0.0100
[02/21 18:03:19 cifar100-global-fpgm-3.0-resnet56]: Epoch 36/100, Acc=0.6163, Val Loss=1.4571, lr=0.0100
[02/21 18:03:46 cifar100-global-fpgm-3.0-resnet56]: Epoch 37/100, Acc=0.6271, Val Loss=1.4101, lr=0.0100
[02/21 18:04:14 cifar100-global-fpgm-3.0-resnet56]: Epoch 38/100, Acc=0.6191, Val Loss=1.4989, lr=0.0100
[02/21 18:04:42 cifar100-global-fpgm-3.0-resnet56]: Epoch 39/100, Acc=0.6282, Val Loss=1.4387, lr=0.0100
[02/21 18:05:09 cifar100-global-fpgm-3.0-resnet56]: Epoch 40/100, Acc=0.6086, Val Loss=1.4862, lr=0.0100
[02/21 18:05:36 cifar100-global-fpgm-3.0-resnet56]: Epoch 41/100, Acc=0.6014, Val Loss=1.5582, lr=0.0100
[02/21 18:06:03 cifar100-global-fpgm-3.0-resnet56]: Epoch 42/100, Acc=0.6182, Val Loss=1.4508, lr=0.0100
[02/21 18:06:30 cifar100-global-fpgm-3.0-resnet56]: Epoch 43/100, Acc=0.6251, Val Loss=1.4568, lr=0.0100
[02/21 18:06:57 cifar100-global-fpgm-3.0-resnet56]: Epoch 44/100, Acc=0.6347, Val Loss=1.3721, lr=0.0100
[02/21 18:07:24 cifar100-global-fpgm-3.0-resnet56]: Epoch 45/100, Acc=0.6292, Val Loss=1.4179, lr=0.0100
[02/21 18:07:51 cifar100-global-fpgm-3.0-resnet56]: Epoch 46/100, Acc=0.6128, Val Loss=1.5214, lr=0.0100
[02/21 18:08:18 cifar100-global-fpgm-3.0-resnet56]: Epoch 47/100, Acc=0.6287, Val Loss=1.4577, lr=0.0100
[02/21 18:08:45 cifar100-global-fpgm-3.0-resnet56]: Epoch 48/100, Acc=0.6069, Val Loss=1.5202, lr=0.0100
[02/21 18:09:12 cifar100-global-fpgm-3.0-resnet56]: Epoch 49/100, Acc=0.6270, Val Loss=1.4035, lr=0.0100
[02/21 18:09:39 cifar100-global-fpgm-3.0-resnet56]: Epoch 50/100, Acc=0.6173, Val Loss=1.4964, lr=0.0100
[02/21 18:10:07 cifar100-global-fpgm-3.0-resnet56]: Epoch 51/100, Acc=0.6167, Val Loss=1.4742, lr=0.0100
[02/21 18:10:35 cifar100-global-fpgm-3.0-resnet56]: Epoch 52/100, Acc=0.5911, Val Loss=1.6572, lr=0.0100
[02/21 18:11:02 cifar100-global-fpgm-3.0-resnet56]: Epoch 53/100, Acc=0.5973, Val Loss=1.5692, lr=0.0100
[02/21 18:11:30 cifar100-global-fpgm-3.0-resnet56]: Epoch 54/100, Acc=0.6129, Val Loss=1.5102, lr=0.0100
[02/21 18:11:57 cifar100-global-fpgm-3.0-resnet56]: Epoch 55/100, Acc=0.6246, Val Loss=1.4592, lr=0.0100
[02/21 18:12:25 cifar100-global-fpgm-3.0-resnet56]: Epoch 56/100, Acc=0.6219, Val Loss=1.4798, lr=0.0100
[02/21 18:12:53 cifar100-global-fpgm-3.0-resnet56]: Epoch 57/100, Acc=0.6219, Val Loss=1.4805, lr=0.0100
[02/21 18:13:20 cifar100-global-fpgm-3.0-resnet56]: Epoch 58/100, Acc=0.6226, Val Loss=1.4572, lr=0.0100
[02/21 18:13:48 cifar100-global-fpgm-3.0-resnet56]: Epoch 59/100, Acc=0.5840, Val Loss=1.6393, lr=0.0100
[02/21 18:14:15 cifar100-global-fpgm-3.0-resnet56]: Epoch 60/100, Acc=0.6856, Val Loss=1.1591, lr=0.0010
[02/21 18:14:43 cifar100-global-fpgm-3.0-resnet56]: Epoch 61/100, Acc=0.6883, Val Loss=1.1535, lr=0.0010
[02/21 18:15:11 cifar100-global-fpgm-3.0-resnet56]: Epoch 62/100, Acc=0.6900, Val Loss=1.1491, lr=0.0010
[02/21 18:15:38 cifar100-global-fpgm-3.0-resnet56]: Epoch 63/100, Acc=0.6885, Val Loss=1.1654, lr=0.0010
[02/21 18:16:06 cifar100-global-fpgm-3.0-resnet56]: Epoch 64/100, Acc=0.6897, Val Loss=1.1673, lr=0.0010
[02/21 18:16:34 cifar100-global-fpgm-3.0-resnet56]: Epoch 65/100, Acc=0.6900, Val Loss=1.1690, lr=0.0010
[02/21 18:17:01 cifar100-global-fpgm-3.0-resnet56]: Epoch 66/100, Acc=0.6919, Val Loss=1.1778, lr=0.0010
[02/21 18:17:29 cifar100-global-fpgm-3.0-resnet56]: Epoch 67/100, Acc=0.6907, Val Loss=1.1820, lr=0.0010
[02/21 18:17:56 cifar100-global-fpgm-3.0-resnet56]: Epoch 68/100, Acc=0.6904, Val Loss=1.1806, lr=0.0010
[02/21 18:18:24 cifar100-global-fpgm-3.0-resnet56]: Epoch 69/100, Acc=0.6883, Val Loss=1.1925, lr=0.0010
[02/21 18:18:51 cifar100-global-fpgm-3.0-resnet56]: Epoch 70/100, Acc=0.6882, Val Loss=1.1919, lr=0.0010
[02/21 18:19:19 cifar100-global-fpgm-3.0-resnet56]: Epoch 71/100, Acc=0.6867, Val Loss=1.1979, lr=0.0010
[02/21 18:19:46 cifar100-global-fpgm-3.0-resnet56]: Epoch 72/100, Acc=0.6873, Val Loss=1.1942, lr=0.0010
[02/21 18:20:13 cifar100-global-fpgm-3.0-resnet56]: Epoch 73/100, Acc=0.6886, Val Loss=1.2101, lr=0.0010
[02/21 18:20:40 cifar100-global-fpgm-3.0-resnet56]: Epoch 74/100, Acc=0.6883, Val Loss=1.2001, lr=0.0010
[02/21 18:21:07 cifar100-global-fpgm-3.0-resnet56]: Epoch 75/100, Acc=0.6838, Val Loss=1.2182, lr=0.0010
[02/21 18:21:34 cifar100-global-fpgm-3.0-resnet56]: Epoch 76/100, Acc=0.6868, Val Loss=1.2203, lr=0.0010
[02/21 18:22:01 cifar100-global-fpgm-3.0-resnet56]: Epoch 77/100, Acc=0.6856, Val Loss=1.2190, lr=0.0010
[02/21 18:22:28 cifar100-global-fpgm-3.0-resnet56]: Epoch 78/100, Acc=0.6878, Val Loss=1.2210, lr=0.0010
[02/21 18:22:56 cifar100-global-fpgm-3.0-resnet56]: Epoch 79/100, Acc=0.6869, Val Loss=1.2353, lr=0.0010
[02/21 18:23:23 cifar100-global-fpgm-3.0-resnet56]: Epoch 80/100, Acc=0.6887, Val Loss=1.2241, lr=0.0001
[02/21 18:23:50 cifar100-global-fpgm-3.0-resnet56]: Epoch 81/100, Acc=0.6876, Val Loss=1.2199, lr=0.0001
[02/21 18:24:17 cifar100-global-fpgm-3.0-resnet56]: Epoch 82/100, Acc=0.6889, Val Loss=1.2183, lr=0.0001
[02/21 18:24:45 cifar100-global-fpgm-3.0-resnet56]: Epoch 83/100, Acc=0.6872, Val Loss=1.2201, lr=0.0001
[02/21 18:25:12 cifar100-global-fpgm-3.0-resnet56]: Epoch 84/100, Acc=0.6868, Val Loss=1.2248, lr=0.0001
[02/21 18:25:39 cifar100-global-fpgm-3.0-resnet56]: Epoch 85/100, Acc=0.6876, Val Loss=1.2223, lr=0.0001
[02/21 18:26:07 cifar100-global-fpgm-3.0-resnet56]: Epoch 86/100, Acc=0.6890, Val Loss=1.2211, lr=0.0001
[02/21 18:26:34 cifar100-global-fpgm-3.0-resnet56]: Epoch 87/100, Acc=0.6883, Val Loss=1.2226, lr=0.0001
[02/21 18:27:02 cifar100-global-fpgm-3.0-resnet56]: Epoch 88/100, Acc=0.6878, Val Loss=1.2221, lr=0.0001
[02/21 18:27:29 cifar100-global-fpgm-3.0-resnet56]: Epoch 89/100, Acc=0.6884, Val Loss=1.2218, lr=0.0001
[02/21 18:27:56 cifar100-global-fpgm-3.0-resnet56]: Epoch 90/100, Acc=0.6883, Val Loss=1.2202, lr=0.0001
[02/21 18:28:24 cifar100-global-fpgm-3.0-resnet56]: Epoch 91/100, Acc=0.6870, Val Loss=1.2311, lr=0.0001
[02/21 18:28:51 cifar100-global-fpgm-3.0-resnet56]: Epoch 92/100, Acc=0.6901, Val Loss=1.2172, lr=0.0001
[02/21 18:29:19 cifar100-global-fpgm-3.0-resnet56]: Epoch 93/100, Acc=0.6871, Val Loss=1.2256, lr=0.0001
[02/21 18:29:46 cifar100-global-fpgm-3.0-resnet56]: Epoch 94/100, Acc=0.6878, Val Loss=1.2215, lr=0.0001
[02/21 18:30:14 cifar100-global-fpgm-3.0-resnet56]: Epoch 95/100, Acc=0.6861, Val Loss=1.2304, lr=0.0001
[02/21 18:30:41 cifar100-global-fpgm-3.0-resnet56]: Epoch 96/100, Acc=0.6900, Val Loss=1.2252, lr=0.0001
[02/21 18:31:09 cifar100-global-fpgm-3.0-resnet56]: Epoch 97/100, Acc=0.6865, Val Loss=1.2248, lr=0.0001
[02/21 18:31:36 cifar100-global-fpgm-3.0-resnet56]: Epoch 98/100, Acc=0.6881, Val Loss=1.2231, lr=0.0001
[02/21 18:32:04 cifar100-global-fpgm-3.0-resnet56]: Epoch 99/100, Acc=0.6862, Val Loss=1.2246, lr=0.0001
[02/21 18:32:04 cifar100-global-fpgm-3.0-resnet56]: Best Acc=0.6919
[02/21 18:32:04 cifar100-global-fpgm-3.0-resnet56]: Params: 0.44 M
[02/21 18:32:04 cifar100-global-fpgm-3.0-resnet56]: ops: 42.26 M
[02/21 18:32:07 cifar100-global-fpgm-3.0-resnet56]: Acc: 0.6862 Val Loss: 1.2246

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: mode: prune
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: model: resnet56
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: verbose: False
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: dataset: cifar100
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: dataroot: data
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: batch_size: 128
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: total_epochs: 100
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: lr_decay_milestones: 60,80
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: lr_decay_gamma: 0.1
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: lr: 0.01
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: output_dir: run/cifar100/prune/cifar100-global-obdc-3.0-resnet56
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: finetune: True
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: last_epochs: 100
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: reps: 1
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: method: obdc
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: speed_up: 3.0
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: max_pruning_ratio: 1.0
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: reg: 1e-05
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: delta_reg: 0.0001
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: weight_decay: 0.0005
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: seed: 1
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: global_pruning: True
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: sl_total_epochs: 100
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: sl_lr: 0.01
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: sl_reg_warmup: 0
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: sl_restore: None
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: iterative_steps: 400
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: logger: <Logger cifar100-global-obdc-3.0-resnet56 (DEBUG)>
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: device: cuda
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: num_classes: 100
[02/21 18:32:14 cifar100-global-obdc-3.0-resnet56]: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 18:32:17 cifar100-global-obdc-3.0-resnet56]: Pruning...
Traceback (most recent call last):
  File "/home/REDACTED/paper1/main.py", line 429, in <module>
    main()
  File "/home/REDACTED/paper1/main.py", line 369, in main
    progressive_pruning(
  File "/home/REDACTED/paper1/main.py", line 65, in progressive_pruning
    imp._prepare_model(model, pruner)
  File "/home/REDACTED/paper1/torch_pruning/pruner/importance.py", line 872, in _prepare_model
    for i, (dep, idxs) in enumerate(group):
TypeError: 'NoneType' object is not iterable
/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: mode: prune
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: model: resnet56
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: verbose: False
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: dataset: cifar100
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: dataroot: data
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: batch_size: 128
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: total_epochs: 100
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: lr_decay_milestones: 60,80
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: lr_decay_gamma: 0.1
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: lr: 0.01
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: output_dir: run/cifar100/prune/cifar100-global-lamp-3.0-resnet56
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: finetune: True
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: last_epochs: 100
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: reps: 1
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: method: lamp
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: speed_up: 3.0
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: max_pruning_ratio: 1.0
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: reg: 1e-05
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: delta_reg: 0.0001
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: weight_decay: 0.0005
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: seed: 1
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: global_pruning: True
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: sl_total_epochs: 100
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: sl_lr: 0.01
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: sl_reg_warmup: 0
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: sl_restore: None
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: iterative_steps: 400
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: logger: <Logger cifar100-global-lamp-3.0-resnet56 (DEBUG)>
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: device: cuda
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: num_classes: 100
[02/21 18:32:23 cifar100-global-lamp-3.0-resnet56]: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 18:32:26 cifar100-global-lamp-3.0-resnet56]: Pruning...
[02/21 18:32:51 cifar100-global-lamp-3.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(14, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(14, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(14, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(14, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(14, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(14, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(14, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(14, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(14, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(14, 19, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(14, 18, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(18, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(18, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(18, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(18, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(18, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(18, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(18, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(18, 22, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(22, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(21, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(22, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(22, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(22, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(21, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(22, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(21, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(22, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(22, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=22, out_features=100, bias=True)
)
[02/21 18:32:54 cifar100-global-lamp-3.0-resnet56]: Params: 0.86 M => 0.15 M (17.64%)
[02/21 18:32:54 cifar100-global-lamp-3.0-resnet56]: FLOPs: 127.12 M => 42.14 M (33.15%, 3.02X )
[02/21 18:32:54 cifar100-global-lamp-3.0-resnet56]: Acc: 0.7269 => 0.0109
[02/21 18:32:54 cifar100-global-lamp-3.0-resnet56]: Val Loss: 1.1578 => 25.7632
[02/21 18:32:54 cifar100-global-lamp-3.0-resnet56]: Finetuning...
[02/21 18:33:21 cifar100-global-lamp-3.0-resnet56]: Epoch 0/100, Acc=0.2799, Val Loss=2.7470, lr=0.0100
[02/21 18:33:48 cifar100-global-lamp-3.0-resnet56]: Epoch 1/100, Acc=0.3707, Val Loss=2.3242, lr=0.0100
[02/21 18:34:15 cifar100-global-lamp-3.0-resnet56]: Epoch 2/100, Acc=0.4036, Val Loss=2.2138, lr=0.0100
[02/21 18:34:43 cifar100-global-lamp-3.0-resnet56]: Epoch 3/100, Acc=0.4056, Val Loss=2.2973, lr=0.0100
[02/21 18:35:10 cifar100-global-lamp-3.0-resnet56]: Epoch 4/100, Acc=0.4319, Val Loss=2.1367, lr=0.0100
[02/21 18:35:37 cifar100-global-lamp-3.0-resnet56]: Epoch 5/100, Acc=0.4488, Val Loss=2.0157, lr=0.0100
[02/21 18:36:04 cifar100-global-lamp-3.0-resnet56]: Epoch 6/100, Acc=0.4796, Val Loss=1.8683, lr=0.0100
[02/21 18:36:31 cifar100-global-lamp-3.0-resnet56]: Epoch 7/100, Acc=0.4640, Val Loss=1.9882, lr=0.0100
[02/21 18:36:58 cifar100-global-lamp-3.0-resnet56]: Epoch 8/100, Acc=0.4714, Val Loss=1.9116, lr=0.0100
[02/21 18:37:25 cifar100-global-lamp-3.0-resnet56]: Epoch 9/100, Acc=0.4810, Val Loss=1.9361, lr=0.0100
[02/21 18:37:52 cifar100-global-lamp-3.0-resnet56]: Epoch 10/100, Acc=0.4839, Val Loss=1.9148, lr=0.0100
[02/21 18:38:19 cifar100-global-lamp-3.0-resnet56]: Epoch 11/100, Acc=0.5003, Val Loss=1.8110, lr=0.0100
[02/21 18:38:46 cifar100-global-lamp-3.0-resnet56]: Epoch 12/100, Acc=0.4861, Val Loss=1.8702, lr=0.0100
[02/21 18:39:13 cifar100-global-lamp-3.0-resnet56]: Epoch 13/100, Acc=0.4926, Val Loss=1.8885, lr=0.0100
[02/21 18:39:40 cifar100-global-lamp-3.0-resnet56]: Epoch 14/100, Acc=0.5232, Val Loss=1.7036, lr=0.0100
[02/21 18:40:07 cifar100-global-lamp-3.0-resnet56]: Epoch 15/100, Acc=0.5112, Val Loss=1.8031, lr=0.0100
[02/21 18:40:34 cifar100-global-lamp-3.0-resnet56]: Epoch 16/100, Acc=0.5265, Val Loss=1.7248, lr=0.0100
[02/21 18:41:01 cifar100-global-lamp-3.0-resnet56]: Epoch 17/100, Acc=0.5055, Val Loss=1.7738, lr=0.0100
[02/21 18:41:28 cifar100-global-lamp-3.0-resnet56]: Epoch 18/100, Acc=0.5059, Val Loss=1.8261, lr=0.0100
[02/21 18:41:55 cifar100-global-lamp-3.0-resnet56]: Epoch 19/100, Acc=0.5099, Val Loss=1.8280, lr=0.0100
[02/21 18:42:22 cifar100-global-lamp-3.0-resnet56]: Epoch 20/100, Acc=0.5182, Val Loss=1.7676, lr=0.0100
[02/21 18:42:50 cifar100-global-lamp-3.0-resnet56]: Epoch 21/100, Acc=0.5292, Val Loss=1.7436, lr=0.0100
[02/21 18:43:17 cifar100-global-lamp-3.0-resnet56]: Epoch 22/100, Acc=0.5076, Val Loss=1.8610, lr=0.0100
[02/21 18:43:44 cifar100-global-lamp-3.0-resnet56]: Epoch 23/100, Acc=0.5315, Val Loss=1.6742, lr=0.0100
[02/21 18:44:12 cifar100-global-lamp-3.0-resnet56]: Epoch 24/100, Acc=0.5167, Val Loss=1.7890, lr=0.0100
[02/21 18:44:39 cifar100-global-lamp-3.0-resnet56]: Epoch 25/100, Acc=0.5264, Val Loss=1.7302, lr=0.0100
[02/21 18:45:07 cifar100-global-lamp-3.0-resnet56]: Epoch 26/100, Acc=0.5441, Val Loss=1.6426, lr=0.0100
[02/21 18:45:34 cifar100-global-lamp-3.0-resnet56]: Epoch 27/100, Acc=0.5304, Val Loss=1.7116, lr=0.0100
[02/21 18:46:01 cifar100-global-lamp-3.0-resnet56]: Epoch 28/100, Acc=0.5411, Val Loss=1.6588, lr=0.0100
[02/21 18:46:29 cifar100-global-lamp-3.0-resnet56]: Epoch 29/100, Acc=0.5306, Val Loss=1.7243, lr=0.0100
[02/21 18:46:56 cifar100-global-lamp-3.0-resnet56]: Epoch 30/100, Acc=0.4860, Val Loss=1.9328, lr=0.0100
[02/21 18:47:24 cifar100-global-lamp-3.0-resnet56]: Epoch 31/100, Acc=0.5270, Val Loss=1.7219, lr=0.0100
[02/21 18:47:51 cifar100-global-lamp-3.0-resnet56]: Epoch 32/100, Acc=0.5395, Val Loss=1.7242, lr=0.0100
[02/21 18:48:18 cifar100-global-lamp-3.0-resnet56]: Epoch 33/100, Acc=0.5339, Val Loss=1.7129, lr=0.0100
[02/21 18:48:45 cifar100-global-lamp-3.0-resnet56]: Epoch 34/100, Acc=0.5187, Val Loss=1.8022, lr=0.0100
[02/21 18:49:12 cifar100-global-lamp-3.0-resnet56]: Epoch 35/100, Acc=0.5306, Val Loss=1.7099, lr=0.0100
[02/21 18:49:39 cifar100-global-lamp-3.0-resnet56]: Epoch 36/100, Acc=0.5341, Val Loss=1.7121, lr=0.0100
[02/21 18:50:06 cifar100-global-lamp-3.0-resnet56]: Epoch 37/100, Acc=0.5374, Val Loss=1.7047, lr=0.0100
[02/21 18:50:33 cifar100-global-lamp-3.0-resnet56]: Epoch 38/100, Acc=0.5390, Val Loss=1.7014, lr=0.0100
[02/21 18:51:00 cifar100-global-lamp-3.0-resnet56]: Epoch 39/100, Acc=0.5349, Val Loss=1.7154, lr=0.0100
[02/21 18:51:27 cifar100-global-lamp-3.0-resnet56]: Epoch 40/100, Acc=0.5435, Val Loss=1.7356, lr=0.0100
[02/21 18:51:54 cifar100-global-lamp-3.0-resnet56]: Epoch 41/100, Acc=0.5294, Val Loss=1.7731, lr=0.0100
[02/21 18:52:22 cifar100-global-lamp-3.0-resnet56]: Epoch 42/100, Acc=0.5172, Val Loss=1.8203, lr=0.0100
[02/21 18:52:49 cifar100-global-lamp-3.0-resnet56]: Epoch 43/100, Acc=0.5506, Val Loss=1.6313, lr=0.0100
[02/21 18:53:16 cifar100-global-lamp-3.0-resnet56]: Epoch 44/100, Acc=0.5259, Val Loss=1.7659, lr=0.0100
[02/21 18:53:43 cifar100-global-lamp-3.0-resnet56]: Epoch 45/100, Acc=0.5575, Val Loss=1.6341, lr=0.0100
[02/21 18:54:10 cifar100-global-lamp-3.0-resnet56]: Epoch 46/100, Acc=0.5278, Val Loss=1.7523, lr=0.0100
[02/21 18:54:37 cifar100-global-lamp-3.0-resnet56]: Epoch 47/100, Acc=0.5270, Val Loss=1.7894, lr=0.0100
[02/21 18:55:04 cifar100-global-lamp-3.0-resnet56]: Epoch 48/100, Acc=0.5278, Val Loss=1.7678, lr=0.0100
[02/21 18:55:31 cifar100-global-lamp-3.0-resnet56]: Epoch 49/100, Acc=0.5659, Val Loss=1.5754, lr=0.0100
[02/21 18:55:59 cifar100-global-lamp-3.0-resnet56]: Epoch 50/100, Acc=0.5522, Val Loss=1.6335, lr=0.0100
[02/21 18:56:26 cifar100-global-lamp-3.0-resnet56]: Epoch 51/100, Acc=0.5318, Val Loss=1.7868, lr=0.0100
[02/21 18:56:53 cifar100-global-lamp-3.0-resnet56]: Epoch 52/100, Acc=0.5517, Val Loss=1.6284, lr=0.0100
[02/21 18:57:20 cifar100-global-lamp-3.0-resnet56]: Epoch 53/100, Acc=0.5383, Val Loss=1.7115, lr=0.0100
[02/21 18:57:48 cifar100-global-lamp-3.0-resnet56]: Epoch 54/100, Acc=0.5246, Val Loss=1.7681, lr=0.0100
[02/21 18:58:15 cifar100-global-lamp-3.0-resnet56]: Epoch 55/100, Acc=0.5401, Val Loss=1.6944, lr=0.0100
[02/21 18:58:42 cifar100-global-lamp-3.0-resnet56]: Epoch 56/100, Acc=0.5472, Val Loss=1.6713, lr=0.0100
[02/21 18:59:09 cifar100-global-lamp-3.0-resnet56]: Epoch 57/100, Acc=0.5542, Val Loss=1.6475, lr=0.0100
[02/21 18:59:36 cifar100-global-lamp-3.0-resnet56]: Epoch 58/100, Acc=0.5617, Val Loss=1.6442, lr=0.0100
[02/21 19:00:03 cifar100-global-lamp-3.0-resnet56]: Epoch 59/100, Acc=0.5091, Val Loss=1.8898, lr=0.0100
[02/21 19:00:30 cifar100-global-lamp-3.0-resnet56]: Epoch 60/100, Acc=0.6224, Val Loss=1.3340, lr=0.0010
[02/21 19:00:58 cifar100-global-lamp-3.0-resnet56]: Epoch 61/100, Acc=0.6276, Val Loss=1.3317, lr=0.0010
[02/21 19:01:25 cifar100-global-lamp-3.0-resnet56]: Epoch 62/100, Acc=0.6287, Val Loss=1.3223, lr=0.0010
[02/21 19:01:52 cifar100-global-lamp-3.0-resnet56]: Epoch 63/100, Acc=0.6295, Val Loss=1.3182, lr=0.0010
[02/21 19:02:20 cifar100-global-lamp-3.0-resnet56]: Epoch 64/100, Acc=0.6276, Val Loss=1.3228, lr=0.0010
[02/21 19:02:47 cifar100-global-lamp-3.0-resnet56]: Epoch 65/100, Acc=0.6280, Val Loss=1.3237, lr=0.0010
[02/21 19:03:14 cifar100-global-lamp-3.0-resnet56]: Epoch 66/100, Acc=0.6279, Val Loss=1.3318, lr=0.0010
[02/21 19:03:41 cifar100-global-lamp-3.0-resnet56]: Epoch 67/100, Acc=0.6285, Val Loss=1.3260, lr=0.0010
[02/21 19:04:08 cifar100-global-lamp-3.0-resnet56]: Epoch 68/100, Acc=0.6284, Val Loss=1.3205, lr=0.0010
[02/21 19:04:35 cifar100-global-lamp-3.0-resnet56]: Epoch 69/100, Acc=0.6290, Val Loss=1.3382, lr=0.0010
[02/21 19:05:03 cifar100-global-lamp-3.0-resnet56]: Epoch 70/100, Acc=0.6330, Val Loss=1.3287, lr=0.0010
[02/21 19:05:30 cifar100-global-lamp-3.0-resnet56]: Epoch 71/100, Acc=0.6289, Val Loss=1.3443, lr=0.0010
[02/21 19:05:58 cifar100-global-lamp-3.0-resnet56]: Epoch 72/100, Acc=0.6300, Val Loss=1.3367, lr=0.0010
[02/21 19:06:25 cifar100-global-lamp-3.0-resnet56]: Epoch 73/100, Acc=0.6278, Val Loss=1.3440, lr=0.0010
[02/21 19:06:53 cifar100-global-lamp-3.0-resnet56]: Epoch 74/100, Acc=0.6309, Val Loss=1.3388, lr=0.0010
[02/21 19:07:20 cifar100-global-lamp-3.0-resnet56]: Epoch 75/100, Acc=0.6319, Val Loss=1.3376, lr=0.0010
[02/21 19:07:48 cifar100-global-lamp-3.0-resnet56]: Epoch 76/100, Acc=0.6292, Val Loss=1.3337, lr=0.0010
[02/21 19:08:15 cifar100-global-lamp-3.0-resnet56]: Epoch 77/100, Acc=0.6279, Val Loss=1.3351, lr=0.0010
[02/21 19:08:43 cifar100-global-lamp-3.0-resnet56]: Epoch 78/100, Acc=0.6303, Val Loss=1.3455, lr=0.0010
[02/21 19:09:10 cifar100-global-lamp-3.0-resnet56]: Epoch 79/100, Acc=0.6286, Val Loss=1.3609, lr=0.0010
[02/21 19:09:38 cifar100-global-lamp-3.0-resnet56]: Epoch 80/100, Acc=0.6330, Val Loss=1.3336, lr=0.0001
[02/21 19:10:05 cifar100-global-lamp-3.0-resnet56]: Epoch 81/100, Acc=0.6337, Val Loss=1.3287, lr=0.0001
[02/21 19:10:33 cifar100-global-lamp-3.0-resnet56]: Epoch 82/100, Acc=0.6363, Val Loss=1.3263, lr=0.0001
[02/21 19:11:00 cifar100-global-lamp-3.0-resnet56]: Epoch 83/100, Acc=0.6367, Val Loss=1.3285, lr=0.0001
[02/21 19:11:28 cifar100-global-lamp-3.0-resnet56]: Epoch 84/100, Acc=0.6344, Val Loss=1.3301, lr=0.0001
[02/21 19:11:55 cifar100-global-lamp-3.0-resnet56]: Epoch 85/100, Acc=0.6344, Val Loss=1.3316, lr=0.0001
[02/21 19:12:23 cifar100-global-lamp-3.0-resnet56]: Epoch 86/100, Acc=0.6356, Val Loss=1.3315, lr=0.0001
[02/21 19:12:51 cifar100-global-lamp-3.0-resnet56]: Epoch 87/100, Acc=0.6348, Val Loss=1.3362, lr=0.0001
[02/21 19:13:18 cifar100-global-lamp-3.0-resnet56]: Epoch 88/100, Acc=0.6331, Val Loss=1.3321, lr=0.0001
[02/21 19:13:46 cifar100-global-lamp-3.0-resnet56]: Epoch 89/100, Acc=0.6344, Val Loss=1.3326, lr=0.0001
[02/21 19:14:13 cifar100-global-lamp-3.0-resnet56]: Epoch 90/100, Acc=0.6327, Val Loss=1.3291, lr=0.0001
[02/21 19:14:41 cifar100-global-lamp-3.0-resnet56]: Epoch 91/100, Acc=0.6331, Val Loss=1.3343, lr=0.0001
[02/21 19:15:08 cifar100-global-lamp-3.0-resnet56]: Epoch 92/100, Acc=0.6354, Val Loss=1.3244, lr=0.0001
[02/21 19:15:36 cifar100-global-lamp-3.0-resnet56]: Epoch 93/100, Acc=0.6349, Val Loss=1.3340, lr=0.0001
[02/21 19:16:03 cifar100-global-lamp-3.0-resnet56]: Epoch 94/100, Acc=0.6351, Val Loss=1.3332, lr=0.0001
[02/21 19:16:31 cifar100-global-lamp-3.0-resnet56]: Epoch 95/100, Acc=0.6345, Val Loss=1.3318, lr=0.0001
[02/21 19:16:59 cifar100-global-lamp-3.0-resnet56]: Epoch 96/100, Acc=0.6339, Val Loss=1.3296, lr=0.0001
[02/21 19:17:26 cifar100-global-lamp-3.0-resnet56]: Epoch 97/100, Acc=0.6362, Val Loss=1.3269, lr=0.0001
[02/21 19:17:54 cifar100-global-lamp-3.0-resnet56]: Epoch 98/100, Acc=0.6378, Val Loss=1.3267, lr=0.0001
[02/21 19:18:22 cifar100-global-lamp-3.0-resnet56]: Epoch 99/100, Acc=0.6366, Val Loss=1.3295, lr=0.0001
[02/21 19:18:22 cifar100-global-lamp-3.0-resnet56]: Best Acc=0.6378
[02/21 19:18:22 cifar100-global-lamp-3.0-resnet56]: Params: 0.15 M
[02/21 19:18:22 cifar100-global-lamp-3.0-resnet56]: ops: 42.14 M
[02/21 19:18:24 cifar100-global-lamp-3.0-resnet56]: Acc: 0.6366 Val Loss: 1.3295

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: mode: prune
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: model: resnet56
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: verbose: False
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: dataset: cifar100
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: dataroot: data
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: batch_size: 128
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: total_epochs: 100
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: lr_decay_milestones: 60,80
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: lr_decay_gamma: 0.1
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: lr: 0.01
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: output_dir: run/cifar100/prune/cifar100-global-slim-3.0-resnet56
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: finetune: True
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: last_epochs: 100
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: reps: 1
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: method: slim
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: speed_up: 3.0
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: max_pruning_ratio: 1.0
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: reg: 1e-05
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: delta_reg: 0.0001
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: weight_decay: 0.0005
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: seed: 1
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: global_pruning: True
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: sl_total_epochs: 100
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: sl_lr: 0.01
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: sl_reg_warmup: 0
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: sl_restore: None
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: iterative_steps: 400
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: logger: <Logger cifar100-global-slim-3.0-resnet56 (DEBUG)>
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: device: cuda
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: num_classes: 100
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 19:18:32 cifar100-global-slim-3.0-resnet56]: Regularizing...
[02/21 19:19:00 cifar100-global-slim-3.0-resnet56]: Epoch 0/100, Acc=0.6381, Val Loss=1.4864, lr=0.0100
[02/21 19:19:27 cifar100-global-slim-3.0-resnet56]: Epoch 1/100, Acc=0.6632, Val Loss=1.4136, lr=0.0100
[02/21 19:19:54 cifar100-global-slim-3.0-resnet56]: Epoch 2/100, Acc=0.6452, Val Loss=1.5159, lr=0.0100
[02/21 19:20:22 cifar100-global-slim-3.0-resnet56]: Epoch 3/100, Acc=0.6647, Val Loss=1.4189, lr=0.0100
[02/21 19:20:49 cifar100-global-slim-3.0-resnet56]: Epoch 4/100, Acc=0.6566, Val Loss=1.5047, lr=0.0100
[02/21 19:21:16 cifar100-global-slim-3.0-resnet56]: Epoch 5/100, Acc=0.6681, Val Loss=1.4348, lr=0.0100
[02/21 19:21:44 cifar100-global-slim-3.0-resnet56]: Epoch 6/100, Acc=0.6736, Val Loss=1.4165, lr=0.0100
[02/21 19:22:11 cifar100-global-slim-3.0-resnet56]: Epoch 7/100, Acc=0.6752, Val Loss=1.3863, lr=0.0100
[02/21 19:22:39 cifar100-global-slim-3.0-resnet56]: Epoch 8/100, Acc=0.6669, Val Loss=1.4575, lr=0.0100
[02/21 19:23:06 cifar100-global-slim-3.0-resnet56]: Epoch 9/100, Acc=0.6783, Val Loss=1.4315, lr=0.0100
[02/21 19:23:33 cifar100-global-slim-3.0-resnet56]: Epoch 10/100, Acc=0.6788, Val Loss=1.3842, lr=0.0100
[02/21 19:24:01 cifar100-global-slim-3.0-resnet56]: Epoch 11/100, Acc=0.6674, Val Loss=1.4595, lr=0.0100
[02/21 19:24:28 cifar100-global-slim-3.0-resnet56]: Epoch 12/100, Acc=0.6808, Val Loss=1.4244, lr=0.0100
[02/21 19:24:56 cifar100-global-slim-3.0-resnet56]: Epoch 13/100, Acc=0.6782, Val Loss=1.4473, lr=0.0100
[02/21 19:25:23 cifar100-global-slim-3.0-resnet56]: Epoch 14/100, Acc=0.6814, Val Loss=1.4471, lr=0.0100
[02/21 19:25:52 cifar100-global-slim-3.0-resnet56]: Epoch 15/100, Acc=0.6814, Val Loss=1.4495, lr=0.0100
[02/21 19:26:20 cifar100-global-slim-3.0-resnet56]: Epoch 16/100, Acc=0.6844, Val Loss=1.4731, lr=0.0100
[02/21 19:26:49 cifar100-global-slim-3.0-resnet56]: Epoch 17/100, Acc=0.6753, Val Loss=1.4980, lr=0.0100
[02/21 19:27:18 cifar100-global-slim-3.0-resnet56]: Epoch 18/100, Acc=0.6826, Val Loss=1.4661, lr=0.0100
[02/21 19:27:48 cifar100-global-slim-3.0-resnet56]: Epoch 19/100, Acc=0.6792, Val Loss=1.5003, lr=0.0100
[02/21 19:28:17 cifar100-global-slim-3.0-resnet56]: Epoch 20/100, Acc=0.6755, Val Loss=1.5291, lr=0.0100
[02/21 19:28:46 cifar100-global-slim-3.0-resnet56]: Epoch 21/100, Acc=0.6867, Val Loss=1.4918, lr=0.0100
[02/21 19:29:14 cifar100-global-slim-3.0-resnet56]: Epoch 22/100, Acc=0.6803, Val Loss=1.5590, lr=0.0100
[02/21 19:29:41 cifar100-global-slim-3.0-resnet56]: Epoch 23/100, Acc=0.6880, Val Loss=1.5431, lr=0.0100
[02/21 19:30:09 cifar100-global-slim-3.0-resnet56]: Epoch 24/100, Acc=0.6813, Val Loss=1.5701, lr=0.0100
[02/21 19:30:37 cifar100-global-slim-3.0-resnet56]: Epoch 25/100, Acc=0.6844, Val Loss=1.5133, lr=0.0100
[02/21 19:31:05 cifar100-global-slim-3.0-resnet56]: Epoch 26/100, Acc=0.6818, Val Loss=1.5892, lr=0.0100
[02/21 19:31:33 cifar100-global-slim-3.0-resnet56]: Epoch 27/100, Acc=0.6781, Val Loss=1.5770, lr=0.0100
[02/21 19:32:01 cifar100-global-slim-3.0-resnet56]: Epoch 28/100, Acc=0.6843, Val Loss=1.6057, lr=0.0100
[02/21 19:32:29 cifar100-global-slim-3.0-resnet56]: Epoch 29/100, Acc=0.6845, Val Loss=1.5620, lr=0.0100
[02/21 19:32:57 cifar100-global-slim-3.0-resnet56]: Epoch 30/100, Acc=0.6916, Val Loss=1.5349, lr=0.0100
[02/21 19:33:25 cifar100-global-slim-3.0-resnet56]: Epoch 31/100, Acc=0.6878, Val Loss=1.5659, lr=0.0100
[02/21 19:33:53 cifar100-global-slim-3.0-resnet56]: Epoch 32/100, Acc=0.6888, Val Loss=1.5908, lr=0.0100
[02/21 19:34:21 cifar100-global-slim-3.0-resnet56]: Epoch 33/100, Acc=0.6917, Val Loss=1.5786, lr=0.0100
[02/21 19:34:49 cifar100-global-slim-3.0-resnet56]: Epoch 34/100, Acc=0.6922, Val Loss=1.5529, lr=0.0100
[02/21 19:35:17 cifar100-global-slim-3.0-resnet56]: Epoch 35/100, Acc=0.6914, Val Loss=1.6152, lr=0.0100
[02/21 19:35:45 cifar100-global-slim-3.0-resnet56]: Epoch 36/100, Acc=0.6915, Val Loss=1.5819, lr=0.0100
[02/21 19:36:13 cifar100-global-slim-3.0-resnet56]: Epoch 37/100, Acc=0.6912, Val Loss=1.6178, lr=0.0100
[02/21 19:36:41 cifar100-global-slim-3.0-resnet56]: Epoch 38/100, Acc=0.6907, Val Loss=1.6539, lr=0.0100
[02/21 19:37:08 cifar100-global-slim-3.0-resnet56]: Epoch 39/100, Acc=0.6877, Val Loss=1.6565, lr=0.0100
[02/21 19:37:36 cifar100-global-slim-3.0-resnet56]: Epoch 40/100, Acc=0.6869, Val Loss=1.6781, lr=0.0100
[02/21 19:38:04 cifar100-global-slim-3.0-resnet56]: Epoch 41/100, Acc=0.6950, Val Loss=1.6490, lr=0.0100
[02/21 19:38:32 cifar100-global-slim-3.0-resnet56]: Epoch 42/100, Acc=0.6997, Val Loss=1.6151, lr=0.0100
[02/21 19:38:59 cifar100-global-slim-3.0-resnet56]: Epoch 43/100, Acc=0.6912, Val Loss=1.6787, lr=0.0100
[02/21 19:39:27 cifar100-global-slim-3.0-resnet56]: Epoch 44/100, Acc=0.6946, Val Loss=1.7070, lr=0.0100
[02/21 19:39:55 cifar100-global-slim-3.0-resnet56]: Epoch 45/100, Acc=0.6988, Val Loss=1.6167, lr=0.0100
[02/21 19:40:23 cifar100-global-slim-3.0-resnet56]: Epoch 46/100, Acc=0.6903, Val Loss=1.6805, lr=0.0100
[02/21 19:40:50 cifar100-global-slim-3.0-resnet56]: Epoch 47/100, Acc=0.6912, Val Loss=1.6589, lr=0.0100
[02/21 19:41:18 cifar100-global-slim-3.0-resnet56]: Epoch 48/100, Acc=0.6953, Val Loss=1.6719, lr=0.0100
[02/21 19:41:46 cifar100-global-slim-3.0-resnet56]: Epoch 49/100, Acc=0.6988, Val Loss=1.6894, lr=0.0100
[02/21 19:42:14 cifar100-global-slim-3.0-resnet56]: Epoch 50/100, Acc=0.6893, Val Loss=1.7396, lr=0.0100
[02/21 19:42:42 cifar100-global-slim-3.0-resnet56]: Epoch 51/100, Acc=0.6952, Val Loss=1.7285, lr=0.0100
[02/21 19:43:10 cifar100-global-slim-3.0-resnet56]: Epoch 52/100, Acc=0.6926, Val Loss=1.7160, lr=0.0100
[02/21 19:43:38 cifar100-global-slim-3.0-resnet56]: Epoch 53/100, Acc=0.6985, Val Loss=1.6982, lr=0.0100
[02/21 19:44:06 cifar100-global-slim-3.0-resnet56]: Epoch 54/100, Acc=0.6915, Val Loss=1.7475, lr=0.0100
[02/21 19:44:34 cifar100-global-slim-3.0-resnet56]: Epoch 55/100, Acc=0.6925, Val Loss=1.7208, lr=0.0100
[02/21 19:45:02 cifar100-global-slim-3.0-resnet56]: Epoch 56/100, Acc=0.6975, Val Loss=1.7495, lr=0.0100
[02/21 19:45:30 cifar100-global-slim-3.0-resnet56]: Epoch 57/100, Acc=0.6972, Val Loss=1.6987, lr=0.0100
[02/21 19:45:58 cifar100-global-slim-3.0-resnet56]: Epoch 58/100, Acc=0.6858, Val Loss=1.8548, lr=0.0100
[02/21 19:46:26 cifar100-global-slim-3.0-resnet56]: Epoch 59/100, Acc=0.6970, Val Loss=1.7711, lr=0.0100
[02/21 19:46:54 cifar100-global-slim-3.0-resnet56]: Epoch 60/100, Acc=0.7136, Val Loss=1.6236, lr=0.0010
[02/21 19:47:22 cifar100-global-slim-3.0-resnet56]: Epoch 61/100, Acc=0.7165, Val Loss=1.6248, lr=0.0010
[02/21 19:47:51 cifar100-global-slim-3.0-resnet56]: Epoch 62/100, Acc=0.7184, Val Loss=1.6179, lr=0.0010
[02/21 19:48:19 cifar100-global-slim-3.0-resnet56]: Epoch 63/100, Acc=0.7182, Val Loss=1.6264, lr=0.0010
[02/21 19:48:47 cifar100-global-slim-3.0-resnet56]: Epoch 64/100, Acc=0.7229, Val Loss=1.6095, lr=0.0010
[02/21 19:49:15 cifar100-global-slim-3.0-resnet56]: Epoch 65/100, Acc=0.7199, Val Loss=1.6294, lr=0.0010
[02/21 19:49:43 cifar100-global-slim-3.0-resnet56]: Epoch 66/100, Acc=0.7233, Val Loss=1.6115, lr=0.0010
[02/21 19:50:11 cifar100-global-slim-3.0-resnet56]: Epoch 67/100, Acc=0.7227, Val Loss=1.6124, lr=0.0010
[02/21 19:50:38 cifar100-global-slim-3.0-resnet56]: Epoch 68/100, Acc=0.7212, Val Loss=1.6174, lr=0.0010
[02/21 19:51:06 cifar100-global-slim-3.0-resnet56]: Epoch 69/100, Acc=0.7211, Val Loss=1.6179, lr=0.0010
[02/21 19:51:34 cifar100-global-slim-3.0-resnet56]: Epoch 70/100, Acc=0.7232, Val Loss=1.6135, lr=0.0010
[02/21 19:52:02 cifar100-global-slim-3.0-resnet56]: Epoch 71/100, Acc=0.7228, Val Loss=1.6201, lr=0.0010
[02/21 19:52:29 cifar100-global-slim-3.0-resnet56]: Epoch 72/100, Acc=0.7226, Val Loss=1.6231, lr=0.0010
[02/21 19:52:57 cifar100-global-slim-3.0-resnet56]: Epoch 73/100, Acc=0.7232, Val Loss=1.6425, lr=0.0010
[02/21 19:53:25 cifar100-global-slim-3.0-resnet56]: Epoch 74/100, Acc=0.7231, Val Loss=1.6348, lr=0.0010
[02/21 19:53:53 cifar100-global-slim-3.0-resnet56]: Epoch 75/100, Acc=0.7235, Val Loss=1.6255, lr=0.0010
[02/21 19:54:20 cifar100-global-slim-3.0-resnet56]: Epoch 76/100, Acc=0.7219, Val Loss=1.6418, lr=0.0010
[02/21 19:54:48 cifar100-global-slim-3.0-resnet56]: Epoch 77/100, Acc=0.7255, Val Loss=1.6285, lr=0.0010
[02/21 19:55:16 cifar100-global-slim-3.0-resnet56]: Epoch 78/100, Acc=0.7235, Val Loss=1.6460, lr=0.0010
[02/21 19:55:43 cifar100-global-slim-3.0-resnet56]: Epoch 79/100, Acc=0.7233, Val Loss=1.6424, lr=0.0010
[02/21 19:56:11 cifar100-global-slim-3.0-resnet56]: Epoch 80/100, Acc=0.7228, Val Loss=1.6470, lr=0.0001
[02/21 19:56:39 cifar100-global-slim-3.0-resnet56]: Epoch 81/100, Acc=0.7245, Val Loss=1.6406, lr=0.0001
[02/21 19:57:06 cifar100-global-slim-3.0-resnet56]: Epoch 82/100, Acc=0.7233, Val Loss=1.6432, lr=0.0001
[02/21 19:57:34 cifar100-global-slim-3.0-resnet56]: Epoch 83/100, Acc=0.7236, Val Loss=1.6466, lr=0.0001
[02/21 19:58:02 cifar100-global-slim-3.0-resnet56]: Epoch 84/100, Acc=0.7229, Val Loss=1.6462, lr=0.0001
[02/21 19:58:30 cifar100-global-slim-3.0-resnet56]: Epoch 85/100, Acc=0.7228, Val Loss=1.6462, lr=0.0001
[02/21 19:58:58 cifar100-global-slim-3.0-resnet56]: Epoch 86/100, Acc=0.7247, Val Loss=1.6467, lr=0.0001
[02/21 19:59:25 cifar100-global-slim-3.0-resnet56]: Epoch 87/100, Acc=0.7248, Val Loss=1.6352, lr=0.0001
[02/21 19:59:53 cifar100-global-slim-3.0-resnet56]: Epoch 88/100, Acc=0.7240, Val Loss=1.6422, lr=0.0001
[02/21 20:00:21 cifar100-global-slim-3.0-resnet56]: Epoch 89/100, Acc=0.7240, Val Loss=1.6398, lr=0.0001
[02/21 20:00:49 cifar100-global-slim-3.0-resnet56]: Epoch 90/100, Acc=0.7255, Val Loss=1.6357, lr=0.0001
[02/21 20:01:17 cifar100-global-slim-3.0-resnet56]: Epoch 91/100, Acc=0.7237, Val Loss=1.6441, lr=0.0001
[02/21 20:01:44 cifar100-global-slim-3.0-resnet56]: Epoch 92/100, Acc=0.7236, Val Loss=1.6444, lr=0.0001
[02/21 20:02:12 cifar100-global-slim-3.0-resnet56]: Epoch 93/100, Acc=0.7242, Val Loss=1.6367, lr=0.0001
[02/21 20:02:40 cifar100-global-slim-3.0-resnet56]: Epoch 94/100, Acc=0.7245, Val Loss=1.6332, lr=0.0001
[02/21 20:03:08 cifar100-global-slim-3.0-resnet56]: Epoch 95/100, Acc=0.7249, Val Loss=1.6396, lr=0.0001
[02/21 20:03:36 cifar100-global-slim-3.0-resnet56]: Epoch 96/100, Acc=0.7252, Val Loss=1.6390, lr=0.0001
[02/21 20:04:04 cifar100-global-slim-3.0-resnet56]: Epoch 97/100, Acc=0.7257, Val Loss=1.6481, lr=0.0001
[02/21 20:04:32 cifar100-global-slim-3.0-resnet56]: Epoch 98/100, Acc=0.7257, Val Loss=1.6363, lr=0.0001
[02/21 20:05:00 cifar100-global-slim-3.0-resnet56]: Epoch 99/100, Acc=0.7247, Val Loss=1.6365, lr=0.0001
[02/21 20:05:00 cifar100-global-slim-3.0-resnet56]: Best Acc=0.7257
[02/21 20:05:00 cifar100-global-slim-3.0-resnet56]: Loading the sparse model from run/cifar100/prune/cifar100-global-slim-3.0-resnet56/reg_cifar100_resnet56_slim_1e-05.pth...
[02/21 20:05:03 cifar100-global-slim-3.0-resnet56]: Pruning...
[02/21 20:05:16 cifar100-global-slim-3.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(5, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(5, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(5, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(5, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(5, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(5, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(5, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(5, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(5, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(5, 13, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(5, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(16, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(16, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(16, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(16, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 41, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(41, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 56, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(56, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(56, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(46, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(56, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(51, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(56, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(56, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(44, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(56, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(56, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(56, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(33, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=56, out_features=100, bias=True)
)
[02/21 20:05:19 cifar100-global-slim-3.0-resnet56]: Params: 0.86 M => 0.48 M (55.83%)
[02/21 20:05:19 cifar100-global-slim-3.0-resnet56]: FLOPs: 127.12 M => 42.24 M (33.23%, 3.01X )
[02/21 20:05:19 cifar100-global-slim-3.0-resnet56]: Acc: 0.7257 => 0.0100
[02/21 20:05:19 cifar100-global-slim-3.0-resnet56]: Val Loss: 1.6481 => 6.0996
[02/21 20:05:19 cifar100-global-slim-3.0-resnet56]: Finetuning...
[02/21 20:05:46 cifar100-global-slim-3.0-resnet56]: Epoch 0/100, Acc=0.0519, Val Loss=4.2491, lr=0.0100
[02/21 20:06:14 cifar100-global-slim-3.0-resnet56]: Epoch 1/100, Acc=0.1502, Val Loss=3.6324, lr=0.0100
[02/21 20:06:41 cifar100-global-slim-3.0-resnet56]: Epoch 2/100, Acc=0.2732, Val Loss=3.0251, lr=0.0100
[02/21 20:07:08 cifar100-global-slim-3.0-resnet56]: Epoch 3/100, Acc=0.3912, Val Loss=2.3548, lr=0.0100
[02/21 20:07:36 cifar100-global-slim-3.0-resnet56]: Epoch 4/100, Acc=0.4628, Val Loss=2.0485, lr=0.0100
[02/21 20:08:03 cifar100-global-slim-3.0-resnet56]: Epoch 5/100, Acc=0.4308, Val Loss=2.2979, lr=0.0100
[02/21 20:08:31 cifar100-global-slim-3.0-resnet56]: Epoch 6/100, Acc=0.5143, Val Loss=1.8944, lr=0.0100
[02/21 20:08:58 cifar100-global-slim-3.0-resnet56]: Epoch 7/100, Acc=0.5536, Val Loss=1.6856, lr=0.0100
[02/21 20:09:26 cifar100-global-slim-3.0-resnet56]: Epoch 8/100, Acc=0.5560, Val Loss=1.6912, lr=0.0100
[02/21 20:09:53 cifar100-global-slim-3.0-resnet56]: Epoch 9/100, Acc=0.5513, Val Loss=1.7560, lr=0.0100
[02/21 20:10:20 cifar100-global-slim-3.0-resnet56]: Epoch 10/100, Acc=0.5866, Val Loss=1.5928, lr=0.0100
[02/21 20:10:48 cifar100-global-slim-3.0-resnet56]: Epoch 11/100, Acc=0.5223, Val Loss=1.8971, lr=0.0100
[02/21 20:11:15 cifar100-global-slim-3.0-resnet56]: Epoch 12/100, Acc=0.5581, Val Loss=1.6931, lr=0.0100
[02/21 20:11:42 cifar100-global-slim-3.0-resnet56]: Epoch 13/100, Acc=0.5722, Val Loss=1.6339, lr=0.0100
[02/21 20:12:10 cifar100-global-slim-3.0-resnet56]: Epoch 14/100, Acc=0.5973, Val Loss=1.5302, lr=0.0100
[02/21 20:12:37 cifar100-global-slim-3.0-resnet56]: Epoch 15/100, Acc=0.5179, Val Loss=2.0114, lr=0.0100
[02/21 20:13:04 cifar100-global-slim-3.0-resnet56]: Epoch 16/100, Acc=0.5824, Val Loss=1.5892, lr=0.0100
[02/21 20:13:32 cifar100-global-slim-3.0-resnet56]: Epoch 17/100, Acc=0.5420, Val Loss=1.8727, lr=0.0100
[02/21 20:13:59 cifar100-global-slim-3.0-resnet56]: Epoch 18/100, Acc=0.5835, Val Loss=1.5890, lr=0.0100
[02/21 20:14:26 cifar100-global-slim-3.0-resnet56]: Epoch 19/100, Acc=0.6149, Val Loss=1.4516, lr=0.0100
[02/21 20:14:54 cifar100-global-slim-3.0-resnet56]: Epoch 20/100, Acc=0.6142, Val Loss=1.4541, lr=0.0100
[02/21 20:15:21 cifar100-global-slim-3.0-resnet56]: Epoch 21/100, Acc=0.5603, Val Loss=1.7480, lr=0.0100
[02/21 20:15:48 cifar100-global-slim-3.0-resnet56]: Epoch 22/100, Acc=0.6026, Val Loss=1.5282, lr=0.0100
[02/21 20:16:15 cifar100-global-slim-3.0-resnet56]: Epoch 23/100, Acc=0.6163, Val Loss=1.4727, lr=0.0100
[02/21 20:16:42 cifar100-global-slim-3.0-resnet56]: Epoch 24/100, Acc=0.6036, Val Loss=1.5475, lr=0.0100
[02/21 20:17:09 cifar100-global-slim-3.0-resnet56]: Epoch 25/100, Acc=0.6024, Val Loss=1.5287, lr=0.0100
[02/21 20:17:36 cifar100-global-slim-3.0-resnet56]: Epoch 26/100, Acc=0.6034, Val Loss=1.5406, lr=0.0100
[02/21 20:18:04 cifar100-global-slim-3.0-resnet56]: Epoch 27/100, Acc=0.6043, Val Loss=1.5251, lr=0.0100
[02/21 20:18:32 cifar100-global-slim-3.0-resnet56]: Epoch 28/100, Acc=0.5821, Val Loss=1.6352, lr=0.0100
[02/21 20:18:59 cifar100-global-slim-3.0-resnet56]: Epoch 29/100, Acc=0.5984, Val Loss=1.5531, lr=0.0100
[02/21 20:19:27 cifar100-global-slim-3.0-resnet56]: Epoch 30/100, Acc=0.6071, Val Loss=1.5300, lr=0.0100
[02/21 20:19:55 cifar100-global-slim-3.0-resnet56]: Epoch 31/100, Acc=0.5962, Val Loss=1.5824, lr=0.0100
[02/21 20:20:22 cifar100-global-slim-3.0-resnet56]: Epoch 32/100, Acc=0.6172, Val Loss=1.4601, lr=0.0100
[02/21 20:20:50 cifar100-global-slim-3.0-resnet56]: Epoch 33/100, Acc=0.6004, Val Loss=1.5561, lr=0.0100
[02/21 20:21:18 cifar100-global-slim-3.0-resnet56]: Epoch 34/100, Acc=0.6001, Val Loss=1.5590, lr=0.0100
[02/21 20:21:46 cifar100-global-slim-3.0-resnet56]: Epoch 35/100, Acc=0.6191, Val Loss=1.4629, lr=0.0100
[02/21 20:22:14 cifar100-global-slim-3.0-resnet56]: Epoch 36/100, Acc=0.5904, Val Loss=1.6410, lr=0.0100
[02/21 20:22:42 cifar100-global-slim-3.0-resnet56]: Epoch 37/100, Acc=0.6066, Val Loss=1.5126, lr=0.0100
[02/21 20:23:10 cifar100-global-slim-3.0-resnet56]: Epoch 38/100, Acc=0.6142, Val Loss=1.4903, lr=0.0100
[02/21 20:23:38 cifar100-global-slim-3.0-resnet56]: Epoch 39/100, Acc=0.6223, Val Loss=1.4567, lr=0.0100
[02/21 20:24:06 cifar100-global-slim-3.0-resnet56]: Epoch 40/100, Acc=0.6108, Val Loss=1.5287, lr=0.0100
[02/21 20:24:33 cifar100-global-slim-3.0-resnet56]: Epoch 41/100, Acc=0.6174, Val Loss=1.5059, lr=0.0100
[02/21 20:25:01 cifar100-global-slim-3.0-resnet56]: Epoch 42/100, Acc=0.6084, Val Loss=1.5007, lr=0.0100
[02/21 20:25:29 cifar100-global-slim-3.0-resnet56]: Epoch 43/100, Acc=0.6116, Val Loss=1.5098, lr=0.0100
[02/21 20:25:56 cifar100-global-slim-3.0-resnet56]: Epoch 44/100, Acc=0.5882, Val Loss=1.6194, lr=0.0100
[02/21 20:26:24 cifar100-global-slim-3.0-resnet56]: Epoch 45/100, Acc=0.6101, Val Loss=1.5115, lr=0.0100
[02/21 20:26:51 cifar100-global-slim-3.0-resnet56]: Epoch 46/100, Acc=0.6279, Val Loss=1.4466, lr=0.0100
[02/21 20:27:19 cifar100-global-slim-3.0-resnet56]: Epoch 47/100, Acc=0.6093, Val Loss=1.5202, lr=0.0100
[02/21 20:27:47 cifar100-global-slim-3.0-resnet56]: Epoch 48/100, Acc=0.6143, Val Loss=1.5303, lr=0.0100
[02/21 20:28:15 cifar100-global-slim-3.0-resnet56]: Epoch 49/100, Acc=0.6196, Val Loss=1.4956, lr=0.0100
[02/21 20:28:44 cifar100-global-slim-3.0-resnet56]: Epoch 50/100, Acc=0.6094, Val Loss=1.5370, lr=0.0100
[02/21 20:29:12 cifar100-global-slim-3.0-resnet56]: Epoch 51/100, Acc=0.5807, Val Loss=1.7218, lr=0.0100
[02/21 20:29:40 cifar100-global-slim-3.0-resnet56]: Epoch 52/100, Acc=0.5996, Val Loss=1.5710, lr=0.0100
[02/21 20:30:07 cifar100-global-slim-3.0-resnet56]: Epoch 53/100, Acc=0.6008, Val Loss=1.5941, lr=0.0100
[02/21 20:30:35 cifar100-global-slim-3.0-resnet56]: Epoch 54/100, Acc=0.6088, Val Loss=1.5449, lr=0.0100
[02/21 20:31:03 cifar100-global-slim-3.0-resnet56]: Epoch 55/100, Acc=0.6126, Val Loss=1.5345, lr=0.0100
[02/21 20:31:31 cifar100-global-slim-3.0-resnet56]: Epoch 56/100, Acc=0.5826, Val Loss=1.6963, lr=0.0100
[02/21 20:31:59 cifar100-global-slim-3.0-resnet56]: Epoch 57/100, Acc=0.6112, Val Loss=1.5933, lr=0.0100
[02/21 20:32:27 cifar100-global-slim-3.0-resnet56]: Epoch 58/100, Acc=0.5850, Val Loss=1.6349, lr=0.0100
[02/21 20:32:54 cifar100-global-slim-3.0-resnet56]: Epoch 59/100, Acc=0.6024, Val Loss=1.6110, lr=0.0100
[02/21 20:33:22 cifar100-global-slim-3.0-resnet56]: Epoch 60/100, Acc=0.6810, Val Loss=1.2034, lr=0.0010
[02/21 20:33:49 cifar100-global-slim-3.0-resnet56]: Epoch 61/100, Acc=0.6845, Val Loss=1.2057, lr=0.0010
[02/21 20:34:17 cifar100-global-slim-3.0-resnet56]: Epoch 62/100, Acc=0.6818, Val Loss=1.2020, lr=0.0010
[02/21 20:34:44 cifar100-global-slim-3.0-resnet56]: Epoch 63/100, Acc=0.6858, Val Loss=1.2104, lr=0.0010
[02/21 20:35:12 cifar100-global-slim-3.0-resnet56]: Epoch 64/100, Acc=0.6841, Val Loss=1.2211, lr=0.0010
[02/21 20:35:40 cifar100-global-slim-3.0-resnet56]: Epoch 65/100, Acc=0.6852, Val Loss=1.2171, lr=0.0010
[02/21 20:36:08 cifar100-global-slim-3.0-resnet56]: Epoch 66/100, Acc=0.6864, Val Loss=1.2290, lr=0.0010
[02/21 20:36:35 cifar100-global-slim-3.0-resnet56]: Epoch 67/100, Acc=0.6881, Val Loss=1.2301, lr=0.0010
[02/21 20:37:03 cifar100-global-slim-3.0-resnet56]: Epoch 68/100, Acc=0.6861, Val Loss=1.2374, lr=0.0010
[02/21 20:37:30 cifar100-global-slim-3.0-resnet56]: Epoch 69/100, Acc=0.6852, Val Loss=1.2417, lr=0.0010
[02/21 20:37:58 cifar100-global-slim-3.0-resnet56]: Epoch 70/100, Acc=0.6831, Val Loss=1.2375, lr=0.0010
[02/21 20:38:26 cifar100-global-slim-3.0-resnet56]: Epoch 71/100, Acc=0.6837, Val Loss=1.2492, lr=0.0010
[02/21 20:38:55 cifar100-global-slim-3.0-resnet56]: Epoch 72/100, Acc=0.6837, Val Loss=1.2531, lr=0.0010
[02/21 20:39:23 cifar100-global-slim-3.0-resnet56]: Epoch 73/100, Acc=0.6853, Val Loss=1.2571, lr=0.0010
[02/21 20:39:51 cifar100-global-slim-3.0-resnet56]: Epoch 74/100, Acc=0.6844, Val Loss=1.2530, lr=0.0010
[02/21 20:40:19 cifar100-global-slim-3.0-resnet56]: Epoch 75/100, Acc=0.6849, Val Loss=1.2627, lr=0.0010
[02/21 20:40:47 cifar100-global-slim-3.0-resnet56]: Epoch 76/100, Acc=0.6851, Val Loss=1.2646, lr=0.0010
[02/21 20:41:15 cifar100-global-slim-3.0-resnet56]: Epoch 77/100, Acc=0.6855, Val Loss=1.2722, lr=0.0010
[02/21 20:41:42 cifar100-global-slim-3.0-resnet56]: Epoch 78/100, Acc=0.6789, Val Loss=1.2885, lr=0.0010
[02/21 20:42:10 cifar100-global-slim-3.0-resnet56]: Epoch 79/100, Acc=0.6810, Val Loss=1.2992, lr=0.0010
[02/21 20:42:37 cifar100-global-slim-3.0-resnet56]: Epoch 80/100, Acc=0.6814, Val Loss=1.2854, lr=0.0001
[02/21 20:43:05 cifar100-global-slim-3.0-resnet56]: Epoch 81/100, Acc=0.6837, Val Loss=1.2838, lr=0.0001
[02/21 20:43:32 cifar100-global-slim-3.0-resnet56]: Epoch 82/100, Acc=0.6846, Val Loss=1.2771, lr=0.0001
[02/21 20:43:59 cifar100-global-slim-3.0-resnet56]: Epoch 83/100, Acc=0.6845, Val Loss=1.2785, lr=0.0001
[02/21 20:44:26 cifar100-global-slim-3.0-resnet56]: Epoch 84/100, Acc=0.6834, Val Loss=1.2772, lr=0.0001
[02/21 20:44:53 cifar100-global-slim-3.0-resnet56]: Epoch 85/100, Acc=0.6841, Val Loss=1.2776, lr=0.0001
[02/21 20:45:20 cifar100-global-slim-3.0-resnet56]: Epoch 86/100, Acc=0.6833, Val Loss=1.2814, lr=0.0001
[02/21 20:45:47 cifar100-global-slim-3.0-resnet56]: Epoch 87/100, Acc=0.6834, Val Loss=1.2786, lr=0.0001
[02/21 20:46:14 cifar100-global-slim-3.0-resnet56]: Epoch 88/100, Acc=0.6837, Val Loss=1.2787, lr=0.0001
[02/21 20:46:41 cifar100-global-slim-3.0-resnet56]: Epoch 89/100, Acc=0.6843, Val Loss=1.2824, lr=0.0001
[02/21 20:47:09 cifar100-global-slim-3.0-resnet56]: Epoch 90/100, Acc=0.6841, Val Loss=1.2843, lr=0.0001
[02/21 20:47:36 cifar100-global-slim-3.0-resnet56]: Epoch 91/100, Acc=0.6847, Val Loss=1.2788, lr=0.0001
[02/21 20:48:03 cifar100-global-slim-3.0-resnet56]: Epoch 92/100, Acc=0.6848, Val Loss=1.2728, lr=0.0001
[02/21 20:48:31 cifar100-global-slim-3.0-resnet56]: Epoch 93/100, Acc=0.6826, Val Loss=1.2830, lr=0.0001
[02/21 20:48:59 cifar100-global-slim-3.0-resnet56]: Epoch 94/100, Acc=0.6841, Val Loss=1.2821, lr=0.0001
[02/21 20:49:27 cifar100-global-slim-3.0-resnet56]: Epoch 95/100, Acc=0.6837, Val Loss=1.2776, lr=0.0001
[02/21 20:49:55 cifar100-global-slim-3.0-resnet56]: Epoch 96/100, Acc=0.6833, Val Loss=1.2788, lr=0.0001
[02/21 20:50:22 cifar100-global-slim-3.0-resnet56]: Epoch 97/100, Acc=0.6822, Val Loss=1.2877, lr=0.0001
[02/21 20:50:50 cifar100-global-slim-3.0-resnet56]: Epoch 98/100, Acc=0.6832, Val Loss=1.2792, lr=0.0001
[02/21 20:51:17 cifar100-global-slim-3.0-resnet56]: Epoch 99/100, Acc=0.6838, Val Loss=1.2861, lr=0.0001
[02/21 20:51:17 cifar100-global-slim-3.0-resnet56]: Best Acc=0.6881
[02/21 20:51:18 cifar100-global-slim-3.0-resnet56]: Params: 0.48 M
[02/21 20:51:18 cifar100-global-slim-3.0-resnet56]: ops: 42.24 M
[02/21 20:51:20 cifar100-global-slim-3.0-resnet56]: Acc: 0.6838 Val Loss: 1.2861

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: mode: prune
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: model: resnet56
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: verbose: False
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: dataset: cifar100
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: dataroot: data
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: batch_size: 128
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: total_epochs: 100
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: lr_decay_milestones: 60,80
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: lr_decay_gamma: 0.1
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: lr: 0.01
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: output_dir: run/cifar100/prune/cifar100-global-group_norm-3.0-resnet56
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: finetune: True
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: last_epochs: 100
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: reps: 1
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: method: group_norm
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: speed_up: 3.0
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: max_pruning_ratio: 1.0
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: reg: 1e-05
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: delta_reg: 0.0001
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: weight_decay: 0.0005
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: seed: 1
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: global_pruning: True
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: sl_total_epochs: 100
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: sl_lr: 0.01
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: sl_reg_warmup: 0
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: sl_restore: None
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: iterative_steps: 400
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: logger: <Logger cifar100-global-group_norm-3.0-resnet56 (DEBUG)>
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: device: cuda
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: num_classes: 100
[02/21 20:51:28 cifar100-global-group_norm-3.0-resnet56]: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 20:51:31 cifar100-global-group_norm-3.0-resnet56]: Pruning...
[02/21 20:51:47 cifar100-global-group_norm-3.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(7, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(7, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(7, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(7, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(7, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(7, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(7, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(7, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(7, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(7, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(7, 9, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(9, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(9, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(9, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(9, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(9, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(9, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(9, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(9, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(40, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(9, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 43, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(43, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(64, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(64, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(64, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(64, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(58, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=100, bias=True)
)
[02/21 20:51:50 cifar100-global-group_norm-3.0-resnet56]: Params: 0.86 M => 0.58 M (66.81%)
[02/21 20:51:50 cifar100-global-group_norm-3.0-resnet56]: FLOPs: 127.12 M => 42.33 M (33.29%, 3.00X )
[02/21 20:51:50 cifar100-global-group_norm-3.0-resnet56]: Acc: 0.7269 => 0.0147
[02/21 20:51:50 cifar100-global-group_norm-3.0-resnet56]: Val Loss: 1.1578 => 15.9639
[02/21 20:51:50 cifar100-global-group_norm-3.0-resnet56]: Finetuning...
[02/21 20:52:17 cifar100-global-group_norm-3.0-resnet56]: Epoch 0/100, Acc=0.4876, Val Loss=2.0285, lr=0.0100
[02/21 20:52:44 cifar100-global-group_norm-3.0-resnet56]: Epoch 1/100, Acc=0.5586, Val Loss=1.6453, lr=0.0100
[02/21 20:53:11 cifar100-global-group_norm-3.0-resnet56]: Epoch 2/100, Acc=0.5551, Val Loss=1.6679, lr=0.0100
[02/21 20:53:38 cifar100-global-group_norm-3.0-resnet56]: Epoch 3/100, Acc=0.5682, Val Loss=1.6645, lr=0.0100
[02/21 20:54:05 cifar100-global-group_norm-3.0-resnet56]: Epoch 4/100, Acc=0.5512, Val Loss=1.7686, lr=0.0100
[02/21 20:54:32 cifar100-global-group_norm-3.0-resnet56]: Epoch 5/100, Acc=0.5607, Val Loss=1.6861, lr=0.0100
[02/21 20:54:59 cifar100-global-group_norm-3.0-resnet56]: Epoch 6/100, Acc=0.6037, Val Loss=1.5036, lr=0.0100
[02/21 20:55:26 cifar100-global-group_norm-3.0-resnet56]: Epoch 7/100, Acc=0.5959, Val Loss=1.5352, lr=0.0100
[02/21 20:55:53 cifar100-global-group_norm-3.0-resnet56]: Epoch 8/100, Acc=0.6087, Val Loss=1.4693, lr=0.0100
[02/21 20:56:21 cifar100-global-group_norm-3.0-resnet56]: Epoch 9/100, Acc=0.6159, Val Loss=1.4289, lr=0.0100
[02/21 20:56:48 cifar100-global-group_norm-3.0-resnet56]: Epoch 10/100, Acc=0.5850, Val Loss=1.6057, lr=0.0100
[02/21 20:57:15 cifar100-global-group_norm-3.0-resnet56]: Epoch 11/100, Acc=0.6038, Val Loss=1.5035, lr=0.0100
[02/21 20:57:42 cifar100-global-group_norm-3.0-resnet56]: Epoch 12/100, Acc=0.5905, Val Loss=1.5469, lr=0.0100
[02/21 20:58:09 cifar100-global-group_norm-3.0-resnet56]: Epoch 13/100, Acc=0.6207, Val Loss=1.4088, lr=0.0100
[02/21 20:58:37 cifar100-global-group_norm-3.0-resnet56]: Epoch 14/100, Acc=0.6085, Val Loss=1.4807, lr=0.0100
[02/21 20:59:04 cifar100-global-group_norm-3.0-resnet56]: Epoch 15/100, Acc=0.6116, Val Loss=1.4787, lr=0.0100
[02/21 20:59:31 cifar100-global-group_norm-3.0-resnet56]: Epoch 16/100, Acc=0.6061, Val Loss=1.5113, lr=0.0100
[02/21 20:59:58 cifar100-global-group_norm-3.0-resnet56]: Epoch 17/100, Acc=0.6076, Val Loss=1.5150, lr=0.0100
[02/21 21:00:26 cifar100-global-group_norm-3.0-resnet56]: Epoch 18/100, Acc=0.6126, Val Loss=1.4976, lr=0.0100
[02/21 21:00:53 cifar100-global-group_norm-3.0-resnet56]: Epoch 19/100, Acc=0.6058, Val Loss=1.5412, lr=0.0100
[02/21 21:01:20 cifar100-global-group_norm-3.0-resnet56]: Epoch 20/100, Acc=0.5948, Val Loss=1.5405, lr=0.0100
[02/21 21:01:47 cifar100-global-group_norm-3.0-resnet56]: Epoch 21/100, Acc=0.6025, Val Loss=1.5339, lr=0.0100
[02/21 21:02:15 cifar100-global-group_norm-3.0-resnet56]: Epoch 22/100, Acc=0.6274, Val Loss=1.4267, lr=0.0100
[02/21 21:02:42 cifar100-global-group_norm-3.0-resnet56]: Epoch 23/100, Acc=0.5799, Val Loss=1.6924, lr=0.0100
[02/21 21:03:09 cifar100-global-group_norm-3.0-resnet56]: Epoch 24/100, Acc=0.6277, Val Loss=1.4377, lr=0.0100
[02/21 21:03:37 cifar100-global-group_norm-3.0-resnet56]: Epoch 25/100, Acc=0.6170, Val Loss=1.4990, lr=0.0100
[02/21 21:04:04 cifar100-global-group_norm-3.0-resnet56]: Epoch 26/100, Acc=0.6246, Val Loss=1.4718, lr=0.0100
[02/21 21:04:31 cifar100-global-group_norm-3.0-resnet56]: Epoch 27/100, Acc=0.6123, Val Loss=1.5011, lr=0.0100
[02/21 21:04:59 cifar100-global-group_norm-3.0-resnet56]: Epoch 28/100, Acc=0.6160, Val Loss=1.4830, lr=0.0100
[02/21 21:05:26 cifar100-global-group_norm-3.0-resnet56]: Epoch 29/100, Acc=0.6164, Val Loss=1.4796, lr=0.0100
[02/21 21:05:53 cifar100-global-group_norm-3.0-resnet56]: Epoch 30/100, Acc=0.6159, Val Loss=1.5193, lr=0.0100
[02/21 21:06:21 cifar100-global-group_norm-3.0-resnet56]: Epoch 31/100, Acc=0.6092, Val Loss=1.5351, lr=0.0100
[02/21 21:06:48 cifar100-global-group_norm-3.0-resnet56]: Epoch 32/100, Acc=0.6075, Val Loss=1.5733, lr=0.0100
[02/21 21:07:16 cifar100-global-group_norm-3.0-resnet56]: Epoch 33/100, Acc=0.6102, Val Loss=1.5305, lr=0.0100
[02/21 21:07:43 cifar100-global-group_norm-3.0-resnet56]: Epoch 34/100, Acc=0.5945, Val Loss=1.6228, lr=0.0100
[02/21 21:08:10 cifar100-global-group_norm-3.0-resnet56]: Epoch 35/100, Acc=0.6136, Val Loss=1.5132, lr=0.0100
[02/21 21:08:38 cifar100-global-group_norm-3.0-resnet56]: Epoch 36/100, Acc=0.6118, Val Loss=1.5554, lr=0.0100
[02/21 21:09:05 cifar100-global-group_norm-3.0-resnet56]: Epoch 37/100, Acc=0.6271, Val Loss=1.4758, lr=0.0100
[02/21 21:09:32 cifar100-global-group_norm-3.0-resnet56]: Epoch 38/100, Acc=0.6302, Val Loss=1.4495, lr=0.0100
[02/21 21:10:00 cifar100-global-group_norm-3.0-resnet56]: Epoch 39/100, Acc=0.6324, Val Loss=1.3955, lr=0.0100
[02/21 21:10:27 cifar100-global-group_norm-3.0-resnet56]: Epoch 40/100, Acc=0.6161, Val Loss=1.4953, lr=0.0100
[02/21 21:10:54 cifar100-global-group_norm-3.0-resnet56]: Epoch 41/100, Acc=0.6202, Val Loss=1.5074, lr=0.0100
[02/21 21:11:22 cifar100-global-group_norm-3.0-resnet56]: Epoch 42/100, Acc=0.6320, Val Loss=1.4545, lr=0.0100
[02/21 21:11:49 cifar100-global-group_norm-3.0-resnet56]: Epoch 43/100, Acc=0.6033, Val Loss=1.5764, lr=0.0100
[02/21 21:12:16 cifar100-global-group_norm-3.0-resnet56]: Epoch 44/100, Acc=0.6379, Val Loss=1.3996, lr=0.0100
[02/21 21:12:43 cifar100-global-group_norm-3.0-resnet56]: Epoch 45/100, Acc=0.6162, Val Loss=1.5032, lr=0.0100
[02/21 21:13:10 cifar100-global-group_norm-3.0-resnet56]: Epoch 46/100, Acc=0.6274, Val Loss=1.4822, lr=0.0100
[02/21 21:13:37 cifar100-global-group_norm-3.0-resnet56]: Epoch 47/100, Acc=0.6095, Val Loss=1.5593, lr=0.0100
[02/21 21:14:04 cifar100-global-group_norm-3.0-resnet56]: Epoch 48/100, Acc=0.6176, Val Loss=1.5457, lr=0.0100
[02/21 21:14:32 cifar100-global-group_norm-3.0-resnet56]: Epoch 49/100, Acc=0.5918, Val Loss=1.6612, lr=0.0100
[02/21 21:14:59 cifar100-global-group_norm-3.0-resnet56]: Epoch 50/100, Acc=0.6265, Val Loss=1.4551, lr=0.0100
[02/21 21:15:27 cifar100-global-group_norm-3.0-resnet56]: Epoch 51/100, Acc=0.6330, Val Loss=1.4278, lr=0.0100
[02/21 21:15:54 cifar100-global-group_norm-3.0-resnet56]: Epoch 52/100, Acc=0.6314, Val Loss=1.4651, lr=0.0100
[02/21 21:16:22 cifar100-global-group_norm-3.0-resnet56]: Epoch 53/100, Acc=0.6150, Val Loss=1.5055, lr=0.0100
[02/21 21:16:49 cifar100-global-group_norm-3.0-resnet56]: Epoch 54/100, Acc=0.6073, Val Loss=1.5580, lr=0.0100
[02/21 21:17:17 cifar100-global-group_norm-3.0-resnet56]: Epoch 55/100, Acc=0.6302, Val Loss=1.4369, lr=0.0100
[02/21 21:17:44 cifar100-global-group_norm-3.0-resnet56]: Epoch 56/100, Acc=0.6265, Val Loss=1.4865, lr=0.0100
[02/21 21:18:12 cifar100-global-group_norm-3.0-resnet56]: Epoch 57/100, Acc=0.6199, Val Loss=1.5360, lr=0.0100
[02/21 21:18:40 cifar100-global-group_norm-3.0-resnet56]: Epoch 58/100, Acc=0.6211, Val Loss=1.5266, lr=0.0100
[02/21 21:19:07 cifar100-global-group_norm-3.0-resnet56]: Epoch 59/100, Acc=0.6265, Val Loss=1.4657, lr=0.0100
[02/21 21:19:35 cifar100-global-group_norm-3.0-resnet56]: Epoch 60/100, Acc=0.6878, Val Loss=1.1956, lr=0.0010
[02/21 21:20:02 cifar100-global-group_norm-3.0-resnet56]: Epoch 61/100, Acc=0.6883, Val Loss=1.1992, lr=0.0010
[02/21 21:20:30 cifar100-global-group_norm-3.0-resnet56]: Epoch 62/100, Acc=0.6881, Val Loss=1.2038, lr=0.0010
[02/21 21:20:58 cifar100-global-group_norm-3.0-resnet56]: Epoch 63/100, Acc=0.6914, Val Loss=1.2132, lr=0.0010
[02/21 21:21:25 cifar100-global-group_norm-3.0-resnet56]: Epoch 64/100, Acc=0.6890, Val Loss=1.2054, lr=0.0010
[02/21 21:21:53 cifar100-global-group_norm-3.0-resnet56]: Epoch 65/100, Acc=0.6911, Val Loss=1.2171, lr=0.0010
[02/21 21:22:20 cifar100-global-group_norm-3.0-resnet56]: Epoch 66/100, Acc=0.6896, Val Loss=1.2261, lr=0.0010
[02/21 21:22:48 cifar100-global-group_norm-3.0-resnet56]: Epoch 67/100, Acc=0.6881, Val Loss=1.2305, lr=0.0010
[02/21 21:23:15 cifar100-global-group_norm-3.0-resnet56]: Epoch 68/100, Acc=0.6900, Val Loss=1.2355, lr=0.0010
[02/21 21:23:42 cifar100-global-group_norm-3.0-resnet56]: Epoch 69/100, Acc=0.6893, Val Loss=1.2408, lr=0.0010
[02/21 21:24:09 cifar100-global-group_norm-3.0-resnet56]: Epoch 70/100, Acc=0.6916, Val Loss=1.2420, lr=0.0010
[02/21 21:24:36 cifar100-global-group_norm-3.0-resnet56]: Epoch 71/100, Acc=0.6925, Val Loss=1.2509, lr=0.0010
[02/21 21:25:03 cifar100-global-group_norm-3.0-resnet56]: Epoch 72/100, Acc=0.6901, Val Loss=1.2504, lr=0.0010
[02/21 21:25:30 cifar100-global-group_norm-3.0-resnet56]: Epoch 73/100, Acc=0.6860, Val Loss=1.2639, lr=0.0010
[02/21 21:25:57 cifar100-global-group_norm-3.0-resnet56]: Epoch 74/100, Acc=0.6890, Val Loss=1.2646, lr=0.0010
[02/21 21:26:25 cifar100-global-group_norm-3.0-resnet56]: Epoch 75/100, Acc=0.6883, Val Loss=1.2691, lr=0.0010
[02/21 21:26:52 cifar100-global-group_norm-3.0-resnet56]: Epoch 76/100, Acc=0.6905, Val Loss=1.2727, lr=0.0010
[02/21 21:27:19 cifar100-global-group_norm-3.0-resnet56]: Epoch 77/100, Acc=0.6902, Val Loss=1.2823, lr=0.0010
[02/21 21:27:46 cifar100-global-group_norm-3.0-resnet56]: Epoch 78/100, Acc=0.6867, Val Loss=1.2809, lr=0.0010
[02/21 21:28:14 cifar100-global-group_norm-3.0-resnet56]: Epoch 79/100, Acc=0.6891, Val Loss=1.2863, lr=0.0010
[02/21 21:28:41 cifar100-global-group_norm-3.0-resnet56]: Epoch 80/100, Acc=0.6879, Val Loss=1.2738, lr=0.0001
[02/21 21:29:09 cifar100-global-group_norm-3.0-resnet56]: Epoch 81/100, Acc=0.6868, Val Loss=1.2729, lr=0.0001
[02/21 21:29:36 cifar100-global-group_norm-3.0-resnet56]: Epoch 82/100, Acc=0.6883, Val Loss=1.2760, lr=0.0001
[02/21 21:30:03 cifar100-global-group_norm-3.0-resnet56]: Epoch 83/100, Acc=0.6881, Val Loss=1.2758, lr=0.0001
[02/21 21:30:30 cifar100-global-group_norm-3.0-resnet56]: Epoch 84/100, Acc=0.6880, Val Loss=1.2825, lr=0.0001
[02/21 21:30:58 cifar100-global-group_norm-3.0-resnet56]: Epoch 85/100, Acc=0.6893, Val Loss=1.2763, lr=0.0001
[02/21 21:31:25 cifar100-global-group_norm-3.0-resnet56]: Epoch 86/100, Acc=0.6872, Val Loss=1.2823, lr=0.0001
[02/21 21:31:52 cifar100-global-group_norm-3.0-resnet56]: Epoch 87/100, Acc=0.6896, Val Loss=1.2804, lr=0.0001
[02/21 21:32:20 cifar100-global-group_norm-3.0-resnet56]: Epoch 88/100, Acc=0.6895, Val Loss=1.2765, lr=0.0001
[02/21 21:32:47 cifar100-global-group_norm-3.0-resnet56]: Epoch 89/100, Acc=0.6880, Val Loss=1.2772, lr=0.0001
[02/21 21:33:14 cifar100-global-group_norm-3.0-resnet56]: Epoch 90/100, Acc=0.6895, Val Loss=1.2766, lr=0.0001
[02/21 21:33:42 cifar100-global-group_norm-3.0-resnet56]: Epoch 91/100, Acc=0.6873, Val Loss=1.2853, lr=0.0001
[02/21 21:34:10 cifar100-global-group_norm-3.0-resnet56]: Epoch 92/100, Acc=0.6873, Val Loss=1.2810, lr=0.0001
[02/21 21:34:37 cifar100-global-group_norm-3.0-resnet56]: Epoch 93/100, Acc=0.6867, Val Loss=1.2846, lr=0.0001
[02/21 21:35:05 cifar100-global-group_norm-3.0-resnet56]: Epoch 94/100, Acc=0.6869, Val Loss=1.2790, lr=0.0001
[02/21 21:35:32 cifar100-global-group_norm-3.0-resnet56]: Epoch 95/100, Acc=0.6883, Val Loss=1.2834, lr=0.0001
[02/21 21:36:00 cifar100-global-group_norm-3.0-resnet56]: Epoch 96/100, Acc=0.6884, Val Loss=1.2856, lr=0.0001
[02/21 21:36:27 cifar100-global-group_norm-3.0-resnet56]: Epoch 97/100, Acc=0.6873, Val Loss=1.2832, lr=0.0001
[02/21 21:36:54 cifar100-global-group_norm-3.0-resnet56]: Epoch 98/100, Acc=0.6871, Val Loss=1.2836, lr=0.0001
[02/21 21:37:22 cifar100-global-group_norm-3.0-resnet56]: Epoch 99/100, Acc=0.6859, Val Loss=1.2861, lr=0.0001
[02/21 21:37:22 cifar100-global-group_norm-3.0-resnet56]: Best Acc=0.6925
[02/21 21:37:22 cifar100-global-group_norm-3.0-resnet56]: Params: 0.58 M
[02/21 21:37:22 cifar100-global-group_norm-3.0-resnet56]: ops: 42.33 M
[02/21 21:37:24 cifar100-global-group_norm-3.0-resnet56]: Acc: 0.6859 Val Loss: 1.2861

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: mode: prune
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: model: resnet56
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: verbose: False
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: dataset: cifar100
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: dataroot: data
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: batch_size: 128
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: total_epochs: 100
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: lr_decay_milestones: 60,80
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: lr_decay_gamma: 0.1
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: lr: 0.01
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: output_dir: run/cifar100/prune/cifar100-global-group_sl-3.0-resnet56
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: finetune: True
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: last_epochs: 100
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: reps: 1
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: method: group_sl
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: speed_up: 3.0
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: max_pruning_ratio: 1.0
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: reg: 1e-05
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: delta_reg: 0.0001
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: weight_decay: 0.0005
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: seed: 1
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: global_pruning: True
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: sl_total_epochs: 100
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: sl_lr: 0.01
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: sl_reg_warmup: 0
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: sl_restore: None
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: iterative_steps: 400
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: logger: <Logger cifar100-global-group_sl-3.0-resnet56 (DEBUG)>
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: device: cuda
[02/21 21:37:31 cifar100-global-group_sl-3.0-resnet56]: num_classes: 100
[02/21 21:37:32 cifar100-global-group_sl-3.0-resnet56]: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 21:37:32 cifar100-global-group_sl-3.0-resnet56]: Regularizing...
[02/21 21:38:30 cifar100-global-group_sl-3.0-resnet56]: Epoch 0/100, Acc=0.6568, Val Loss=1.4284, lr=0.0100
[02/21 21:39:28 cifar100-global-group_sl-3.0-resnet56]: Epoch 1/100, Acc=0.6673, Val Loss=1.3766, lr=0.0100
[02/21 21:40:26 cifar100-global-group_sl-3.0-resnet56]: Epoch 2/100, Acc=0.6588, Val Loss=1.4751, lr=0.0100
[02/21 21:41:24 cifar100-global-group_sl-3.0-resnet56]: Epoch 3/100, Acc=0.6714, Val Loss=1.3532, lr=0.0100
[02/21 21:42:23 cifar100-global-group_sl-3.0-resnet56]: Epoch 4/100, Acc=0.6626, Val Loss=1.4321, lr=0.0100
[02/21 21:43:21 cifar100-global-group_sl-3.0-resnet56]: Epoch 5/100, Acc=0.6727, Val Loss=1.4147, lr=0.0100
[02/21 21:44:19 cifar100-global-group_sl-3.0-resnet56]: Epoch 6/100, Acc=0.6631, Val Loss=1.4638, lr=0.0100
[02/21 21:45:17 cifar100-global-group_sl-3.0-resnet56]: Epoch 7/100, Acc=0.6817, Val Loss=1.3882, lr=0.0100
[02/21 21:46:16 cifar100-global-group_sl-3.0-resnet56]: Epoch 8/100, Acc=0.6669, Val Loss=1.4328, lr=0.0100
[02/21 21:47:14 cifar100-global-group_sl-3.0-resnet56]: Epoch 9/100, Acc=0.6788, Val Loss=1.4348, lr=0.0100
[02/21 21:48:12 cifar100-global-group_sl-3.0-resnet56]: Epoch 10/100, Acc=0.6762, Val Loss=1.4056, lr=0.0100
[02/21 21:49:11 cifar100-global-group_sl-3.0-resnet56]: Epoch 11/100, Acc=0.6735, Val Loss=1.4293, lr=0.0100
[02/21 21:50:09 cifar100-global-group_sl-3.0-resnet56]: Epoch 12/100, Acc=0.6693, Val Loss=1.4858, lr=0.0100
[02/21 21:51:08 cifar100-global-group_sl-3.0-resnet56]: Epoch 13/100, Acc=0.6814, Val Loss=1.4139, lr=0.0100
[02/21 21:52:06 cifar100-global-group_sl-3.0-resnet56]: Epoch 14/100, Acc=0.6708, Val Loss=1.4601, lr=0.0100
[02/21 21:53:05 cifar100-global-group_sl-3.0-resnet56]: Epoch 15/100, Acc=0.6845, Val Loss=1.3917, lr=0.0100
[02/21 21:54:04 cifar100-global-group_sl-3.0-resnet56]: Epoch 16/100, Acc=0.6799, Val Loss=1.4377, lr=0.0100
[02/21 21:55:02 cifar100-global-group_sl-3.0-resnet56]: Epoch 17/100, Acc=0.6764, Val Loss=1.4481, lr=0.0100
[02/21 21:56:02 cifar100-global-group_sl-3.0-resnet56]: Epoch 18/100, Acc=0.6742, Val Loss=1.5238, lr=0.0100
[02/21 21:57:01 cifar100-global-group_sl-3.0-resnet56]: Epoch 19/100, Acc=0.6866, Val Loss=1.4380, lr=0.0100
[02/21 21:58:00 cifar100-global-group_sl-3.0-resnet56]: Epoch 20/100, Acc=0.6728, Val Loss=1.4795, lr=0.0100
[02/21 21:58:59 cifar100-global-group_sl-3.0-resnet56]: Epoch 21/100, Acc=0.6890, Val Loss=1.4269, lr=0.0100
[02/21 21:59:58 cifar100-global-group_sl-3.0-resnet56]: Epoch 22/100, Acc=0.6818, Val Loss=1.5285, lr=0.0100
[02/21 22:00:56 cifar100-global-group_sl-3.0-resnet56]: Epoch 23/100, Acc=0.6766, Val Loss=1.5421, lr=0.0100
[02/21 22:01:55 cifar100-global-group_sl-3.0-resnet56]: Epoch 24/100, Acc=0.6765, Val Loss=1.5517, lr=0.0100
[02/21 22:02:54 cifar100-global-group_sl-3.0-resnet56]: Epoch 25/100, Acc=0.6829, Val Loss=1.4950, lr=0.0100
[02/21 22:03:53 cifar100-global-group_sl-3.0-resnet56]: Epoch 26/100, Acc=0.6866, Val Loss=1.5258, lr=0.0100
[02/21 22:04:52 cifar100-global-group_sl-3.0-resnet56]: Epoch 27/100, Acc=0.6788, Val Loss=1.5036, lr=0.0100
[02/21 22:05:52 cifar100-global-group_sl-3.0-resnet56]: Epoch 28/100, Acc=0.6862, Val Loss=1.5289, lr=0.0100
[02/21 22:06:51 cifar100-global-group_sl-3.0-resnet56]: Epoch 29/100, Acc=0.6847, Val Loss=1.5183, lr=0.0100
[02/21 22:07:50 cifar100-global-group_sl-3.0-resnet56]: Epoch 30/100, Acc=0.6846, Val Loss=1.5493, lr=0.0100
[02/21 22:08:49 cifar100-global-group_sl-3.0-resnet56]: Epoch 31/100, Acc=0.6862, Val Loss=1.5508, lr=0.0100
[02/21 22:09:49 cifar100-global-group_sl-3.0-resnet56]: Epoch 32/100, Acc=0.6760, Val Loss=1.5850, lr=0.0100
[02/21 22:10:47 cifar100-global-group_sl-3.0-resnet56]: Epoch 33/100, Acc=0.6868, Val Loss=1.5134, lr=0.0100
[02/21 22:11:46 cifar100-global-group_sl-3.0-resnet56]: Epoch 34/100, Acc=0.6907, Val Loss=1.5030, lr=0.0100
[02/21 22:12:45 cifar100-global-group_sl-3.0-resnet56]: Epoch 35/100, Acc=0.6752, Val Loss=1.6392, lr=0.0100
[02/21 22:13:44 cifar100-global-group_sl-3.0-resnet56]: Epoch 36/100, Acc=0.6816, Val Loss=1.5627, lr=0.0100
[02/21 22:14:43 cifar100-global-group_sl-3.0-resnet56]: Epoch 37/100, Acc=0.6808, Val Loss=1.6393, lr=0.0100
[02/21 22:15:42 cifar100-global-group_sl-3.0-resnet56]: Epoch 38/100, Acc=0.6817, Val Loss=1.5963, lr=0.0100
[02/21 22:16:40 cifar100-global-group_sl-3.0-resnet56]: Epoch 39/100, Acc=0.6736, Val Loss=1.6851, lr=0.0100
[02/21 22:17:39 cifar100-global-group_sl-3.0-resnet56]: Epoch 40/100, Acc=0.6861, Val Loss=1.6254, lr=0.0100
[02/21 22:18:38 cifar100-global-group_sl-3.0-resnet56]: Epoch 41/100, Acc=0.6844, Val Loss=1.6011, lr=0.0100
[02/21 22:19:37 cifar100-global-group_sl-3.0-resnet56]: Epoch 42/100, Acc=0.6911, Val Loss=1.5525, lr=0.0100
[02/21 22:20:37 cifar100-global-group_sl-3.0-resnet56]: Epoch 43/100, Acc=0.6783, Val Loss=1.7020, lr=0.0100
[02/21 22:21:37 cifar100-global-group_sl-3.0-resnet56]: Epoch 44/100, Acc=0.6889, Val Loss=1.6193, lr=0.0100
[02/21 22:22:37 cifar100-global-group_sl-3.0-resnet56]: Epoch 45/100, Acc=0.6822, Val Loss=1.6474, lr=0.0100
[02/21 22:23:37 cifar100-global-group_sl-3.0-resnet56]: Epoch 46/100, Acc=0.6863, Val Loss=1.6377, lr=0.0100
[02/21 22:24:37 cifar100-global-group_sl-3.0-resnet56]: Epoch 47/100, Acc=0.6833, Val Loss=1.6565, lr=0.0100
[02/21 22:25:38 cifar100-global-group_sl-3.0-resnet56]: Epoch 48/100, Acc=0.6901, Val Loss=1.6405, lr=0.0100
[02/21 22:26:38 cifar100-global-group_sl-3.0-resnet56]: Epoch 49/100, Acc=0.6889, Val Loss=1.6530, lr=0.0100
[02/21 22:27:39 cifar100-global-group_sl-3.0-resnet56]: Epoch 50/100, Acc=0.6875, Val Loss=1.6420, lr=0.0100
[02/21 22:28:39 cifar100-global-group_sl-3.0-resnet56]: Epoch 51/100, Acc=0.6850, Val Loss=1.6906, lr=0.0100
[02/21 22:29:39 cifar100-global-group_sl-3.0-resnet56]: Epoch 52/100, Acc=0.6891, Val Loss=1.6473, lr=0.0100
[02/21 22:30:40 cifar100-global-group_sl-3.0-resnet56]: Epoch 53/100, Acc=0.6884, Val Loss=1.6824, lr=0.0100
[02/21 22:31:40 cifar100-global-group_sl-3.0-resnet56]: Epoch 54/100, Acc=0.6984, Val Loss=1.6603, lr=0.0100
[02/21 22:32:40 cifar100-global-group_sl-3.0-resnet56]: Epoch 55/100, Acc=0.6952, Val Loss=1.6354, lr=0.0100
[02/21 22:33:40 cifar100-global-group_sl-3.0-resnet56]: Epoch 56/100, Acc=0.6887, Val Loss=1.6809, lr=0.0100
[02/21 22:34:41 cifar100-global-group_sl-3.0-resnet56]: Epoch 57/100, Acc=0.6918, Val Loss=1.6572, lr=0.0100
[02/21 22:35:41 cifar100-global-group_sl-3.0-resnet56]: Epoch 58/100, Acc=0.6897, Val Loss=1.7075, lr=0.0100
[02/21 22:36:41 cifar100-global-group_sl-3.0-resnet56]: Epoch 59/100, Acc=0.6957, Val Loss=1.6509, lr=0.0100
[02/21 22:37:41 cifar100-global-group_sl-3.0-resnet56]: Epoch 60/100, Acc=0.7151, Val Loss=1.5035, lr=0.0010
[02/21 22:38:40 cifar100-global-group_sl-3.0-resnet56]: Epoch 61/100, Acc=0.7171, Val Loss=1.4991, lr=0.0010
[02/21 22:39:39 cifar100-global-group_sl-3.0-resnet56]: Epoch 62/100, Acc=0.7205, Val Loss=1.4957, lr=0.0010
[02/21 22:40:38 cifar100-global-group_sl-3.0-resnet56]: Epoch 63/100, Acc=0.7214, Val Loss=1.5025, lr=0.0010
[02/21 22:41:37 cifar100-global-group_sl-3.0-resnet56]: Epoch 64/100, Acc=0.7221, Val Loss=1.4885, lr=0.0010
[02/21 22:42:36 cifar100-global-group_sl-3.0-resnet56]: Epoch 65/100, Acc=0.7208, Val Loss=1.5103, lr=0.0010
[02/21 22:43:34 cifar100-global-group_sl-3.0-resnet56]: Epoch 66/100, Acc=0.7209, Val Loss=1.5011, lr=0.0010
[02/21 22:44:33 cifar100-global-group_sl-3.0-resnet56]: Epoch 67/100, Acc=0.7210, Val Loss=1.5038, lr=0.0010
[02/21 22:45:32 cifar100-global-group_sl-3.0-resnet56]: Epoch 68/100, Acc=0.7233, Val Loss=1.5083, lr=0.0010
[02/21 22:46:31 cifar100-global-group_sl-3.0-resnet56]: Epoch 69/100, Acc=0.7236, Val Loss=1.5091, lr=0.0010
[02/21 22:47:30 cifar100-global-group_sl-3.0-resnet56]: Epoch 70/100, Acc=0.7234, Val Loss=1.5109, lr=0.0010
[02/21 22:48:28 cifar100-global-group_sl-3.0-resnet56]: Epoch 71/100, Acc=0.7251, Val Loss=1.5101, lr=0.0010
[02/21 22:49:27 cifar100-global-group_sl-3.0-resnet56]: Epoch 72/100, Acc=0.7232, Val Loss=1.5199, lr=0.0010
[02/21 22:50:26 cifar100-global-group_sl-3.0-resnet56]: Epoch 73/100, Acc=0.7219, Val Loss=1.5391, lr=0.0010
[02/21 22:51:25 cifar100-global-group_sl-3.0-resnet56]: Epoch 74/100, Acc=0.7226, Val Loss=1.5289, lr=0.0010
[02/21 22:52:23 cifar100-global-group_sl-3.0-resnet56]: Epoch 75/100, Acc=0.7228, Val Loss=1.5316, lr=0.0010
[02/21 22:53:22 cifar100-global-group_sl-3.0-resnet56]: Epoch 76/100, Acc=0.7223, Val Loss=1.5347, lr=0.0010
[02/21 22:54:20 cifar100-global-group_sl-3.0-resnet56]: Epoch 77/100, Acc=0.7253, Val Loss=1.5315, lr=0.0010
[02/21 22:55:19 cifar100-global-group_sl-3.0-resnet56]: Epoch 78/100, Acc=0.7221, Val Loss=1.5459, lr=0.0010
[02/21 22:56:18 cifar100-global-group_sl-3.0-resnet56]: Epoch 79/100, Acc=0.7213, Val Loss=1.5437, lr=0.0010
[02/21 22:57:17 cifar100-global-group_sl-3.0-resnet56]: Epoch 80/100, Acc=0.7224, Val Loss=1.5436, lr=0.0001
[02/21 22:58:16 cifar100-global-group_sl-3.0-resnet56]: Epoch 81/100, Acc=0.7228, Val Loss=1.5407, lr=0.0001
[02/21 22:59:15 cifar100-global-group_sl-3.0-resnet56]: Epoch 82/100, Acc=0.7228, Val Loss=1.5445, lr=0.0001
[02/21 23:00:14 cifar100-global-group_sl-3.0-resnet56]: Epoch 83/100, Acc=0.7215, Val Loss=1.5477, lr=0.0001
[02/21 23:01:13 cifar100-global-group_sl-3.0-resnet56]: Epoch 84/100, Acc=0.7233, Val Loss=1.5411, lr=0.0001
[02/21 23:02:12 cifar100-global-group_sl-3.0-resnet56]: Epoch 85/100, Acc=0.7228, Val Loss=1.5414, lr=0.0001
[02/21 23:03:11 cifar100-global-group_sl-3.0-resnet56]: Epoch 86/100, Acc=0.7237, Val Loss=1.5451, lr=0.0001
[02/21 23:04:10 cifar100-global-group_sl-3.0-resnet56]: Epoch 87/100, Acc=0.7239, Val Loss=1.5366, lr=0.0001
[02/21 23:05:09 cifar100-global-group_sl-3.0-resnet56]: Epoch 88/100, Acc=0.7235, Val Loss=1.5390, lr=0.0001
[02/21 23:06:08 cifar100-global-group_sl-3.0-resnet56]: Epoch 89/100, Acc=0.7225, Val Loss=1.5382, lr=0.0001
[02/21 23:07:07 cifar100-global-group_sl-3.0-resnet56]: Epoch 90/100, Acc=0.7232, Val Loss=1.5346, lr=0.0001
[02/21 23:08:06 cifar100-global-group_sl-3.0-resnet56]: Epoch 91/100, Acc=0.7224, Val Loss=1.5434, lr=0.0001
[02/21 23:09:05 cifar100-global-group_sl-3.0-resnet56]: Epoch 92/100, Acc=0.7245, Val Loss=1.5399, lr=0.0001
[02/21 23:10:04 cifar100-global-group_sl-3.0-resnet56]: Epoch 93/100, Acc=0.7236, Val Loss=1.5367, lr=0.0001
[02/21 23:11:03 cifar100-global-group_sl-3.0-resnet56]: Epoch 94/100, Acc=0.7237, Val Loss=1.5370, lr=0.0001
[02/21 23:12:02 cifar100-global-group_sl-3.0-resnet56]: Epoch 95/100, Acc=0.7248, Val Loss=1.5387, lr=0.0001
[02/21 23:13:02 cifar100-global-group_sl-3.0-resnet56]: Epoch 96/100, Acc=0.7236, Val Loss=1.5394, lr=0.0001
[02/21 23:14:01 cifar100-global-group_sl-3.0-resnet56]: Epoch 97/100, Acc=0.7229, Val Loss=1.5492, lr=0.0001
[02/21 23:15:01 cifar100-global-group_sl-3.0-resnet56]: Epoch 98/100, Acc=0.7239, Val Loss=1.5374, lr=0.0001
[02/21 23:16:00 cifar100-global-group_sl-3.0-resnet56]: Epoch 99/100, Acc=0.7236, Val Loss=1.5399, lr=0.0001
[02/21 23:16:00 cifar100-global-group_sl-3.0-resnet56]: Best Acc=0.7253
[02/21 23:16:00 cifar100-global-group_sl-3.0-resnet56]: Loading the sparse model from run/cifar100/prune/cifar100-global-group_sl-3.0-resnet56/reg_cifar100_resnet56_group_sl_1e-05.pth...
[02/21 23:16:03 cifar100-global-group_sl-3.0-resnet56]: Pruning...
[02/21 23:16:20 cifar100-global-group_sl-3.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(6, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(6, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(6, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(6, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(6, 7, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(6, 15, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(15, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(15, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(15, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(15, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(15, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(15, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(15, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(15, 6, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(15, 58, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(58, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(58, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(58, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(46, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(58, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(58, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(58, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(49, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(58, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(53, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(58, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(40, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(58, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(45, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=58, out_features=100, bias=True)
)
[02/21 23:16:23 cifar100-global-group_sl-3.0-resnet56]: Params: 0.86 M => 0.49 M (56.38%)
[02/21 23:16:23 cifar100-global-group_sl-3.0-resnet56]: FLOPs: 127.12 M => 41.97 M (33.01%, 3.03X )
[02/21 23:16:23 cifar100-global-group_sl-3.0-resnet56]: Acc: 0.7253 => 0.0102
[02/21 23:16:23 cifar100-global-group_sl-3.0-resnet56]: Val Loss: 1.5315 => 23.8920
[02/21 23:16:23 cifar100-global-group_sl-3.0-resnet56]: Finetuning...
[02/21 23:16:51 cifar100-global-group_sl-3.0-resnet56]: Epoch 0/100, Acc=0.4764, Val Loss=1.9979, lr=0.0100
[02/21 23:17:19 cifar100-global-group_sl-3.0-resnet56]: Epoch 1/100, Acc=0.5180, Val Loss=1.8850, lr=0.0100
[02/21 23:17:47 cifar100-global-group_sl-3.0-resnet56]: Epoch 2/100, Acc=0.5726, Val Loss=1.6244, lr=0.0100
[02/21 23:18:15 cifar100-global-group_sl-3.0-resnet56]: Epoch 3/100, Acc=0.5887, Val Loss=1.5556, lr=0.0100
[02/21 23:18:44 cifar100-global-group_sl-3.0-resnet56]: Epoch 4/100, Acc=0.5933, Val Loss=1.5592, lr=0.0100
[02/21 23:19:12 cifar100-global-group_sl-3.0-resnet56]: Epoch 5/100, Acc=0.5983, Val Loss=1.5577, lr=0.0100
[02/21 23:19:40 cifar100-global-group_sl-3.0-resnet56]: Epoch 6/100, Acc=0.6088, Val Loss=1.4744, lr=0.0100
[02/21 23:20:08 cifar100-global-group_sl-3.0-resnet56]: Epoch 7/100, Acc=0.6063, Val Loss=1.5213, lr=0.0100
[02/21 23:20:36 cifar100-global-group_sl-3.0-resnet56]: Epoch 8/100, Acc=0.6265, Val Loss=1.4333, lr=0.0100
[02/21 23:21:04 cifar100-global-group_sl-3.0-resnet56]: Epoch 9/100, Acc=0.5909, Val Loss=1.5896, lr=0.0100
[02/21 23:21:33 cifar100-global-group_sl-3.0-resnet56]: Epoch 10/100, Acc=0.6161, Val Loss=1.4779, lr=0.0100
[02/21 23:22:01 cifar100-global-group_sl-3.0-resnet56]: Epoch 11/100, Acc=0.6317, Val Loss=1.3805, lr=0.0100
[02/21 23:22:29 cifar100-global-group_sl-3.0-resnet56]: Epoch 12/100, Acc=0.6236, Val Loss=1.4237, lr=0.0100
[02/21 23:22:57 cifar100-global-group_sl-3.0-resnet56]: Epoch 13/100, Acc=0.6123, Val Loss=1.4956, lr=0.0100
[02/21 23:23:25 cifar100-global-group_sl-3.0-resnet56]: Epoch 14/100, Acc=0.6288, Val Loss=1.4035, lr=0.0100
[02/21 23:23:53 cifar100-global-group_sl-3.0-resnet56]: Epoch 15/100, Acc=0.6188, Val Loss=1.5019, lr=0.0100
[02/21 23:24:21 cifar100-global-group_sl-3.0-resnet56]: Epoch 16/100, Acc=0.6188, Val Loss=1.4660, lr=0.0100
[02/21 23:24:49 cifar100-global-group_sl-3.0-resnet56]: Epoch 17/100, Acc=0.6084, Val Loss=1.5706, lr=0.0100
[02/21 23:25:17 cifar100-global-group_sl-3.0-resnet56]: Epoch 18/100, Acc=0.6195, Val Loss=1.4441, lr=0.0100
[02/21 23:25:45 cifar100-global-group_sl-3.0-resnet56]: Epoch 19/100, Acc=0.6124, Val Loss=1.5155, lr=0.0100
[02/21 23:26:12 cifar100-global-group_sl-3.0-resnet56]: Epoch 20/100, Acc=0.6112, Val Loss=1.4905, lr=0.0100
[02/21 23:26:40 cifar100-global-group_sl-3.0-resnet56]: Epoch 21/100, Acc=0.6186, Val Loss=1.4616, lr=0.0100
[02/21 23:27:08 cifar100-global-group_sl-3.0-resnet56]: Epoch 22/100, Acc=0.6326, Val Loss=1.4141, lr=0.0100
[02/21 23:27:36 cifar100-global-group_sl-3.0-resnet56]: Epoch 23/100, Acc=0.6189, Val Loss=1.4537, lr=0.0100
[02/21 23:28:04 cifar100-global-group_sl-3.0-resnet56]: Epoch 24/100, Acc=0.6263, Val Loss=1.4481, lr=0.0100
[02/21 23:28:31 cifar100-global-group_sl-3.0-resnet56]: Epoch 25/100, Acc=0.6149, Val Loss=1.5158, lr=0.0100
[02/21 23:28:59 cifar100-global-group_sl-3.0-resnet56]: Epoch 26/100, Acc=0.6329, Val Loss=1.4198, lr=0.0100
[02/21 23:29:27 cifar100-global-group_sl-3.0-resnet56]: Epoch 27/100, Acc=0.6006, Val Loss=1.5953, lr=0.0100
[02/21 23:29:55 cifar100-global-group_sl-3.0-resnet56]: Epoch 28/100, Acc=0.6193, Val Loss=1.4976, lr=0.0100
[02/21 23:30:23 cifar100-global-group_sl-3.0-resnet56]: Epoch 29/100, Acc=0.6234, Val Loss=1.4585, lr=0.0100
[02/21 23:30:51 cifar100-global-group_sl-3.0-resnet56]: Epoch 30/100, Acc=0.5994, Val Loss=1.6228, lr=0.0100
[02/21 23:31:19 cifar100-global-group_sl-3.0-resnet56]: Epoch 31/100, Acc=0.6191, Val Loss=1.5003, lr=0.0100
[02/21 23:31:47 cifar100-global-group_sl-3.0-resnet56]: Epoch 32/100, Acc=0.6075, Val Loss=1.5742, lr=0.0100
[02/21 23:32:14 cifar100-global-group_sl-3.0-resnet56]: Epoch 33/100, Acc=0.5931, Val Loss=1.6651, lr=0.0100
[02/21 23:32:42 cifar100-global-group_sl-3.0-resnet56]: Epoch 34/100, Acc=0.6346, Val Loss=1.4075, lr=0.0100
[02/21 23:33:10 cifar100-global-group_sl-3.0-resnet56]: Epoch 35/100, Acc=0.6148, Val Loss=1.4604, lr=0.0100
[02/21 23:33:38 cifar100-global-group_sl-3.0-resnet56]: Epoch 36/100, Acc=0.6255, Val Loss=1.4632, lr=0.0100
[02/21 23:34:06 cifar100-global-group_sl-3.0-resnet56]: Epoch 37/100, Acc=0.6083, Val Loss=1.5535, lr=0.0100
[02/21 23:34:34 cifar100-global-group_sl-3.0-resnet56]: Epoch 38/100, Acc=0.6252, Val Loss=1.4545, lr=0.0100
[02/21 23:35:02 cifar100-global-group_sl-3.0-resnet56]: Epoch 39/100, Acc=0.6284, Val Loss=1.4428, lr=0.0100
[02/21 23:35:29 cifar100-global-group_sl-3.0-resnet56]: Epoch 40/100, Acc=0.6373, Val Loss=1.4009, lr=0.0100
[02/21 23:35:58 cifar100-global-group_sl-3.0-resnet56]: Epoch 41/100, Acc=0.6200, Val Loss=1.5064, lr=0.0100
[02/21 23:36:25 cifar100-global-group_sl-3.0-resnet56]: Epoch 42/100, Acc=0.6255, Val Loss=1.4749, lr=0.0100
[02/21 23:36:53 cifar100-global-group_sl-3.0-resnet56]: Epoch 43/100, Acc=0.6392, Val Loss=1.3867, lr=0.0100
[02/21 23:37:21 cifar100-global-group_sl-3.0-resnet56]: Epoch 44/100, Acc=0.6114, Val Loss=1.5521, lr=0.0100
[02/21 23:37:48 cifar100-global-group_sl-3.0-resnet56]: Epoch 45/100, Acc=0.6218, Val Loss=1.4843, lr=0.0100
[02/21 23:38:16 cifar100-global-group_sl-3.0-resnet56]: Epoch 46/100, Acc=0.6336, Val Loss=1.4479, lr=0.0100
[02/21 23:38:44 cifar100-global-group_sl-3.0-resnet56]: Epoch 47/100, Acc=0.6233, Val Loss=1.4715, lr=0.0100
[02/21 23:39:11 cifar100-global-group_sl-3.0-resnet56]: Epoch 48/100, Acc=0.6331, Val Loss=1.4269, lr=0.0100
[02/21 23:39:39 cifar100-global-group_sl-3.0-resnet56]: Epoch 49/100, Acc=0.6103, Val Loss=1.5671, lr=0.0100
[02/21 23:40:06 cifar100-global-group_sl-3.0-resnet56]: Epoch 50/100, Acc=0.6343, Val Loss=1.4178, lr=0.0100
[02/21 23:40:34 cifar100-global-group_sl-3.0-resnet56]: Epoch 51/100, Acc=0.6042, Val Loss=1.5684, lr=0.0100
[02/21 23:41:01 cifar100-global-group_sl-3.0-resnet56]: Epoch 52/100, Acc=0.6152, Val Loss=1.5360, lr=0.0100
[02/21 23:41:29 cifar100-global-group_sl-3.0-resnet56]: Epoch 53/100, Acc=0.6144, Val Loss=1.5147, lr=0.0100
[02/21 23:41:56 cifar100-global-group_sl-3.0-resnet56]: Epoch 54/100, Acc=0.6134, Val Loss=1.5456, lr=0.0100
[02/21 23:42:24 cifar100-global-group_sl-3.0-resnet56]: Epoch 55/100, Acc=0.6101, Val Loss=1.5803, lr=0.0100
[02/21 23:42:51 cifar100-global-group_sl-3.0-resnet56]: Epoch 56/100, Acc=0.6045, Val Loss=1.5956, lr=0.0100
[02/21 23:43:19 cifar100-global-group_sl-3.0-resnet56]: Epoch 57/100, Acc=0.6242, Val Loss=1.5127, lr=0.0100
[02/21 23:43:46 cifar100-global-group_sl-3.0-resnet56]: Epoch 58/100, Acc=0.6122, Val Loss=1.5277, lr=0.0100
[02/21 23:44:14 cifar100-global-group_sl-3.0-resnet56]: Epoch 59/100, Acc=0.6256, Val Loss=1.5225, lr=0.0100
[02/21 23:44:41 cifar100-global-group_sl-3.0-resnet56]: Epoch 60/100, Acc=0.6879, Val Loss=1.1772, lr=0.0010
[02/21 23:45:09 cifar100-global-group_sl-3.0-resnet56]: Epoch 61/100, Acc=0.6894, Val Loss=1.1745, lr=0.0010
[02/21 23:45:37 cifar100-global-group_sl-3.0-resnet56]: Epoch 62/100, Acc=0.6912, Val Loss=1.1753, lr=0.0010
[02/21 23:46:05 cifar100-global-group_sl-3.0-resnet56]: Epoch 63/100, Acc=0.6896, Val Loss=1.1791, lr=0.0010
[02/21 23:46:33 cifar100-global-group_sl-3.0-resnet56]: Epoch 64/100, Acc=0.6896, Val Loss=1.1896, lr=0.0010
[02/21 23:47:00 cifar100-global-group_sl-3.0-resnet56]: Epoch 65/100, Acc=0.6887, Val Loss=1.1957, lr=0.0010
[02/21 23:47:28 cifar100-global-group_sl-3.0-resnet56]: Epoch 66/100, Acc=0.6867, Val Loss=1.2025, lr=0.0010
[02/21 23:47:55 cifar100-global-group_sl-3.0-resnet56]: Epoch 67/100, Acc=0.6903, Val Loss=1.2052, lr=0.0010
[02/21 23:48:23 cifar100-global-group_sl-3.0-resnet56]: Epoch 68/100, Acc=0.6885, Val Loss=1.2071, lr=0.0010
[02/21 23:48:51 cifar100-global-group_sl-3.0-resnet56]: Epoch 69/100, Acc=0.6892, Val Loss=1.2087, lr=0.0010
[02/21 23:49:18 cifar100-global-group_sl-3.0-resnet56]: Epoch 70/100, Acc=0.6879, Val Loss=1.2189, lr=0.0010
[02/21 23:49:45 cifar100-global-group_sl-3.0-resnet56]: Epoch 71/100, Acc=0.6861, Val Loss=1.2224, lr=0.0010
[02/21 23:50:12 cifar100-global-group_sl-3.0-resnet56]: Epoch 72/100, Acc=0.6901, Val Loss=1.2213, lr=0.0010
[02/21 23:50:39 cifar100-global-group_sl-3.0-resnet56]: Epoch 73/100, Acc=0.6896, Val Loss=1.2326, lr=0.0010
[02/21 23:51:07 cifar100-global-group_sl-3.0-resnet56]: Epoch 74/100, Acc=0.6884, Val Loss=1.2266, lr=0.0010
[02/21 23:51:34 cifar100-global-group_sl-3.0-resnet56]: Epoch 75/100, Acc=0.6886, Val Loss=1.2423, lr=0.0010
[02/21 23:52:02 cifar100-global-group_sl-3.0-resnet56]: Epoch 76/100, Acc=0.6892, Val Loss=1.2382, lr=0.0010
[02/21 23:52:30 cifar100-global-group_sl-3.0-resnet56]: Epoch 77/100, Acc=0.6872, Val Loss=1.2479, lr=0.0010
[02/21 23:52:57 cifar100-global-group_sl-3.0-resnet56]: Epoch 78/100, Acc=0.6849, Val Loss=1.2657, lr=0.0010
[02/21 23:53:25 cifar100-global-group_sl-3.0-resnet56]: Epoch 79/100, Acc=0.6901, Val Loss=1.2531, lr=0.0010
[02/21 23:53:53 cifar100-global-group_sl-3.0-resnet56]: Epoch 80/100, Acc=0.6882, Val Loss=1.2489, lr=0.0001
[02/21 23:54:20 cifar100-global-group_sl-3.0-resnet56]: Epoch 81/100, Acc=0.6877, Val Loss=1.2485, lr=0.0001
[02/21 23:54:48 cifar100-global-group_sl-3.0-resnet56]: Epoch 82/100, Acc=0.6896, Val Loss=1.2443, lr=0.0001
[02/21 23:55:16 cifar100-global-group_sl-3.0-resnet56]: Epoch 83/100, Acc=0.6900, Val Loss=1.2448, lr=0.0001
[02/21 23:55:43 cifar100-global-group_sl-3.0-resnet56]: Epoch 84/100, Acc=0.6896, Val Loss=1.2404, lr=0.0001
[02/21 23:56:11 cifar100-global-group_sl-3.0-resnet56]: Epoch 85/100, Acc=0.6885, Val Loss=1.2434, lr=0.0001
[02/21 23:56:39 cifar100-global-group_sl-3.0-resnet56]: Epoch 86/100, Acc=0.6900, Val Loss=1.2509, lr=0.0001
[02/21 23:57:06 cifar100-global-group_sl-3.0-resnet56]: Epoch 87/100, Acc=0.6881, Val Loss=1.2438, lr=0.0001
[02/21 23:57:34 cifar100-global-group_sl-3.0-resnet56]: Epoch 88/100, Acc=0.6880, Val Loss=1.2420, lr=0.0001
[02/21 23:58:01 cifar100-global-group_sl-3.0-resnet56]: Epoch 89/100, Acc=0.6887, Val Loss=1.2445, lr=0.0001
[02/21 23:58:28 cifar100-global-group_sl-3.0-resnet56]: Epoch 90/100, Acc=0.6887, Val Loss=1.2478, lr=0.0001
[02/21 23:58:55 cifar100-global-group_sl-3.0-resnet56]: Epoch 91/100, Acc=0.6890, Val Loss=1.2474, lr=0.0001
[02/21 23:59:22 cifar100-global-group_sl-3.0-resnet56]: Epoch 92/100, Acc=0.6902, Val Loss=1.2438, lr=0.0001
[02/21 23:59:49 cifar100-global-group_sl-3.0-resnet56]: Epoch 93/100, Acc=0.6890, Val Loss=1.2519, lr=0.0001
[02/22 00:00:16 cifar100-global-group_sl-3.0-resnet56]: Epoch 94/100, Acc=0.6874, Val Loss=1.2508, lr=0.0001
[02/22 00:00:43 cifar100-global-group_sl-3.0-resnet56]: Epoch 95/100, Acc=0.6886, Val Loss=1.2504, lr=0.0001
[02/22 00:01:11 cifar100-global-group_sl-3.0-resnet56]: Epoch 96/100, Acc=0.6882, Val Loss=1.2511, lr=0.0001
[02/22 00:01:38 cifar100-global-group_sl-3.0-resnet56]: Epoch 97/100, Acc=0.6881, Val Loss=1.2556, lr=0.0001
[02/22 00:02:05 cifar100-global-group_sl-3.0-resnet56]: Epoch 98/100, Acc=0.6880, Val Loss=1.2482, lr=0.0001
[02/22 00:02:32 cifar100-global-group_sl-3.0-resnet56]: Epoch 99/100, Acc=0.6879, Val Loss=1.2547, lr=0.0001
[02/22 00:02:32 cifar100-global-group_sl-3.0-resnet56]: Best Acc=0.6912
[02/22 00:02:32 cifar100-global-group_sl-3.0-resnet56]: Params: 0.49 M
[02/22 00:02:32 cifar100-global-group_sl-3.0-resnet56]: ops: 41.97 M
[02/22 00:02:35 cifar100-global-group_sl-3.0-resnet56]: Acc: 0.6879 Val Loss: 1.2547

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: mode: prune
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: model: resnet56
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: verbose: False
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: dataset: cifar100
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: dataroot: data
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: batch_size: 128
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: total_epochs: 100
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: lr_decay_milestones: 60,80
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: lr_decay_gamma: 0.1
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: lr: 0.01
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: output_dir: run/cifar100/prune/cifar100-global-proj-3.0-resnet56
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: finetune: True
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: last_epochs: 100
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: reps: 1
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: method: proj
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: speed_up: 3.0
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: max_pruning_ratio: 1.0
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: soft_keeping_ratio: 0.0
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: reg: 1e-05
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: delta_reg: 0.0001
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: weight_decay: 0.0005
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: seed: 1
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: global_pruning: True
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: sl_total_epochs: 100
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: sl_lr: 0.01
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: sl_reg_warmup: 0
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: sl_restore: None
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: iterative_steps: 400
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: logger: <Logger cifar100-global-proj-3.0-resnet56 (DEBUG)>
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: device: cuda
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: num_classes: 100
[02/22 00:02:42 cifar100-global-proj-3.0-resnet56]: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/22 00:02:46 cifar100-global-proj-3.0-resnet56]: Pruning...
[02/22 00:05:13 cifar100-global-proj-3.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(2, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(2, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(2, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(2, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(2, 24, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(24, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(24, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(24, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(24, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(24, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(24, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(24, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(24, 52, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(52, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(24, 59, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(59, 43, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(43, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(43, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(59, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(59, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(52, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(59, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(57, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(59, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(46, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(59, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(49, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(59, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(59, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(46, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=59, out_features=100, bias=True)
)
[02/22 00:05:15 cifar100-global-proj-3.0-resnet56]: Params: 0.86 M => 0.51 M (59.59%)
[02/22 00:05:15 cifar100-global-proj-3.0-resnet56]: FLOPs: 127.12 M => 42.27 M (33.25%, 3.01X )
[02/22 00:05:15 cifar100-global-proj-3.0-resnet56]: Acc: 0.7269 => 0.0100
[02/22 00:05:15 cifar100-global-proj-3.0-resnet56]: Val Loss: 1.1578 => 15.5368
[02/22 00:05:15 cifar100-global-proj-3.0-resnet56]: Finetuning...
[02/22 00:05:42 cifar100-global-proj-3.0-resnet56]: Epoch 0/100, Acc=0.1598, Val Loss=3.6702, lr=0.0100
[02/22 00:06:09 cifar100-global-proj-3.0-resnet56]: Epoch 1/100, Acc=0.3828, Val Loss=2.4342, lr=0.0100
[02/22 00:06:36 cifar100-global-proj-3.0-resnet56]: Epoch 2/100, Acc=0.3946, Val Loss=2.4514, lr=0.0100
[02/22 00:07:03 cifar100-global-proj-3.0-resnet56]: Epoch 3/100, Acc=0.3942, Val Loss=2.5161, lr=0.0100
[02/22 00:07:30 cifar100-global-proj-3.0-resnet56]: Epoch 4/100, Acc=0.5330, Val Loss=1.7343, lr=0.0100
[02/22 00:07:57 cifar100-global-proj-3.0-resnet56]: Epoch 5/100, Acc=0.5221, Val Loss=1.8042, lr=0.0100
[02/22 00:08:24 cifar100-global-proj-3.0-resnet56]: Epoch 6/100, Acc=0.5433, Val Loss=1.7224, lr=0.0100
[02/22 00:08:50 cifar100-global-proj-3.0-resnet56]: Epoch 7/100, Acc=0.5420, Val Loss=1.7261, lr=0.0100
[02/22 00:09:17 cifar100-global-proj-3.0-resnet56]: Epoch 8/100, Acc=0.5619, Val Loss=1.6346, lr=0.0100
[02/22 00:09:44 cifar100-global-proj-3.0-resnet56]: Epoch 9/100, Acc=0.5791, Val Loss=1.5569, lr=0.0100
[02/22 00:10:11 cifar100-global-proj-3.0-resnet56]: Epoch 10/100, Acc=0.5681, Val Loss=1.6065, lr=0.0100
[02/22 00:10:39 cifar100-global-proj-3.0-resnet56]: Epoch 11/100, Acc=0.5698, Val Loss=1.6196, lr=0.0100
[02/22 00:11:06 cifar100-global-proj-3.0-resnet56]: Epoch 12/100, Acc=0.5551, Val Loss=1.6814, lr=0.0100
[02/22 00:11:33 cifar100-global-proj-3.0-resnet56]: Epoch 13/100, Acc=0.5891, Val Loss=1.5297, lr=0.0100
[02/22 00:12:01 cifar100-global-proj-3.0-resnet56]: Epoch 14/100, Acc=0.5528, Val Loss=1.6936, lr=0.0100
[02/22 00:12:28 cifar100-global-proj-3.0-resnet56]: Epoch 15/100, Acc=0.5926, Val Loss=1.5276, lr=0.0100
[02/22 00:12:55 cifar100-global-proj-3.0-resnet56]: Epoch 16/100, Acc=0.5811, Val Loss=1.5555, lr=0.0100
[02/22 00:13:22 cifar100-global-proj-3.0-resnet56]: Epoch 17/100, Acc=0.5871, Val Loss=1.5532, lr=0.0100
[02/22 00:13:50 cifar100-global-proj-3.0-resnet56]: Epoch 18/100, Acc=0.5856, Val Loss=1.5692, lr=0.0100
[02/22 00:14:17 cifar100-global-proj-3.0-resnet56]: Epoch 19/100, Acc=0.5847, Val Loss=1.5646, lr=0.0100
[02/22 00:14:45 cifar100-global-proj-3.0-resnet56]: Epoch 20/100, Acc=0.5912, Val Loss=1.5668, lr=0.0100
[02/22 00:15:12 cifar100-global-proj-3.0-resnet56]: Epoch 21/100, Acc=0.6002, Val Loss=1.4827, lr=0.0100
[02/22 00:15:39 cifar100-global-proj-3.0-resnet56]: Epoch 22/100, Acc=0.6037, Val Loss=1.4823, lr=0.0100
[02/22 00:16:07 cifar100-global-proj-3.0-resnet56]: Epoch 23/100, Acc=0.6042, Val Loss=1.4817, lr=0.0100
[02/22 00:16:34 cifar100-global-proj-3.0-resnet56]: Epoch 24/100, Acc=0.6115, Val Loss=1.4632, lr=0.0100
[02/22 00:17:02 cifar100-global-proj-3.0-resnet56]: Epoch 25/100, Acc=0.5963, Val Loss=1.5317, lr=0.0100
[02/22 00:17:29 cifar100-global-proj-3.0-resnet56]: Epoch 26/100, Acc=0.6170, Val Loss=1.4618, lr=0.0100
[02/22 00:17:56 cifar100-global-proj-3.0-resnet56]: Epoch 27/100, Acc=0.5914, Val Loss=1.5782, lr=0.0100
[02/22 00:18:23 cifar100-global-proj-3.0-resnet56]: Epoch 28/100, Acc=0.6009, Val Loss=1.5285, lr=0.0100
[02/22 00:18:50 cifar100-global-proj-3.0-resnet56]: Epoch 29/100, Acc=0.5872, Val Loss=1.5763, lr=0.0100
[02/22 00:19:17 cifar100-global-proj-3.0-resnet56]: Epoch 30/100, Acc=0.6021, Val Loss=1.5361, lr=0.0100
[02/22 00:19:44 cifar100-global-proj-3.0-resnet56]: Epoch 31/100, Acc=0.6083, Val Loss=1.4636, lr=0.0100
[02/22 00:20:11 cifar100-global-proj-3.0-resnet56]: Epoch 32/100, Acc=0.6044, Val Loss=1.5093, lr=0.0100
[02/22 00:20:38 cifar100-global-proj-3.0-resnet56]: Epoch 33/100, Acc=0.5860, Val Loss=1.5775, lr=0.0100
[02/22 00:21:05 cifar100-global-proj-3.0-resnet56]: Epoch 34/100, Acc=0.5992, Val Loss=1.5362, lr=0.0100
[02/22 00:21:32 cifar100-global-proj-3.0-resnet56]: Epoch 35/100, Acc=0.6060, Val Loss=1.4837, lr=0.0100
[02/22 00:21:59 cifar100-global-proj-3.0-resnet56]: Epoch 36/100, Acc=0.6038, Val Loss=1.5367, lr=0.0100
[02/22 00:22:26 cifar100-global-proj-3.0-resnet56]: Epoch 37/100, Acc=0.6085, Val Loss=1.5591, lr=0.0100
[02/22 00:22:53 cifar100-global-proj-3.0-resnet56]: Epoch 38/100, Acc=0.6052, Val Loss=1.5199, lr=0.0100
[02/22 00:23:20 cifar100-global-proj-3.0-resnet56]: Epoch 39/100, Acc=0.5932, Val Loss=1.5613, lr=0.0100
[02/22 00:23:47 cifar100-global-proj-3.0-resnet56]: Epoch 40/100, Acc=0.6175, Val Loss=1.4517, lr=0.0100
[02/22 00:24:14 cifar100-global-proj-3.0-resnet56]: Epoch 41/100, Acc=0.6025, Val Loss=1.5483, lr=0.0100
[02/22 00:24:41 cifar100-global-proj-3.0-resnet56]: Epoch 42/100, Acc=0.6070, Val Loss=1.5202, lr=0.0100
[02/22 00:25:08 cifar100-global-proj-3.0-resnet56]: Epoch 43/100, Acc=0.5948, Val Loss=1.5697, lr=0.0100
[02/22 00:25:35 cifar100-global-proj-3.0-resnet56]: Epoch 44/100, Acc=0.6199, Val Loss=1.4373, lr=0.0100
[02/22 00:26:02 cifar100-global-proj-3.0-resnet56]: Epoch 45/100, Acc=0.5987, Val Loss=1.5779, lr=0.0100
[02/22 00:26:29 cifar100-global-proj-3.0-resnet56]: Epoch 46/100, Acc=0.6157, Val Loss=1.4836, lr=0.0100
[02/22 00:26:56 cifar100-global-proj-3.0-resnet56]: Epoch 47/100, Acc=0.6069, Val Loss=1.5293, lr=0.0100
[02/22 00:27:23 cifar100-global-proj-3.0-resnet56]: Epoch 48/100, Acc=0.6024, Val Loss=1.5729, lr=0.0100
[02/22 00:27:51 cifar100-global-proj-3.0-resnet56]: Epoch 49/100, Acc=0.6172, Val Loss=1.4682, lr=0.0100
[02/22 00:28:18 cifar100-global-proj-3.0-resnet56]: Epoch 50/100, Acc=0.6106, Val Loss=1.4783, lr=0.0100
[02/22 00:28:45 cifar100-global-proj-3.0-resnet56]: Epoch 51/100, Acc=0.6095, Val Loss=1.5366, lr=0.0100
[02/22 00:29:12 cifar100-global-proj-3.0-resnet56]: Epoch 52/100, Acc=0.6084, Val Loss=1.4850, lr=0.0100
[02/22 00:29:39 cifar100-global-proj-3.0-resnet56]: Epoch 53/100, Acc=0.6065, Val Loss=1.5413, lr=0.0100
[02/22 00:30:07 cifar100-global-proj-3.0-resnet56]: Epoch 54/100, Acc=0.6048, Val Loss=1.5603, lr=0.0100
[02/22 00:30:34 cifar100-global-proj-3.0-resnet56]: Epoch 55/100, Acc=0.6163, Val Loss=1.5404, lr=0.0100
[02/22 00:31:01 cifar100-global-proj-3.0-resnet56]: Epoch 56/100, Acc=0.5953, Val Loss=1.6064, lr=0.0100
[02/22 00:31:28 cifar100-global-proj-3.0-resnet56]: Epoch 57/100, Acc=0.5967, Val Loss=1.6220, lr=0.0100
[02/22 00:31:55 cifar100-global-proj-3.0-resnet56]: Epoch 58/100, Acc=0.6224, Val Loss=1.4590, lr=0.0100
[02/22 00:32:23 cifar100-global-proj-3.0-resnet56]: Epoch 59/100, Acc=0.6104, Val Loss=1.5063, lr=0.0100
[02/22 00:32:50 cifar100-global-proj-3.0-resnet56]: Epoch 60/100, Acc=0.6793, Val Loss=1.2101, lr=0.0010
[02/22 00:33:17 cifar100-global-proj-3.0-resnet56]: Epoch 61/100, Acc=0.6812, Val Loss=1.2002, lr=0.0010
[02/22 00:33:44 cifar100-global-proj-3.0-resnet56]: Epoch 62/100, Acc=0.6808, Val Loss=1.2005, lr=0.0010
[02/22 00:34:12 cifar100-global-proj-3.0-resnet56]: Epoch 63/100, Acc=0.6799, Val Loss=1.2116, lr=0.0010
[02/22 00:34:39 cifar100-global-proj-3.0-resnet56]: Epoch 64/100, Acc=0.6803, Val Loss=1.2262, lr=0.0010
[02/22 00:35:06 cifar100-global-proj-3.0-resnet56]: Epoch 65/100, Acc=0.6790, Val Loss=1.2161, lr=0.0010
[02/22 00:35:33 cifar100-global-proj-3.0-resnet56]: Epoch 66/100, Acc=0.6750, Val Loss=1.2384, lr=0.0010
[02/22 00:36:01 cifar100-global-proj-3.0-resnet56]: Epoch 67/100, Acc=0.6771, Val Loss=1.2333, lr=0.0010
[02/22 00:36:28 cifar100-global-proj-3.0-resnet56]: Epoch 68/100, Acc=0.6791, Val Loss=1.2314, lr=0.0010
[02/22 00:36:55 cifar100-global-proj-3.0-resnet56]: Epoch 69/100, Acc=0.6797, Val Loss=1.2391, lr=0.0010
[02/22 00:37:22 cifar100-global-proj-3.0-resnet56]: Epoch 70/100, Acc=0.6804, Val Loss=1.2452, lr=0.0010
[02/22 00:37:50 cifar100-global-proj-3.0-resnet56]: Epoch 71/100, Acc=0.6781, Val Loss=1.2544, lr=0.0010
[02/22 00:38:17 cifar100-global-proj-3.0-resnet56]: Epoch 72/100, Acc=0.6782, Val Loss=1.2504, lr=0.0010
[02/22 00:38:44 cifar100-global-proj-3.0-resnet56]: Epoch 73/100, Acc=0.6790, Val Loss=1.2614, lr=0.0010
[02/22 00:39:12 cifar100-global-proj-3.0-resnet56]: Epoch 74/100, Acc=0.6779, Val Loss=1.2594, lr=0.0010
[02/22 00:39:39 cifar100-global-proj-3.0-resnet56]: Epoch 75/100, Acc=0.6800, Val Loss=1.2598, lr=0.0010
[02/22 00:40:06 cifar100-global-proj-3.0-resnet56]: Epoch 76/100, Acc=0.6757, Val Loss=1.2753, lr=0.0010
[02/22 00:40:34 cifar100-global-proj-3.0-resnet56]: Epoch 77/100, Acc=0.6770, Val Loss=1.2748, lr=0.0010
[02/22 00:41:01 cifar100-global-proj-3.0-resnet56]: Epoch 78/100, Acc=0.6774, Val Loss=1.2848, lr=0.0010
[02/22 00:41:30 cifar100-global-proj-3.0-resnet56]: Epoch 79/100, Acc=0.6743, Val Loss=1.2797, lr=0.0010
[02/22 00:41:58 cifar100-global-proj-3.0-resnet56]: Epoch 80/100, Acc=0.6774, Val Loss=1.2688, lr=0.0001
[02/22 00:42:26 cifar100-global-proj-3.0-resnet56]: Epoch 81/100, Acc=0.6763, Val Loss=1.2704, lr=0.0001
[02/22 00:42:55 cifar100-global-proj-3.0-resnet56]: Epoch 82/100, Acc=0.6779, Val Loss=1.2681, lr=0.0001
[02/22 00:43:23 cifar100-global-proj-3.0-resnet56]: Epoch 83/100, Acc=0.6782, Val Loss=1.2720, lr=0.0001
[02/22 00:43:52 cifar100-global-proj-3.0-resnet56]: Epoch 84/100, Acc=0.6785, Val Loss=1.2726, lr=0.0001
[02/22 00:44:20 cifar100-global-proj-3.0-resnet56]: Epoch 85/100, Acc=0.6783, Val Loss=1.2731, lr=0.0001
[02/22 00:44:48 cifar100-global-proj-3.0-resnet56]: Epoch 86/100, Acc=0.6808, Val Loss=1.2740, lr=0.0001
[02/22 00:45:15 cifar100-global-proj-3.0-resnet56]: Epoch 87/100, Acc=0.6798, Val Loss=1.2729, lr=0.0001
[02/22 00:45:43 cifar100-global-proj-3.0-resnet56]: Epoch 88/100, Acc=0.6798, Val Loss=1.2699, lr=0.0001
[02/22 00:46:10 cifar100-global-proj-3.0-resnet56]: Epoch 89/100, Acc=0.6783, Val Loss=1.2709, lr=0.0001
[02/22 00:46:37 cifar100-global-proj-3.0-resnet56]: Epoch 90/100, Acc=0.6801, Val Loss=1.2682, lr=0.0001
[02/22 00:47:05 cifar100-global-proj-3.0-resnet56]: Epoch 91/100, Acc=0.6777, Val Loss=1.2788, lr=0.0001
[02/22 00:47:32 cifar100-global-proj-3.0-resnet56]: Epoch 92/100, Acc=0.6812, Val Loss=1.2677, lr=0.0001
[02/22 00:48:00 cifar100-global-proj-3.0-resnet56]: Epoch 93/100, Acc=0.6777, Val Loss=1.2770, lr=0.0001
[02/22 00:48:27 cifar100-global-proj-3.0-resnet56]: Epoch 94/100, Acc=0.6791, Val Loss=1.2720, lr=0.0001
[02/22 00:48:54 cifar100-global-proj-3.0-resnet56]: Epoch 95/100, Acc=0.6792, Val Loss=1.2763, lr=0.0001
[02/22 00:49:22 cifar100-global-proj-3.0-resnet56]: Epoch 96/100, Acc=0.6796, Val Loss=1.2745, lr=0.0001
[02/22 00:49:49 cifar100-global-proj-3.0-resnet56]: Epoch 97/100, Acc=0.6798, Val Loss=1.2741, lr=0.0001
[02/22 00:50:17 cifar100-global-proj-3.0-resnet56]: Epoch 98/100, Acc=0.6807, Val Loss=1.2727, lr=0.0001
[02/22 00:50:44 cifar100-global-proj-3.0-resnet56]: Epoch 99/100, Acc=0.6772, Val Loss=1.2771, lr=0.0001
[02/22 00:50:44 cifar100-global-proj-3.0-resnet56]: Best Acc=0.6812
[02/22 00:50:44 cifar100-global-proj-3.0-resnet56]: Params: 0.51 M
[02/22 00:50:44 cifar100-global-proj-3.0-resnet56]: ops: 42.27 M
[02/22 00:50:47 cifar100-global-proj-3.0-resnet56]: Acc: 0.6772 Val Loss: 1.2771

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: mode: prune
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: model: resnet56
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: verbose: False
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: dataset: cifar100
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: dataroot: data
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: batch_size: 128
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: total_epochs: 100
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: lr_decay_milestones: 60,80
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: lr_decay_gamma: 0.1
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: lr: 0.01
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: output_dir: run/cifar100/prune/cifar100-global-proj_sl-3.0-resnet56
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: finetune: True
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: last_epochs: 100
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: reps: 1
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: method: proj_sl
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: speed_up: 3.0
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: max_pruning_ratio: 1.0
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: soft_keeping_ratio: 0.0
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: reg: 1e-05
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: delta_reg: 0.0001
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: weight_decay: 0.0005
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: seed: 1
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: global_pruning: True
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: sl_total_epochs: 100
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: sl_lr: 0.01
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: sl_reg_warmup: 0
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: sl_restore: None
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: iterative_steps: 400
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: logger: <Logger cifar100-global-proj_sl-3.0-resnet56 (DEBUG)>
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: device: cuda
[02/22 00:50:54 cifar100-global-proj_sl-3.0-resnet56]: num_classes: 100
[02/22 00:50:55 cifar100-global-proj_sl-3.0-resnet56]: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/22 00:50:55 cifar100-global-proj_sl-3.0-resnet56]: Regularizing...
[02/22 00:57:39 cifar100-global-proj_sl-3.0-resnet56]: Epoch 0/100, Acc=0.6377, Val Loss=1.5375, lr=0.0100
[02/22 01:04:26 cifar100-global-proj_sl-3.0-resnet56]: Epoch 1/100, Acc=0.6527, Val Loss=1.4562, lr=0.0100
[02/22 01:11:13 cifar100-global-proj_sl-3.0-resnet56]: Epoch 2/100, Acc=0.6656, Val Loss=1.4228, lr=0.0100
[02/22 01:18:00 cifar100-global-proj_sl-3.0-resnet56]: Epoch 3/100, Acc=0.6634, Val Loss=1.3831, lr=0.0100
[02/22 01:24:48 cifar100-global-proj_sl-3.0-resnet56]: Epoch 4/100, Acc=0.6597, Val Loss=1.4386, lr=0.0100
[02/22 01:31:35 cifar100-global-proj_sl-3.0-resnet56]: Epoch 5/100, Acc=0.6709, Val Loss=1.4038, lr=0.0100
[02/22 01:38:22 cifar100-global-proj_sl-3.0-resnet56]: Epoch 6/100, Acc=0.6632, Val Loss=1.4514, lr=0.0100
[02/22 01:45:08 cifar100-global-proj_sl-3.0-resnet56]: Epoch 7/100, Acc=0.6715, Val Loss=1.3842, lr=0.0100
[02/22 01:51:53 cifar100-global-proj_sl-3.0-resnet56]: Epoch 8/100, Acc=0.6728, Val Loss=1.3778, lr=0.0100
[02/22 01:58:41 cifar100-global-proj_sl-3.0-resnet56]: Epoch 9/100, Acc=0.6774, Val Loss=1.4485, lr=0.0100
[02/22 02:05:26 cifar100-global-proj_sl-3.0-resnet56]: Epoch 10/100, Acc=0.6828, Val Loss=1.3929, lr=0.0100
[02/22 02:12:11 cifar100-global-proj_sl-3.0-resnet56]: Epoch 11/100, Acc=0.6753, Val Loss=1.4364, lr=0.0100
[02/22 02:18:58 cifar100-global-proj_sl-3.0-resnet56]: Epoch 12/100, Acc=0.6768, Val Loss=1.4354, lr=0.0100
[02/22 02:25:46 cifar100-global-proj_sl-3.0-resnet56]: Epoch 13/100, Acc=0.6771, Val Loss=1.4794, lr=0.0100
[02/22 02:32:33 cifar100-global-proj_sl-3.0-resnet56]: Epoch 14/100, Acc=0.6790, Val Loss=1.4397, lr=0.0100
[02/22 02:39:19 cifar100-global-proj_sl-3.0-resnet56]: Epoch 15/100, Acc=0.6773, Val Loss=1.4394, lr=0.0100
[02/22 02:46:06 cifar100-global-proj_sl-3.0-resnet56]: Epoch 16/100, Acc=0.6856, Val Loss=1.4235, lr=0.0100
[02/22 02:52:53 cifar100-global-proj_sl-3.0-resnet56]: Epoch 17/100, Acc=0.6716, Val Loss=1.4873, lr=0.0100
[02/22 02:59:37 cifar100-global-proj_sl-3.0-resnet56]: Epoch 18/100, Acc=0.6895, Val Loss=1.4615, lr=0.0100
[02/22 03:06:24 cifar100-global-proj_sl-3.0-resnet56]: Epoch 19/100, Acc=0.6758, Val Loss=1.5407, lr=0.0100
[02/22 03:13:13 cifar100-global-proj_sl-3.0-resnet56]: Epoch 20/100, Acc=0.6729, Val Loss=1.5128, lr=0.0100
[02/22 03:20:01 cifar100-global-proj_sl-3.0-resnet56]: Epoch 21/100, Acc=0.6794, Val Loss=1.4783, lr=0.0100
[02/22 03:26:48 cifar100-global-proj_sl-3.0-resnet56]: Epoch 22/100, Acc=0.6736, Val Loss=1.5364, lr=0.0100
[02/22 03:33:33 cifar100-global-proj_sl-3.0-resnet56]: Epoch 23/100, Acc=0.6823, Val Loss=1.5239, lr=0.0100
[02/22 03:40:19 cifar100-global-proj_sl-3.0-resnet56]: Epoch 24/100, Acc=0.6773, Val Loss=1.5699, lr=0.0100
[02/22 03:47:05 cifar100-global-proj_sl-3.0-resnet56]: Epoch 25/100, Acc=0.6909, Val Loss=1.4293, lr=0.0100
[02/22 03:53:54 cifar100-global-proj_sl-3.0-resnet56]: Epoch 26/100, Acc=0.6793, Val Loss=1.5625, lr=0.0100
[02/22 04:00:42 cifar100-global-proj_sl-3.0-resnet56]: Epoch 27/100, Acc=0.6770, Val Loss=1.5642, lr=0.0100
[02/22 04:07:31 cifar100-global-proj_sl-3.0-resnet56]: Epoch 28/100, Acc=0.6805, Val Loss=1.5694, lr=0.0100
[02/22 04:14:19 cifar100-global-proj_sl-3.0-resnet56]: Epoch 29/100, Acc=0.6853, Val Loss=1.5376, lr=0.0100
[02/22 04:21:05 cifar100-global-proj_sl-3.0-resnet56]: Epoch 30/100, Acc=0.6833, Val Loss=1.6033, lr=0.0100
[02/22 04:27:50 cifar100-global-proj_sl-3.0-resnet56]: Epoch 31/100, Acc=0.6862, Val Loss=1.5249, lr=0.0100
[02/22 04:34:38 cifar100-global-proj_sl-3.0-resnet56]: Epoch 32/100, Acc=0.6834, Val Loss=1.5486, lr=0.0100
[02/22 04:41:25 cifar100-global-proj_sl-3.0-resnet56]: Epoch 33/100, Acc=0.6910, Val Loss=1.4812, lr=0.0100
[02/22 04:48:12 cifar100-global-proj_sl-3.0-resnet56]: Epoch 34/100, Acc=0.6871, Val Loss=1.5863, lr=0.0100
[02/22 04:54:57 cifar100-global-proj_sl-3.0-resnet56]: Epoch 35/100, Acc=0.6890, Val Loss=1.5653, lr=0.0100
[02/22 05:01:42 cifar100-global-proj_sl-3.0-resnet56]: Epoch 36/100, Acc=0.6791, Val Loss=1.6072, lr=0.0100
[02/22 05:08:30 cifar100-global-proj_sl-3.0-resnet56]: Epoch 37/100, Acc=0.6872, Val Loss=1.5946, lr=0.0100
[02/22 05:15:17 cifar100-global-proj_sl-3.0-resnet56]: Epoch 38/100, Acc=0.6902, Val Loss=1.5887, lr=0.0100
[02/22 05:22:01 cifar100-global-proj_sl-3.0-resnet56]: Epoch 39/100, Acc=0.6923, Val Loss=1.5889, lr=0.0100
[02/22 05:28:47 cifar100-global-proj_sl-3.0-resnet56]: Epoch 40/100, Acc=0.6864, Val Loss=1.5704, lr=0.0100
[02/22 05:35:33 cifar100-global-proj_sl-3.0-resnet56]: Epoch 41/100, Acc=0.6792, Val Loss=1.6591, lr=0.0100
[02/22 05:42:19 cifar100-global-proj_sl-3.0-resnet56]: Epoch 42/100, Acc=0.6885, Val Loss=1.5957, lr=0.0100
[02/22 05:49:03 cifar100-global-proj_sl-3.0-resnet56]: Epoch 43/100, Acc=0.6765, Val Loss=1.6587, lr=0.0100
[02/22 05:55:49 cifar100-global-proj_sl-3.0-resnet56]: Epoch 44/100, Acc=0.6895, Val Loss=1.6444, lr=0.0100
[02/22 06:02:37 cifar100-global-proj_sl-3.0-resnet56]: Epoch 45/100, Acc=0.6845, Val Loss=1.6588, lr=0.0100
[02/22 06:09:23 cifar100-global-proj_sl-3.0-resnet56]: Epoch 46/100, Acc=0.6789, Val Loss=1.6733, lr=0.0100
[02/22 06:16:10 cifar100-global-proj_sl-3.0-resnet56]: Epoch 47/100, Acc=0.6929, Val Loss=1.6375, lr=0.0100
[02/22 06:22:59 cifar100-global-proj_sl-3.0-resnet56]: Epoch 48/100, Acc=0.6923, Val Loss=1.6322, lr=0.0100
[02/22 06:29:46 cifar100-global-proj_sl-3.0-resnet56]: Epoch 49/100, Acc=0.6857, Val Loss=1.6410, lr=0.0100
[02/22 06:36:34 cifar100-global-proj_sl-3.0-resnet56]: Epoch 50/100, Acc=0.6862, Val Loss=1.6978, lr=0.0100
[02/22 06:43:22 cifar100-global-proj_sl-3.0-resnet56]: Epoch 51/100, Acc=0.6852, Val Loss=1.6858, lr=0.0100
[02/22 06:50:09 cifar100-global-proj_sl-3.0-resnet56]: Epoch 52/100, Acc=0.6866, Val Loss=1.6986, lr=0.0100
[02/22 06:56:56 cifar100-global-proj_sl-3.0-resnet56]: Epoch 53/100, Acc=0.6856, Val Loss=1.6712, lr=0.0100
[02/22 07:03:43 cifar100-global-proj_sl-3.0-resnet56]: Epoch 54/100, Acc=0.6830, Val Loss=1.7331, lr=0.0100
[02/22 07:10:28 cifar100-global-proj_sl-3.0-resnet56]: Epoch 55/100, Acc=0.6881, Val Loss=1.6991, lr=0.0100
[02/22 07:17:15 cifar100-global-proj_sl-3.0-resnet56]: Epoch 56/100, Acc=0.6892, Val Loss=1.7007, lr=0.0100
[02/22 07:24:03 cifar100-global-proj_sl-3.0-resnet56]: Epoch 57/100, Acc=0.6947, Val Loss=1.7030, lr=0.0100
[02/22 07:30:49 cifar100-global-proj_sl-3.0-resnet56]: Epoch 58/100, Acc=0.6873, Val Loss=1.7507, lr=0.0100
[02/22 07:37:36 cifar100-global-proj_sl-3.0-resnet56]: Epoch 59/100, Acc=0.6912, Val Loss=1.7144, lr=0.0100
[02/22 07:44:25 cifar100-global-proj_sl-3.0-resnet56]: Epoch 60/100, Acc=0.7158, Val Loss=1.5267, lr=0.0010
[02/22 07:51:13 cifar100-global-proj_sl-3.0-resnet56]: Epoch 61/100, Acc=0.7203, Val Loss=1.5155, lr=0.0010
[02/22 07:57:59 cifar100-global-proj_sl-3.0-resnet56]: Epoch 62/100, Acc=0.7220, Val Loss=1.5155, lr=0.0010
[02/22 08:04:45 cifar100-global-proj_sl-3.0-resnet56]: Epoch 63/100, Acc=0.7196, Val Loss=1.5203, lr=0.0010
[02/22 08:11:30 cifar100-global-proj_sl-3.0-resnet56]: Epoch 64/100, Acc=0.7222, Val Loss=1.5143, lr=0.0010
[02/22 08:18:16 cifar100-global-proj_sl-3.0-resnet56]: Epoch 65/100, Acc=0.7220, Val Loss=1.5345, lr=0.0010
[02/22 08:25:02 cifar100-global-proj_sl-3.0-resnet56]: Epoch 66/100, Acc=0.7242, Val Loss=1.5241, lr=0.0010
[02/22 08:31:49 cifar100-global-proj_sl-3.0-resnet56]: Epoch 67/100, Acc=0.7231, Val Loss=1.5273, lr=0.0010
[02/22 08:38:36 cifar100-global-proj_sl-3.0-resnet56]: Epoch 68/100, Acc=0.7228, Val Loss=1.5295, lr=0.0010
[02/22 08:45:21 cifar100-global-proj_sl-3.0-resnet56]: Epoch 69/100, Acc=0.7224, Val Loss=1.5338, lr=0.0010
[02/22 08:52:08 cifar100-global-proj_sl-3.0-resnet56]: Epoch 70/100, Acc=0.7218, Val Loss=1.5357, lr=0.0010
[02/22 08:58:54 cifar100-global-proj_sl-3.0-resnet56]: Epoch 71/100, Acc=0.7214, Val Loss=1.5386, lr=0.0010
[02/22 09:05:40 cifar100-global-proj_sl-3.0-resnet56]: Epoch 72/100, Acc=0.7207, Val Loss=1.5455, lr=0.0010
[02/22 09:12:26 cifar100-global-proj_sl-3.0-resnet56]: Epoch 73/100, Acc=0.7211, Val Loss=1.5600, lr=0.0010
[02/22 09:19:11 cifar100-global-proj_sl-3.0-resnet56]: Epoch 74/100, Acc=0.7214, Val Loss=1.5549, lr=0.0010
[02/22 09:25:56 cifar100-global-proj_sl-3.0-resnet56]: Epoch 75/100, Acc=0.7217, Val Loss=1.5613, lr=0.0010
[02/22 09:32:41 cifar100-global-proj_sl-3.0-resnet56]: Epoch 76/100, Acc=0.7211, Val Loss=1.5662, lr=0.0010
[02/22 09:39:26 cifar100-global-proj_sl-3.0-resnet56]: Epoch 77/100, Acc=0.7219, Val Loss=1.5530, lr=0.0010
[02/22 09:46:14 cifar100-global-proj_sl-3.0-resnet56]: Epoch 78/100, Acc=0.7199, Val Loss=1.5684, lr=0.0010
[02/22 09:53:02 cifar100-global-proj_sl-3.0-resnet56]: Epoch 79/100, Acc=0.7219, Val Loss=1.5653, lr=0.0010
[02/22 09:59:50 cifar100-global-proj_sl-3.0-resnet56]: Epoch 80/100, Acc=0.7223, Val Loss=1.5681, lr=0.0001
[02/22 10:06:37 cifar100-global-proj_sl-3.0-resnet56]: Epoch 81/100, Acc=0.7208, Val Loss=1.5653, lr=0.0001
[02/22 10:13:24 cifar100-global-proj_sl-3.0-resnet56]: Epoch 82/100, Acc=0.7221, Val Loss=1.5674, lr=0.0001
[02/22 10:20:11 cifar100-global-proj_sl-3.0-resnet56]: Epoch 83/100, Acc=0.7205, Val Loss=1.5707, lr=0.0001
[02/22 10:26:57 cifar100-global-proj_sl-3.0-resnet56]: Epoch 84/100, Acc=0.7210, Val Loss=1.5665, lr=0.0001
[02/22 10:33:44 cifar100-global-proj_sl-3.0-resnet56]: Epoch 85/100, Acc=0.7211, Val Loss=1.5705, lr=0.0001
[02/22 10:40:28 cifar100-global-proj_sl-3.0-resnet56]: Epoch 86/100, Acc=0.7213, Val Loss=1.5679, lr=0.0001
[02/22 10:47:14 cifar100-global-proj_sl-3.0-resnet56]: Epoch 87/100, Acc=0.7216, Val Loss=1.5605, lr=0.0001
[02/22 10:54:02 cifar100-global-proj_sl-3.0-resnet56]: Epoch 88/100, Acc=0.7227, Val Loss=1.5647, lr=0.0001
[02/22 11:00:49 cifar100-global-proj_sl-3.0-resnet56]: Epoch 89/100, Acc=0.7226, Val Loss=1.5679, lr=0.0001
[02/22 11:07:37 cifar100-global-proj_sl-3.0-resnet56]: Epoch 90/100, Acc=0.7215, Val Loss=1.5595, lr=0.0001
[02/22 11:14:25 cifar100-global-proj_sl-3.0-resnet56]: Epoch 91/100, Acc=0.7213, Val Loss=1.5644, lr=0.0001
[02/22 11:21:11 cifar100-global-proj_sl-3.0-resnet56]: Epoch 92/100, Acc=0.7180, Val Loss=1.5635, lr=0.0001
[02/22 11:27:58 cifar100-global-proj_sl-3.0-resnet56]: Epoch 93/100, Acc=0.7228, Val Loss=1.5627, lr=0.0001
[02/22 11:34:46 cifar100-global-proj_sl-3.0-resnet56]: Epoch 94/100, Acc=0.7213, Val Loss=1.5622, lr=0.0001
[02/22 11:41:33 cifar100-global-proj_sl-3.0-resnet56]: Epoch 95/100, Acc=0.7212, Val Loss=1.5677, lr=0.0001
[02/22 11:48:22 cifar100-global-proj_sl-3.0-resnet56]: Epoch 96/100, Acc=0.7213, Val Loss=1.5634, lr=0.0001
[02/22 11:55:07 cifar100-global-proj_sl-3.0-resnet56]: Epoch 97/100, Acc=0.7215, Val Loss=1.5728, lr=0.0001
[02/22 12:01:52 cifar100-global-proj_sl-3.0-resnet56]: Epoch 98/100, Acc=0.7223, Val Loss=1.5625, lr=0.0001
[02/22 12:08:40 cifar100-global-proj_sl-3.0-resnet56]: Epoch 99/100, Acc=0.7223, Val Loss=1.5640, lr=0.0001
[02/22 12:08:40 cifar100-global-proj_sl-3.0-resnet56]: Best Acc=0.7242
[02/22 12:08:40 cifar100-global-proj_sl-3.0-resnet56]: Loading the sparse model from run/cifar100/prune/cifar100-global-proj_sl-3.0-resnet56/reg_cifar100_resnet56_proj_sl_1e-05.pth...
[02/22 12:08:43 cifar100-global-proj_sl-3.0-resnet56]: Pruning...
[02/22 12:11:10 cifar100-global-proj_sl-3.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(5, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(5, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(5, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(5, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(5, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(5, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(5, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(5, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(5, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(5, 23, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(23, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(23, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(23, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(23, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(23, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(23, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(23, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(23, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(23, 14, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(23, 55, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(55, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(49, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(55, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(35, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(55, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(55, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(55, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(52, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(55, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(53, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(55, 37, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(37, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(55, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(36, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=55, out_features=100, bias=True)
)
[02/22 12:11:13 cifar100-global-proj_sl-3.0-resnet56]: Params: 0.86 M => 0.45 M (52.20%)
[02/22 12:11:13 cifar100-global-proj_sl-3.0-resnet56]: FLOPs: 127.12 M => 42.25 M (33.23%, 3.01X )
[02/22 12:11:13 cifar100-global-proj_sl-3.0-resnet56]: Acc: 0.7242 => 0.0106
[02/22 12:11:13 cifar100-global-proj_sl-3.0-resnet56]: Val Loss: 1.5241 => 28.0679
[02/22 12:11:13 cifar100-global-proj_sl-3.0-resnet56]: Finetuning...
[02/22 12:11:41 cifar100-global-proj_sl-3.0-resnet56]: Epoch 0/100, Acc=0.3118, Val Loss=2.7081, lr=0.0100
[02/22 12:12:09 cifar100-global-proj_sl-3.0-resnet56]: Epoch 1/100, Acc=0.4462, Val Loss=2.2327, lr=0.0100
[02/22 12:12:36 cifar100-global-proj_sl-3.0-resnet56]: Epoch 2/100, Acc=0.4979, Val Loss=1.9346, lr=0.0100
[02/22 12:13:04 cifar100-global-proj_sl-3.0-resnet56]: Epoch 3/100, Acc=0.4587, Val Loss=2.2110, lr=0.0100
[02/22 12:13:32 cifar100-global-proj_sl-3.0-resnet56]: Epoch 4/100, Acc=0.4922, Val Loss=2.0221, lr=0.0100
[02/22 12:14:00 cifar100-global-proj_sl-3.0-resnet56]: Epoch 5/100, Acc=0.5612, Val Loss=1.6607, lr=0.0100
[02/22 12:14:28 cifar100-global-proj_sl-3.0-resnet56]: Epoch 6/100, Acc=0.5703, Val Loss=1.6180, lr=0.0100
[02/22 12:14:56 cifar100-global-proj_sl-3.0-resnet56]: Epoch 7/100, Acc=0.5214, Val Loss=1.9200, lr=0.0100
[02/22 12:15:24 cifar100-global-proj_sl-3.0-resnet56]: Epoch 8/100, Acc=0.5673, Val Loss=1.6856, lr=0.0100
[02/22 12:15:52 cifar100-global-proj_sl-3.0-resnet56]: Epoch 9/100, Acc=0.5761, Val Loss=1.5929, lr=0.0100
[02/22 12:16:21 cifar100-global-proj_sl-3.0-resnet56]: Epoch 10/100, Acc=0.6129, Val Loss=1.4455, lr=0.0100
[02/22 12:16:49 cifar100-global-proj_sl-3.0-resnet56]: Epoch 11/100, Acc=0.5898, Val Loss=1.5578, lr=0.0100
[02/22 12:17:17 cifar100-global-proj_sl-3.0-resnet56]: Epoch 12/100, Acc=0.5869, Val Loss=1.5531, lr=0.0100
[02/22 12:17:44 cifar100-global-proj_sl-3.0-resnet56]: Epoch 13/100, Acc=0.5959, Val Loss=1.5419, lr=0.0100
[02/22 12:18:12 cifar100-global-proj_sl-3.0-resnet56]: Epoch 14/100, Acc=0.5892, Val Loss=1.5731, lr=0.0100
[02/22 12:18:40 cifar100-global-proj_sl-3.0-resnet56]: Epoch 15/100, Acc=0.5801, Val Loss=1.6252, lr=0.0100
[02/22 12:19:08 cifar100-global-proj_sl-3.0-resnet56]: Epoch 16/100, Acc=0.6109, Val Loss=1.4632, lr=0.0100
[02/22 12:19:35 cifar100-global-proj_sl-3.0-resnet56]: Epoch 17/100, Acc=0.5844, Val Loss=1.6341, lr=0.0100
[02/22 12:20:03 cifar100-global-proj_sl-3.0-resnet56]: Epoch 18/100, Acc=0.6047, Val Loss=1.5036, lr=0.0100
[02/22 12:20:31 cifar100-global-proj_sl-3.0-resnet56]: Epoch 19/100, Acc=0.5915, Val Loss=1.5564, lr=0.0100
[02/22 12:20:58 cifar100-global-proj_sl-3.0-resnet56]: Epoch 20/100, Acc=0.5634, Val Loss=1.7256, lr=0.0100
[02/22 12:21:26 cifar100-global-proj_sl-3.0-resnet56]: Epoch 21/100, Acc=0.5705, Val Loss=1.6966, lr=0.0100
[02/22 12:21:54 cifar100-global-proj_sl-3.0-resnet56]: Epoch 22/100, Acc=0.6144, Val Loss=1.4069, lr=0.0100
[02/22 12:22:22 cifar100-global-proj_sl-3.0-resnet56]: Epoch 23/100, Acc=0.6140, Val Loss=1.4922, lr=0.0100
[02/22 12:22:50 cifar100-global-proj_sl-3.0-resnet56]: Epoch 24/100, Acc=0.6171, Val Loss=1.4909, lr=0.0100
[02/22 12:23:18 cifar100-global-proj_sl-3.0-resnet56]: Epoch 25/100, Acc=0.5814, Val Loss=1.6618, lr=0.0100
[02/22 12:23:45 cifar100-global-proj_sl-3.0-resnet56]: Epoch 26/100, Acc=0.6126, Val Loss=1.4775, lr=0.0100
[02/22 12:24:13 cifar100-global-proj_sl-3.0-resnet56]: Epoch 27/100, Acc=0.6144, Val Loss=1.4470, lr=0.0100
[02/22 12:24:41 cifar100-global-proj_sl-3.0-resnet56]: Epoch 28/100, Acc=0.6218, Val Loss=1.4515, lr=0.0100
[02/22 12:25:09 cifar100-global-proj_sl-3.0-resnet56]: Epoch 29/100, Acc=0.6060, Val Loss=1.5379, lr=0.0100
[02/22 12:25:37 cifar100-global-proj_sl-3.0-resnet56]: Epoch 30/100, Acc=0.6176, Val Loss=1.4612, lr=0.0100
[02/22 12:26:05 cifar100-global-proj_sl-3.0-resnet56]: Epoch 31/100, Acc=0.6019, Val Loss=1.5398, lr=0.0100
[02/22 12:26:33 cifar100-global-proj_sl-3.0-resnet56]: Epoch 32/100, Acc=0.6214, Val Loss=1.4706, lr=0.0100
[02/22 12:27:01 cifar100-global-proj_sl-3.0-resnet56]: Epoch 33/100, Acc=0.5799, Val Loss=1.6685, lr=0.0100
[02/22 12:27:29 cifar100-global-proj_sl-3.0-resnet56]: Epoch 34/100, Acc=0.6159, Val Loss=1.4653, lr=0.0100
[02/22 12:27:58 cifar100-global-proj_sl-3.0-resnet56]: Epoch 35/100, Acc=0.6162, Val Loss=1.4852, lr=0.0100
[02/22 12:28:26 cifar100-global-proj_sl-3.0-resnet56]: Epoch 36/100, Acc=0.6008, Val Loss=1.5324, lr=0.0100
[02/22 12:28:54 cifar100-global-proj_sl-3.0-resnet56]: Epoch 37/100, Acc=0.6127, Val Loss=1.5128, lr=0.0100
[02/22 12:29:22 cifar100-global-proj_sl-3.0-resnet56]: Epoch 38/100, Acc=0.6010, Val Loss=1.5885, lr=0.0100
[02/22 12:29:50 cifar100-global-proj_sl-3.0-resnet56]: Epoch 39/100, Acc=0.6262, Val Loss=1.4055, lr=0.0100
[02/22 12:30:19 cifar100-global-proj_sl-3.0-resnet56]: Epoch 40/100, Acc=0.6150, Val Loss=1.5195, lr=0.0100
[02/22 12:30:47 cifar100-global-proj_sl-3.0-resnet56]: Epoch 41/100, Acc=0.6177, Val Loss=1.4777, lr=0.0100
[02/22 12:31:15 cifar100-global-proj_sl-3.0-resnet56]: Epoch 42/100, Acc=0.5991, Val Loss=1.6287, lr=0.0100
[02/22 12:31:43 cifar100-global-proj_sl-3.0-resnet56]: Epoch 43/100, Acc=0.6243, Val Loss=1.4479, lr=0.0100
[02/22 12:32:12 cifar100-global-proj_sl-3.0-resnet56]: Epoch 44/100, Acc=0.5747, Val Loss=1.7419, lr=0.0100
[02/22 12:32:40 cifar100-global-proj_sl-3.0-resnet56]: Epoch 45/100, Acc=0.6090, Val Loss=1.5511, lr=0.0100
[02/22 12:33:08 cifar100-global-proj_sl-3.0-resnet56]: Epoch 46/100, Acc=0.6068, Val Loss=1.5360, lr=0.0100
[02/22 12:33:36 cifar100-global-proj_sl-3.0-resnet56]: Epoch 47/100, Acc=0.5941, Val Loss=1.5986, lr=0.0100
[02/22 12:34:04 cifar100-global-proj_sl-3.0-resnet56]: Epoch 48/100, Acc=0.6244, Val Loss=1.4600, lr=0.0100
[02/22 12:34:32 cifar100-global-proj_sl-3.0-resnet56]: Epoch 49/100, Acc=0.5946, Val Loss=1.5738, lr=0.0100
[02/22 12:35:00 cifar100-global-proj_sl-3.0-resnet56]: Epoch 50/100, Acc=0.6244, Val Loss=1.4490, lr=0.0100
[02/22 12:35:28 cifar100-global-proj_sl-3.0-resnet56]: Epoch 51/100, Acc=0.6036, Val Loss=1.5876, lr=0.0100
[02/22 12:35:55 cifar100-global-proj_sl-3.0-resnet56]: Epoch 52/100, Acc=0.6064, Val Loss=1.5769, lr=0.0100
[02/22 12:36:23 cifar100-global-proj_sl-3.0-resnet56]: Epoch 53/100, Acc=0.5987, Val Loss=1.5810, lr=0.0100
[02/22 12:36:51 cifar100-global-proj_sl-3.0-resnet56]: Epoch 54/100, Acc=0.6097, Val Loss=1.5057, lr=0.0100
[02/22 12:37:18 cifar100-global-proj_sl-3.0-resnet56]: Epoch 55/100, Acc=0.5987, Val Loss=1.6083, lr=0.0100
[02/22 12:37:46 cifar100-global-proj_sl-3.0-resnet56]: Epoch 56/100, Acc=0.5903, Val Loss=1.6303, lr=0.0100
[02/22 12:38:13 cifar100-global-proj_sl-3.0-resnet56]: Epoch 57/100, Acc=0.5894, Val Loss=1.6228, lr=0.0100
[02/22 12:38:41 cifar100-global-proj_sl-3.0-resnet56]: Epoch 58/100, Acc=0.6060, Val Loss=1.5352, lr=0.0100
[02/22 12:39:09 cifar100-global-proj_sl-3.0-resnet56]: Epoch 59/100, Acc=0.6110, Val Loss=1.5469, lr=0.0100
[02/22 12:39:37 cifar100-global-proj_sl-3.0-resnet56]: Epoch 60/100, Acc=0.6823, Val Loss=1.1912, lr=0.0010
[02/22 12:40:05 cifar100-global-proj_sl-3.0-resnet56]: Epoch 61/100, Acc=0.6863, Val Loss=1.1774, lr=0.0010
[02/22 12:40:33 cifar100-global-proj_sl-3.0-resnet56]: Epoch 62/100, Acc=0.6851, Val Loss=1.1824, lr=0.0010
[02/22 12:41:01 cifar100-global-proj_sl-3.0-resnet56]: Epoch 63/100, Acc=0.6869, Val Loss=1.1842, lr=0.0010
[02/22 12:41:29 cifar100-global-proj_sl-3.0-resnet56]: Epoch 64/100, Acc=0.6868, Val Loss=1.1946, lr=0.0010
[02/22 12:41:58 cifar100-global-proj_sl-3.0-resnet56]: Epoch 65/100, Acc=0.6853, Val Loss=1.2041, lr=0.0010
[02/22 12:42:26 cifar100-global-proj_sl-3.0-resnet56]: Epoch 66/100, Acc=0.6884, Val Loss=1.2058, lr=0.0010
[02/22 12:42:54 cifar100-global-proj_sl-3.0-resnet56]: Epoch 67/100, Acc=0.6894, Val Loss=1.2120, lr=0.0010
[02/22 12:43:22 cifar100-global-proj_sl-3.0-resnet56]: Epoch 68/100, Acc=0.6867, Val Loss=1.2084, lr=0.0010
[02/22 12:43:50 cifar100-global-proj_sl-3.0-resnet56]: Epoch 69/100, Acc=0.6849, Val Loss=1.2193, lr=0.0010
[02/22 12:44:18 cifar100-global-proj_sl-3.0-resnet56]: Epoch 70/100, Acc=0.6845, Val Loss=1.2209, lr=0.0010
[02/22 12:44:46 cifar100-global-proj_sl-3.0-resnet56]: Epoch 71/100, Acc=0.6856, Val Loss=1.2252, lr=0.0010
[02/22 12:45:14 cifar100-global-proj_sl-3.0-resnet56]: Epoch 72/100, Acc=0.6876, Val Loss=1.2288, lr=0.0010
[02/22 12:45:43 cifar100-global-proj_sl-3.0-resnet56]: Epoch 73/100, Acc=0.6820, Val Loss=1.2292, lr=0.0010
[02/22 12:46:11 cifar100-global-proj_sl-3.0-resnet56]: Epoch 74/100, Acc=0.6863, Val Loss=1.2334, lr=0.0010
[02/22 12:46:39 cifar100-global-proj_sl-3.0-resnet56]: Epoch 75/100, Acc=0.6827, Val Loss=1.2481, lr=0.0010
[02/22 12:47:07 cifar100-global-proj_sl-3.0-resnet56]: Epoch 76/100, Acc=0.6849, Val Loss=1.2560, lr=0.0010
[02/22 12:47:35 cifar100-global-proj_sl-3.0-resnet56]: Epoch 77/100, Acc=0.6852, Val Loss=1.2531, lr=0.0010
[02/22 12:48:03 cifar100-global-proj_sl-3.0-resnet56]: Epoch 78/100, Acc=0.6843, Val Loss=1.2660, lr=0.0010
[02/22 12:48:31 cifar100-global-proj_sl-3.0-resnet56]: Epoch 79/100, Acc=0.6861, Val Loss=1.2567, lr=0.0010
[02/22 12:48:59 cifar100-global-proj_sl-3.0-resnet56]: Epoch 80/100, Acc=0.6879, Val Loss=1.2509, lr=0.0001
[02/22 12:49:27 cifar100-global-proj_sl-3.0-resnet56]: Epoch 81/100, Acc=0.6880, Val Loss=1.2475, lr=0.0001
[02/22 12:49:55 cifar100-global-proj_sl-3.0-resnet56]: Epoch 82/100, Acc=0.6891, Val Loss=1.2493, lr=0.0001
[02/22 12:50:23 cifar100-global-proj_sl-3.0-resnet56]: Epoch 83/100, Acc=0.6879, Val Loss=1.2465, lr=0.0001
[02/22 12:50:51 cifar100-global-proj_sl-3.0-resnet56]: Epoch 84/100, Acc=0.6886, Val Loss=1.2447, lr=0.0001
[02/22 12:51:19 cifar100-global-proj_sl-3.0-resnet56]: Epoch 85/100, Acc=0.6882, Val Loss=1.2480, lr=0.0001
[02/22 12:51:47 cifar100-global-proj_sl-3.0-resnet56]: Epoch 86/100, Acc=0.6868, Val Loss=1.2516, lr=0.0001
[02/22 12:52:16 cifar100-global-proj_sl-3.0-resnet56]: Epoch 87/100, Acc=0.6860, Val Loss=1.2488, lr=0.0001
[02/22 12:52:44 cifar100-global-proj_sl-3.0-resnet56]: Epoch 88/100, Acc=0.6877, Val Loss=1.2533, lr=0.0001
[02/22 12:53:12 cifar100-global-proj_sl-3.0-resnet56]: Epoch 89/100, Acc=0.6868, Val Loss=1.2513, lr=0.0001
[02/22 12:53:40 cifar100-global-proj_sl-3.0-resnet56]: Epoch 90/100, Acc=0.6858, Val Loss=1.2525, lr=0.0001
[02/22 12:54:08 cifar100-global-proj_sl-3.0-resnet56]: Epoch 91/100, Acc=0.6862, Val Loss=1.2503, lr=0.0001
[02/22 12:54:36 cifar100-global-proj_sl-3.0-resnet56]: Epoch 92/100, Acc=0.6878, Val Loss=1.2438, lr=0.0001
[02/22 12:55:05 cifar100-global-proj_sl-3.0-resnet56]: Epoch 93/100, Acc=0.6888, Val Loss=1.2532, lr=0.0001
[02/22 12:55:33 cifar100-global-proj_sl-3.0-resnet56]: Epoch 94/100, Acc=0.6866, Val Loss=1.2583, lr=0.0001
[02/22 12:56:01 cifar100-global-proj_sl-3.0-resnet56]: Epoch 95/100, Acc=0.6878, Val Loss=1.2514, lr=0.0001
[02/22 12:56:29 cifar100-global-proj_sl-3.0-resnet56]: Epoch 96/100, Acc=0.6879, Val Loss=1.2504, lr=0.0001
[02/22 12:56:56 cifar100-global-proj_sl-3.0-resnet56]: Epoch 97/100, Acc=0.6875, Val Loss=1.2584, lr=0.0001
[02/22 12:57:25 cifar100-global-proj_sl-3.0-resnet56]: Epoch 98/100, Acc=0.6875, Val Loss=1.2490, lr=0.0001
[02/22 12:57:52 cifar100-global-proj_sl-3.0-resnet56]: Epoch 99/100, Acc=0.6873, Val Loss=1.2529, lr=0.0001
[02/22 12:57:52 cifar100-global-proj_sl-3.0-resnet56]: Best Acc=0.6894
[02/22 12:57:52 cifar100-global-proj_sl-3.0-resnet56]: Params: 0.45 M
[02/22 12:57:52 cifar100-global-proj_sl-3.0-resnet56]: ops: 42.25 M
[02/22 12:57:55 cifar100-global-proj_sl-3.0-resnet56]: Acc: 0.6873 Val Loss: 1.2529

TIME TAKEN: 20:44:05
SLURM WORKLOAD FINISH: Sat Feb 22 12:57:56 CET 2025
