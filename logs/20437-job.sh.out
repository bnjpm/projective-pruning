SLURM WORKLOAD START: Thu Feb 20 20:54:57 CET 2025
Thu Feb 20 20:54:57 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:0B:00.0 Off |                    0 |
| N/A   41C    P0             26W /  250W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: mode: prune
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: model: resnet56
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: verbose: False
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: dataset: cifar10
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: dataroot: data
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: batch_size: 128
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: total_epochs: 100
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: lr_decay_milestones: 60,80
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: lr_decay_gamma: 0.1
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: lr: 0.01
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: output_dir: run/cifar10/prune/cifar10-global-random-2.0-resnet56
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: finetune: True
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: last_epochs: 100
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: reps: 1
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: method: random
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: speed_up: 2.0
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: max_pruning_ratio: 1.0
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: soft_keeping_ratio: 0.0
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: reg: 1e-05
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: delta_reg: 0.0001
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: weight_decay: 0.0005
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: seed: 1
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: global_pruning: True
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: sl_total_epochs: 100
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: sl_lr: 0.01
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: sl_reg_warmup: 0
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: sl_restore: None
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: iterative_steps: 400
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: logger: <Logger cifar10-global-random-2.0-resnet56 (DEBUG)>
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: device: cuda
[02/20 20:55:03 cifar10-global-random-2.0-resnet56]: num_classes: 10
[02/20 20:55:04 cifar10-global-random-2.0-resnet56]: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/20 20:55:07 cifar10-global-random-2.0-resnet56]: Pruning...
[02/20 20:55:14 cifar10-global-random-2.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(7, 23, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(7, 23, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(23, 55, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(23, 55, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=55, out_features=10, bias=True)
)
[02/20 20:55:17 cifar10-global-random-2.0-resnet56]: Params: 0.86 M => 0.57 M (66.60%)
[02/20 20:55:17 cifar10-global-random-2.0-resnet56]: FLOPs: 127.12 M => 60.67 M (47.73%, 2.10X )
[02/20 20:55:17 cifar10-global-random-2.0-resnet56]: Acc: 0.9392 => 0.1000
[02/20 20:55:17 cifar10-global-random-2.0-resnet56]: Val Loss: 0.2587 => 16.9797
[02/20 20:55:17 cifar10-global-random-2.0-resnet56]: Finetuning...
[02/20 20:55:54 cifar10-global-random-2.0-resnet56]: Epoch 0/100, Acc=0.8144, Val Loss=0.5495, lr=0.0100
[02/20 20:56:26 cifar10-global-random-2.0-resnet56]: Epoch 1/100, Acc=0.8451, Val Loss=0.4658, lr=0.0100
[02/20 20:56:58 cifar10-global-random-2.0-resnet56]: Epoch 2/100, Acc=0.8621, Val Loss=0.4213, lr=0.0100
[02/20 20:57:29 cifar10-global-random-2.0-resnet56]: Epoch 3/100, Acc=0.8752, Val Loss=0.3676, lr=0.0100
[02/20 20:58:01 cifar10-global-random-2.0-resnet56]: Epoch 4/100, Acc=0.8280, Val Loss=0.5444, lr=0.0100
[02/20 20:58:32 cifar10-global-random-2.0-resnet56]: Epoch 5/100, Acc=0.8690, Val Loss=0.4165, lr=0.0100
[02/20 20:59:02 cifar10-global-random-2.0-resnet56]: Epoch 6/100, Acc=0.8658, Val Loss=0.4217, lr=0.0100
[02/20 20:59:33 cifar10-global-random-2.0-resnet56]: Epoch 7/100, Acc=0.8769, Val Loss=0.3833, lr=0.0100
[02/20 21:00:05 cifar10-global-random-2.0-resnet56]: Epoch 8/100, Acc=0.8707, Val Loss=0.4066, lr=0.0100
[02/20 21:00:37 cifar10-global-random-2.0-resnet56]: Epoch 9/100, Acc=0.8941, Val Loss=0.3289, lr=0.0100
[02/20 21:01:09 cifar10-global-random-2.0-resnet56]: Epoch 10/100, Acc=0.8557, Val Loss=0.4521, lr=0.0100
[02/20 21:01:41 cifar10-global-random-2.0-resnet56]: Epoch 11/100, Acc=0.8809, Val Loss=0.3858, lr=0.0100
[02/20 21:02:13 cifar10-global-random-2.0-resnet56]: Epoch 12/100, Acc=0.8664, Val Loss=0.4186, lr=0.0100
[02/20 21:02:45 cifar10-global-random-2.0-resnet56]: Epoch 13/100, Acc=0.8874, Val Loss=0.3591, lr=0.0100
[02/20 21:03:17 cifar10-global-random-2.0-resnet56]: Epoch 14/100, Acc=0.8699, Val Loss=0.4163, lr=0.0100
[02/20 21:03:49 cifar10-global-random-2.0-resnet56]: Epoch 15/100, Acc=0.8746, Val Loss=0.4130, lr=0.0100
[02/20 21:04:21 cifar10-global-random-2.0-resnet56]: Epoch 16/100, Acc=0.8911, Val Loss=0.3435, lr=0.0100
[02/20 21:04:54 cifar10-global-random-2.0-resnet56]: Epoch 17/100, Acc=0.8929, Val Loss=0.3435, lr=0.0100
[02/20 21:05:26 cifar10-global-random-2.0-resnet56]: Epoch 18/100, Acc=0.8811, Val Loss=0.3870, lr=0.0100
[02/20 21:05:58 cifar10-global-random-2.0-resnet56]: Epoch 19/100, Acc=0.8901, Val Loss=0.3564, lr=0.0100
[02/20 21:06:30 cifar10-global-random-2.0-resnet56]: Epoch 20/100, Acc=0.8773, Val Loss=0.4005, lr=0.0100
[02/20 21:07:02 cifar10-global-random-2.0-resnet56]: Epoch 21/100, Acc=0.8681, Val Loss=0.4387, lr=0.0100
[02/20 21:07:34 cifar10-global-random-2.0-resnet56]: Epoch 22/100, Acc=0.8919, Val Loss=0.3654, lr=0.0100
[02/20 21:08:06 cifar10-global-random-2.0-resnet56]: Epoch 23/100, Acc=0.8703, Val Loss=0.4360, lr=0.0100
[02/20 21:08:38 cifar10-global-random-2.0-resnet56]: Epoch 24/100, Acc=0.8605, Val Loss=0.4872, lr=0.0100
[02/20 21:09:10 cifar10-global-random-2.0-resnet56]: Epoch 25/100, Acc=0.8893, Val Loss=0.3695, lr=0.0100
[02/20 21:09:42 cifar10-global-random-2.0-resnet56]: Epoch 26/100, Acc=0.8832, Val Loss=0.3839, lr=0.0100
[02/20 21:10:13 cifar10-global-random-2.0-resnet56]: Epoch 27/100, Acc=0.8849, Val Loss=0.3972, lr=0.0100
[02/20 21:10:44 cifar10-global-random-2.0-resnet56]: Epoch 28/100, Acc=0.8884, Val Loss=0.3753, lr=0.0100
[02/20 21:11:16 cifar10-global-random-2.0-resnet56]: Epoch 29/100, Acc=0.8798, Val Loss=0.3859, lr=0.0100
[02/20 21:11:47 cifar10-global-random-2.0-resnet56]: Epoch 30/100, Acc=0.8971, Val Loss=0.3411, lr=0.0100
[02/20 21:12:18 cifar10-global-random-2.0-resnet56]: Epoch 31/100, Acc=0.8916, Val Loss=0.3663, lr=0.0100
[02/20 21:12:49 cifar10-global-random-2.0-resnet56]: Epoch 32/100, Acc=0.8775, Val Loss=0.4406, lr=0.0100
[02/20 21:13:21 cifar10-global-random-2.0-resnet56]: Epoch 33/100, Acc=0.8844, Val Loss=0.3854, lr=0.0100
[02/20 21:13:53 cifar10-global-random-2.0-resnet56]: Epoch 34/100, Acc=0.8795, Val Loss=0.3984, lr=0.0100
[02/20 21:14:25 cifar10-global-random-2.0-resnet56]: Epoch 35/100, Acc=0.8693, Val Loss=0.4456, lr=0.0100
[02/20 21:14:57 cifar10-global-random-2.0-resnet56]: Epoch 36/100, Acc=0.8887, Val Loss=0.3689, lr=0.0100
[02/20 21:15:28 cifar10-global-random-2.0-resnet56]: Epoch 37/100, Acc=0.8973, Val Loss=0.3482, lr=0.0100
[02/20 21:16:01 cifar10-global-random-2.0-resnet56]: Epoch 38/100, Acc=0.8939, Val Loss=0.3462, lr=0.0100
[02/20 21:16:32 cifar10-global-random-2.0-resnet56]: Epoch 39/100, Acc=0.8945, Val Loss=0.3599, lr=0.0100
[02/20 21:17:03 cifar10-global-random-2.0-resnet56]: Epoch 40/100, Acc=0.8962, Val Loss=0.3452, lr=0.0100
[02/20 21:17:34 cifar10-global-random-2.0-resnet56]: Epoch 41/100, Acc=0.8950, Val Loss=0.3529, lr=0.0100
[02/20 21:18:05 cifar10-global-random-2.0-resnet56]: Epoch 42/100, Acc=0.9056, Val Loss=0.3200, lr=0.0100
[02/20 21:18:36 cifar10-global-random-2.0-resnet56]: Epoch 43/100, Acc=0.9040, Val Loss=0.3270, lr=0.0100
[02/20 21:19:08 cifar10-global-random-2.0-resnet56]: Epoch 44/100, Acc=0.8895, Val Loss=0.3754, lr=0.0100
[02/20 21:19:39 cifar10-global-random-2.0-resnet56]: Epoch 45/100, Acc=0.8866, Val Loss=0.3969, lr=0.0100
[02/20 21:20:10 cifar10-global-random-2.0-resnet56]: Epoch 46/100, Acc=0.8965, Val Loss=0.3602, lr=0.0100
[02/20 21:20:42 cifar10-global-random-2.0-resnet56]: Epoch 47/100, Acc=0.8973, Val Loss=0.3587, lr=0.0100
[02/20 21:21:13 cifar10-global-random-2.0-resnet56]: Epoch 48/100, Acc=0.8788, Val Loss=0.4343, lr=0.0100
[02/20 21:21:44 cifar10-global-random-2.0-resnet56]: Epoch 49/100, Acc=0.8982, Val Loss=0.3464, lr=0.0100
[02/20 21:22:15 cifar10-global-random-2.0-resnet56]: Epoch 50/100, Acc=0.8971, Val Loss=0.3640, lr=0.0100
[02/20 21:22:46 cifar10-global-random-2.0-resnet56]: Epoch 51/100, Acc=0.8932, Val Loss=0.3737, lr=0.0100
[02/20 21:23:17 cifar10-global-random-2.0-resnet56]: Epoch 52/100, Acc=0.8828, Val Loss=0.3993, lr=0.0100
[02/20 21:23:48 cifar10-global-random-2.0-resnet56]: Epoch 53/100, Acc=0.9024, Val Loss=0.3379, lr=0.0100
[02/20 21:24:19 cifar10-global-random-2.0-resnet56]: Epoch 54/100, Acc=0.8922, Val Loss=0.3608, lr=0.0100
[02/20 21:24:51 cifar10-global-random-2.0-resnet56]: Epoch 55/100, Acc=0.8999, Val Loss=0.3397, lr=0.0100
[02/20 21:25:22 cifar10-global-random-2.0-resnet56]: Epoch 56/100, Acc=0.9029, Val Loss=0.3346, lr=0.0100
[02/20 21:25:54 cifar10-global-random-2.0-resnet56]: Epoch 57/100, Acc=0.9018, Val Loss=0.3472, lr=0.0100
[02/20 21:26:25 cifar10-global-random-2.0-resnet56]: Epoch 58/100, Acc=0.8830, Val Loss=0.4011, lr=0.0100
[02/20 21:26:57 cifar10-global-random-2.0-resnet56]: Epoch 59/100, Acc=0.8823, Val Loss=0.4261, lr=0.0100
[02/20 21:27:28 cifar10-global-random-2.0-resnet56]: Epoch 60/100, Acc=0.9225, Val Loss=0.2654, lr=0.0010
[02/20 21:27:59 cifar10-global-random-2.0-resnet56]: Epoch 61/100, Acc=0.9231, Val Loss=0.2668, lr=0.0010
[02/20 21:28:30 cifar10-global-random-2.0-resnet56]: Epoch 62/100, Acc=0.9250, Val Loss=0.2680, lr=0.0010
[02/20 21:29:01 cifar10-global-random-2.0-resnet56]: Epoch 63/100, Acc=0.9264, Val Loss=0.2665, lr=0.0010
[02/20 21:29:32 cifar10-global-random-2.0-resnet56]: Epoch 64/100, Acc=0.9267, Val Loss=0.2648, lr=0.0010
[02/20 21:30:04 cifar10-global-random-2.0-resnet56]: Epoch 65/100, Acc=0.9255, Val Loss=0.2673, lr=0.0010
[02/20 21:30:35 cifar10-global-random-2.0-resnet56]: Epoch 66/100, Acc=0.9257, Val Loss=0.2727, lr=0.0010
[02/20 21:31:06 cifar10-global-random-2.0-resnet56]: Epoch 67/100, Acc=0.9264, Val Loss=0.2729, lr=0.0010
[02/20 21:31:37 cifar10-global-random-2.0-resnet56]: Epoch 68/100, Acc=0.9270, Val Loss=0.2725, lr=0.0010
[02/20 21:32:11 cifar10-global-random-2.0-resnet56]: Epoch 69/100, Acc=0.9288, Val Loss=0.2734, lr=0.0010
[02/20 21:32:42 cifar10-global-random-2.0-resnet56]: Epoch 70/100, Acc=0.9277, Val Loss=0.2779, lr=0.0010
[02/20 21:33:13 cifar10-global-random-2.0-resnet56]: Epoch 71/100, Acc=0.9292, Val Loss=0.2729, lr=0.0010
[02/20 21:33:44 cifar10-global-random-2.0-resnet56]: Epoch 72/100, Acc=0.9276, Val Loss=0.2754, lr=0.0010
[02/20 21:34:14 cifar10-global-random-2.0-resnet56]: Epoch 73/100, Acc=0.9276, Val Loss=0.2796, lr=0.0010
[02/20 21:34:45 cifar10-global-random-2.0-resnet56]: Epoch 74/100, Acc=0.9276, Val Loss=0.2789, lr=0.0010
[02/20 21:35:16 cifar10-global-random-2.0-resnet56]: Epoch 75/100, Acc=0.9268, Val Loss=0.2812, lr=0.0010
[02/20 21:35:47 cifar10-global-random-2.0-resnet56]: Epoch 76/100, Acc=0.9259, Val Loss=0.2816, lr=0.0010
[02/20 21:36:18 cifar10-global-random-2.0-resnet56]: Epoch 77/100, Acc=0.9273, Val Loss=0.2810, lr=0.0010
[02/20 21:36:49 cifar10-global-random-2.0-resnet56]: Epoch 78/100, Acc=0.9266, Val Loss=0.2854, lr=0.0010
[02/20 21:37:21 cifar10-global-random-2.0-resnet56]: Epoch 79/100, Acc=0.9285, Val Loss=0.2839, lr=0.0010
[02/20 21:37:52 cifar10-global-random-2.0-resnet56]: Epoch 80/100, Acc=0.9279, Val Loss=0.2851, lr=0.0001
[02/20 21:38:23 cifar10-global-random-2.0-resnet56]: Epoch 81/100, Acc=0.9292, Val Loss=0.2833, lr=0.0001
[02/20 21:38:53 cifar10-global-random-2.0-resnet56]: Epoch 82/100, Acc=0.9291, Val Loss=0.2809, lr=0.0001
[02/20 21:39:24 cifar10-global-random-2.0-resnet56]: Epoch 83/100, Acc=0.9289, Val Loss=0.2831, lr=0.0001
[02/20 21:39:55 cifar10-global-random-2.0-resnet56]: Epoch 84/100, Acc=0.9284, Val Loss=0.2830, lr=0.0001
[02/20 21:40:26 cifar10-global-random-2.0-resnet56]: Epoch 85/100, Acc=0.9283, Val Loss=0.2824, lr=0.0001
[02/20 21:40:57 cifar10-global-random-2.0-resnet56]: Epoch 86/100, Acc=0.9294, Val Loss=0.2829, lr=0.0001
[02/20 21:41:28 cifar10-global-random-2.0-resnet56]: Epoch 87/100, Acc=0.9274, Val Loss=0.2842, lr=0.0001
[02/20 21:41:59 cifar10-global-random-2.0-resnet56]: Epoch 88/100, Acc=0.9290, Val Loss=0.2814, lr=0.0001
[02/20 21:42:30 cifar10-global-random-2.0-resnet56]: Epoch 89/100, Acc=0.9290, Val Loss=0.2825, lr=0.0001
[02/20 21:43:01 cifar10-global-random-2.0-resnet56]: Epoch 90/100, Acc=0.9283, Val Loss=0.2835, lr=0.0001
[02/20 21:43:31 cifar10-global-random-2.0-resnet56]: Epoch 91/100, Acc=0.9287, Val Loss=0.2824, lr=0.0001
[02/20 21:44:02 cifar10-global-random-2.0-resnet56]: Epoch 92/100, Acc=0.9295, Val Loss=0.2834, lr=0.0001
[02/20 21:44:33 cifar10-global-random-2.0-resnet56]: Epoch 93/100, Acc=0.9288, Val Loss=0.2827, lr=0.0001
[02/20 21:45:04 cifar10-global-random-2.0-resnet56]: Epoch 94/100, Acc=0.9279, Val Loss=0.2837, lr=0.0001
[02/20 21:45:35 cifar10-global-random-2.0-resnet56]: Epoch 95/100, Acc=0.9277, Val Loss=0.2852, lr=0.0001
[02/20 21:46:06 cifar10-global-random-2.0-resnet56]: Epoch 96/100, Acc=0.9290, Val Loss=0.2817, lr=0.0001
[02/20 21:46:37 cifar10-global-random-2.0-resnet56]: Epoch 97/100, Acc=0.9292, Val Loss=0.2824, lr=0.0001
[02/20 21:47:08 cifar10-global-random-2.0-resnet56]: Epoch 98/100, Acc=0.9287, Val Loss=0.2842, lr=0.0001
[02/20 21:47:39 cifar10-global-random-2.0-resnet56]: Epoch 99/100, Acc=0.9282, Val Loss=0.2855, lr=0.0001
[02/20 21:47:39 cifar10-global-random-2.0-resnet56]: Best Acc=0.9295
[02/20 21:47:39 cifar10-global-random-2.0-resnet56]: Params: 0.57 M
[02/20 21:47:39 cifar10-global-random-2.0-resnet56]: ops: 60.67 M
[02/20 21:47:43 cifar10-global-random-2.0-resnet56]: Acc: 0.9282 Val Loss: 0.2855

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: mode: prune
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: model: resnet56
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: verbose: False
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: dataset: cifar10
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: dataroot: data
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: batch_size: 128
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: total_epochs: 100
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: lr_decay_milestones: 60,80
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: lr_decay_gamma: 0.1
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: lr: 0.01
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: output_dir: run/cifar10/prune/cifar10-global-l2-2.0-resnet56
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: finetune: True
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: last_epochs: 100
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: reps: 1
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: method: l2
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: speed_up: 2.0
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: max_pruning_ratio: 1.0
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: soft_keeping_ratio: 0.0
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: reg: 1e-05
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: delta_reg: 0.0001
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: weight_decay: 0.0005
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: seed: 1
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: global_pruning: True
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: sl_total_epochs: 100
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: sl_lr: 0.01
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: sl_reg_warmup: 0
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: sl_restore: None
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: iterative_steps: 400
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: logger: <Logger cifar10-global-l2-2.0-resnet56 (DEBUG)>
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: device: cuda
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: num_classes: 10
[02/20 21:47:50 cifar10-global-l2-2.0-resnet56]: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/20 21:47:54 cifar10-global-l2-2.0-resnet56]: Pruning...
[02/20 21:48:10 cifar10-global-l2-2.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(12, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(12, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(12, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(12, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(12, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(12, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(12, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(12, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(12, 25, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(12, 31, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(31, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(31, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(31, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(31, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(31, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(31, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(31, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(31, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(31, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(31, 61, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(61, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(61, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(61, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(47, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(61, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(61, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(46, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(61, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(40, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(61, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(29, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(61, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=61, out_features=10, bias=True)
)
[02/20 21:48:13 cifar10-global-l2-2.0-resnet56]: Params: 0.86 M => 0.56 M (64.90%)
[02/20 21:48:13 cifar10-global-l2-2.0-resnet56]: FLOPs: 127.12 M => 63.06 M (49.61%, 2.02X )
[02/20 21:48:13 cifar10-global-l2-2.0-resnet56]: Acc: 0.9392 => 0.2899
[02/20 21:48:13 cifar10-global-l2-2.0-resnet56]: Val Loss: 0.2587 => 4.5965
[02/20 21:48:13 cifar10-global-l2-2.0-resnet56]: Finetuning...
[02/20 21:48:46 cifar10-global-l2-2.0-resnet56]: Epoch 0/100, Acc=0.8868, Val Loss=0.3735, lr=0.0100
[02/20 21:49:19 cifar10-global-l2-2.0-resnet56]: Epoch 1/100, Acc=0.8984, Val Loss=0.3324, lr=0.0100
[02/20 21:49:52 cifar10-global-l2-2.0-resnet56]: Epoch 2/100, Acc=0.9087, Val Loss=0.2905, lr=0.0100
[02/20 21:50:25 cifar10-global-l2-2.0-resnet56]: Epoch 3/100, Acc=0.9011, Val Loss=0.3122, lr=0.0100
[02/20 21:50:58 cifar10-global-l2-2.0-resnet56]: Epoch 4/100, Acc=0.8956, Val Loss=0.3360, lr=0.0100
[02/20 21:51:31 cifar10-global-l2-2.0-resnet56]: Epoch 5/100, Acc=0.8936, Val Loss=0.3637, lr=0.0100
[02/20 21:52:05 cifar10-global-l2-2.0-resnet56]: Epoch 6/100, Acc=0.9066, Val Loss=0.3133, lr=0.0100
[02/20 21:52:38 cifar10-global-l2-2.0-resnet56]: Epoch 7/100, Acc=0.8958, Val Loss=0.3531, lr=0.0100
[02/20 21:53:11 cifar10-global-l2-2.0-resnet56]: Epoch 8/100, Acc=0.9087, Val Loss=0.3057, lr=0.0100
[02/20 21:53:44 cifar10-global-l2-2.0-resnet56]: Epoch 9/100, Acc=0.9022, Val Loss=0.3361, lr=0.0100
[02/20 21:54:17 cifar10-global-l2-2.0-resnet56]: Epoch 10/100, Acc=0.9043, Val Loss=0.3179, lr=0.0100
[02/20 21:54:50 cifar10-global-l2-2.0-resnet56]: Epoch 11/100, Acc=0.9010, Val Loss=0.3467, lr=0.0100
[02/20 21:55:23 cifar10-global-l2-2.0-resnet56]: Epoch 12/100, Acc=0.9042, Val Loss=0.3366, lr=0.0100
[02/20 21:55:57 cifar10-global-l2-2.0-resnet56]: Epoch 13/100, Acc=0.8979, Val Loss=0.3507, lr=0.0100
[02/20 21:56:30 cifar10-global-l2-2.0-resnet56]: Epoch 14/100, Acc=0.9052, Val Loss=0.3220, lr=0.0100
[02/20 21:57:03 cifar10-global-l2-2.0-resnet56]: Epoch 15/100, Acc=0.9092, Val Loss=0.3150, lr=0.0100
[02/20 21:57:37 cifar10-global-l2-2.0-resnet56]: Epoch 16/100, Acc=0.9063, Val Loss=0.3175, lr=0.0100
[02/20 21:58:09 cifar10-global-l2-2.0-resnet56]: Epoch 17/100, Acc=0.9081, Val Loss=0.3173, lr=0.0100
[02/20 21:58:42 cifar10-global-l2-2.0-resnet56]: Epoch 18/100, Acc=0.9060, Val Loss=0.3111, lr=0.0100
[02/20 21:59:15 cifar10-global-l2-2.0-resnet56]: Epoch 19/100, Acc=0.8970, Val Loss=0.3537, lr=0.0100
[02/20 21:59:48 cifar10-global-l2-2.0-resnet56]: Epoch 20/100, Acc=0.9016, Val Loss=0.3207, lr=0.0100
[02/20 22:00:22 cifar10-global-l2-2.0-resnet56]: Epoch 21/100, Acc=0.9001, Val Loss=0.3424, lr=0.0100
[02/20 22:00:55 cifar10-global-l2-2.0-resnet56]: Epoch 22/100, Acc=0.8941, Val Loss=0.3825, lr=0.0100
[02/20 22:01:28 cifar10-global-l2-2.0-resnet56]: Epoch 23/100, Acc=0.9118, Val Loss=0.2928, lr=0.0100
[02/20 22:02:01 cifar10-global-l2-2.0-resnet56]: Epoch 24/100, Acc=0.9032, Val Loss=0.3353, lr=0.0100
[02/20 22:02:34 cifar10-global-l2-2.0-resnet56]: Epoch 25/100, Acc=0.9121, Val Loss=0.3062, lr=0.0100
[02/20 22:03:06 cifar10-global-l2-2.0-resnet56]: Epoch 26/100, Acc=0.8998, Val Loss=0.3569, lr=0.0100
[02/20 22:03:39 cifar10-global-l2-2.0-resnet56]: Epoch 27/100, Acc=0.9039, Val Loss=0.3329, lr=0.0100
[02/20 22:04:12 cifar10-global-l2-2.0-resnet56]: Epoch 28/100, Acc=0.9087, Val Loss=0.3067, lr=0.0100
[02/20 22:04:45 cifar10-global-l2-2.0-resnet56]: Epoch 29/100, Acc=0.9104, Val Loss=0.3116, lr=0.0100
[02/20 22:05:17 cifar10-global-l2-2.0-resnet56]: Epoch 30/100, Acc=0.9064, Val Loss=0.3296, lr=0.0100
[02/20 22:05:50 cifar10-global-l2-2.0-resnet56]: Epoch 31/100, Acc=0.9124, Val Loss=0.3013, lr=0.0100
[02/20 22:06:23 cifar10-global-l2-2.0-resnet56]: Epoch 32/100, Acc=0.9046, Val Loss=0.3164, lr=0.0100
[02/20 22:06:56 cifar10-global-l2-2.0-resnet56]: Epoch 33/100, Acc=0.9052, Val Loss=0.3151, lr=0.0100
[02/20 22:07:30 cifar10-global-l2-2.0-resnet56]: Epoch 34/100, Acc=0.9015, Val Loss=0.3304, lr=0.0100
[02/20 22:08:03 cifar10-global-l2-2.0-resnet56]: Epoch 35/100, Acc=0.9022, Val Loss=0.3164, lr=0.0100
[02/20 22:08:37 cifar10-global-l2-2.0-resnet56]: Epoch 36/100, Acc=0.9051, Val Loss=0.3349, lr=0.0100
[02/20 22:09:10 cifar10-global-l2-2.0-resnet56]: Epoch 37/100, Acc=0.8994, Val Loss=0.3607, lr=0.0100
[02/20 22:09:43 cifar10-global-l2-2.0-resnet56]: Epoch 38/100, Acc=0.9025, Val Loss=0.3537, lr=0.0100
[02/20 22:10:16 cifar10-global-l2-2.0-resnet56]: Epoch 39/100, Acc=0.9024, Val Loss=0.3379, lr=0.0100
[02/20 22:10:49 cifar10-global-l2-2.0-resnet56]: Epoch 40/100, Acc=0.9056, Val Loss=0.3371, lr=0.0100
[02/20 22:11:23 cifar10-global-l2-2.0-resnet56]: Epoch 41/100, Acc=0.9136, Val Loss=0.3029, lr=0.0100
[02/20 22:11:56 cifar10-global-l2-2.0-resnet56]: Epoch 42/100, Acc=0.9160, Val Loss=0.2885, lr=0.0100
[02/20 22:12:30 cifar10-global-l2-2.0-resnet56]: Epoch 43/100, Acc=0.9029, Val Loss=0.3422, lr=0.0100
[02/20 22:13:03 cifar10-global-l2-2.0-resnet56]: Epoch 44/100, Acc=0.8947, Val Loss=0.3530, lr=0.0100
[02/20 22:13:37 cifar10-global-l2-2.0-resnet56]: Epoch 45/100, Acc=0.9107, Val Loss=0.3090, lr=0.0100
[02/20 22:14:10 cifar10-global-l2-2.0-resnet56]: Epoch 46/100, Acc=0.9028, Val Loss=0.3327, lr=0.0100
[02/20 22:14:44 cifar10-global-l2-2.0-resnet56]: Epoch 47/100, Acc=0.9055, Val Loss=0.3396, lr=0.0100
[02/20 22:15:17 cifar10-global-l2-2.0-resnet56]: Epoch 48/100, Acc=0.8938, Val Loss=0.4007, lr=0.0100
[02/20 22:15:51 cifar10-global-l2-2.0-resnet56]: Epoch 49/100, Acc=0.9070, Val Loss=0.3099, lr=0.0100
[02/20 22:16:24 cifar10-global-l2-2.0-resnet56]: Epoch 50/100, Acc=0.9102, Val Loss=0.3238, lr=0.0100
[02/20 22:16:57 cifar10-global-l2-2.0-resnet56]: Epoch 51/100, Acc=0.9041, Val Loss=0.3442, lr=0.0100
[02/20 22:17:30 cifar10-global-l2-2.0-resnet56]: Epoch 52/100, Acc=0.9084, Val Loss=0.3282, lr=0.0100
[02/20 22:18:04 cifar10-global-l2-2.0-resnet56]: Epoch 53/100, Acc=0.9167, Val Loss=0.2907, lr=0.0100
[02/20 22:18:37 cifar10-global-l2-2.0-resnet56]: Epoch 54/100, Acc=0.8963, Val Loss=0.3678, lr=0.0100
[02/20 22:19:10 cifar10-global-l2-2.0-resnet56]: Epoch 55/100, Acc=0.9101, Val Loss=0.3189, lr=0.0100
[02/20 22:19:43 cifar10-global-l2-2.0-resnet56]: Epoch 56/100, Acc=0.9063, Val Loss=0.3376, lr=0.0100
[02/20 22:20:16 cifar10-global-l2-2.0-resnet56]: Epoch 57/100, Acc=0.9088, Val Loss=0.3129, lr=0.0100
[02/20 22:20:49 cifar10-global-l2-2.0-resnet56]: Epoch 58/100, Acc=0.9082, Val Loss=0.3221, lr=0.0100
[02/20 22:21:22 cifar10-global-l2-2.0-resnet56]: Epoch 59/100, Acc=0.8975, Val Loss=0.3972, lr=0.0100
[02/20 22:21:56 cifar10-global-l2-2.0-resnet56]: Epoch 60/100, Acc=0.9275, Val Loss=0.2564, lr=0.0010
[02/20 22:22:29 cifar10-global-l2-2.0-resnet56]: Epoch 61/100, Acc=0.9287, Val Loss=0.2514, lr=0.0010
[02/20 22:23:02 cifar10-global-l2-2.0-resnet56]: Epoch 62/100, Acc=0.9301, Val Loss=0.2534, lr=0.0010
[02/20 22:23:35 cifar10-global-l2-2.0-resnet56]: Epoch 63/100, Acc=0.9298, Val Loss=0.2539, lr=0.0010
[02/20 22:24:08 cifar10-global-l2-2.0-resnet56]: Epoch 64/100, Acc=0.9316, Val Loss=0.2547, lr=0.0010
[02/20 22:24:42 cifar10-global-l2-2.0-resnet56]: Epoch 65/100, Acc=0.9304, Val Loss=0.2528, lr=0.0010
[02/20 22:25:16 cifar10-global-l2-2.0-resnet56]: Epoch 66/100, Acc=0.9323, Val Loss=0.2542, lr=0.0010
[02/20 22:25:49 cifar10-global-l2-2.0-resnet56]: Epoch 67/100, Acc=0.9310, Val Loss=0.2566, lr=0.0010
[02/20 22:26:23 cifar10-global-l2-2.0-resnet56]: Epoch 68/100, Acc=0.9301, Val Loss=0.2607, lr=0.0010
[02/20 22:26:56 cifar10-global-l2-2.0-resnet56]: Epoch 69/100, Acc=0.9321, Val Loss=0.2552, lr=0.0010
[02/20 22:27:30 cifar10-global-l2-2.0-resnet56]: Epoch 70/100, Acc=0.9315, Val Loss=0.2613, lr=0.0010
[02/20 22:28:04 cifar10-global-l2-2.0-resnet56]: Epoch 71/100, Acc=0.9335, Val Loss=0.2571, lr=0.0010
[02/20 22:28:37 cifar10-global-l2-2.0-resnet56]: Epoch 72/100, Acc=0.9331, Val Loss=0.2598, lr=0.0010
[02/20 22:29:11 cifar10-global-l2-2.0-resnet56]: Epoch 73/100, Acc=0.9338, Val Loss=0.2581, lr=0.0010
[02/20 22:29:44 cifar10-global-l2-2.0-resnet56]: Epoch 74/100, Acc=0.9331, Val Loss=0.2596, lr=0.0010
[02/20 22:30:17 cifar10-global-l2-2.0-resnet56]: Epoch 75/100, Acc=0.9326, Val Loss=0.2597, lr=0.0010
[02/20 22:30:50 cifar10-global-l2-2.0-resnet56]: Epoch 76/100, Acc=0.9318, Val Loss=0.2632, lr=0.0010
[02/20 22:31:23 cifar10-global-l2-2.0-resnet56]: Epoch 77/100, Acc=0.9321, Val Loss=0.2646, lr=0.0010
[02/20 22:31:56 cifar10-global-l2-2.0-resnet56]: Epoch 78/100, Acc=0.9329, Val Loss=0.2621, lr=0.0010
[02/20 22:32:29 cifar10-global-l2-2.0-resnet56]: Epoch 79/100, Acc=0.9336, Val Loss=0.2621, lr=0.0010
[02/20 22:33:02 cifar10-global-l2-2.0-resnet56]: Epoch 80/100, Acc=0.9344, Val Loss=0.2644, lr=0.0001
[02/20 22:33:36 cifar10-global-l2-2.0-resnet56]: Epoch 81/100, Acc=0.9338, Val Loss=0.2609, lr=0.0001
[02/20 22:34:10 cifar10-global-l2-2.0-resnet56]: Epoch 82/100, Acc=0.9338, Val Loss=0.2614, lr=0.0001
[02/20 22:34:43 cifar10-global-l2-2.0-resnet56]: Epoch 83/100, Acc=0.9341, Val Loss=0.2638, lr=0.0001
[02/20 22:35:17 cifar10-global-l2-2.0-resnet56]: Epoch 84/100, Acc=0.9341, Val Loss=0.2632, lr=0.0001
[02/20 22:35:50 cifar10-global-l2-2.0-resnet56]: Epoch 85/100, Acc=0.9336, Val Loss=0.2646, lr=0.0001
[02/20 22:36:24 cifar10-global-l2-2.0-resnet56]: Epoch 86/100, Acc=0.9333, Val Loss=0.2633, lr=0.0001
[02/20 22:36:57 cifar10-global-l2-2.0-resnet56]: Epoch 87/100, Acc=0.9340, Val Loss=0.2655, lr=0.0001
[02/20 22:37:31 cifar10-global-l2-2.0-resnet56]: Epoch 88/100, Acc=0.9338, Val Loss=0.2611, lr=0.0001
[02/20 22:38:04 cifar10-global-l2-2.0-resnet56]: Epoch 89/100, Acc=0.9337, Val Loss=0.2624, lr=0.0001
[02/20 22:38:37 cifar10-global-l2-2.0-resnet56]: Epoch 90/100, Acc=0.9343, Val Loss=0.2645, lr=0.0001
[02/20 22:39:10 cifar10-global-l2-2.0-resnet56]: Epoch 91/100, Acc=0.9324, Val Loss=0.2631, lr=0.0001
[02/20 22:39:44 cifar10-global-l2-2.0-resnet56]: Epoch 92/100, Acc=0.9333, Val Loss=0.2625, lr=0.0001
[02/20 22:40:17 cifar10-global-l2-2.0-resnet56]: Epoch 93/100, Acc=0.9337, Val Loss=0.2612, lr=0.0001
[02/20 22:40:51 cifar10-global-l2-2.0-resnet56]: Epoch 94/100, Acc=0.9338, Val Loss=0.2632, lr=0.0001
[02/20 22:41:25 cifar10-global-l2-2.0-resnet56]: Epoch 95/100, Acc=0.9345, Val Loss=0.2642, lr=0.0001
[02/20 22:41:58 cifar10-global-l2-2.0-resnet56]: Epoch 96/100, Acc=0.9331, Val Loss=0.2634, lr=0.0001
[02/20 22:42:32 cifar10-global-l2-2.0-resnet56]: Epoch 97/100, Acc=0.9343, Val Loss=0.2637, lr=0.0001
[02/20 22:43:05 cifar10-global-l2-2.0-resnet56]: Epoch 98/100, Acc=0.9337, Val Loss=0.2638, lr=0.0001
[02/20 22:43:40 cifar10-global-l2-2.0-resnet56]: Epoch 99/100, Acc=0.9346, Val Loss=0.2651, lr=0.0001
[02/20 22:43:40 cifar10-global-l2-2.0-resnet56]: Best Acc=0.9346
[02/20 22:43:40 cifar10-global-l2-2.0-resnet56]: Params: 0.56 M
[02/20 22:43:40 cifar10-global-l2-2.0-resnet56]: ops: 63.06 M
[02/20 22:43:44 cifar10-global-l2-2.0-resnet56]: Acc: 0.9346 Val Loss: 0.2651

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: mode: prune
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: model: resnet56
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: verbose: False
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: dataset: cifar10
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: dataroot: data
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: batch_size: 128
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: total_epochs: 100
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: lr_decay_milestones: 60,80
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: lr_decay_gamma: 0.1
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: lr: 0.01
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: output_dir: run/cifar10/prune/cifar10-global-fpgm-2.0-resnet56
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: finetune: True
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: last_epochs: 100
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: reps: 1
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: method: fpgm
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: speed_up: 2.0
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: max_pruning_ratio: 1.0
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: soft_keeping_ratio: 0.0
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: reg: 1e-05
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: delta_reg: 0.0001
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: weight_decay: 0.0005
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: seed: 1
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: global_pruning: True
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: sl_total_epochs: 100
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: sl_lr: 0.01
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: sl_reg_warmup: 0
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: sl_restore: None
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: iterative_steps: 400
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: logger: <Logger cifar10-global-fpgm-2.0-resnet56 (DEBUG)>
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: device: cuda
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: num_classes: 10
[02/20 22:43:52 cifar10-global-fpgm-2.0-resnet56]: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/20 22:43:56 cifar10-global-fpgm-2.0-resnet56]: Pruning...
[02/20 22:44:11 cifar10-global-fpgm-2.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(15, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(15, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(15, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(15, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(15, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(15, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(15, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(15, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(15, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(15, 25, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(15, 24, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(24, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(21, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(24, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(24, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(24, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(24, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(24, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(24, 36, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(36, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(36, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(57, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(36, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(36, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(53, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(36, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(36, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(53, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(36, 45, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(45, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=36, out_features=10, bias=True)
)
[02/20 22:44:14 cifar10-global-fpgm-2.0-resnet56]: Params: 0.86 M => 0.40 M (46.84%)
[02/20 22:44:14 cifar10-global-fpgm-2.0-resnet56]: FLOPs: 127.12 M => 62.99 M (49.55%, 2.02X )
[02/20 22:44:14 cifar10-global-fpgm-2.0-resnet56]: Acc: 0.9392 => 0.3201
[02/20 22:44:14 cifar10-global-fpgm-2.0-resnet56]: Val Loss: 0.2587 => 2.3332
[02/20 22:44:14 cifar10-global-fpgm-2.0-resnet56]: Finetuning...
[02/20 22:44:48 cifar10-global-fpgm-2.0-resnet56]: Epoch 0/100, Acc=0.8561, Val Loss=0.4425, lr=0.0100
[02/20 22:45:22 cifar10-global-fpgm-2.0-resnet56]: Epoch 1/100, Acc=0.8901, Val Loss=0.3474, lr=0.0100
[02/20 22:45:56 cifar10-global-fpgm-2.0-resnet56]: Epoch 2/100, Acc=0.8949, Val Loss=0.3222, lr=0.0100
[02/20 22:46:30 cifar10-global-fpgm-2.0-resnet56]: Epoch 3/100, Acc=0.8796, Val Loss=0.3746, lr=0.0100
[02/20 22:47:04 cifar10-global-fpgm-2.0-resnet56]: Epoch 4/100, Acc=0.8852, Val Loss=0.3478, lr=0.0100
[02/20 22:47:38 cifar10-global-fpgm-2.0-resnet56]: Epoch 5/100, Acc=0.8762, Val Loss=0.4083, lr=0.0100
[02/20 22:48:12 cifar10-global-fpgm-2.0-resnet56]: Epoch 6/100, Acc=0.8921, Val Loss=0.3394, lr=0.0100
[02/20 22:48:46 cifar10-global-fpgm-2.0-resnet56]: Epoch 7/100, Acc=0.8734, Val Loss=0.4232, lr=0.0100
[02/20 22:49:20 cifar10-global-fpgm-2.0-resnet56]: Epoch 8/100, Acc=0.9035, Val Loss=0.3097, lr=0.0100
[02/20 22:49:55 cifar10-global-fpgm-2.0-resnet56]: Epoch 9/100, Acc=0.8818, Val Loss=0.3888, lr=0.0100
[02/20 22:50:29 cifar10-global-fpgm-2.0-resnet56]: Epoch 10/100, Acc=0.8875, Val Loss=0.3550, lr=0.0100
[02/20 22:51:03 cifar10-global-fpgm-2.0-resnet56]: Epoch 11/100, Acc=0.9024, Val Loss=0.3128, lr=0.0100
[02/20 22:51:37 cifar10-global-fpgm-2.0-resnet56]: Epoch 12/100, Acc=0.8916, Val Loss=0.3534, lr=0.0100
[02/20 22:52:11 cifar10-global-fpgm-2.0-resnet56]: Epoch 13/100, Acc=0.8927, Val Loss=0.3279, lr=0.0100
[02/20 22:52:45 cifar10-global-fpgm-2.0-resnet56]: Epoch 14/100, Acc=0.9030, Val Loss=0.3099, lr=0.0100
[02/20 22:53:18 cifar10-global-fpgm-2.0-resnet56]: Epoch 15/100, Acc=0.8990, Val Loss=0.3364, lr=0.0100
[02/20 22:53:51 cifar10-global-fpgm-2.0-resnet56]: Epoch 16/100, Acc=0.8956, Val Loss=0.3406, lr=0.0100
[02/20 22:54:24 cifar10-global-fpgm-2.0-resnet56]: Epoch 17/100, Acc=0.8972, Val Loss=0.3317, lr=0.0100
[02/20 22:54:58 cifar10-global-fpgm-2.0-resnet56]: Epoch 18/100, Acc=0.8972, Val Loss=0.3289, lr=0.0100
[02/20 22:55:31 cifar10-global-fpgm-2.0-resnet56]: Epoch 19/100, Acc=0.8922, Val Loss=0.3331, lr=0.0100
[02/20 22:56:04 cifar10-global-fpgm-2.0-resnet56]: Epoch 20/100, Acc=0.8923, Val Loss=0.3465, lr=0.0100
[02/20 22:56:38 cifar10-global-fpgm-2.0-resnet56]: Epoch 21/100, Acc=0.9020, Val Loss=0.3158, lr=0.0100
[02/20 22:57:11 cifar10-global-fpgm-2.0-resnet56]: Epoch 22/100, Acc=0.9016, Val Loss=0.3234, lr=0.0100
[02/20 22:57:44 cifar10-global-fpgm-2.0-resnet56]: Epoch 23/100, Acc=0.8905, Val Loss=0.3600, lr=0.0100
[02/20 22:58:18 cifar10-global-fpgm-2.0-resnet56]: Epoch 24/100, Acc=0.8996, Val Loss=0.3312, lr=0.0100
[02/20 22:58:51 cifar10-global-fpgm-2.0-resnet56]: Epoch 25/100, Acc=0.8957, Val Loss=0.3408, lr=0.0100
[02/20 22:59:24 cifar10-global-fpgm-2.0-resnet56]: Epoch 26/100, Acc=0.9024, Val Loss=0.3214, lr=0.0100
[02/20 22:59:58 cifar10-global-fpgm-2.0-resnet56]: Epoch 27/100, Acc=0.8885, Val Loss=0.3652, lr=0.0100
[02/20 23:00:32 cifar10-global-fpgm-2.0-resnet56]: Epoch 28/100, Acc=0.8973, Val Loss=0.3518, lr=0.0100
[02/20 23:01:05 cifar10-global-fpgm-2.0-resnet56]: Epoch 29/100, Acc=0.9020, Val Loss=0.3255, lr=0.0100
[02/20 23:01:39 cifar10-global-fpgm-2.0-resnet56]: Epoch 30/100, Acc=0.8887, Val Loss=0.3743, lr=0.0100
[02/20 23:02:13 cifar10-global-fpgm-2.0-resnet56]: Epoch 31/100, Acc=0.9033, Val Loss=0.3107, lr=0.0100
[02/20 23:02:46 cifar10-global-fpgm-2.0-resnet56]: Epoch 32/100, Acc=0.8991, Val Loss=0.3316, lr=0.0100
[02/20 23:03:19 cifar10-global-fpgm-2.0-resnet56]: Epoch 33/100, Acc=0.8973, Val Loss=0.3329, lr=0.0100
[02/20 23:03:53 cifar10-global-fpgm-2.0-resnet56]: Epoch 34/100, Acc=0.8996, Val Loss=0.3480, lr=0.0100
[02/20 23:04:26 cifar10-global-fpgm-2.0-resnet56]: Epoch 35/100, Acc=0.8861, Val Loss=0.3927, lr=0.0100
[02/20 23:05:00 cifar10-global-fpgm-2.0-resnet56]: Epoch 36/100, Acc=0.8932, Val Loss=0.3708, lr=0.0100
[02/20 23:05:34 cifar10-global-fpgm-2.0-resnet56]: Epoch 37/100, Acc=0.8768, Val Loss=0.4395, lr=0.0100
[02/20 23:06:08 cifar10-global-fpgm-2.0-resnet56]: Epoch 38/100, Acc=0.9017, Val Loss=0.3369, lr=0.0100
[02/20 23:06:43 cifar10-global-fpgm-2.0-resnet56]: Epoch 39/100, Acc=0.8985, Val Loss=0.3527, lr=0.0100
[02/20 23:07:23 cifar10-global-fpgm-2.0-resnet56]: Epoch 40/100, Acc=0.8894, Val Loss=0.3811, lr=0.0100
[02/20 23:07:57 cifar10-global-fpgm-2.0-resnet56]: Epoch 41/100, Acc=0.9023, Val Loss=0.3334, lr=0.0100
[02/20 23:08:31 cifar10-global-fpgm-2.0-resnet56]: Epoch 42/100, Acc=0.8924, Val Loss=0.3703, lr=0.0100
[02/20 23:09:04 cifar10-global-fpgm-2.0-resnet56]: Epoch 43/100, Acc=0.9001, Val Loss=0.3417, lr=0.0100
[02/20 23:09:38 cifar10-global-fpgm-2.0-resnet56]: Epoch 44/100, Acc=0.9055, Val Loss=0.3194, lr=0.0100
[02/20 23:10:11 cifar10-global-fpgm-2.0-resnet56]: Epoch 45/100, Acc=0.9013, Val Loss=0.3476, lr=0.0100
[02/20 23:10:45 cifar10-global-fpgm-2.0-resnet56]: Epoch 46/100, Acc=0.9037, Val Loss=0.3135, lr=0.0100
[02/20 23:11:18 cifar10-global-fpgm-2.0-resnet56]: Epoch 47/100, Acc=0.9025, Val Loss=0.3438, lr=0.0100
[02/20 23:11:52 cifar10-global-fpgm-2.0-resnet56]: Epoch 48/100, Acc=0.9015, Val Loss=0.3380, lr=0.0100
[02/20 23:12:25 cifar10-global-fpgm-2.0-resnet56]: Epoch 49/100, Acc=0.8990, Val Loss=0.3390, lr=0.0100
[02/20 23:12:59 cifar10-global-fpgm-2.0-resnet56]: Epoch 50/100, Acc=0.8952, Val Loss=0.3585, lr=0.0100
[02/20 23:13:32 cifar10-global-fpgm-2.0-resnet56]: Epoch 51/100, Acc=0.9007, Val Loss=0.3325, lr=0.0100
[02/20 23:14:06 cifar10-global-fpgm-2.0-resnet56]: Epoch 52/100, Acc=0.8906, Val Loss=0.3857, lr=0.0100
[02/20 23:14:40 cifar10-global-fpgm-2.0-resnet56]: Epoch 53/100, Acc=0.8960, Val Loss=0.3437, lr=0.0100
[02/20 23:15:14 cifar10-global-fpgm-2.0-resnet56]: Epoch 54/100, Acc=0.8941, Val Loss=0.3425, lr=0.0100
[02/20 23:15:47 cifar10-global-fpgm-2.0-resnet56]: Epoch 55/100, Acc=0.8978, Val Loss=0.3602, lr=0.0100
[02/20 23:16:21 cifar10-global-fpgm-2.0-resnet56]: Epoch 56/100, Acc=0.9013, Val Loss=0.3373, lr=0.0100
[02/20 23:16:55 cifar10-global-fpgm-2.0-resnet56]: Epoch 57/100, Acc=0.9072, Val Loss=0.3273, lr=0.0100
[02/20 23:17:29 cifar10-global-fpgm-2.0-resnet56]: Epoch 58/100, Acc=0.8891, Val Loss=0.3910, lr=0.0100
[02/20 23:18:02 cifar10-global-fpgm-2.0-resnet56]: Epoch 59/100, Acc=0.9106, Val Loss=0.3176, lr=0.0100
[02/20 23:18:36 cifar10-global-fpgm-2.0-resnet56]: Epoch 60/100, Acc=0.9259, Val Loss=0.2560, lr=0.0010
[02/20 23:19:10 cifar10-global-fpgm-2.0-resnet56]: Epoch 61/100, Acc=0.9288, Val Loss=0.2580, lr=0.0010
[02/20 23:19:43 cifar10-global-fpgm-2.0-resnet56]: Epoch 62/100, Acc=0.9289, Val Loss=0.2608, lr=0.0010
[02/20 23:20:17 cifar10-global-fpgm-2.0-resnet56]: Epoch 63/100, Acc=0.9302, Val Loss=0.2578, lr=0.0010
[02/20 23:20:50 cifar10-global-fpgm-2.0-resnet56]: Epoch 64/100, Acc=0.9299, Val Loss=0.2581, lr=0.0010
[02/20 23:21:24 cifar10-global-fpgm-2.0-resnet56]: Epoch 65/100, Acc=0.9306, Val Loss=0.2582, lr=0.0010
[02/20 23:21:58 cifar10-global-fpgm-2.0-resnet56]: Epoch 66/100, Acc=0.9305, Val Loss=0.2589, lr=0.0010
[02/20 23:22:31 cifar10-global-fpgm-2.0-resnet56]: Epoch 67/100, Acc=0.9302, Val Loss=0.2631, lr=0.0010
[02/20 23:23:05 cifar10-global-fpgm-2.0-resnet56]: Epoch 68/100, Acc=0.9307, Val Loss=0.2619, lr=0.0010
[02/20 23:23:39 cifar10-global-fpgm-2.0-resnet56]: Epoch 69/100, Acc=0.9326, Val Loss=0.2632, lr=0.0010
[02/20 23:24:12 cifar10-global-fpgm-2.0-resnet56]: Epoch 70/100, Acc=0.9310, Val Loss=0.2668, lr=0.0010
[02/20 23:24:46 cifar10-global-fpgm-2.0-resnet56]: Epoch 71/100, Acc=0.9322, Val Loss=0.2657, lr=0.0010
[02/20 23:25:20 cifar10-global-fpgm-2.0-resnet56]: Epoch 72/100, Acc=0.9330, Val Loss=0.2681, lr=0.0010
[02/20 23:25:53 cifar10-global-fpgm-2.0-resnet56]: Epoch 73/100, Acc=0.9323, Val Loss=0.2701, lr=0.0010
[02/20 23:26:26 cifar10-global-fpgm-2.0-resnet56]: Epoch 74/100, Acc=0.9319, Val Loss=0.2718, lr=0.0010
[02/20 23:26:59 cifar10-global-fpgm-2.0-resnet56]: Epoch 75/100, Acc=0.9321, Val Loss=0.2742, lr=0.0010
[02/20 23:27:33 cifar10-global-fpgm-2.0-resnet56]: Epoch 76/100, Acc=0.9318, Val Loss=0.2742, lr=0.0010
[02/20 23:28:07 cifar10-global-fpgm-2.0-resnet56]: Epoch 77/100, Acc=0.9311, Val Loss=0.2734, lr=0.0010
[02/20 23:28:46 cifar10-global-fpgm-2.0-resnet56]: Epoch 78/100, Acc=0.9322, Val Loss=0.2748, lr=0.0010
[02/20 23:29:25 cifar10-global-fpgm-2.0-resnet56]: Epoch 79/100, Acc=0.9307, Val Loss=0.2798, lr=0.0010
[02/20 23:29:59 cifar10-global-fpgm-2.0-resnet56]: Epoch 80/100, Acc=0.9339, Val Loss=0.2796, lr=0.0001
[02/20 23:30:32 cifar10-global-fpgm-2.0-resnet56]: Epoch 81/100, Acc=0.9325, Val Loss=0.2767, lr=0.0001
[02/20 23:31:05 cifar10-global-fpgm-2.0-resnet56]: Epoch 82/100, Acc=0.9326, Val Loss=0.2770, lr=0.0001
[02/20 23:31:39 cifar10-global-fpgm-2.0-resnet56]: Epoch 83/100, Acc=0.9331, Val Loss=0.2788, lr=0.0001
[02/20 23:32:12 cifar10-global-fpgm-2.0-resnet56]: Epoch 84/100, Acc=0.9332, Val Loss=0.2781, lr=0.0001
[02/20 23:32:45 cifar10-global-fpgm-2.0-resnet56]: Epoch 85/100, Acc=0.9329, Val Loss=0.2787, lr=0.0001
[02/20 23:33:19 cifar10-global-fpgm-2.0-resnet56]: Epoch 86/100, Acc=0.9332, Val Loss=0.2772, lr=0.0001
[02/20 23:33:52 cifar10-global-fpgm-2.0-resnet56]: Epoch 87/100, Acc=0.9319, Val Loss=0.2786, lr=0.0001
[02/20 23:34:26 cifar10-global-fpgm-2.0-resnet56]: Epoch 88/100, Acc=0.9328, Val Loss=0.2750, lr=0.0001
[02/20 23:35:00 cifar10-global-fpgm-2.0-resnet56]: Epoch 89/100, Acc=0.9339, Val Loss=0.2769, lr=0.0001
[02/20 23:35:34 cifar10-global-fpgm-2.0-resnet56]: Epoch 90/100, Acc=0.9314, Val Loss=0.2777, lr=0.0001
[02/20 23:36:07 cifar10-global-fpgm-2.0-resnet56]: Epoch 91/100, Acc=0.9335, Val Loss=0.2768, lr=0.0001
[02/20 23:36:41 cifar10-global-fpgm-2.0-resnet56]: Epoch 92/100, Acc=0.9328, Val Loss=0.2770, lr=0.0001
[02/20 23:37:14 cifar10-global-fpgm-2.0-resnet56]: Epoch 93/100, Acc=0.9324, Val Loss=0.2767, lr=0.0001
[02/20 23:37:48 cifar10-global-fpgm-2.0-resnet56]: Epoch 94/100, Acc=0.9331, Val Loss=0.2780, lr=0.0001
[02/20 23:38:21 cifar10-global-fpgm-2.0-resnet56]: Epoch 95/100, Acc=0.9340, Val Loss=0.2782, lr=0.0001
[02/20 23:38:54 cifar10-global-fpgm-2.0-resnet56]: Epoch 96/100, Acc=0.9327, Val Loss=0.2766, lr=0.0001
[02/20 23:39:27 cifar10-global-fpgm-2.0-resnet56]: Epoch 97/100, Acc=0.9329, Val Loss=0.2777, lr=0.0001
[02/20 23:40:00 cifar10-global-fpgm-2.0-resnet56]: Epoch 98/100, Acc=0.9339, Val Loss=0.2770, lr=0.0001
[02/20 23:40:33 cifar10-global-fpgm-2.0-resnet56]: Epoch 99/100, Acc=0.9317, Val Loss=0.2790, lr=0.0001
[02/20 23:40:33 cifar10-global-fpgm-2.0-resnet56]: Best Acc=0.9340
[02/20 23:40:33 cifar10-global-fpgm-2.0-resnet56]: Params: 0.40 M
[02/20 23:40:33 cifar10-global-fpgm-2.0-resnet56]: ops: 62.99 M
[02/20 23:40:36 cifar10-global-fpgm-2.0-resnet56]: Acc: 0.9317 Val Loss: 0.2790

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: mode: prune
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: model: resnet56
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: verbose: False
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: dataset: cifar10
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: dataroot: data
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: batch_size: 128
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: total_epochs: 100
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: lr_decay_milestones: 60,80
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: lr_decay_gamma: 0.1
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: lr: 0.01
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: output_dir: run/cifar10/prune/cifar10-global-obdc-2.0-resnet56
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: finetune: True
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: last_epochs: 100
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: reps: 1
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: method: obdc
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: speed_up: 2.0
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: max_pruning_ratio: 1.0
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: soft_keeping_ratio: 0.0
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: reg: 1e-05
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: delta_reg: 0.0001
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: weight_decay: 0.0005
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: seed: 1
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: global_pruning: True
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: sl_total_epochs: 100
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: sl_lr: 0.01
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: sl_reg_warmup: 0
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: sl_restore: None
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: iterative_steps: 400
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: logger: <Logger cifar10-global-obdc-2.0-resnet56 (DEBUG)>
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: device: cuda
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: num_classes: 10
[02/20 23:40:44 cifar10-global-obdc-2.0-resnet56]: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/20 23:40:47 cifar10-global-obdc-2.0-resnet56]: Pruning...
Traceback (most recent call last):
  File "/home/REDACTED/paper1/main.py", line 429, in <module>
    main()
  File "/home/REDACTED/paper1/main.py", line 369, in main
    progressive_pruning(
  File "/home/REDACTED/paper1/main.py", line 65, in progressive_pruning
    imp._prepare_model(model, pruner)
  File "/home/REDACTED/paper1/torch_pruning/pruner/importance.py", line 872, in _prepare_model
    for i, (dep, idxs) in enumerate(group):
TypeError: 'NoneType' object is not iterable
/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: mode: prune
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: model: resnet56
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: verbose: False
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: dataset: cifar10
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: dataroot: data
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: batch_size: 128
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: total_epochs: 100
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: lr_decay_milestones: 60,80
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: lr_decay_gamma: 0.1
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: lr: 0.01
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: output_dir: run/cifar10/prune/cifar10-global-lamp-2.0-resnet56
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: finetune: True
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: last_epochs: 100
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: reps: 1
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: method: lamp
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: speed_up: 2.0
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: max_pruning_ratio: 1.0
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: soft_keeping_ratio: 0.0
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: reg: 1e-05
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: delta_reg: 0.0001
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: weight_decay: 0.0005
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: seed: 1
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: global_pruning: True
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: sl_total_epochs: 100
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: sl_lr: 0.01
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: sl_reg_warmup: 0
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: sl_restore: None
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: iterative_steps: 400
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: logger: <Logger cifar10-global-lamp-2.0-resnet56 (DEBUG)>
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: device: cuda
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: num_classes: 10
[02/20 23:40:53 cifar10-global-lamp-2.0-resnet56]: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/20 23:40:56 cifar10-global-lamp-2.0-resnet56]: Pruning...
[02/20 23:41:19 cifar10-global-lamp-2.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(16, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 25, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 27, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(27, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(27, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(27, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(27, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(27, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(21, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(27, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(27, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(27, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(27, 31, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(31, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(27, 28, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(28, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(31, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(28, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(30, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(28, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(29, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(28, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(29, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(28, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(29, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(28, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(26, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(28, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(28, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=28, out_features=10, bias=True)
)
[02/20 23:41:22 cifar10-global-lamp-2.0-resnet56]: Params: 0.86 M => 0.25 M (28.95%)
[02/20 23:41:22 cifar10-global-lamp-2.0-resnet56]: FLOPs: 127.12 M => 63.48 M (49.94%, 2.00X )
[02/20 23:41:22 cifar10-global-lamp-2.0-resnet56]: Acc: 0.9392 => 0.2653
[02/20 23:41:22 cifar10-global-lamp-2.0-resnet56]: Val Loss: 0.2587 => 2.0727
[02/20 23:41:22 cifar10-global-lamp-2.0-resnet56]: Finetuning...
[02/20 23:41:56 cifar10-global-lamp-2.0-resnet56]: Epoch 0/100, Acc=0.8219, Val Loss=0.5363, lr=0.0100
[02/20 23:42:31 cifar10-global-lamp-2.0-resnet56]: Epoch 1/100, Acc=0.8630, Val Loss=0.4119, lr=0.0100
[02/20 23:43:05 cifar10-global-lamp-2.0-resnet56]: Epoch 2/100, Acc=0.8749, Val Loss=0.3718, lr=0.0100
[02/20 23:43:39 cifar10-global-lamp-2.0-resnet56]: Epoch 3/100, Acc=0.8857, Val Loss=0.3386, lr=0.0100
[02/20 23:44:13 cifar10-global-lamp-2.0-resnet56]: Epoch 4/100, Acc=0.8844, Val Loss=0.3612, lr=0.0100
[02/20 23:44:47 cifar10-global-lamp-2.0-resnet56]: Epoch 5/100, Acc=0.8728, Val Loss=0.4066, lr=0.0100
[02/20 23:45:21 cifar10-global-lamp-2.0-resnet56]: Epoch 6/100, Acc=0.8658, Val Loss=0.4204, lr=0.0100
[02/20 23:45:56 cifar10-global-lamp-2.0-resnet56]: Epoch 7/100, Acc=0.8912, Val Loss=0.3381, lr=0.0100
[02/20 23:46:30 cifar10-global-lamp-2.0-resnet56]: Epoch 8/100, Acc=0.8820, Val Loss=0.3563, lr=0.0100
[02/20 23:47:04 cifar10-global-lamp-2.0-resnet56]: Epoch 9/100, Acc=0.8819, Val Loss=0.3773, lr=0.0100
[02/20 23:47:39 cifar10-global-lamp-2.0-resnet56]: Epoch 10/100, Acc=0.8622, Val Loss=0.4274, lr=0.0100
[02/20 23:48:14 cifar10-global-lamp-2.0-resnet56]: Epoch 11/100, Acc=0.8912, Val Loss=0.3393, lr=0.0100
[02/20 23:48:49 cifar10-global-lamp-2.0-resnet56]: Epoch 12/100, Acc=0.8791, Val Loss=0.3810, lr=0.0100
[02/20 23:49:23 cifar10-global-lamp-2.0-resnet56]: Epoch 13/100, Acc=0.8899, Val Loss=0.3418, lr=0.0100
[02/20 23:49:58 cifar10-global-lamp-2.0-resnet56]: Epoch 14/100, Acc=0.8831, Val Loss=0.3650, lr=0.0100
[02/20 23:50:32 cifar10-global-lamp-2.0-resnet56]: Epoch 15/100, Acc=0.8940, Val Loss=0.3300, lr=0.0100
[02/20 23:51:07 cifar10-global-lamp-2.0-resnet56]: Epoch 16/100, Acc=0.8933, Val Loss=0.3330, lr=0.0100
[02/20 23:51:41 cifar10-global-lamp-2.0-resnet56]: Epoch 17/100, Acc=0.8981, Val Loss=0.3196, lr=0.0100
[02/20 23:52:16 cifar10-global-lamp-2.0-resnet56]: Epoch 18/100, Acc=0.8780, Val Loss=0.3790, lr=0.0100
[02/20 23:52:50 cifar10-global-lamp-2.0-resnet56]: Epoch 19/100, Acc=0.8937, Val Loss=0.3442, lr=0.0100
[02/20 23:53:25 cifar10-global-lamp-2.0-resnet56]: Epoch 20/100, Acc=0.8867, Val Loss=0.3643, lr=0.0100
[02/20 23:53:59 cifar10-global-lamp-2.0-resnet56]: Epoch 21/100, Acc=0.8922, Val Loss=0.3358, lr=0.0100
[02/20 23:54:34 cifar10-global-lamp-2.0-resnet56]: Epoch 22/100, Acc=0.8950, Val Loss=0.3355, lr=0.0100
[02/20 23:55:08 cifar10-global-lamp-2.0-resnet56]: Epoch 23/100, Acc=0.8871, Val Loss=0.3604, lr=0.0100
[02/20 23:55:42 cifar10-global-lamp-2.0-resnet56]: Epoch 24/100, Acc=0.8781, Val Loss=0.4120, lr=0.0100
[02/20 23:56:16 cifar10-global-lamp-2.0-resnet56]: Epoch 25/100, Acc=0.9000, Val Loss=0.3221, lr=0.0100
[02/20 23:56:50 cifar10-global-lamp-2.0-resnet56]: Epoch 26/100, Acc=0.8855, Val Loss=0.3841, lr=0.0100
[02/20 23:57:25 cifar10-global-lamp-2.0-resnet56]: Epoch 27/100, Acc=0.8970, Val Loss=0.3381, lr=0.0100
[02/20 23:57:59 cifar10-global-lamp-2.0-resnet56]: Epoch 28/100, Acc=0.8995, Val Loss=0.3186, lr=0.0100
[02/20 23:58:34 cifar10-global-lamp-2.0-resnet56]: Epoch 29/100, Acc=0.8951, Val Loss=0.3571, lr=0.0100
[02/20 23:59:09 cifar10-global-lamp-2.0-resnet56]: Epoch 30/100, Acc=0.9002, Val Loss=0.3248, lr=0.0100
[02/20 23:59:45 cifar10-global-lamp-2.0-resnet56]: Epoch 31/100, Acc=0.9026, Val Loss=0.3091, lr=0.0100
[02/21 00:00:20 cifar10-global-lamp-2.0-resnet56]: Epoch 32/100, Acc=0.8894, Val Loss=0.3528, lr=0.0100
[02/21 00:00:54 cifar10-global-lamp-2.0-resnet56]: Epoch 33/100, Acc=0.8998, Val Loss=0.3214, lr=0.0100
[02/21 00:01:29 cifar10-global-lamp-2.0-resnet56]: Epoch 34/100, Acc=0.8897, Val Loss=0.3577, lr=0.0100
[02/21 00:02:03 cifar10-global-lamp-2.0-resnet56]: Epoch 35/100, Acc=0.8954, Val Loss=0.3420, lr=0.0100
[02/21 00:02:38 cifar10-global-lamp-2.0-resnet56]: Epoch 36/100, Acc=0.8903, Val Loss=0.3519, lr=0.0100
[02/21 00:03:12 cifar10-global-lamp-2.0-resnet56]: Epoch 37/100, Acc=0.8944, Val Loss=0.3532, lr=0.0100
[02/21 00:03:46 cifar10-global-lamp-2.0-resnet56]: Epoch 38/100, Acc=0.9073, Val Loss=0.2998, lr=0.0100
[02/21 00:04:21 cifar10-global-lamp-2.0-resnet56]: Epoch 39/100, Acc=0.8920, Val Loss=0.3499, lr=0.0100
[02/21 00:04:55 cifar10-global-lamp-2.0-resnet56]: Epoch 40/100, Acc=0.8957, Val Loss=0.3354, lr=0.0100
[02/21 00:05:29 cifar10-global-lamp-2.0-resnet56]: Epoch 41/100, Acc=0.8920, Val Loss=0.3494, lr=0.0100
[02/21 00:06:03 cifar10-global-lamp-2.0-resnet56]: Epoch 42/100, Acc=0.8889, Val Loss=0.3570, lr=0.0100
[02/21 00:06:37 cifar10-global-lamp-2.0-resnet56]: Epoch 43/100, Acc=0.8950, Val Loss=0.3517, lr=0.0100
[02/21 00:07:11 cifar10-global-lamp-2.0-resnet56]: Epoch 44/100, Acc=0.8992, Val Loss=0.3181, lr=0.0100
[02/21 00:07:45 cifar10-global-lamp-2.0-resnet56]: Epoch 45/100, Acc=0.8949, Val Loss=0.3580, lr=0.0100
[02/21 00:08:20 cifar10-global-lamp-2.0-resnet56]: Epoch 46/100, Acc=0.8976, Val Loss=0.3299, lr=0.0100
[02/21 00:08:53 cifar10-global-lamp-2.0-resnet56]: Epoch 47/100, Acc=0.8976, Val Loss=0.3306, lr=0.0100
[02/21 00:09:28 cifar10-global-lamp-2.0-resnet56]: Epoch 48/100, Acc=0.8912, Val Loss=0.3474, lr=0.0100
[02/21 00:10:02 cifar10-global-lamp-2.0-resnet56]: Epoch 49/100, Acc=0.9012, Val Loss=0.3302, lr=0.0100
[02/21 00:10:36 cifar10-global-lamp-2.0-resnet56]: Epoch 50/100, Acc=0.8931, Val Loss=0.3404, lr=0.0100
[02/21 00:11:10 cifar10-global-lamp-2.0-resnet56]: Epoch 51/100, Acc=0.8934, Val Loss=0.3507, lr=0.0100
[02/21 00:11:45 cifar10-global-lamp-2.0-resnet56]: Epoch 52/100, Acc=0.8901, Val Loss=0.3521, lr=0.0100
[02/21 00:12:19 cifar10-global-lamp-2.0-resnet56]: Epoch 53/100, Acc=0.8970, Val Loss=0.3399, lr=0.0100
[02/21 00:12:53 cifar10-global-lamp-2.0-resnet56]: Epoch 54/100, Acc=0.9000, Val Loss=0.3309, lr=0.0100
[02/21 00:13:27 cifar10-global-lamp-2.0-resnet56]: Epoch 55/100, Acc=0.8919, Val Loss=0.3781, lr=0.0100
[02/21 00:14:02 cifar10-global-lamp-2.0-resnet56]: Epoch 56/100, Acc=0.8961, Val Loss=0.3531, lr=0.0100
[02/21 00:14:36 cifar10-global-lamp-2.0-resnet56]: Epoch 57/100, Acc=0.8913, Val Loss=0.3716, lr=0.0100
[02/21 00:15:10 cifar10-global-lamp-2.0-resnet56]: Epoch 58/100, Acc=0.8915, Val Loss=0.3610, lr=0.0100
[02/21 00:15:44 cifar10-global-lamp-2.0-resnet56]: Epoch 59/100, Acc=0.8752, Val Loss=0.4115, lr=0.0100
[02/21 00:16:18 cifar10-global-lamp-2.0-resnet56]: Epoch 60/100, Acc=0.9206, Val Loss=0.2543, lr=0.0010
[02/21 00:16:53 cifar10-global-lamp-2.0-resnet56]: Epoch 61/100, Acc=0.9228, Val Loss=0.2505, lr=0.0010
[02/21 00:17:29 cifar10-global-lamp-2.0-resnet56]: Epoch 62/100, Acc=0.9245, Val Loss=0.2516, lr=0.0010
[02/21 00:18:03 cifar10-global-lamp-2.0-resnet56]: Epoch 63/100, Acc=0.9256, Val Loss=0.2483, lr=0.0010
[02/21 00:18:38 cifar10-global-lamp-2.0-resnet56]: Epoch 64/100, Acc=0.9263, Val Loss=0.2478, lr=0.0010
[02/21 00:19:12 cifar10-global-lamp-2.0-resnet56]: Epoch 65/100, Acc=0.9275, Val Loss=0.2508, lr=0.0010
[02/21 00:19:47 cifar10-global-lamp-2.0-resnet56]: Epoch 66/100, Acc=0.9273, Val Loss=0.2553, lr=0.0010
[02/21 00:20:22 cifar10-global-lamp-2.0-resnet56]: Epoch 67/100, Acc=0.9286, Val Loss=0.2523, lr=0.0010
[02/21 00:20:56 cifar10-global-lamp-2.0-resnet56]: Epoch 68/100, Acc=0.9276, Val Loss=0.2579, lr=0.0010
[02/21 00:21:31 cifar10-global-lamp-2.0-resnet56]: Epoch 69/100, Acc=0.9280, Val Loss=0.2534, lr=0.0010
[02/21 00:22:06 cifar10-global-lamp-2.0-resnet56]: Epoch 70/100, Acc=0.9274, Val Loss=0.2572, lr=0.0010
[02/21 00:22:40 cifar10-global-lamp-2.0-resnet56]: Epoch 71/100, Acc=0.9274, Val Loss=0.2574, lr=0.0010
[02/21 00:23:15 cifar10-global-lamp-2.0-resnet56]: Epoch 72/100, Acc=0.9267, Val Loss=0.2593, lr=0.0010
[02/21 00:23:50 cifar10-global-lamp-2.0-resnet56]: Epoch 73/100, Acc=0.9265, Val Loss=0.2649, lr=0.0010
[02/21 00:24:24 cifar10-global-lamp-2.0-resnet56]: Epoch 74/100, Acc=0.9263, Val Loss=0.2694, lr=0.0010
[02/21 00:24:59 cifar10-global-lamp-2.0-resnet56]: Epoch 75/100, Acc=0.9278, Val Loss=0.2685, lr=0.0010
[02/21 00:25:34 cifar10-global-lamp-2.0-resnet56]: Epoch 76/100, Acc=0.9281, Val Loss=0.2689, lr=0.0010
[02/21 00:26:10 cifar10-global-lamp-2.0-resnet56]: Epoch 77/100, Acc=0.9279, Val Loss=0.2717, lr=0.0010
[02/21 00:26:45 cifar10-global-lamp-2.0-resnet56]: Epoch 78/100, Acc=0.9262, Val Loss=0.2706, lr=0.0010
[02/21 00:27:19 cifar10-global-lamp-2.0-resnet56]: Epoch 79/100, Acc=0.9266, Val Loss=0.2721, lr=0.0010
[02/21 00:27:54 cifar10-global-lamp-2.0-resnet56]: Epoch 80/100, Acc=0.9280, Val Loss=0.2719, lr=0.0001
[02/21 00:28:29 cifar10-global-lamp-2.0-resnet56]: Epoch 81/100, Acc=0.9279, Val Loss=0.2698, lr=0.0001
[02/21 00:29:03 cifar10-global-lamp-2.0-resnet56]: Epoch 82/100, Acc=0.9269, Val Loss=0.2696, lr=0.0001
[02/21 00:29:37 cifar10-global-lamp-2.0-resnet56]: Epoch 83/100, Acc=0.9269, Val Loss=0.2710, lr=0.0001
[02/21 00:30:11 cifar10-global-lamp-2.0-resnet56]: Epoch 84/100, Acc=0.9279, Val Loss=0.2706, lr=0.0001
[02/21 00:30:45 cifar10-global-lamp-2.0-resnet56]: Epoch 85/100, Acc=0.9266, Val Loss=0.2708, lr=0.0001
[02/21 00:31:19 cifar10-global-lamp-2.0-resnet56]: Epoch 86/100, Acc=0.9278, Val Loss=0.2718, lr=0.0001
[02/21 00:31:54 cifar10-global-lamp-2.0-resnet56]: Epoch 87/100, Acc=0.9281, Val Loss=0.2724, lr=0.0001
[02/21 00:32:28 cifar10-global-lamp-2.0-resnet56]: Epoch 88/100, Acc=0.9276, Val Loss=0.2702, lr=0.0001
[02/21 00:33:02 cifar10-global-lamp-2.0-resnet56]: Epoch 89/100, Acc=0.9292, Val Loss=0.2715, lr=0.0001
[02/21 00:33:36 cifar10-global-lamp-2.0-resnet56]: Epoch 90/100, Acc=0.9280, Val Loss=0.2712, lr=0.0001
[02/21 00:34:11 cifar10-global-lamp-2.0-resnet56]: Epoch 91/100, Acc=0.9293, Val Loss=0.2704, lr=0.0001
[02/21 00:34:45 cifar10-global-lamp-2.0-resnet56]: Epoch 92/100, Acc=0.9280, Val Loss=0.2717, lr=0.0001
[02/21 00:35:19 cifar10-global-lamp-2.0-resnet56]: Epoch 93/100, Acc=0.9280, Val Loss=0.2708, lr=0.0001
[02/21 00:35:53 cifar10-global-lamp-2.0-resnet56]: Epoch 94/100, Acc=0.9297, Val Loss=0.2732, lr=0.0001
[02/21 00:36:27 cifar10-global-lamp-2.0-resnet56]: Epoch 95/100, Acc=0.9276, Val Loss=0.2708, lr=0.0001
[02/21 00:37:01 cifar10-global-lamp-2.0-resnet56]: Epoch 96/100, Acc=0.9284, Val Loss=0.2715, lr=0.0001
[02/21 00:37:35 cifar10-global-lamp-2.0-resnet56]: Epoch 97/100, Acc=0.9286, Val Loss=0.2732, lr=0.0001
[02/21 00:38:09 cifar10-global-lamp-2.0-resnet56]: Epoch 98/100, Acc=0.9288, Val Loss=0.2727, lr=0.0001
[02/21 00:38:43 cifar10-global-lamp-2.0-resnet56]: Epoch 99/100, Acc=0.9287, Val Loss=0.2728, lr=0.0001
[02/21 00:38:43 cifar10-global-lamp-2.0-resnet56]: Best Acc=0.9297
[02/21 00:38:43 cifar10-global-lamp-2.0-resnet56]: Params: 0.25 M
[02/21 00:38:43 cifar10-global-lamp-2.0-resnet56]: ops: 63.48 M
[02/21 00:38:46 cifar10-global-lamp-2.0-resnet56]: Acc: 0.9287 Val Loss: 0.2728

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: mode: prune
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: model: resnet56
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: verbose: False
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: dataset: cifar10
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: dataroot: data
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: batch_size: 128
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: total_epochs: 100
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: lr_decay_milestones: 60,80
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: lr_decay_gamma: 0.1
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: lr: 0.01
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: output_dir: run/cifar10/prune/cifar10-global-slim-2.0-resnet56
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: finetune: True
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: last_epochs: 100
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: reps: 1
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: method: slim
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: speed_up: 2.0
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: max_pruning_ratio: 1.0
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: reg: 1e-05
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: delta_reg: 0.0001
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: weight_decay: 0.0005
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: seed: 1
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: global_pruning: True
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: sl_total_epochs: 100
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: sl_lr: 0.01
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: sl_reg_warmup: 0
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: sl_restore: None
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: iterative_steps: 400
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: logger: <Logger cifar10-global-slim-2.0-resnet56 (DEBUG)>
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: device: cuda
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: num_classes: 10
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 00:38:54 cifar10-global-slim-2.0-resnet56]: Regularizing...
[02/21 00:39:29 cifar10-global-slim-2.0-resnet56]: Epoch 0/100, Acc=0.9048, Val Loss=0.3422, lr=0.0100
[02/21 00:40:03 cifar10-global-slim-2.0-resnet56]: Epoch 1/100, Acc=0.9129, Val Loss=0.3065, lr=0.0100
[02/21 00:40:39 cifar10-global-slim-2.0-resnet56]: Epoch 2/100, Acc=0.9126, Val Loss=0.3092, lr=0.0100
[02/21 00:41:14 cifar10-global-slim-2.0-resnet56]: Epoch 3/100, Acc=0.9123, Val Loss=0.3083, lr=0.0100
[02/21 00:41:49 cifar10-global-slim-2.0-resnet56]: Epoch 4/100, Acc=0.9096, Val Loss=0.3242, lr=0.0100
[02/21 00:42:24 cifar10-global-slim-2.0-resnet56]: Epoch 5/100, Acc=0.9151, Val Loss=0.3038, lr=0.0100
[02/21 00:42:59 cifar10-global-slim-2.0-resnet56]: Epoch 6/100, Acc=0.9155, Val Loss=0.3136, lr=0.0100
[02/21 00:43:34 cifar10-global-slim-2.0-resnet56]: Epoch 7/100, Acc=0.9241, Val Loss=0.2801, lr=0.0100
[02/21 00:44:10 cifar10-global-slim-2.0-resnet56]: Epoch 8/100, Acc=0.9178, Val Loss=0.3120, lr=0.0100
[02/21 00:44:45 cifar10-global-slim-2.0-resnet56]: Epoch 9/100, Acc=0.9146, Val Loss=0.3428, lr=0.0100
[02/21 00:45:20 cifar10-global-slim-2.0-resnet56]: Epoch 10/100, Acc=0.9110, Val Loss=0.3453, lr=0.0100
[02/21 00:45:55 cifar10-global-slim-2.0-resnet56]: Epoch 11/100, Acc=0.9214, Val Loss=0.3052, lr=0.0100
[02/21 00:46:30 cifar10-global-slim-2.0-resnet56]: Epoch 12/100, Acc=0.9215, Val Loss=0.3105, lr=0.0100
[02/21 00:47:05 cifar10-global-slim-2.0-resnet56]: Epoch 13/100, Acc=0.9234, Val Loss=0.3057, lr=0.0100
[02/21 00:47:40 cifar10-global-slim-2.0-resnet56]: Epoch 14/100, Acc=0.9237, Val Loss=0.3017, lr=0.0100
[02/21 00:48:15 cifar10-global-slim-2.0-resnet56]: Epoch 15/100, Acc=0.9201, Val Loss=0.3234, lr=0.0100
[02/21 00:48:50 cifar10-global-slim-2.0-resnet56]: Epoch 16/100, Acc=0.9277, Val Loss=0.3000, lr=0.0100
[02/21 00:49:24 cifar10-global-slim-2.0-resnet56]: Epoch 17/100, Acc=0.9264, Val Loss=0.2970, lr=0.0100
[02/21 00:49:59 cifar10-global-slim-2.0-resnet56]: Epoch 18/100, Acc=0.9224, Val Loss=0.3260, lr=0.0100
[02/21 00:50:34 cifar10-global-slim-2.0-resnet56]: Epoch 19/100, Acc=0.9158, Val Loss=0.3495, lr=0.0100
[02/21 00:51:09 cifar10-global-slim-2.0-resnet56]: Epoch 20/100, Acc=0.9237, Val Loss=0.3149, lr=0.0100
[02/21 00:51:44 cifar10-global-slim-2.0-resnet56]: Epoch 21/100, Acc=0.9254, Val Loss=0.3062, lr=0.0100
[02/21 00:52:19 cifar10-global-slim-2.0-resnet56]: Epoch 22/100, Acc=0.9243, Val Loss=0.3206, lr=0.0100
[02/21 00:52:54 cifar10-global-slim-2.0-resnet56]: Epoch 23/100, Acc=0.9221, Val Loss=0.3576, lr=0.0100
[02/21 00:53:29 cifar10-global-slim-2.0-resnet56]: Epoch 24/100, Acc=0.9265, Val Loss=0.3115, lr=0.0100
[02/21 00:54:04 cifar10-global-slim-2.0-resnet56]: Epoch 25/100, Acc=0.9220, Val Loss=0.3290, lr=0.0100
[02/21 00:54:40 cifar10-global-slim-2.0-resnet56]: Epoch 26/100, Acc=0.9218, Val Loss=0.3218, lr=0.0100
[02/21 00:55:15 cifar10-global-slim-2.0-resnet56]: Epoch 27/100, Acc=0.9232, Val Loss=0.3344, lr=0.0100
[02/21 00:55:50 cifar10-global-slim-2.0-resnet56]: Epoch 28/100, Acc=0.9198, Val Loss=0.3718, lr=0.0100
[02/21 00:56:25 cifar10-global-slim-2.0-resnet56]: Epoch 29/100, Acc=0.9256, Val Loss=0.3340, lr=0.0100
[02/21 00:57:01 cifar10-global-slim-2.0-resnet56]: Epoch 30/100, Acc=0.9225, Val Loss=0.3387, lr=0.0100
[02/21 00:57:36 cifar10-global-slim-2.0-resnet56]: Epoch 31/100, Acc=0.9274, Val Loss=0.3251, lr=0.0100
[02/21 00:58:11 cifar10-global-slim-2.0-resnet56]: Epoch 32/100, Acc=0.9251, Val Loss=0.3201, lr=0.0100
[02/21 00:58:46 cifar10-global-slim-2.0-resnet56]: Epoch 33/100, Acc=0.9258, Val Loss=0.3435, lr=0.0100
[02/21 00:59:21 cifar10-global-slim-2.0-resnet56]: Epoch 34/100, Acc=0.9264, Val Loss=0.3211, lr=0.0100
[02/21 00:59:57 cifar10-global-slim-2.0-resnet56]: Epoch 35/100, Acc=0.9253, Val Loss=0.3498, lr=0.0100
[02/21 01:00:32 cifar10-global-slim-2.0-resnet56]: Epoch 36/100, Acc=0.9317, Val Loss=0.3196, lr=0.0100
[02/21 01:01:07 cifar10-global-slim-2.0-resnet56]: Epoch 37/100, Acc=0.9286, Val Loss=0.3221, lr=0.0100
[02/21 01:01:42 cifar10-global-slim-2.0-resnet56]: Epoch 38/100, Acc=0.9284, Val Loss=0.3370, lr=0.0100
[02/21 01:02:17 cifar10-global-slim-2.0-resnet56]: Epoch 39/100, Acc=0.9291, Val Loss=0.3299, lr=0.0100
[02/21 01:02:52 cifar10-global-slim-2.0-resnet56]: Epoch 40/100, Acc=0.9245, Val Loss=0.3467, lr=0.0100
[02/21 01:03:27 cifar10-global-slim-2.0-resnet56]: Epoch 41/100, Acc=0.9305, Val Loss=0.3275, lr=0.0100
[02/21 01:04:02 cifar10-global-slim-2.0-resnet56]: Epoch 42/100, Acc=0.9299, Val Loss=0.3394, lr=0.0100
[02/21 01:04:37 cifar10-global-slim-2.0-resnet56]: Epoch 43/100, Acc=0.9329, Val Loss=0.3332, lr=0.0100
[02/21 01:05:13 cifar10-global-slim-2.0-resnet56]: Epoch 44/100, Acc=0.9316, Val Loss=0.3257, lr=0.0100
[02/21 01:05:48 cifar10-global-slim-2.0-resnet56]: Epoch 45/100, Acc=0.9320, Val Loss=0.3154, lr=0.0100
[02/21 01:06:23 cifar10-global-slim-2.0-resnet56]: Epoch 46/100, Acc=0.9319, Val Loss=0.3291, lr=0.0100
[02/21 01:06:59 cifar10-global-slim-2.0-resnet56]: Epoch 47/100, Acc=0.9240, Val Loss=0.3435, lr=0.0100
[02/21 01:07:36 cifar10-global-slim-2.0-resnet56]: Epoch 48/100, Acc=0.9306, Val Loss=0.3467, lr=0.0100
[02/21 01:08:14 cifar10-global-slim-2.0-resnet56]: Epoch 49/100, Acc=0.9300, Val Loss=0.3305, lr=0.0100
[02/21 01:08:49 cifar10-global-slim-2.0-resnet56]: Epoch 50/100, Acc=0.9285, Val Loss=0.3482, lr=0.0100
[02/21 01:09:25 cifar10-global-slim-2.0-resnet56]: Epoch 51/100, Acc=0.9291, Val Loss=0.3495, lr=0.0100
[02/21 01:09:59 cifar10-global-slim-2.0-resnet56]: Epoch 52/100, Acc=0.9283, Val Loss=0.3311, lr=0.0100
[02/21 01:10:34 cifar10-global-slim-2.0-resnet56]: Epoch 53/100, Acc=0.9303, Val Loss=0.3333, lr=0.0100
[02/21 01:11:15 cifar10-global-slim-2.0-resnet56]: Epoch 54/100, Acc=0.9300, Val Loss=0.3407, lr=0.0100
[02/21 01:11:55 cifar10-global-slim-2.0-resnet56]: Epoch 55/100, Acc=0.9297, Val Loss=0.3534, lr=0.0100
[02/21 01:12:32 cifar10-global-slim-2.0-resnet56]: Epoch 56/100, Acc=0.9327, Val Loss=0.3385, lr=0.0100
[02/21 01:13:07 cifar10-global-slim-2.0-resnet56]: Epoch 57/100, Acc=0.9282, Val Loss=0.3603, lr=0.0100
[02/21 01:13:42 cifar10-global-slim-2.0-resnet56]: Epoch 58/100, Acc=0.9291, Val Loss=0.3438, lr=0.0100
[02/21 01:14:17 cifar10-global-slim-2.0-resnet56]: Epoch 59/100, Acc=0.9284, Val Loss=0.3639, lr=0.0100
[02/21 01:14:52 cifar10-global-slim-2.0-resnet56]: Epoch 60/100, Acc=0.9338, Val Loss=0.3292, lr=0.0010
[02/21 01:15:27 cifar10-global-slim-2.0-resnet56]: Epoch 61/100, Acc=0.9355, Val Loss=0.3222, lr=0.0010
[02/21 01:16:03 cifar10-global-slim-2.0-resnet56]: Epoch 62/100, Acc=0.9354, Val Loss=0.3214, lr=0.0010
[02/21 01:16:38 cifar10-global-slim-2.0-resnet56]: Epoch 63/100, Acc=0.9379, Val Loss=0.3217, lr=0.0010
[02/21 01:17:15 cifar10-global-slim-2.0-resnet56]: Epoch 64/100, Acc=0.9376, Val Loss=0.3216, lr=0.0010
[02/21 01:17:52 cifar10-global-slim-2.0-resnet56]: Epoch 65/100, Acc=0.9382, Val Loss=0.3210, lr=0.0010
[02/21 01:18:27 cifar10-global-slim-2.0-resnet56]: Epoch 66/100, Acc=0.9378, Val Loss=0.3220, lr=0.0010
[02/21 01:19:02 cifar10-global-slim-2.0-resnet56]: Epoch 67/100, Acc=0.9390, Val Loss=0.3154, lr=0.0010
[02/21 01:19:37 cifar10-global-slim-2.0-resnet56]: Epoch 68/100, Acc=0.9394, Val Loss=0.3197, lr=0.0010
[02/21 01:20:13 cifar10-global-slim-2.0-resnet56]: Epoch 69/100, Acc=0.9383, Val Loss=0.3177, lr=0.0010
[02/21 01:20:49 cifar10-global-slim-2.0-resnet56]: Epoch 70/100, Acc=0.9384, Val Loss=0.3205, lr=0.0010
[02/21 01:21:25 cifar10-global-slim-2.0-resnet56]: Epoch 71/100, Acc=0.9395, Val Loss=0.3209, lr=0.0010
[02/21 01:22:00 cifar10-global-slim-2.0-resnet56]: Epoch 72/100, Acc=0.9408, Val Loss=0.3180, lr=0.0010
[02/21 01:22:35 cifar10-global-slim-2.0-resnet56]: Epoch 73/100, Acc=0.9395, Val Loss=0.3224, lr=0.0010
[02/21 01:23:11 cifar10-global-slim-2.0-resnet56]: Epoch 74/100, Acc=0.9385, Val Loss=0.3196, lr=0.0010
[02/21 01:23:46 cifar10-global-slim-2.0-resnet56]: Epoch 75/100, Acc=0.9394, Val Loss=0.3227, lr=0.0010
[02/21 01:24:21 cifar10-global-slim-2.0-resnet56]: Epoch 76/100, Acc=0.9389, Val Loss=0.3181, lr=0.0010
[02/21 01:24:56 cifar10-global-slim-2.0-resnet56]: Epoch 77/100, Acc=0.9396, Val Loss=0.3218, lr=0.0010
[02/21 01:25:31 cifar10-global-slim-2.0-resnet56]: Epoch 78/100, Acc=0.9398, Val Loss=0.3184, lr=0.0010
[02/21 01:26:07 cifar10-global-slim-2.0-resnet56]: Epoch 79/100, Acc=0.9404, Val Loss=0.3213, lr=0.0010
[02/21 01:26:42 cifar10-global-slim-2.0-resnet56]: Epoch 80/100, Acc=0.9395, Val Loss=0.3226, lr=0.0001
[02/21 01:27:18 cifar10-global-slim-2.0-resnet56]: Epoch 81/100, Acc=0.9392, Val Loss=0.3245, lr=0.0001
[02/21 01:27:53 cifar10-global-slim-2.0-resnet56]: Epoch 82/100, Acc=0.9384, Val Loss=0.3224, lr=0.0001
[02/21 01:28:28 cifar10-global-slim-2.0-resnet56]: Epoch 83/100, Acc=0.9393, Val Loss=0.3211, lr=0.0001
[02/21 01:29:03 cifar10-global-slim-2.0-resnet56]: Epoch 84/100, Acc=0.9390, Val Loss=0.3224, lr=0.0001
[02/21 01:29:39 cifar10-global-slim-2.0-resnet56]: Epoch 85/100, Acc=0.9395, Val Loss=0.3237, lr=0.0001
[02/21 01:30:14 cifar10-global-slim-2.0-resnet56]: Epoch 86/100, Acc=0.9391, Val Loss=0.3231, lr=0.0001
[02/21 01:30:50 cifar10-global-slim-2.0-resnet56]: Epoch 87/100, Acc=0.9394, Val Loss=0.3232, lr=0.0001
[02/21 01:31:25 cifar10-global-slim-2.0-resnet56]: Epoch 88/100, Acc=0.9394, Val Loss=0.3236, lr=0.0001
[02/21 01:32:01 cifar10-global-slim-2.0-resnet56]: Epoch 89/100, Acc=0.9394, Val Loss=0.3233, lr=0.0001
[02/21 01:32:37 cifar10-global-slim-2.0-resnet56]: Epoch 90/100, Acc=0.9392, Val Loss=0.3254, lr=0.0001
[02/21 01:33:12 cifar10-global-slim-2.0-resnet56]: Epoch 91/100, Acc=0.9394, Val Loss=0.3254, lr=0.0001
[02/21 01:33:47 cifar10-global-slim-2.0-resnet56]: Epoch 92/100, Acc=0.9392, Val Loss=0.3206, lr=0.0001
[02/21 01:34:22 cifar10-global-slim-2.0-resnet56]: Epoch 93/100, Acc=0.9397, Val Loss=0.3188, lr=0.0001
[02/21 01:34:58 cifar10-global-slim-2.0-resnet56]: Epoch 94/100, Acc=0.9401, Val Loss=0.3220, lr=0.0001
[02/21 01:35:33 cifar10-global-slim-2.0-resnet56]: Epoch 95/100, Acc=0.9393, Val Loss=0.3226, lr=0.0001
[02/21 01:36:08 cifar10-global-slim-2.0-resnet56]: Epoch 96/100, Acc=0.9404, Val Loss=0.3201, lr=0.0001
[02/21 01:36:44 cifar10-global-slim-2.0-resnet56]: Epoch 97/100, Acc=0.9392, Val Loss=0.3229, lr=0.0001
[02/21 01:37:19 cifar10-global-slim-2.0-resnet56]: Epoch 98/100, Acc=0.9394, Val Loss=0.3236, lr=0.0001
[02/21 01:37:54 cifar10-global-slim-2.0-resnet56]: Epoch 99/100, Acc=0.9393, Val Loss=0.3240, lr=0.0001
[02/21 01:37:54 cifar10-global-slim-2.0-resnet56]: Best Acc=0.9408
[02/21 01:37:54 cifar10-global-slim-2.0-resnet56]: Loading the sparse model from run/cifar10/prune/cifar10-global-slim-2.0-resnet56/reg_cifar10_resnet56_slim_1e-05.pth...
[02/21 01:37:57 cifar10-global-slim-2.0-resnet56]: Pruning...
[02/21 01:38:08 cifar10-global-slim-2.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(10, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(10, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(10, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(10, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(10, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(10, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(10, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(10, 22, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(10, 28, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(28, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(26, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(28, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(28, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(30, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(28, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(29, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(28, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(28, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(28, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(28, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(28, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(28, 50, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(50, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(57, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(50, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(58, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(50, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(57, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(50, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(50, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(54, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(50, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(49, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(50, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(51, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=50, out_features=10, bias=True)
)
[02/21 01:38:11 cifar10-global-slim-2.0-resnet56]: Params: 0.86 M => 0.54 M (62.77%)
[02/21 01:38:11 cifar10-global-slim-2.0-resnet56]: FLOPs: 127.12 M => 63.21 M (49.73%, 2.01X )
[02/21 01:38:11 cifar10-global-slim-2.0-resnet56]: Acc: 0.9408 => 0.1000
[02/21 01:38:11 cifar10-global-slim-2.0-resnet56]: Val Loss: 0.3180 => 2.8136
[02/21 01:38:11 cifar10-global-slim-2.0-resnet56]: Finetuning...
[02/21 01:38:44 cifar10-global-slim-2.0-resnet56]: Epoch 0/100, Acc=0.8690, Val Loss=0.4178, lr=0.0100
[02/21 01:39:17 cifar10-global-slim-2.0-resnet56]: Epoch 1/100, Acc=0.8869, Val Loss=0.3776, lr=0.0100
[02/21 01:39:50 cifar10-global-slim-2.0-resnet56]: Epoch 2/100, Acc=0.8898, Val Loss=0.3690, lr=0.0100
[02/21 01:40:23 cifar10-global-slim-2.0-resnet56]: Epoch 3/100, Acc=0.8981, Val Loss=0.3646, lr=0.0100
[02/21 01:40:55 cifar10-global-slim-2.0-resnet56]: Epoch 4/100, Acc=0.8952, Val Loss=0.3674, lr=0.0100
[02/21 01:41:28 cifar10-global-slim-2.0-resnet56]: Epoch 5/100, Acc=0.8990, Val Loss=0.3427, lr=0.0100
[02/21 01:42:01 cifar10-global-slim-2.0-resnet56]: Epoch 6/100, Acc=0.8985, Val Loss=0.3594, lr=0.0100
[02/21 01:42:34 cifar10-global-slim-2.0-resnet56]: Epoch 7/100, Acc=0.9013, Val Loss=0.3530, lr=0.0100
[02/21 01:43:09 cifar10-global-slim-2.0-resnet56]: Epoch 8/100, Acc=0.8870, Val Loss=0.4108, lr=0.0100
[02/21 01:43:48 cifar10-global-slim-2.0-resnet56]: Epoch 9/100, Acc=0.8995, Val Loss=0.3358, lr=0.0100
[02/21 01:44:22 cifar10-global-slim-2.0-resnet56]: Epoch 10/100, Acc=0.8954, Val Loss=0.3681, lr=0.0100
[02/21 01:44:55 cifar10-global-slim-2.0-resnet56]: Epoch 11/100, Acc=0.8915, Val Loss=0.3971, lr=0.0100
[02/21 01:45:28 cifar10-global-slim-2.0-resnet56]: Epoch 12/100, Acc=0.8894, Val Loss=0.4130, lr=0.0100
[02/21 01:46:01 cifar10-global-slim-2.0-resnet56]: Epoch 13/100, Acc=0.9045, Val Loss=0.3287, lr=0.0100
[02/21 01:46:34 cifar10-global-slim-2.0-resnet56]: Epoch 14/100, Acc=0.8993, Val Loss=0.3581, lr=0.0100
[02/21 01:47:07 cifar10-global-slim-2.0-resnet56]: Epoch 15/100, Acc=0.8898, Val Loss=0.4108, lr=0.0100
[02/21 01:47:40 cifar10-global-slim-2.0-resnet56]: Epoch 16/100, Acc=0.8957, Val Loss=0.3658, lr=0.0100
[02/21 01:48:12 cifar10-global-slim-2.0-resnet56]: Epoch 17/100, Acc=0.9080, Val Loss=0.3387, lr=0.0100
[02/21 01:48:45 cifar10-global-slim-2.0-resnet56]: Epoch 18/100, Acc=0.8999, Val Loss=0.3688, lr=0.0100
[02/21 01:49:18 cifar10-global-slim-2.0-resnet56]: Epoch 19/100, Acc=0.8902, Val Loss=0.3859, lr=0.0100
[02/21 01:49:51 cifar10-global-slim-2.0-resnet56]: Epoch 20/100, Acc=0.8971, Val Loss=0.3757, lr=0.0100
[02/21 01:50:24 cifar10-global-slim-2.0-resnet56]: Epoch 21/100, Acc=0.8998, Val Loss=0.3509, lr=0.0100
[02/21 01:50:57 cifar10-global-slim-2.0-resnet56]: Epoch 22/100, Acc=0.9092, Val Loss=0.3096, lr=0.0100
[02/21 01:51:31 cifar10-global-slim-2.0-resnet56]: Epoch 23/100, Acc=0.9020, Val Loss=0.3577, lr=0.0100
[02/21 01:52:04 cifar10-global-slim-2.0-resnet56]: Epoch 24/100, Acc=0.8966, Val Loss=0.3679, lr=0.0100
[02/21 01:52:37 cifar10-global-slim-2.0-resnet56]: Epoch 25/100, Acc=0.9052, Val Loss=0.3188, lr=0.0100
[02/21 01:53:10 cifar10-global-slim-2.0-resnet56]: Epoch 26/100, Acc=0.9007, Val Loss=0.3712, lr=0.0100
[02/21 01:53:45 cifar10-global-slim-2.0-resnet56]: Epoch 27/100, Acc=0.9067, Val Loss=0.3471, lr=0.0100
[02/21 01:54:19 cifar10-global-slim-2.0-resnet56]: Epoch 28/100, Acc=0.9027, Val Loss=0.3409, lr=0.0100
[02/21 01:54:55 cifar10-global-slim-2.0-resnet56]: Epoch 29/100, Acc=0.9011, Val Loss=0.3630, lr=0.0100
[02/21 01:55:28 cifar10-global-slim-2.0-resnet56]: Epoch 30/100, Acc=0.8959, Val Loss=0.3746, lr=0.0100
[02/21 01:56:01 cifar10-global-slim-2.0-resnet56]: Epoch 31/100, Acc=0.9011, Val Loss=0.3405, lr=0.0100
[02/21 01:56:34 cifar10-global-slim-2.0-resnet56]: Epoch 32/100, Acc=0.8851, Val Loss=0.4026, lr=0.0100
[02/21 01:57:07 cifar10-global-slim-2.0-resnet56]: Epoch 33/100, Acc=0.9060, Val Loss=0.3301, lr=0.0100
[02/21 01:57:43 cifar10-global-slim-2.0-resnet56]: Epoch 34/100, Acc=0.8991, Val Loss=0.3603, lr=0.0100
[02/21 01:58:16 cifar10-global-slim-2.0-resnet56]: Epoch 35/100, Acc=0.9093, Val Loss=0.3189, lr=0.0100
[02/21 01:58:49 cifar10-global-slim-2.0-resnet56]: Epoch 36/100, Acc=0.8973, Val Loss=0.3549, lr=0.0100
[02/21 01:59:22 cifar10-global-slim-2.0-resnet56]: Epoch 37/100, Acc=0.8900, Val Loss=0.3840, lr=0.0100
[02/21 01:59:54 cifar10-global-slim-2.0-resnet56]: Epoch 38/100, Acc=0.8921, Val Loss=0.3907, lr=0.0100
[02/21 02:00:27 cifar10-global-slim-2.0-resnet56]: Epoch 39/100, Acc=0.9026, Val Loss=0.3683, lr=0.0100
[02/21 02:01:00 cifar10-global-slim-2.0-resnet56]: Epoch 40/100, Acc=0.9065, Val Loss=0.3311, lr=0.0100
[02/21 02:01:33 cifar10-global-slim-2.0-resnet56]: Epoch 41/100, Acc=0.8895, Val Loss=0.3985, lr=0.0100
[02/21 02:02:06 cifar10-global-slim-2.0-resnet56]: Epoch 42/100, Acc=0.8943, Val Loss=0.3894, lr=0.0100
[02/21 02:02:39 cifar10-global-slim-2.0-resnet56]: Epoch 43/100, Acc=0.8957, Val Loss=0.3529, lr=0.0100
[02/21 02:03:12 cifar10-global-slim-2.0-resnet56]: Epoch 44/100, Acc=0.9036, Val Loss=0.3309, lr=0.0100
[02/21 02:03:45 cifar10-global-slim-2.0-resnet56]: Epoch 45/100, Acc=0.9017, Val Loss=0.3568, lr=0.0100
[02/21 02:04:18 cifar10-global-slim-2.0-resnet56]: Epoch 46/100, Acc=0.8787, Val Loss=0.4523, lr=0.0100
[02/21 02:04:51 cifar10-global-slim-2.0-resnet56]: Epoch 47/100, Acc=0.8808, Val Loss=0.4552, lr=0.0100
[02/21 02:05:25 cifar10-global-slim-2.0-resnet56]: Epoch 48/100, Acc=0.9093, Val Loss=0.3157, lr=0.0100
[02/21 02:05:57 cifar10-global-slim-2.0-resnet56]: Epoch 49/100, Acc=0.9056, Val Loss=0.3380, lr=0.0100
[02/21 02:06:31 cifar10-global-slim-2.0-resnet56]: Epoch 50/100, Acc=0.8927, Val Loss=0.3769, lr=0.0100
[02/21 02:07:04 cifar10-global-slim-2.0-resnet56]: Epoch 51/100, Acc=0.8970, Val Loss=0.3619, lr=0.0100
[02/21 02:07:38 cifar10-global-slim-2.0-resnet56]: Epoch 52/100, Acc=0.8896, Val Loss=0.3977, lr=0.0100
[02/21 02:08:12 cifar10-global-slim-2.0-resnet56]: Epoch 53/100, Acc=0.9020, Val Loss=0.3508, lr=0.0100
[02/21 02:08:45 cifar10-global-slim-2.0-resnet56]: Epoch 54/100, Acc=0.9017, Val Loss=0.3543, lr=0.0100
[02/21 02:09:18 cifar10-global-slim-2.0-resnet56]: Epoch 55/100, Acc=0.8985, Val Loss=0.3652, lr=0.0100
[02/21 02:09:51 cifar10-global-slim-2.0-resnet56]: Epoch 56/100, Acc=0.9005, Val Loss=0.3599, lr=0.0100
[02/21 02:10:24 cifar10-global-slim-2.0-resnet56]: Epoch 57/100, Acc=0.8943, Val Loss=0.3754, lr=0.0100
[02/21 02:10:57 cifar10-global-slim-2.0-resnet56]: Epoch 58/100, Acc=0.9000, Val Loss=0.3499, lr=0.0100
[02/21 02:11:31 cifar10-global-slim-2.0-resnet56]: Epoch 59/100, Acc=0.8996, Val Loss=0.3577, lr=0.0100
[02/21 02:12:04 cifar10-global-slim-2.0-resnet56]: Epoch 60/100, Acc=0.9264, Val Loss=0.2659, lr=0.0010
[02/21 02:12:38 cifar10-global-slim-2.0-resnet56]: Epoch 61/100, Acc=0.9300, Val Loss=0.2595, lr=0.0010
[02/21 02:13:11 cifar10-global-slim-2.0-resnet56]: Epoch 62/100, Acc=0.9320, Val Loss=0.2672, lr=0.0010
[02/21 02:13:44 cifar10-global-slim-2.0-resnet56]: Epoch 63/100, Acc=0.9325, Val Loss=0.2623, lr=0.0010
[02/21 02:14:17 cifar10-global-slim-2.0-resnet56]: Epoch 64/100, Acc=0.9323, Val Loss=0.2638, lr=0.0010
[02/21 02:14:50 cifar10-global-slim-2.0-resnet56]: Epoch 65/100, Acc=0.9329, Val Loss=0.2640, lr=0.0010
[02/21 02:15:23 cifar10-global-slim-2.0-resnet56]: Epoch 66/100, Acc=0.9320, Val Loss=0.2682, lr=0.0010
[02/21 02:15:56 cifar10-global-slim-2.0-resnet56]: Epoch 67/100, Acc=0.9317, Val Loss=0.2656, lr=0.0010
[02/21 02:16:29 cifar10-global-slim-2.0-resnet56]: Epoch 68/100, Acc=0.9325, Val Loss=0.2660, lr=0.0010
[02/21 02:17:02 cifar10-global-slim-2.0-resnet56]: Epoch 69/100, Acc=0.9330, Val Loss=0.2701, lr=0.0010
[02/21 02:17:35 cifar10-global-slim-2.0-resnet56]: Epoch 70/100, Acc=0.9330, Val Loss=0.2682, lr=0.0010
[02/21 02:18:08 cifar10-global-slim-2.0-resnet56]: Epoch 71/100, Acc=0.9332, Val Loss=0.2663, lr=0.0010
[02/21 02:18:41 cifar10-global-slim-2.0-resnet56]: Epoch 72/100, Acc=0.9330, Val Loss=0.2688, lr=0.0010
[02/21 02:19:14 cifar10-global-slim-2.0-resnet56]: Epoch 73/100, Acc=0.9334, Val Loss=0.2705, lr=0.0010
[02/21 02:19:47 cifar10-global-slim-2.0-resnet56]: Epoch 74/100, Acc=0.9338, Val Loss=0.2712, lr=0.0010
[02/21 02:20:20 cifar10-global-slim-2.0-resnet56]: Epoch 75/100, Acc=0.9347, Val Loss=0.2710, lr=0.0010
[02/21 02:20:59 cifar10-global-slim-2.0-resnet56]: Epoch 76/100, Acc=0.9344, Val Loss=0.2719, lr=0.0010
[02/21 02:21:38 cifar10-global-slim-2.0-resnet56]: Epoch 77/100, Acc=0.9344, Val Loss=0.2735, lr=0.0010
[02/21 02:22:18 cifar10-global-slim-2.0-resnet56]: Epoch 78/100, Acc=0.9322, Val Loss=0.2766, lr=0.0010
[02/21 02:22:58 cifar10-global-slim-2.0-resnet56]: Epoch 79/100, Acc=0.9342, Val Loss=0.2756, lr=0.0010
[02/21 02:23:31 cifar10-global-slim-2.0-resnet56]: Epoch 80/100, Acc=0.9340, Val Loss=0.2740, lr=0.0001
[02/21 02:24:04 cifar10-global-slim-2.0-resnet56]: Epoch 81/100, Acc=0.9346, Val Loss=0.2738, lr=0.0001
[02/21 02:24:37 cifar10-global-slim-2.0-resnet56]: Epoch 82/100, Acc=0.9338, Val Loss=0.2753, lr=0.0001
[02/21 02:25:10 cifar10-global-slim-2.0-resnet56]: Epoch 83/100, Acc=0.9347, Val Loss=0.2742, lr=0.0001
[02/21 02:25:43 cifar10-global-slim-2.0-resnet56]: Epoch 84/100, Acc=0.9341, Val Loss=0.2750, lr=0.0001
[02/21 02:26:16 cifar10-global-slim-2.0-resnet56]: Epoch 85/100, Acc=0.9341, Val Loss=0.2740, lr=0.0001
[02/21 02:26:49 cifar10-global-slim-2.0-resnet56]: Epoch 86/100, Acc=0.9345, Val Loss=0.2742, lr=0.0001
[02/21 02:27:22 cifar10-global-slim-2.0-resnet56]: Epoch 87/100, Acc=0.9348, Val Loss=0.2716, lr=0.0001
[02/21 02:27:55 cifar10-global-slim-2.0-resnet56]: Epoch 88/100, Acc=0.9348, Val Loss=0.2717, lr=0.0001
[02/21 02:28:28 cifar10-global-slim-2.0-resnet56]: Epoch 89/100, Acc=0.9349, Val Loss=0.2748, lr=0.0001
[02/21 02:29:01 cifar10-global-slim-2.0-resnet56]: Epoch 90/100, Acc=0.9348, Val Loss=0.2731, lr=0.0001
[02/21 02:29:34 cifar10-global-slim-2.0-resnet56]: Epoch 91/100, Acc=0.9351, Val Loss=0.2733, lr=0.0001
[02/21 02:30:08 cifar10-global-slim-2.0-resnet56]: Epoch 92/100, Acc=0.9349, Val Loss=0.2719, lr=0.0001
[02/21 02:30:41 cifar10-global-slim-2.0-resnet56]: Epoch 93/100, Acc=0.9346, Val Loss=0.2732, lr=0.0001
[02/21 02:31:14 cifar10-global-slim-2.0-resnet56]: Epoch 94/100, Acc=0.9349, Val Loss=0.2757, lr=0.0001
[02/21 02:31:47 cifar10-global-slim-2.0-resnet56]: Epoch 95/100, Acc=0.9345, Val Loss=0.2743, lr=0.0001
[02/21 02:32:20 cifar10-global-slim-2.0-resnet56]: Epoch 96/100, Acc=0.9341, Val Loss=0.2732, lr=0.0001
[02/21 02:32:53 cifar10-global-slim-2.0-resnet56]: Epoch 97/100, Acc=0.9348, Val Loss=0.2742, lr=0.0001
[02/21 02:33:26 cifar10-global-slim-2.0-resnet56]: Epoch 98/100, Acc=0.9345, Val Loss=0.2721, lr=0.0001
[02/21 02:33:59 cifar10-global-slim-2.0-resnet56]: Epoch 99/100, Acc=0.9345, Val Loss=0.2757, lr=0.0001
[02/21 02:33:59 cifar10-global-slim-2.0-resnet56]: Best Acc=0.9351
[02/21 02:33:59 cifar10-global-slim-2.0-resnet56]: Params: 0.54 M
[02/21 02:33:59 cifar10-global-slim-2.0-resnet56]: ops: 63.21 M
[02/21 02:34:02 cifar10-global-slim-2.0-resnet56]: Acc: 0.9345 Val Loss: 0.2757

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: mode: prune
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: model: resnet56
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: verbose: False
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: dataset: cifar10
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: dataroot: data
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: batch_size: 128
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: total_epochs: 100
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: lr_decay_milestones: 60,80
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: lr_decay_gamma: 0.1
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: lr: 0.01
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: output_dir: run/cifar10/prune/cifar10-global-group_norm-2.0-resnet56
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: finetune: True
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: last_epochs: 100
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: reps: 1
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: method: group_norm
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: speed_up: 2.0
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: max_pruning_ratio: 1.0
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: reg: 1e-05
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: delta_reg: 0.0001
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: weight_decay: 0.0005
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: seed: 1
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: global_pruning: True
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: sl_total_epochs: 100
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: sl_lr: 0.01
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: sl_reg_warmup: 0
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: sl_restore: None
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: iterative_steps: 400
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: logger: <Logger cifar10-global-group_norm-2.0-resnet56 (DEBUG)>
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: device: cuda
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: num_classes: 10
[02/21 02:34:10 cifar10-global-group_norm-2.0-resnet56]: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 02:34:14 cifar10-global-group_norm-2.0-resnet56]: Pruning...
[02/21 02:34:30 cifar10-global-group_norm-2.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(12, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(12, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(12, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(12, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(12, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(12, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(12, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(12, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(12, 25, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(12, 31, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(31, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(31, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(31, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(31, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(31, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(31, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(31, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(31, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(31, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(31, 61, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(61, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(61, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(61, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(47, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(61, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(61, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(46, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(61, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(40, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(61, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(29, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(61, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=61, out_features=10, bias=True)
)
[02/21 02:34:33 cifar10-global-group_norm-2.0-resnet56]: Params: 0.86 M => 0.56 M (64.90%)
[02/21 02:34:33 cifar10-global-group_norm-2.0-resnet56]: FLOPs: 127.12 M => 63.06 M (49.61%, 2.02X )
[02/21 02:34:33 cifar10-global-group_norm-2.0-resnet56]: Acc: 0.9392 => 0.2899
[02/21 02:34:33 cifar10-global-group_norm-2.0-resnet56]: Val Loss: 0.2587 => 4.5965
[02/21 02:34:33 cifar10-global-group_norm-2.0-resnet56]: Finetuning...
[02/21 02:35:07 cifar10-global-group_norm-2.0-resnet56]: Epoch 0/100, Acc=0.8889, Val Loss=0.3436, lr=0.0100
[02/21 02:35:40 cifar10-global-group_norm-2.0-resnet56]: Epoch 1/100, Acc=0.8804, Val Loss=0.3984, lr=0.0100
[02/21 02:36:13 cifar10-global-group_norm-2.0-resnet56]: Epoch 2/100, Acc=0.8994, Val Loss=0.3246, lr=0.0100
[02/21 02:36:46 cifar10-global-group_norm-2.0-resnet56]: Epoch 3/100, Acc=0.9037, Val Loss=0.3113, lr=0.0100
[02/21 02:37:19 cifar10-global-group_norm-2.0-resnet56]: Epoch 4/100, Acc=0.8933, Val Loss=0.3433, lr=0.0100
[02/21 02:37:51 cifar10-global-group_norm-2.0-resnet56]: Epoch 5/100, Acc=0.8915, Val Loss=0.3698, lr=0.0100
[02/21 02:38:24 cifar10-global-group_norm-2.0-resnet56]: Epoch 6/100, Acc=0.8783, Val Loss=0.4101, lr=0.0100
[02/21 02:38:57 cifar10-global-group_norm-2.0-resnet56]: Epoch 7/100, Acc=0.9100, Val Loss=0.2995, lr=0.0100
[02/21 02:39:30 cifar10-global-group_norm-2.0-resnet56]: Epoch 8/100, Acc=0.8999, Val Loss=0.3243, lr=0.0100
[02/21 02:40:03 cifar10-global-group_norm-2.0-resnet56]: Epoch 9/100, Acc=0.8903, Val Loss=0.3750, lr=0.0100
[02/21 02:40:35 cifar10-global-group_norm-2.0-resnet56]: Epoch 10/100, Acc=0.8975, Val Loss=0.3445, lr=0.0100
[02/21 02:41:08 cifar10-global-group_norm-2.0-resnet56]: Epoch 11/100, Acc=0.8974, Val Loss=0.3425, lr=0.0100
[02/21 02:41:41 cifar10-global-group_norm-2.0-resnet56]: Epoch 12/100, Acc=0.9075, Val Loss=0.3028, lr=0.0100
[02/21 02:42:14 cifar10-global-group_norm-2.0-resnet56]: Epoch 13/100, Acc=0.9002, Val Loss=0.3390, lr=0.0100
[02/21 02:42:47 cifar10-global-group_norm-2.0-resnet56]: Epoch 14/100, Acc=0.9065, Val Loss=0.3183, lr=0.0100
[02/21 02:43:20 cifar10-global-group_norm-2.0-resnet56]: Epoch 15/100, Acc=0.9070, Val Loss=0.3196, lr=0.0100
[02/21 02:43:53 cifar10-global-group_norm-2.0-resnet56]: Epoch 16/100, Acc=0.9056, Val Loss=0.3202, lr=0.0100
[02/21 02:44:26 cifar10-global-group_norm-2.0-resnet56]: Epoch 17/100, Acc=0.9036, Val Loss=0.3320, lr=0.0100
[02/21 02:44:59 cifar10-global-group_norm-2.0-resnet56]: Epoch 18/100, Acc=0.9052, Val Loss=0.3161, lr=0.0100
[02/21 02:45:32 cifar10-global-group_norm-2.0-resnet56]: Epoch 19/100, Acc=0.8986, Val Loss=0.3397, lr=0.0100
[02/21 02:46:04 cifar10-global-group_norm-2.0-resnet56]: Epoch 20/100, Acc=0.8994, Val Loss=0.3348, lr=0.0100
[02/21 02:46:37 cifar10-global-group_norm-2.0-resnet56]: Epoch 21/100, Acc=0.9115, Val Loss=0.2954, lr=0.0100
[02/21 02:47:09 cifar10-global-group_norm-2.0-resnet56]: Epoch 22/100, Acc=0.9068, Val Loss=0.3191, lr=0.0100
[02/21 02:47:42 cifar10-global-group_norm-2.0-resnet56]: Epoch 23/100, Acc=0.9061, Val Loss=0.3204, lr=0.0100
[02/21 02:48:15 cifar10-global-group_norm-2.0-resnet56]: Epoch 24/100, Acc=0.8998, Val Loss=0.3405, lr=0.0100
[02/21 02:48:48 cifar10-global-group_norm-2.0-resnet56]: Epoch 25/100, Acc=0.9050, Val Loss=0.3286, lr=0.0100
[02/21 02:49:21 cifar10-global-group_norm-2.0-resnet56]: Epoch 26/100, Acc=0.8927, Val Loss=0.3846, lr=0.0100
[02/21 02:49:54 cifar10-global-group_norm-2.0-resnet56]: Epoch 27/100, Acc=0.9028, Val Loss=0.3303, lr=0.0100
[02/21 02:50:27 cifar10-global-group_norm-2.0-resnet56]: Epoch 28/100, Acc=0.9021, Val Loss=0.3380, lr=0.0100
[02/21 02:50:59 cifar10-global-group_norm-2.0-resnet56]: Epoch 29/100, Acc=0.9010, Val Loss=0.3326, lr=0.0100
[02/21 02:51:32 cifar10-global-group_norm-2.0-resnet56]: Epoch 30/100, Acc=0.8977, Val Loss=0.3599, lr=0.0100
[02/21 02:52:05 cifar10-global-group_norm-2.0-resnet56]: Epoch 31/100, Acc=0.9058, Val Loss=0.3229, lr=0.0100
[02/21 02:52:42 cifar10-global-group_norm-2.0-resnet56]: Epoch 32/100, Acc=0.9072, Val Loss=0.3378, lr=0.0100
[02/21 02:53:16 cifar10-global-group_norm-2.0-resnet56]: Epoch 33/100, Acc=0.9139, Val Loss=0.2972, lr=0.0100
[02/21 02:53:53 cifar10-global-group_norm-2.0-resnet56]: Epoch 34/100, Acc=0.9047, Val Loss=0.3259, lr=0.0100
[02/21 02:54:26 cifar10-global-group_norm-2.0-resnet56]: Epoch 35/100, Acc=0.9017, Val Loss=0.3388, lr=0.0100
[02/21 02:54:59 cifar10-global-group_norm-2.0-resnet56]: Epoch 36/100, Acc=0.9100, Val Loss=0.2985, lr=0.0100
[02/21 02:55:33 cifar10-global-group_norm-2.0-resnet56]: Epoch 37/100, Acc=0.9054, Val Loss=0.3392, lr=0.0100
[02/21 02:56:06 cifar10-global-group_norm-2.0-resnet56]: Epoch 38/100, Acc=0.8929, Val Loss=0.3858, lr=0.0100
[02/21 02:56:39 cifar10-global-group_norm-2.0-resnet56]: Epoch 39/100, Acc=0.9052, Val Loss=0.3336, lr=0.0100
[02/21 02:57:12 cifar10-global-group_norm-2.0-resnet56]: Epoch 40/100, Acc=0.9044, Val Loss=0.3298, lr=0.0100
[02/21 02:57:49 cifar10-global-group_norm-2.0-resnet56]: Epoch 41/100, Acc=0.8970, Val Loss=0.3575, lr=0.0100
[02/21 02:58:22 cifar10-global-group_norm-2.0-resnet56]: Epoch 42/100, Acc=0.9001, Val Loss=0.3392, lr=0.0100
[02/21 02:58:55 cifar10-global-group_norm-2.0-resnet56]: Epoch 43/100, Acc=0.9081, Val Loss=0.3315, lr=0.0100
[02/21 02:59:28 cifar10-global-group_norm-2.0-resnet56]: Epoch 44/100, Acc=0.9062, Val Loss=0.3207, lr=0.0100
[02/21 03:00:05 cifar10-global-group_norm-2.0-resnet56]: Epoch 45/100, Acc=0.9022, Val Loss=0.3488, lr=0.0100
[02/21 03:00:38 cifar10-global-group_norm-2.0-resnet56]: Epoch 46/100, Acc=0.8970, Val Loss=0.3478, lr=0.0100
[02/21 03:01:10 cifar10-global-group_norm-2.0-resnet56]: Epoch 47/100, Acc=0.9020, Val Loss=0.3443, lr=0.0100
[02/21 03:01:43 cifar10-global-group_norm-2.0-resnet56]: Epoch 48/100, Acc=0.9020, Val Loss=0.3464, lr=0.0100
[02/21 03:02:16 cifar10-global-group_norm-2.0-resnet56]: Epoch 49/100, Acc=0.9012, Val Loss=0.3275, lr=0.0100
[02/21 03:02:48 cifar10-global-group_norm-2.0-resnet56]: Epoch 50/100, Acc=0.9025, Val Loss=0.3302, lr=0.0100
[02/21 03:03:22 cifar10-global-group_norm-2.0-resnet56]: Epoch 51/100, Acc=0.9109, Val Loss=0.3174, lr=0.0100
[02/21 03:03:55 cifar10-global-group_norm-2.0-resnet56]: Epoch 52/100, Acc=0.9024, Val Loss=0.3560, lr=0.0100
[02/21 03:04:28 cifar10-global-group_norm-2.0-resnet56]: Epoch 53/100, Acc=0.9051, Val Loss=0.3350, lr=0.0100
[02/21 03:05:01 cifar10-global-group_norm-2.0-resnet56]: Epoch 54/100, Acc=0.9057, Val Loss=0.3203, lr=0.0100
[02/21 03:05:35 cifar10-global-group_norm-2.0-resnet56]: Epoch 55/100, Acc=0.9064, Val Loss=0.3220, lr=0.0100
[02/21 03:06:08 cifar10-global-group_norm-2.0-resnet56]: Epoch 56/100, Acc=0.9056, Val Loss=0.3321, lr=0.0100
[02/21 03:06:40 cifar10-global-group_norm-2.0-resnet56]: Epoch 57/100, Acc=0.9079, Val Loss=0.3267, lr=0.0100
[02/21 03:07:13 cifar10-global-group_norm-2.0-resnet56]: Epoch 58/100, Acc=0.9036, Val Loss=0.3573, lr=0.0100
[02/21 03:07:46 cifar10-global-group_norm-2.0-resnet56]: Epoch 59/100, Acc=0.9086, Val Loss=0.3134, lr=0.0100
[02/21 03:08:19 cifar10-global-group_norm-2.0-resnet56]: Epoch 60/100, Acc=0.9305, Val Loss=0.2374, lr=0.0010
[02/21 03:08:52 cifar10-global-group_norm-2.0-resnet56]: Epoch 61/100, Acc=0.9320, Val Loss=0.2415, lr=0.0010
[02/21 03:09:27 cifar10-global-group_norm-2.0-resnet56]: Epoch 62/100, Acc=0.9344, Val Loss=0.2422, lr=0.0010
[02/21 03:09:59 cifar10-global-group_norm-2.0-resnet56]: Epoch 63/100, Acc=0.9339, Val Loss=0.2425, lr=0.0010
[02/21 03:10:33 cifar10-global-group_norm-2.0-resnet56]: Epoch 64/100, Acc=0.9342, Val Loss=0.2462, lr=0.0010
[02/21 03:11:07 cifar10-global-group_norm-2.0-resnet56]: Epoch 65/100, Acc=0.9355, Val Loss=0.2476, lr=0.0010
[02/21 03:11:42 cifar10-global-group_norm-2.0-resnet56]: Epoch 66/100, Acc=0.9345, Val Loss=0.2464, lr=0.0010
[02/21 03:12:18 cifar10-global-group_norm-2.0-resnet56]: Epoch 67/100, Acc=0.9342, Val Loss=0.2500, lr=0.0010
[02/21 03:12:51 cifar10-global-group_norm-2.0-resnet56]: Epoch 68/100, Acc=0.9342, Val Loss=0.2520, lr=0.0010
[02/21 03:13:23 cifar10-global-group_norm-2.0-resnet56]: Epoch 69/100, Acc=0.9352, Val Loss=0.2498, lr=0.0010
[02/21 03:13:57 cifar10-global-group_norm-2.0-resnet56]: Epoch 70/100, Acc=0.9348, Val Loss=0.2534, lr=0.0010
[02/21 03:14:30 cifar10-global-group_norm-2.0-resnet56]: Epoch 71/100, Acc=0.9351, Val Loss=0.2540, lr=0.0010
[02/21 03:15:03 cifar10-global-group_norm-2.0-resnet56]: Epoch 72/100, Acc=0.9359, Val Loss=0.2576, lr=0.0010
[02/21 03:15:36 cifar10-global-group_norm-2.0-resnet56]: Epoch 73/100, Acc=0.9355, Val Loss=0.2588, lr=0.0010
[02/21 03:16:09 cifar10-global-group_norm-2.0-resnet56]: Epoch 74/100, Acc=0.9355, Val Loss=0.2621, lr=0.0010
[02/21 03:16:42 cifar10-global-group_norm-2.0-resnet56]: Epoch 75/100, Acc=0.9357, Val Loss=0.2592, lr=0.0010
[02/21 03:17:15 cifar10-global-group_norm-2.0-resnet56]: Epoch 76/100, Acc=0.9352, Val Loss=0.2610, lr=0.0010
[02/21 03:17:48 cifar10-global-group_norm-2.0-resnet56]: Epoch 77/100, Acc=0.9353, Val Loss=0.2589, lr=0.0010
[02/21 03:18:21 cifar10-global-group_norm-2.0-resnet56]: Epoch 78/100, Acc=0.9350, Val Loss=0.2594, lr=0.0010
[02/21 03:18:55 cifar10-global-group_norm-2.0-resnet56]: Epoch 79/100, Acc=0.9344, Val Loss=0.2630, lr=0.0010
[02/21 03:19:28 cifar10-global-group_norm-2.0-resnet56]: Epoch 80/100, Acc=0.9343, Val Loss=0.2626, lr=0.0001
[02/21 03:20:01 cifar10-global-group_norm-2.0-resnet56]: Epoch 81/100, Acc=0.9350, Val Loss=0.2612, lr=0.0001
[02/21 03:20:34 cifar10-global-group_norm-2.0-resnet56]: Epoch 82/100, Acc=0.9355, Val Loss=0.2605, lr=0.0001
[02/21 03:21:07 cifar10-global-group_norm-2.0-resnet56]: Epoch 83/100, Acc=0.9352, Val Loss=0.2623, lr=0.0001
[02/21 03:21:42 cifar10-global-group_norm-2.0-resnet56]: Epoch 84/100, Acc=0.9347, Val Loss=0.2615, lr=0.0001
[02/21 03:22:15 cifar10-global-group_norm-2.0-resnet56]: Epoch 85/100, Acc=0.9350, Val Loss=0.2623, lr=0.0001
[02/21 03:22:50 cifar10-global-group_norm-2.0-resnet56]: Epoch 86/100, Acc=0.9353, Val Loss=0.2608, lr=0.0001
[02/21 03:23:23 cifar10-global-group_norm-2.0-resnet56]: Epoch 87/100, Acc=0.9350, Val Loss=0.2619, lr=0.0001
[02/21 03:23:56 cifar10-global-group_norm-2.0-resnet56]: Epoch 88/100, Acc=0.9356, Val Loss=0.2593, lr=0.0001
[02/21 03:24:30 cifar10-global-group_norm-2.0-resnet56]: Epoch 89/100, Acc=0.9362, Val Loss=0.2602, lr=0.0001
[02/21 03:25:03 cifar10-global-group_norm-2.0-resnet56]: Epoch 90/100, Acc=0.9354, Val Loss=0.2619, lr=0.0001
[02/21 03:25:36 cifar10-global-group_norm-2.0-resnet56]: Epoch 91/100, Acc=0.9355, Val Loss=0.2622, lr=0.0001
[02/21 03:26:12 cifar10-global-group_norm-2.0-resnet56]: Epoch 92/100, Acc=0.9357, Val Loss=0.2616, lr=0.0001
[02/21 03:26:47 cifar10-global-group_norm-2.0-resnet56]: Epoch 93/100, Acc=0.9360, Val Loss=0.2610, lr=0.0001
[02/21 03:27:20 cifar10-global-group_norm-2.0-resnet56]: Epoch 94/100, Acc=0.9360, Val Loss=0.2617, lr=0.0001
[02/21 03:27:55 cifar10-global-group_norm-2.0-resnet56]: Epoch 95/100, Acc=0.9349, Val Loss=0.2622, lr=0.0001
[02/21 03:28:28 cifar10-global-group_norm-2.0-resnet56]: Epoch 96/100, Acc=0.9350, Val Loss=0.2617, lr=0.0001
[02/21 03:29:00 cifar10-global-group_norm-2.0-resnet56]: Epoch 97/100, Acc=0.9355, Val Loss=0.2611, lr=0.0001
[02/21 03:29:33 cifar10-global-group_norm-2.0-resnet56]: Epoch 98/100, Acc=0.9348, Val Loss=0.2625, lr=0.0001
[02/21 03:30:06 cifar10-global-group_norm-2.0-resnet56]: Epoch 99/100, Acc=0.9353, Val Loss=0.2632, lr=0.0001
[02/21 03:30:06 cifar10-global-group_norm-2.0-resnet56]: Best Acc=0.9362
[02/21 03:30:06 cifar10-global-group_norm-2.0-resnet56]: Params: 0.56 M
[02/21 03:30:06 cifar10-global-group_norm-2.0-resnet56]: ops: 63.06 M
[02/21 03:30:09 cifar10-global-group_norm-2.0-resnet56]: Acc: 0.9353 Val Loss: 0.2632

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: mode: prune
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: model: resnet56
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: verbose: False
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: dataset: cifar10
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: dataroot: data
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: batch_size: 128
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: total_epochs: 100
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: lr_decay_milestones: 60,80
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: lr_decay_gamma: 0.1
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: lr: 0.01
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: output_dir: run/cifar10/prune/cifar10-global-group_sl-2.0-resnet56
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: finetune: True
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: last_epochs: 100
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: reps: 1
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: method: group_sl
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: speed_up: 2.0
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: max_pruning_ratio: 1.0
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: reg: 1e-05
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: delta_reg: 0.0001
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: weight_decay: 0.0005
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: seed: 1
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: global_pruning: True
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: sl_total_epochs: 100
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: sl_lr: 0.01
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: sl_reg_warmup: 0
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: sl_restore: None
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: iterative_steps: 400
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: logger: <Logger cifar10-global-group_sl-2.0-resnet56 (DEBUG)>
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: device: cuda
[02/21 03:30:16 cifar10-global-group_sl-2.0-resnet56]: num_classes: 10
[02/21 03:30:17 cifar10-global-group_sl-2.0-resnet56]: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 03:30:17 cifar10-global-group_sl-2.0-resnet56]: Regularizing...
[02/21 03:31:26 cifar10-global-group_sl-2.0-resnet56]: Epoch 0/100, Acc=0.9075, Val Loss=0.3286, lr=0.0100
[02/21 03:32:38 cifar10-global-group_sl-2.0-resnet56]: Epoch 1/100, Acc=0.8942, Val Loss=0.3858, lr=0.0100
[02/21 03:33:47 cifar10-global-group_sl-2.0-resnet56]: Epoch 2/100, Acc=0.9146, Val Loss=0.2984, lr=0.0100
[02/21 03:34:57 cifar10-global-group_sl-2.0-resnet56]: Epoch 3/100, Acc=0.9121, Val Loss=0.2940, lr=0.0100
[02/21 03:36:09 cifar10-global-group_sl-2.0-resnet56]: Epoch 4/100, Acc=0.9172, Val Loss=0.2961, lr=0.0100
[02/21 03:37:19 cifar10-global-group_sl-2.0-resnet56]: Epoch 5/100, Acc=0.9164, Val Loss=0.3065, lr=0.0100
[02/21 03:38:29 cifar10-global-group_sl-2.0-resnet56]: Epoch 6/100, Acc=0.9101, Val Loss=0.3332, lr=0.0100
[02/21 03:39:38 cifar10-global-group_sl-2.0-resnet56]: Epoch 7/100, Acc=0.9088, Val Loss=0.3520, lr=0.0100
[02/21 03:40:47 cifar10-global-group_sl-2.0-resnet56]: Epoch 8/100, Acc=0.9190, Val Loss=0.3055, lr=0.0100
[02/21 03:41:57 cifar10-global-group_sl-2.0-resnet56]: Epoch 9/100, Acc=0.9207, Val Loss=0.3058, lr=0.0100
[02/21 03:43:08 cifar10-global-group_sl-2.0-resnet56]: Epoch 10/100, Acc=0.9191, Val Loss=0.3149, lr=0.0100
[02/21 03:44:19 cifar10-global-group_sl-2.0-resnet56]: Epoch 11/100, Acc=0.9181, Val Loss=0.3273, lr=0.0100
[02/21 03:45:30 cifar10-global-group_sl-2.0-resnet56]: Epoch 12/100, Acc=0.9233, Val Loss=0.3003, lr=0.0100
[02/21 03:46:40 cifar10-global-group_sl-2.0-resnet56]: Epoch 13/100, Acc=0.9233, Val Loss=0.2900, lr=0.0100
[02/21 03:47:50 cifar10-global-group_sl-2.0-resnet56]: Epoch 14/100, Acc=0.9158, Val Loss=0.3348, lr=0.0100
[02/21 03:48:59 cifar10-global-group_sl-2.0-resnet56]: Epoch 15/100, Acc=0.9244, Val Loss=0.2918, lr=0.0100
[02/21 03:50:09 cifar10-global-group_sl-2.0-resnet56]: Epoch 16/100, Acc=0.9173, Val Loss=0.3520, lr=0.0100
[02/21 03:51:19 cifar10-global-group_sl-2.0-resnet56]: Epoch 17/100, Acc=0.9172, Val Loss=0.3332, lr=0.0100
[02/21 03:52:30 cifar10-global-group_sl-2.0-resnet56]: Epoch 18/100, Acc=0.9224, Val Loss=0.3137, lr=0.0100
[02/21 03:53:42 cifar10-global-group_sl-2.0-resnet56]: Epoch 19/100, Acc=0.9230, Val Loss=0.3110, lr=0.0100
[02/21 03:54:52 cifar10-global-group_sl-2.0-resnet56]: Epoch 20/100, Acc=0.9216, Val Loss=0.3198, lr=0.0100
[02/21 03:56:02 cifar10-global-group_sl-2.0-resnet56]: Epoch 21/100, Acc=0.9241, Val Loss=0.3098, lr=0.0100
[02/21 03:57:12 cifar10-global-group_sl-2.0-resnet56]: Epoch 22/100, Acc=0.9182, Val Loss=0.3341, lr=0.0100
[02/21 03:58:22 cifar10-global-group_sl-2.0-resnet56]: Epoch 23/100, Acc=0.9198, Val Loss=0.3462, lr=0.0100
[02/21 03:59:33 cifar10-global-group_sl-2.0-resnet56]: Epoch 24/100, Acc=0.9230, Val Loss=0.3163, lr=0.0100
[02/21 04:00:44 cifar10-global-group_sl-2.0-resnet56]: Epoch 25/100, Acc=0.9178, Val Loss=0.3431, lr=0.0100
[02/21 04:01:55 cifar10-global-group_sl-2.0-resnet56]: Epoch 26/100, Acc=0.9225, Val Loss=0.3176, lr=0.0100
[02/21 04:03:05 cifar10-global-group_sl-2.0-resnet56]: Epoch 27/100, Acc=0.9234, Val Loss=0.3374, lr=0.0100
[02/21 04:04:15 cifar10-global-group_sl-2.0-resnet56]: Epoch 28/100, Acc=0.9220, Val Loss=0.3270, lr=0.0100
[02/21 04:05:26 cifar10-global-group_sl-2.0-resnet56]: Epoch 29/100, Acc=0.9246, Val Loss=0.3229, lr=0.0100
[02/21 04:06:36 cifar10-global-group_sl-2.0-resnet56]: Epoch 30/100, Acc=0.9220, Val Loss=0.3288, lr=0.0100
[02/21 04:07:46 cifar10-global-group_sl-2.0-resnet56]: Epoch 31/100, Acc=0.9224, Val Loss=0.3340, lr=0.0100
[02/21 04:08:56 cifar10-global-group_sl-2.0-resnet56]: Epoch 32/100, Acc=0.9249, Val Loss=0.3378, lr=0.0100
[02/21 04:10:06 cifar10-global-group_sl-2.0-resnet56]: Epoch 33/100, Acc=0.9242, Val Loss=0.3332, lr=0.0100
[02/21 04:11:16 cifar10-global-group_sl-2.0-resnet56]: Epoch 34/100, Acc=0.9214, Val Loss=0.3358, lr=0.0100
[02/21 04:12:26 cifar10-global-group_sl-2.0-resnet56]: Epoch 35/100, Acc=0.9182, Val Loss=0.3515, lr=0.0100
[02/21 04:13:37 cifar10-global-group_sl-2.0-resnet56]: Epoch 36/100, Acc=0.9242, Val Loss=0.3343, lr=0.0100
[02/21 04:14:47 cifar10-global-group_sl-2.0-resnet56]: Epoch 37/100, Acc=0.9176, Val Loss=0.3638, lr=0.0100
[02/21 04:15:57 cifar10-global-group_sl-2.0-resnet56]: Epoch 38/100, Acc=0.9231, Val Loss=0.3531, lr=0.0100
[02/21 04:17:07 cifar10-global-group_sl-2.0-resnet56]: Epoch 39/100, Acc=0.9246, Val Loss=0.3241, lr=0.0100
[02/21 04:18:17 cifar10-global-group_sl-2.0-resnet56]: Epoch 40/100, Acc=0.9216, Val Loss=0.3613, lr=0.0100
[02/21 04:19:28 cifar10-global-group_sl-2.0-resnet56]: Epoch 41/100, Acc=0.9295, Val Loss=0.3115, lr=0.0100
[02/21 04:20:39 cifar10-global-group_sl-2.0-resnet56]: Epoch 42/100, Acc=0.9221, Val Loss=0.3627, lr=0.0100
[02/21 04:21:49 cifar10-global-group_sl-2.0-resnet56]: Epoch 43/100, Acc=0.9258, Val Loss=0.3592, lr=0.0100
[02/21 04:23:00 cifar10-global-group_sl-2.0-resnet56]: Epoch 44/100, Acc=0.9275, Val Loss=0.3335, lr=0.0100
[02/21 04:24:11 cifar10-global-group_sl-2.0-resnet56]: Epoch 45/100, Acc=0.9240, Val Loss=0.3439, lr=0.0100
[02/21 04:25:22 cifar10-global-group_sl-2.0-resnet56]: Epoch 46/100, Acc=0.9277, Val Loss=0.3367, lr=0.0100
[02/21 04:26:33 cifar10-global-group_sl-2.0-resnet56]: Epoch 47/100, Acc=0.9242, Val Loss=0.3562, lr=0.0100
[02/21 04:27:45 cifar10-global-group_sl-2.0-resnet56]: Epoch 48/100, Acc=0.9255, Val Loss=0.3469, lr=0.0100
[02/21 04:28:56 cifar10-global-group_sl-2.0-resnet56]: Epoch 49/100, Acc=0.9298, Val Loss=0.3263, lr=0.0100
[02/21 04:30:22 cifar10-global-group_sl-2.0-resnet56]: Epoch 50/100, Acc=0.9296, Val Loss=0.3296, lr=0.0100
[02/21 04:31:47 cifar10-global-group_sl-2.0-resnet56]: Epoch 51/100, Acc=0.9269, Val Loss=0.3297, lr=0.0100
[02/21 04:32:59 cifar10-global-group_sl-2.0-resnet56]: Epoch 52/100, Acc=0.9219, Val Loss=0.3430, lr=0.0100
[02/21 04:34:11 cifar10-global-group_sl-2.0-resnet56]: Epoch 53/100, Acc=0.9230, Val Loss=0.3642, lr=0.0100
[02/21 04:35:23 cifar10-global-group_sl-2.0-resnet56]: Epoch 54/100, Acc=0.9266, Val Loss=0.3432, lr=0.0100
[02/21 04:36:36 cifar10-global-group_sl-2.0-resnet56]: Epoch 55/100, Acc=0.9259, Val Loss=0.3455, lr=0.0100
[02/21 04:37:51 cifar10-global-group_sl-2.0-resnet56]: Epoch 56/100, Acc=0.9258, Val Loss=0.3329, lr=0.0100
[02/21 04:39:03 cifar10-global-group_sl-2.0-resnet56]: Epoch 57/100, Acc=0.9205, Val Loss=0.3809, lr=0.0100
[02/21 04:40:16 cifar10-global-group_sl-2.0-resnet56]: Epoch 58/100, Acc=0.9242, Val Loss=0.3609, lr=0.0100
[02/21 04:41:28 cifar10-global-group_sl-2.0-resnet56]: Epoch 59/100, Acc=0.9237, Val Loss=0.3527, lr=0.0100
[02/21 04:42:40 cifar10-global-group_sl-2.0-resnet56]: Epoch 60/100, Acc=0.9332, Val Loss=0.3119, lr=0.0010
[02/21 04:43:52 cifar10-global-group_sl-2.0-resnet56]: Epoch 61/100, Acc=0.9358, Val Loss=0.3042, lr=0.0010
[02/21 04:45:03 cifar10-global-group_sl-2.0-resnet56]: Epoch 62/100, Acc=0.9365, Val Loss=0.3052, lr=0.0010
[02/21 04:46:15 cifar10-global-group_sl-2.0-resnet56]: Epoch 63/100, Acc=0.9380, Val Loss=0.3028, lr=0.0010
[02/21 04:47:25 cifar10-global-group_sl-2.0-resnet56]: Epoch 64/100, Acc=0.9379, Val Loss=0.3048, lr=0.0010
[02/21 04:48:35 cifar10-global-group_sl-2.0-resnet56]: Epoch 65/100, Acc=0.9380, Val Loss=0.3023, lr=0.0010
[02/21 04:49:45 cifar10-global-group_sl-2.0-resnet56]: Epoch 66/100, Acc=0.9381, Val Loss=0.3072, lr=0.0010
[02/21 04:50:56 cifar10-global-group_sl-2.0-resnet56]: Epoch 67/100, Acc=0.9390, Val Loss=0.3032, lr=0.0010
[02/21 04:52:12 cifar10-global-group_sl-2.0-resnet56]: Epoch 68/100, Acc=0.9390, Val Loss=0.3062, lr=0.0010
[02/21 04:53:22 cifar10-global-group_sl-2.0-resnet56]: Epoch 69/100, Acc=0.9385, Val Loss=0.3042, lr=0.0010
[02/21 04:54:32 cifar10-global-group_sl-2.0-resnet56]: Epoch 70/100, Acc=0.9376, Val Loss=0.3085, lr=0.0010
[02/21 04:55:42 cifar10-global-group_sl-2.0-resnet56]: Epoch 71/100, Acc=0.9379, Val Loss=0.3117, lr=0.0010
[02/21 04:56:52 cifar10-global-group_sl-2.0-resnet56]: Epoch 72/100, Acc=0.9376, Val Loss=0.3095, lr=0.0010
[02/21 04:58:02 cifar10-global-group_sl-2.0-resnet56]: Epoch 73/100, Acc=0.9382, Val Loss=0.3119, lr=0.0010
[02/21 04:59:12 cifar10-global-group_sl-2.0-resnet56]: Epoch 74/100, Acc=0.9392, Val Loss=0.3095, lr=0.0010
[02/21 05:00:26 cifar10-global-group_sl-2.0-resnet56]: Epoch 75/100, Acc=0.9388, Val Loss=0.3120, lr=0.0010
[02/21 05:01:46 cifar10-global-group_sl-2.0-resnet56]: Epoch 76/100, Acc=0.9386, Val Loss=0.3097, lr=0.0010
[02/21 05:02:55 cifar10-global-group_sl-2.0-resnet56]: Epoch 77/100, Acc=0.9394, Val Loss=0.3121, lr=0.0010
[02/21 05:04:05 cifar10-global-group_sl-2.0-resnet56]: Epoch 78/100, Acc=0.9394, Val Loss=0.3105, lr=0.0010
[02/21 05:05:14 cifar10-global-group_sl-2.0-resnet56]: Epoch 79/100, Acc=0.9389, Val Loss=0.3108, lr=0.0010
[02/21 05:06:24 cifar10-global-group_sl-2.0-resnet56]: Epoch 80/100, Acc=0.9395, Val Loss=0.3124, lr=0.0001
[02/21 05:07:44 cifar10-global-group_sl-2.0-resnet56]: Epoch 81/100, Acc=0.9379, Val Loss=0.3142, lr=0.0001
[02/21 05:08:54 cifar10-global-group_sl-2.0-resnet56]: Epoch 82/100, Acc=0.9380, Val Loss=0.3137, lr=0.0001
[02/21 05:10:04 cifar10-global-group_sl-2.0-resnet56]: Epoch 83/100, Acc=0.9391, Val Loss=0.3122, lr=0.0001
[02/21 05:11:22 cifar10-global-group_sl-2.0-resnet56]: Epoch 84/100, Acc=0.9386, Val Loss=0.3129, lr=0.0001
[02/21 05:12:45 cifar10-global-group_sl-2.0-resnet56]: Epoch 85/100, Acc=0.9391, Val Loss=0.3149, lr=0.0001
[02/21 05:14:00 cifar10-global-group_sl-2.0-resnet56]: Epoch 86/100, Acc=0.9381, Val Loss=0.3130, lr=0.0001
[02/21 05:15:10 cifar10-global-group_sl-2.0-resnet56]: Epoch 87/100, Acc=0.9395, Val Loss=0.3141, lr=0.0001
[02/21 05:16:26 cifar10-global-group_sl-2.0-resnet56]: Epoch 88/100, Acc=0.9387, Val Loss=0.3119, lr=0.0001
[02/21 05:17:36 cifar10-global-group_sl-2.0-resnet56]: Epoch 89/100, Acc=0.9404, Val Loss=0.3134, lr=0.0001
[02/21 05:18:45 cifar10-global-group_sl-2.0-resnet56]: Epoch 90/100, Acc=0.9397, Val Loss=0.3143, lr=0.0001
[02/21 05:19:55 cifar10-global-group_sl-2.0-resnet56]: Epoch 91/100, Acc=0.9390, Val Loss=0.3138, lr=0.0001
[02/21 05:21:05 cifar10-global-group_sl-2.0-resnet56]: Epoch 92/100, Acc=0.9385, Val Loss=0.3111, lr=0.0001
[02/21 05:22:15 cifar10-global-group_sl-2.0-resnet56]: Epoch 93/100, Acc=0.9392, Val Loss=0.3086, lr=0.0001
[02/21 05:23:25 cifar10-global-group_sl-2.0-resnet56]: Epoch 94/100, Acc=0.9386, Val Loss=0.3129, lr=0.0001
[02/21 05:24:35 cifar10-global-group_sl-2.0-resnet56]: Epoch 95/100, Acc=0.9387, Val Loss=0.3128, lr=0.0001
[02/21 05:25:46 cifar10-global-group_sl-2.0-resnet56]: Epoch 96/100, Acc=0.9399, Val Loss=0.3103, lr=0.0001
[02/21 05:26:56 cifar10-global-group_sl-2.0-resnet56]: Epoch 97/100, Acc=0.9385, Val Loss=0.3124, lr=0.0001
[02/21 05:28:07 cifar10-global-group_sl-2.0-resnet56]: Epoch 98/100, Acc=0.9387, Val Loss=0.3142, lr=0.0001
[02/21 05:29:17 cifar10-global-group_sl-2.0-resnet56]: Epoch 99/100, Acc=0.9386, Val Loss=0.3144, lr=0.0001
[02/21 05:29:17 cifar10-global-group_sl-2.0-resnet56]: Best Acc=0.9404
[02/21 05:29:17 cifar10-global-group_sl-2.0-resnet56]: Loading the sparse model from run/cifar10/prune/cifar10-global-group_sl-2.0-resnet56/reg_cifar10_resnet56_group_sl_1e-05.pth...
[02/21 05:29:20 cifar10-global-group_sl-2.0-resnet56]: Pruning...
[02/21 05:29:34 cifar10-global-group_sl-2.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(12, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(12, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(12, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(12, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(12, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(12, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(12, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(12, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(12, 19, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(12, 23, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(23, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(23, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(23, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(23, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(23, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(23, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(23, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(23, 39, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(39, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(39, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(23, 58, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(58, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(58, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(58, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(58, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(58, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(58, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(58, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(58, 38, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(38, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(58, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(44, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=58, out_features=10, bias=True)
)
[02/21 05:29:38 cifar10-global-group_sl-2.0-resnet56]: Params: 0.86 M => 0.56 M (65.09%)
[02/21 05:29:38 cifar10-global-group_sl-2.0-resnet56]: FLOPs: 127.12 M => 62.98 M (49.55%, 2.02X )
[02/21 05:29:38 cifar10-global-group_sl-2.0-resnet56]: Acc: 0.9404 => 0.1774
[02/21 05:29:38 cifar10-global-group_sl-2.0-resnet56]: Val Loss: 0.3134 => 9.3704
[02/21 05:29:38 cifar10-global-group_sl-2.0-resnet56]: Finetuning...
[02/21 05:30:10 cifar10-global-group_sl-2.0-resnet56]: Epoch 0/100, Acc=0.8997, Val Loss=0.3460, lr=0.0100
[02/21 05:30:43 cifar10-global-group_sl-2.0-resnet56]: Epoch 1/100, Acc=0.9054, Val Loss=0.3324, lr=0.0100
[02/21 05:31:16 cifar10-global-group_sl-2.0-resnet56]: Epoch 2/100, Acc=0.9005, Val Loss=0.3550, lr=0.0100
[02/21 05:31:49 cifar10-global-group_sl-2.0-resnet56]: Epoch 3/100, Acc=0.9033, Val Loss=0.3641, lr=0.0100
[02/21 05:32:22 cifar10-global-group_sl-2.0-resnet56]: Epoch 4/100, Acc=0.8976, Val Loss=0.3633, lr=0.0100
[02/21 05:32:55 cifar10-global-group_sl-2.0-resnet56]: Epoch 5/100, Acc=0.9066, Val Loss=0.3428, lr=0.0100
[02/21 05:33:29 cifar10-global-group_sl-2.0-resnet56]: Epoch 6/100, Acc=0.9048, Val Loss=0.3508, lr=0.0100
[02/21 05:34:02 cifar10-global-group_sl-2.0-resnet56]: Epoch 7/100, Acc=0.9034, Val Loss=0.3620, lr=0.0100
[02/21 05:34:35 cifar10-global-group_sl-2.0-resnet56]: Epoch 8/100, Acc=0.9059, Val Loss=0.3578, lr=0.0100
[02/21 05:35:10 cifar10-global-group_sl-2.0-resnet56]: Epoch 9/100, Acc=0.9093, Val Loss=0.3348, lr=0.0100
[02/21 05:35:46 cifar10-global-group_sl-2.0-resnet56]: Epoch 10/100, Acc=0.9086, Val Loss=0.3237, lr=0.0100
[02/21 05:36:21 cifar10-global-group_sl-2.0-resnet56]: Epoch 11/100, Acc=0.9037, Val Loss=0.3454, lr=0.0100
[02/21 05:37:00 cifar10-global-group_sl-2.0-resnet56]: Epoch 12/100, Acc=0.9086, Val Loss=0.3267, lr=0.0100
[02/21 05:37:42 cifar10-global-group_sl-2.0-resnet56]: Epoch 13/100, Acc=0.9058, Val Loss=0.3423, lr=0.0100
[02/21 05:38:23 cifar10-global-group_sl-2.0-resnet56]: Epoch 14/100, Acc=0.9068, Val Loss=0.3261, lr=0.0100
[02/21 05:39:03 cifar10-global-group_sl-2.0-resnet56]: Epoch 15/100, Acc=0.8992, Val Loss=0.3733, lr=0.0100
[02/21 05:39:40 cifar10-global-group_sl-2.0-resnet56]: Epoch 16/100, Acc=0.8994, Val Loss=0.3692, lr=0.0100
[02/21 05:40:13 cifar10-global-group_sl-2.0-resnet56]: Epoch 17/100, Acc=0.9026, Val Loss=0.3459, lr=0.0100
[02/21 05:40:47 cifar10-global-group_sl-2.0-resnet56]: Epoch 18/100, Acc=0.9003, Val Loss=0.3430, lr=0.0100
[02/21 05:41:20 cifar10-global-group_sl-2.0-resnet56]: Epoch 19/100, Acc=0.9074, Val Loss=0.3386, lr=0.0100
[02/21 05:41:53 cifar10-global-group_sl-2.0-resnet56]: Epoch 20/100, Acc=0.9007, Val Loss=0.3605, lr=0.0100
[02/21 05:42:27 cifar10-global-group_sl-2.0-resnet56]: Epoch 21/100, Acc=0.9108, Val Loss=0.3199, lr=0.0100
[02/21 05:43:00 cifar10-global-group_sl-2.0-resnet56]: Epoch 22/100, Acc=0.9132, Val Loss=0.3123, lr=0.0100
[02/21 05:43:34 cifar10-global-group_sl-2.0-resnet56]: Epoch 23/100, Acc=0.8975, Val Loss=0.3648, lr=0.0100
[02/21 05:44:07 cifar10-global-group_sl-2.0-resnet56]: Epoch 24/100, Acc=0.9116, Val Loss=0.3186, lr=0.0100
[02/21 05:44:41 cifar10-global-group_sl-2.0-resnet56]: Epoch 25/100, Acc=0.8957, Val Loss=0.3738, lr=0.0100
[02/21 05:45:14 cifar10-global-group_sl-2.0-resnet56]: Epoch 26/100, Acc=0.8763, Val Loss=0.4417, lr=0.0100
[02/21 05:45:48 cifar10-global-group_sl-2.0-resnet56]: Epoch 27/100, Acc=0.9073, Val Loss=0.3169, lr=0.0100
[02/21 05:46:22 cifar10-global-group_sl-2.0-resnet56]: Epoch 28/100, Acc=0.9008, Val Loss=0.3550, lr=0.0100
[02/21 05:46:55 cifar10-global-group_sl-2.0-resnet56]: Epoch 29/100, Acc=0.8971, Val Loss=0.3914, lr=0.0100
[02/21 05:47:29 cifar10-global-group_sl-2.0-resnet56]: Epoch 30/100, Acc=0.9044, Val Loss=0.3500, lr=0.0100
[02/21 05:48:02 cifar10-global-group_sl-2.0-resnet56]: Epoch 31/100, Acc=0.9040, Val Loss=0.3545, lr=0.0100
[02/21 05:48:35 cifar10-global-group_sl-2.0-resnet56]: Epoch 32/100, Acc=0.8991, Val Loss=0.3466, lr=0.0100
[02/21 05:49:08 cifar10-global-group_sl-2.0-resnet56]: Epoch 33/100, Acc=0.9014, Val Loss=0.3666, lr=0.0100
[02/21 05:49:42 cifar10-global-group_sl-2.0-resnet56]: Epoch 34/100, Acc=0.8988, Val Loss=0.3540, lr=0.0100
[02/21 05:50:15 cifar10-global-group_sl-2.0-resnet56]: Epoch 35/100, Acc=0.9004, Val Loss=0.3591, lr=0.0100
[02/21 05:50:49 cifar10-global-group_sl-2.0-resnet56]: Epoch 36/100, Acc=0.9119, Val Loss=0.3171, lr=0.0100
[02/21 05:51:22 cifar10-global-group_sl-2.0-resnet56]: Epoch 37/100, Acc=0.9052, Val Loss=0.3329, lr=0.0100
[02/21 05:51:56 cifar10-global-group_sl-2.0-resnet56]: Epoch 38/100, Acc=0.8930, Val Loss=0.3896, lr=0.0100
[02/21 05:52:30 cifar10-global-group_sl-2.0-resnet56]: Epoch 39/100, Acc=0.9143, Val Loss=0.2931, lr=0.0100
[02/21 05:53:03 cifar10-global-group_sl-2.0-resnet56]: Epoch 40/100, Acc=0.9104, Val Loss=0.3078, lr=0.0100
[02/21 05:53:36 cifar10-global-group_sl-2.0-resnet56]: Epoch 41/100, Acc=0.9085, Val Loss=0.3230, lr=0.0100
[02/21 05:54:08 cifar10-global-group_sl-2.0-resnet56]: Epoch 42/100, Acc=0.9099, Val Loss=0.3121, lr=0.0100
[02/21 05:54:41 cifar10-global-group_sl-2.0-resnet56]: Epoch 43/100, Acc=0.9079, Val Loss=0.3150, lr=0.0100
[02/21 05:55:14 cifar10-global-group_sl-2.0-resnet56]: Epoch 44/100, Acc=0.9035, Val Loss=0.3411, lr=0.0100
[02/21 05:55:48 cifar10-global-group_sl-2.0-resnet56]: Epoch 45/100, Acc=0.9071, Val Loss=0.3219, lr=0.0100
[02/21 05:56:21 cifar10-global-group_sl-2.0-resnet56]: Epoch 46/100, Acc=0.9055, Val Loss=0.3400, lr=0.0100
[02/21 05:56:54 cifar10-global-group_sl-2.0-resnet56]: Epoch 47/100, Acc=0.8958, Val Loss=0.3769, lr=0.0100
[02/21 05:57:27 cifar10-global-group_sl-2.0-resnet56]: Epoch 48/100, Acc=0.8997, Val Loss=0.3687, lr=0.0100
[02/21 05:58:00 cifar10-global-group_sl-2.0-resnet56]: Epoch 49/100, Acc=0.8998, Val Loss=0.3550, lr=0.0100
[02/21 05:58:33 cifar10-global-group_sl-2.0-resnet56]: Epoch 50/100, Acc=0.9073, Val Loss=0.3251, lr=0.0100
[02/21 05:59:07 cifar10-global-group_sl-2.0-resnet56]: Epoch 51/100, Acc=0.8981, Val Loss=0.3760, lr=0.0100
[02/21 05:59:40 cifar10-global-group_sl-2.0-resnet56]: Epoch 52/100, Acc=0.8986, Val Loss=0.3682, lr=0.0100
[02/21 06:00:13 cifar10-global-group_sl-2.0-resnet56]: Epoch 53/100, Acc=0.8915, Val Loss=0.3972, lr=0.0100
[02/21 06:00:46 cifar10-global-group_sl-2.0-resnet56]: Epoch 54/100, Acc=0.8939, Val Loss=0.4088, lr=0.0100
[02/21 06:01:20 cifar10-global-group_sl-2.0-resnet56]: Epoch 55/100, Acc=0.9079, Val Loss=0.3248, lr=0.0100
[02/21 06:01:52 cifar10-global-group_sl-2.0-resnet56]: Epoch 56/100, Acc=0.9084, Val Loss=0.3229, lr=0.0100
[02/21 06:02:25 cifar10-global-group_sl-2.0-resnet56]: Epoch 57/100, Acc=0.9064, Val Loss=0.3319, lr=0.0100
[02/21 06:02:58 cifar10-global-group_sl-2.0-resnet56]: Epoch 58/100, Acc=0.9066, Val Loss=0.3411, lr=0.0100
[02/21 06:03:32 cifar10-global-group_sl-2.0-resnet56]: Epoch 59/100, Acc=0.9035, Val Loss=0.3446, lr=0.0100
[02/21 06:04:06 cifar10-global-group_sl-2.0-resnet56]: Epoch 60/100, Acc=0.9286, Val Loss=0.2529, lr=0.0010
[02/21 06:04:39 cifar10-global-group_sl-2.0-resnet56]: Epoch 61/100, Acc=0.9315, Val Loss=0.2489, lr=0.0010
[02/21 06:05:12 cifar10-global-group_sl-2.0-resnet56]: Epoch 62/100, Acc=0.9317, Val Loss=0.2543, lr=0.0010
[02/21 06:05:46 cifar10-global-group_sl-2.0-resnet56]: Epoch 63/100, Acc=0.9341, Val Loss=0.2494, lr=0.0010
[02/21 06:06:20 cifar10-global-group_sl-2.0-resnet56]: Epoch 64/100, Acc=0.9347, Val Loss=0.2534, lr=0.0010
[02/21 06:06:53 cifar10-global-group_sl-2.0-resnet56]: Epoch 65/100, Acc=0.9331, Val Loss=0.2549, lr=0.0010
[02/21 06:07:27 cifar10-global-group_sl-2.0-resnet56]: Epoch 66/100, Acc=0.9335, Val Loss=0.2581, lr=0.0010
[02/21 06:08:00 cifar10-global-group_sl-2.0-resnet56]: Epoch 67/100, Acc=0.9341, Val Loss=0.2582, lr=0.0010
[02/21 06:08:34 cifar10-global-group_sl-2.0-resnet56]: Epoch 68/100, Acc=0.9338, Val Loss=0.2597, lr=0.0010
[02/21 06:09:07 cifar10-global-group_sl-2.0-resnet56]: Epoch 69/100, Acc=0.9338, Val Loss=0.2603, lr=0.0010
[02/21 06:09:41 cifar10-global-group_sl-2.0-resnet56]: Epoch 70/100, Acc=0.9341, Val Loss=0.2602, lr=0.0010
[02/21 06:10:14 cifar10-global-group_sl-2.0-resnet56]: Epoch 71/100, Acc=0.9336, Val Loss=0.2644, lr=0.0010
[02/21 06:10:48 cifar10-global-group_sl-2.0-resnet56]: Epoch 72/100, Acc=0.9350, Val Loss=0.2603, lr=0.0010
[02/21 06:11:21 cifar10-global-group_sl-2.0-resnet56]: Epoch 73/100, Acc=0.9340, Val Loss=0.2630, lr=0.0010
[02/21 06:11:55 cifar10-global-group_sl-2.0-resnet56]: Epoch 74/100, Acc=0.9362, Val Loss=0.2649, lr=0.0010
[02/21 06:12:28 cifar10-global-group_sl-2.0-resnet56]: Epoch 75/100, Acc=0.9352, Val Loss=0.2672, lr=0.0010
[02/21 06:13:02 cifar10-global-group_sl-2.0-resnet56]: Epoch 76/100, Acc=0.9341, Val Loss=0.2687, lr=0.0010
[02/21 06:13:35 cifar10-global-group_sl-2.0-resnet56]: Epoch 77/100, Acc=0.9342, Val Loss=0.2658, lr=0.0010
[02/21 06:14:08 cifar10-global-group_sl-2.0-resnet56]: Epoch 78/100, Acc=0.9352, Val Loss=0.2676, lr=0.0010
[02/21 06:14:41 cifar10-global-group_sl-2.0-resnet56]: Epoch 79/100, Acc=0.9360, Val Loss=0.2678, lr=0.0010
[02/21 06:15:15 cifar10-global-group_sl-2.0-resnet56]: Epoch 80/100, Acc=0.9350, Val Loss=0.2646, lr=0.0001
[02/21 06:15:48 cifar10-global-group_sl-2.0-resnet56]: Epoch 81/100, Acc=0.9355, Val Loss=0.2666, lr=0.0001
[02/21 06:16:22 cifar10-global-group_sl-2.0-resnet56]: Epoch 82/100, Acc=0.9356, Val Loss=0.2664, lr=0.0001
[02/21 06:16:55 cifar10-global-group_sl-2.0-resnet56]: Epoch 83/100, Acc=0.9357, Val Loss=0.2663, lr=0.0001
[02/21 06:17:29 cifar10-global-group_sl-2.0-resnet56]: Epoch 84/100, Acc=0.9357, Val Loss=0.2664, lr=0.0001
[02/21 06:18:02 cifar10-global-group_sl-2.0-resnet56]: Epoch 85/100, Acc=0.9356, Val Loss=0.2662, lr=0.0001
[02/21 06:18:35 cifar10-global-group_sl-2.0-resnet56]: Epoch 86/100, Acc=0.9352, Val Loss=0.2660, lr=0.0001
[02/21 06:19:09 cifar10-global-group_sl-2.0-resnet56]: Epoch 87/100, Acc=0.9357, Val Loss=0.2644, lr=0.0001
[02/21 06:19:42 cifar10-global-group_sl-2.0-resnet56]: Epoch 88/100, Acc=0.9348, Val Loss=0.2633, lr=0.0001
[02/21 06:20:15 cifar10-global-group_sl-2.0-resnet56]: Epoch 89/100, Acc=0.9352, Val Loss=0.2658, lr=0.0001
[02/21 06:20:49 cifar10-global-group_sl-2.0-resnet56]: Epoch 90/100, Acc=0.9352, Val Loss=0.2654, lr=0.0001
[02/21 06:21:22 cifar10-global-group_sl-2.0-resnet56]: Epoch 91/100, Acc=0.9359, Val Loss=0.2648, lr=0.0001
[02/21 06:21:55 cifar10-global-group_sl-2.0-resnet56]: Epoch 92/100, Acc=0.9352, Val Loss=0.2645, lr=0.0001
[02/21 06:22:28 cifar10-global-group_sl-2.0-resnet56]: Epoch 93/100, Acc=0.9347, Val Loss=0.2645, lr=0.0001
[02/21 06:23:07 cifar10-global-group_sl-2.0-resnet56]: Epoch 94/100, Acc=0.9351, Val Loss=0.2668, lr=0.0001
[02/21 06:23:47 cifar10-global-group_sl-2.0-resnet56]: Epoch 95/100, Acc=0.9356, Val Loss=0.2651, lr=0.0001
[02/21 06:24:26 cifar10-global-group_sl-2.0-resnet56]: Epoch 96/100, Acc=0.9353, Val Loss=0.2643, lr=0.0001
[02/21 06:25:04 cifar10-global-group_sl-2.0-resnet56]: Epoch 97/100, Acc=0.9356, Val Loss=0.2651, lr=0.0001
[02/21 06:25:37 cifar10-global-group_sl-2.0-resnet56]: Epoch 98/100, Acc=0.9358, Val Loss=0.2662, lr=0.0001
[02/21 06:26:11 cifar10-global-group_sl-2.0-resnet56]: Epoch 99/100, Acc=0.9350, Val Loss=0.2684, lr=0.0001
[02/21 06:26:11 cifar10-global-group_sl-2.0-resnet56]: Best Acc=0.9362
[02/21 06:26:11 cifar10-global-group_sl-2.0-resnet56]: Params: 0.56 M
[02/21 06:26:11 cifar10-global-group_sl-2.0-resnet56]: ops: 62.98 M
[02/21 06:26:14 cifar10-global-group_sl-2.0-resnet56]: Acc: 0.9350 Val Loss: 0.2684

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: mode: prune
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: model: resnet56
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: verbose: False
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: dataset: cifar10
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: dataroot: data
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: batch_size: 128
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: total_epochs: 100
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: lr_decay_milestones: 60,80
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: lr_decay_gamma: 0.1
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: lr: 0.01
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: output_dir: run/cifar10/prune/cifar10-global-proj-2.0-resnet56
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: finetune: True
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: last_epochs: 100
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: reps: 1
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: method: proj
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: speed_up: 2.0
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: max_pruning_ratio: 1.0
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: reg: 1e-05
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: delta_reg: 0.0001
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: weight_decay: 0.0005
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: seed: 1
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: global_pruning: True
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: sl_total_epochs: 100
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: sl_lr: 0.01
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: sl_reg_warmup: 0
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: sl_restore: None
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: iterative_steps: 400
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: logger: <Logger cifar10-global-proj-2.0-resnet56 (DEBUG)>
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: device: cuda
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: num_classes: 10
[02/21 06:26:22 cifar10-global-proj-2.0-resnet56]: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 06:26:25 cifar10-global-proj-2.0-resnet56]: Pruning...
[02/21 06:28:32 cifar10-global-proj-2.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(10, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(10, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(10, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(10, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(10, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(10, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(10, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(10, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(10, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(10, 25, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(10, 31, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(31, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(31, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(31, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(31, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(21, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(31, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(31, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(31, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(31, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(31, 62, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(31, 61, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(61, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(61, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(52, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(61, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(51, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(61, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(61, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(51, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(61, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(46, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(61, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(36, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(61, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(40, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=61, out_features=10, bias=True)
)
[02/21 06:28:35 cifar10-global-proj-2.0-resnet56]: Params: 0.86 M => 0.56 M (65.94%)
[02/21 06:28:35 cifar10-global-proj-2.0-resnet56]: FLOPs: 127.12 M => 63.45 M (49.91%, 2.00X )
[02/21 06:28:35 cifar10-global-proj-2.0-resnet56]: Acc: 0.9392 => 0.1316
[02/21 06:28:35 cifar10-global-proj-2.0-resnet56]: Val Loss: 0.2587 => 5.8787
[02/21 06:28:35 cifar10-global-proj-2.0-resnet56]: Finetuning...
[02/21 06:29:08 cifar10-global-proj-2.0-resnet56]: Epoch 0/100, Acc=0.8656, Val Loss=0.4305, lr=0.0100
[02/21 06:29:41 cifar10-global-proj-2.0-resnet56]: Epoch 1/100, Acc=0.8917, Val Loss=0.3488, lr=0.0100
[02/21 06:30:14 cifar10-global-proj-2.0-resnet56]: Epoch 2/100, Acc=0.8972, Val Loss=0.3388, lr=0.0100
[02/21 06:30:47 cifar10-global-proj-2.0-resnet56]: Epoch 3/100, Acc=0.9015, Val Loss=0.3040, lr=0.0100
[02/21 06:31:20 cifar10-global-proj-2.0-resnet56]: Epoch 4/100, Acc=0.8957, Val Loss=0.3499, lr=0.0100
[02/21 06:31:54 cifar10-global-proj-2.0-resnet56]: Epoch 5/100, Acc=0.8909, Val Loss=0.3646, lr=0.0100
[02/21 06:32:27 cifar10-global-proj-2.0-resnet56]: Epoch 6/100, Acc=0.8786, Val Loss=0.3928, lr=0.0100
[02/21 06:33:01 cifar10-global-proj-2.0-resnet56]: Epoch 7/100, Acc=0.8944, Val Loss=0.3492, lr=0.0100
[02/21 06:33:34 cifar10-global-proj-2.0-resnet56]: Epoch 8/100, Acc=0.8906, Val Loss=0.3644, lr=0.0100
[02/21 06:34:09 cifar10-global-proj-2.0-resnet56]: Epoch 9/100, Acc=0.9035, Val Loss=0.3191, lr=0.0100
[02/21 06:34:42 cifar10-global-proj-2.0-resnet56]: Epoch 10/100, Acc=0.8944, Val Loss=0.3438, lr=0.0100
[02/21 06:35:16 cifar10-global-proj-2.0-resnet56]: Epoch 11/100, Acc=0.9008, Val Loss=0.3312, lr=0.0100
[02/21 06:35:50 cifar10-global-proj-2.0-resnet56]: Epoch 12/100, Acc=0.8859, Val Loss=0.3971, lr=0.0100
[02/21 06:36:24 cifar10-global-proj-2.0-resnet56]: Epoch 13/100, Acc=0.8985, Val Loss=0.3477, lr=0.0100
[02/21 06:36:57 cifar10-global-proj-2.0-resnet56]: Epoch 14/100, Acc=0.9051, Val Loss=0.3254, lr=0.0100
[02/21 06:37:31 cifar10-global-proj-2.0-resnet56]: Epoch 15/100, Acc=0.9131, Val Loss=0.3033, lr=0.0100
[02/21 06:38:04 cifar10-global-proj-2.0-resnet56]: Epoch 16/100, Acc=0.9023, Val Loss=0.3296, lr=0.0100
[02/21 06:38:38 cifar10-global-proj-2.0-resnet56]: Epoch 17/100, Acc=0.9062, Val Loss=0.3037, lr=0.0100
[02/21 06:39:11 cifar10-global-proj-2.0-resnet56]: Epoch 18/100, Acc=0.8895, Val Loss=0.3742, lr=0.0100
[02/21 06:39:45 cifar10-global-proj-2.0-resnet56]: Epoch 19/100, Acc=0.9104, Val Loss=0.3012, lr=0.0100
[02/21 06:40:19 cifar10-global-proj-2.0-resnet56]: Epoch 20/100, Acc=0.9046, Val Loss=0.3185, lr=0.0100
[02/21 06:40:52 cifar10-global-proj-2.0-resnet56]: Epoch 21/100, Acc=0.9009, Val Loss=0.3376, lr=0.0100
[02/21 06:41:25 cifar10-global-proj-2.0-resnet56]: Epoch 22/100, Acc=0.9047, Val Loss=0.3260, lr=0.0100
[02/21 06:41:58 cifar10-global-proj-2.0-resnet56]: Epoch 23/100, Acc=0.9025, Val Loss=0.3282, lr=0.0100
[02/21 06:42:31 cifar10-global-proj-2.0-resnet56]: Epoch 24/100, Acc=0.8965, Val Loss=0.3455, lr=0.0100
[02/21 06:43:04 cifar10-global-proj-2.0-resnet56]: Epoch 25/100, Acc=0.9029, Val Loss=0.3234, lr=0.0100
[02/21 06:43:38 cifar10-global-proj-2.0-resnet56]: Epoch 26/100, Acc=0.8963, Val Loss=0.3552, lr=0.0100
[02/21 06:44:12 cifar10-global-proj-2.0-resnet56]: Epoch 27/100, Acc=0.9094, Val Loss=0.3267, lr=0.0100
[02/21 06:44:45 cifar10-global-proj-2.0-resnet56]: Epoch 28/100, Acc=0.9052, Val Loss=0.3226, lr=0.0100
[02/21 06:45:18 cifar10-global-proj-2.0-resnet56]: Epoch 29/100, Acc=0.9040, Val Loss=0.3343, lr=0.0100
[02/21 06:45:51 cifar10-global-proj-2.0-resnet56]: Epoch 30/100, Acc=0.9074, Val Loss=0.3255, lr=0.0100
[02/21 06:46:25 cifar10-global-proj-2.0-resnet56]: Epoch 31/100, Acc=0.9033, Val Loss=0.3336, lr=0.0100
[02/21 06:46:57 cifar10-global-proj-2.0-resnet56]: Epoch 32/100, Acc=0.9012, Val Loss=0.3291, lr=0.0100
[02/21 06:47:30 cifar10-global-proj-2.0-resnet56]: Epoch 33/100, Acc=0.9146, Val Loss=0.2932, lr=0.0100
[02/21 06:48:09 cifar10-global-proj-2.0-resnet56]: Epoch 34/100, Acc=0.8996, Val Loss=0.3457, lr=0.0100
[02/21 06:48:42 cifar10-global-proj-2.0-resnet56]: Epoch 35/100, Acc=0.9027, Val Loss=0.3446, lr=0.0100
[02/21 06:49:15 cifar10-global-proj-2.0-resnet56]: Epoch 36/100, Acc=0.9001, Val Loss=0.3511, lr=0.0100
[02/21 06:49:48 cifar10-global-proj-2.0-resnet56]: Epoch 37/100, Acc=0.9080, Val Loss=0.3102, lr=0.0100
[02/21 06:50:21 cifar10-global-proj-2.0-resnet56]: Epoch 38/100, Acc=0.9058, Val Loss=0.3256, lr=0.0100
[02/21 06:50:55 cifar10-global-proj-2.0-resnet56]: Epoch 39/100, Acc=0.9022, Val Loss=0.3475, lr=0.0100
[02/21 06:51:28 cifar10-global-proj-2.0-resnet56]: Epoch 40/100, Acc=0.9027, Val Loss=0.3291, lr=0.0100
[02/21 06:52:00 cifar10-global-proj-2.0-resnet56]: Epoch 41/100, Acc=0.8982, Val Loss=0.3499, lr=0.0100
[02/21 06:52:33 cifar10-global-proj-2.0-resnet56]: Epoch 42/100, Acc=0.9075, Val Loss=0.3095, lr=0.0100
[02/21 06:53:07 cifar10-global-proj-2.0-resnet56]: Epoch 43/100, Acc=0.9060, Val Loss=0.3288, lr=0.0100
[02/21 06:53:40 cifar10-global-proj-2.0-resnet56]: Epoch 44/100, Acc=0.9040, Val Loss=0.3210, lr=0.0100
[02/21 06:54:14 cifar10-global-proj-2.0-resnet56]: Epoch 45/100, Acc=0.9086, Val Loss=0.3125, lr=0.0100
[02/21 06:54:47 cifar10-global-proj-2.0-resnet56]: Epoch 46/100, Acc=0.8995, Val Loss=0.3531, lr=0.0100
[02/21 06:55:21 cifar10-global-proj-2.0-resnet56]: Epoch 47/100, Acc=0.8953, Val Loss=0.3767, lr=0.0100
[02/21 06:55:54 cifar10-global-proj-2.0-resnet56]: Epoch 48/100, Acc=0.8959, Val Loss=0.3676, lr=0.0100
[02/21 06:56:27 cifar10-global-proj-2.0-resnet56]: Epoch 49/100, Acc=0.8992, Val Loss=0.3511, lr=0.0100
[02/21 06:57:00 cifar10-global-proj-2.0-resnet56]: Epoch 50/100, Acc=0.9088, Val Loss=0.3402, lr=0.0100
[02/21 06:57:33 cifar10-global-proj-2.0-resnet56]: Epoch 51/100, Acc=0.9047, Val Loss=0.3432, lr=0.0100
[02/21 06:58:06 cifar10-global-proj-2.0-resnet56]: Epoch 52/100, Acc=0.8874, Val Loss=0.3984, lr=0.0100
[02/21 06:58:39 cifar10-global-proj-2.0-resnet56]: Epoch 53/100, Acc=0.9057, Val Loss=0.3420, lr=0.0100
[02/21 06:59:12 cifar10-global-proj-2.0-resnet56]: Epoch 54/100, Acc=0.9004, Val Loss=0.3512, lr=0.0100
[02/21 06:59:44 cifar10-global-proj-2.0-resnet56]: Epoch 55/100, Acc=0.8959, Val Loss=0.3612, lr=0.0100
[02/21 07:00:18 cifar10-global-proj-2.0-resnet56]: Epoch 56/100, Acc=0.9085, Val Loss=0.3227, lr=0.0100
[02/21 07:00:51 cifar10-global-proj-2.0-resnet56]: Epoch 57/100, Acc=0.9061, Val Loss=0.3153, lr=0.0100
[02/21 07:01:24 cifar10-global-proj-2.0-resnet56]: Epoch 58/100, Acc=0.8952, Val Loss=0.3829, lr=0.0100
[02/21 07:01:57 cifar10-global-proj-2.0-resnet56]: Epoch 59/100, Acc=0.8986, Val Loss=0.3560, lr=0.0100
[02/21 07:02:30 cifar10-global-proj-2.0-resnet56]: Epoch 60/100, Acc=0.9303, Val Loss=0.2361, lr=0.0010
[02/21 07:03:03 cifar10-global-proj-2.0-resnet56]: Epoch 61/100, Acc=0.9314, Val Loss=0.2374, lr=0.0010
[02/21 07:03:37 cifar10-global-proj-2.0-resnet56]: Epoch 62/100, Acc=0.9334, Val Loss=0.2397, lr=0.0010
[02/21 07:04:10 cifar10-global-proj-2.0-resnet56]: Epoch 63/100, Acc=0.9330, Val Loss=0.2395, lr=0.0010
[02/21 07:04:43 cifar10-global-proj-2.0-resnet56]: Epoch 64/100, Acc=0.9337, Val Loss=0.2412, lr=0.0010
[02/21 07:05:15 cifar10-global-proj-2.0-resnet56]: Epoch 65/100, Acc=0.9328, Val Loss=0.2411, lr=0.0010
[02/21 07:05:48 cifar10-global-proj-2.0-resnet56]: Epoch 66/100, Acc=0.9342, Val Loss=0.2428, lr=0.0010
[02/21 07:06:21 cifar10-global-proj-2.0-resnet56]: Epoch 67/100, Acc=0.9346, Val Loss=0.2432, lr=0.0010
[02/21 07:06:54 cifar10-global-proj-2.0-resnet56]: Epoch 68/100, Acc=0.9339, Val Loss=0.2451, lr=0.0010
[02/21 07:07:27 cifar10-global-proj-2.0-resnet56]: Epoch 69/100, Acc=0.9341, Val Loss=0.2420, lr=0.0010
[02/21 07:07:59 cifar10-global-proj-2.0-resnet56]: Epoch 70/100, Acc=0.9341, Val Loss=0.2501, lr=0.0010
[02/21 07:08:32 cifar10-global-proj-2.0-resnet56]: Epoch 71/100, Acc=0.9330, Val Loss=0.2484, lr=0.0010
[02/21 07:09:05 cifar10-global-proj-2.0-resnet56]: Epoch 72/100, Acc=0.9338, Val Loss=0.2494, lr=0.0010
[02/21 07:09:37 cifar10-global-proj-2.0-resnet56]: Epoch 73/100, Acc=0.9359, Val Loss=0.2489, lr=0.0010
[02/21 07:10:10 cifar10-global-proj-2.0-resnet56]: Epoch 74/100, Acc=0.9350, Val Loss=0.2512, lr=0.0010
[02/21 07:10:42 cifar10-global-proj-2.0-resnet56]: Epoch 75/100, Acc=0.9349, Val Loss=0.2493, lr=0.0010
[02/21 07:11:15 cifar10-global-proj-2.0-resnet56]: Epoch 76/100, Acc=0.9366, Val Loss=0.2488, lr=0.0010
[02/21 07:11:48 cifar10-global-proj-2.0-resnet56]: Epoch 77/100, Acc=0.9349, Val Loss=0.2514, lr=0.0010
[02/21 07:12:20 cifar10-global-proj-2.0-resnet56]: Epoch 78/100, Acc=0.9369, Val Loss=0.2530, lr=0.0010
[02/21 07:12:53 cifar10-global-proj-2.0-resnet56]: Epoch 79/100, Acc=0.9365, Val Loss=0.2534, lr=0.0010
[02/21 07:13:26 cifar10-global-proj-2.0-resnet56]: Epoch 80/100, Acc=0.9363, Val Loss=0.2529, lr=0.0001
[02/21 07:13:59 cifar10-global-proj-2.0-resnet56]: Epoch 81/100, Acc=0.9363, Val Loss=0.2506, lr=0.0001
[02/21 07:14:32 cifar10-global-proj-2.0-resnet56]: Epoch 82/100, Acc=0.9365, Val Loss=0.2497, lr=0.0001
[02/21 07:15:05 cifar10-global-proj-2.0-resnet56]: Epoch 83/100, Acc=0.9369, Val Loss=0.2504, lr=0.0001
[02/21 07:15:38 cifar10-global-proj-2.0-resnet56]: Epoch 84/100, Acc=0.9364, Val Loss=0.2503, lr=0.0001
[02/21 07:16:11 cifar10-global-proj-2.0-resnet56]: Epoch 85/100, Acc=0.9367, Val Loss=0.2495, lr=0.0001
[02/21 07:16:43 cifar10-global-proj-2.0-resnet56]: Epoch 86/100, Acc=0.9364, Val Loss=0.2494, lr=0.0001
[02/21 07:17:16 cifar10-global-proj-2.0-resnet56]: Epoch 87/100, Acc=0.9366, Val Loss=0.2515, lr=0.0001
[02/21 07:17:49 cifar10-global-proj-2.0-resnet56]: Epoch 88/100, Acc=0.9371, Val Loss=0.2503, lr=0.0001
[02/21 07:18:22 cifar10-global-proj-2.0-resnet56]: Epoch 89/100, Acc=0.9374, Val Loss=0.2493, lr=0.0001
[02/21 07:18:55 cifar10-global-proj-2.0-resnet56]: Epoch 90/100, Acc=0.9363, Val Loss=0.2536, lr=0.0001
[02/21 07:19:28 cifar10-global-proj-2.0-resnet56]: Epoch 91/100, Acc=0.9374, Val Loss=0.2499, lr=0.0001
[02/21 07:20:01 cifar10-global-proj-2.0-resnet56]: Epoch 92/100, Acc=0.9360, Val Loss=0.2523, lr=0.0001
[02/21 07:20:35 cifar10-global-proj-2.0-resnet56]: Epoch 93/100, Acc=0.9363, Val Loss=0.2507, lr=0.0001
[02/21 07:21:08 cifar10-global-proj-2.0-resnet56]: Epoch 94/100, Acc=0.9368, Val Loss=0.2512, lr=0.0001
[02/21 07:21:41 cifar10-global-proj-2.0-resnet56]: Epoch 95/100, Acc=0.9369, Val Loss=0.2521, lr=0.0001
[02/21 07:22:14 cifar10-global-proj-2.0-resnet56]: Epoch 96/100, Acc=0.9363, Val Loss=0.2518, lr=0.0001
[02/21 07:22:48 cifar10-global-proj-2.0-resnet56]: Epoch 97/100, Acc=0.9364, Val Loss=0.2530, lr=0.0001
[02/21 07:23:21 cifar10-global-proj-2.0-resnet56]: Epoch 98/100, Acc=0.9354, Val Loss=0.2522, lr=0.0001
[02/21 07:23:53 cifar10-global-proj-2.0-resnet56]: Epoch 99/100, Acc=0.9363, Val Loss=0.2524, lr=0.0001
[02/21 07:23:53 cifar10-global-proj-2.0-resnet56]: Best Acc=0.9374
[02/21 07:23:53 cifar10-global-proj-2.0-resnet56]: Params: 0.56 M
[02/21 07:23:53 cifar10-global-proj-2.0-resnet56]: ops: 63.45 M
[02/21 07:23:56 cifar10-global-proj-2.0-resnet56]: Acc: 0.9363 Val Loss: 0.2524

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: mode: prune
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: model: resnet56
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: verbose: False
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: dataset: cifar10
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: dataroot: data
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: batch_size: 128
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: total_epochs: 100
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: lr_decay_milestones: 60,80
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: lr_decay_gamma: 0.1
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: lr: 0.01
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: output_dir: run/cifar10/prune/cifar10-global-proj_sl-2.0-resnet56
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: finetune: True
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: last_epochs: 100
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: reps: 1
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: method: proj_sl
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: speed_up: 2.0
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: max_pruning_ratio: 1.0
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: reg: 1e-05
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: delta_reg: 0.0001
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: weight_decay: 0.0005
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: seed: 1
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: global_pruning: True
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: sl_total_epochs: 100
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: sl_lr: 0.01
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: sl_reg_warmup: 0
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: sl_restore: None
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: iterative_steps: 400
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: logger: <Logger cifar10-global-proj_sl-2.0-resnet56 (DEBUG)>
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: device: cuda
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: num_classes: 10
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 07:24:04 cifar10-global-proj_sl-2.0-resnet56]: Regularizing...
[02/21 07:31:28 cifar10-global-proj_sl-2.0-resnet56]: Epoch 0/100, Acc=0.9166, Val Loss=0.2954, lr=0.0100
[02/21 07:38:53 cifar10-global-proj_sl-2.0-resnet56]: Epoch 1/100, Acc=0.9094, Val Loss=0.3258, lr=0.0100
[02/21 07:46:22 cifar10-global-proj_sl-2.0-resnet56]: Epoch 2/100, Acc=0.9102, Val Loss=0.3347, lr=0.0100
[02/21 07:53:45 cifar10-global-proj_sl-2.0-resnet56]: Epoch 3/100, Acc=0.9088, Val Loss=0.3245, lr=0.0100
[02/21 08:01:09 cifar10-global-proj_sl-2.0-resnet56]: Epoch 4/100, Acc=0.9078, Val Loss=0.3198, lr=0.0100
[02/21 08:08:36 cifar10-global-proj_sl-2.0-resnet56]: Epoch 5/100, Acc=0.9145, Val Loss=0.3242, lr=0.0100
[02/21 08:16:00 cifar10-global-proj_sl-2.0-resnet56]: Epoch 6/100, Acc=0.9067, Val Loss=0.3405, lr=0.0100
[02/21 08:23:24 cifar10-global-proj_sl-2.0-resnet56]: Epoch 7/100, Acc=0.9131, Val Loss=0.3142, lr=0.0100
[02/21 08:30:55 cifar10-global-proj_sl-2.0-resnet56]: Epoch 8/100, Acc=0.9137, Val Loss=0.3327, lr=0.0100
[02/21 08:38:41 cifar10-global-proj_sl-2.0-resnet56]: Epoch 9/100, Acc=0.9138, Val Loss=0.3231, lr=0.0100
[02/21 08:46:20 cifar10-global-proj_sl-2.0-resnet56]: Epoch 10/100, Acc=0.9158, Val Loss=0.3144, lr=0.0100
[02/21 08:54:58 cifar10-global-proj_sl-2.0-resnet56]: Epoch 11/100, Acc=0.9149, Val Loss=0.3212, lr=0.0100
[02/21 09:02:35 cifar10-global-proj_sl-2.0-resnet56]: Epoch 12/100, Acc=0.9198, Val Loss=0.3277, lr=0.0100
[02/21 09:10:06 cifar10-global-proj_sl-2.0-resnet56]: Epoch 13/100, Acc=0.9182, Val Loss=0.3031, lr=0.0100
[02/21 09:17:41 cifar10-global-proj_sl-2.0-resnet56]: Epoch 14/100, Acc=0.9178, Val Loss=0.3201, lr=0.0100
[02/21 09:25:09 cifar10-global-proj_sl-2.0-resnet56]: Epoch 15/100, Acc=0.9180, Val Loss=0.3138, lr=0.0100
[02/21 09:32:34 cifar10-global-proj_sl-2.0-resnet56]: Epoch 16/100, Acc=0.9185, Val Loss=0.3065, lr=0.0100
[02/21 09:40:09 cifar10-global-proj_sl-2.0-resnet56]: Epoch 17/100, Acc=0.9113, Val Loss=0.3599, lr=0.0100
[02/21 09:47:42 cifar10-global-proj_sl-2.0-resnet56]: Epoch 18/100, Acc=0.9122, Val Loss=0.3535, lr=0.0100
[02/21 09:55:10 cifar10-global-proj_sl-2.0-resnet56]: Epoch 19/100, Acc=0.9208, Val Loss=0.3045, lr=0.0100
[02/21 10:02:33 cifar10-global-proj_sl-2.0-resnet56]: Epoch 20/100, Acc=0.9234, Val Loss=0.3134, lr=0.0100
[02/21 10:09:56 cifar10-global-proj_sl-2.0-resnet56]: Epoch 21/100, Acc=0.9202, Val Loss=0.3396, lr=0.0100
[02/21 10:17:32 cifar10-global-proj_sl-2.0-resnet56]: Epoch 22/100, Acc=0.9215, Val Loss=0.3151, lr=0.0100
[02/21 10:24:59 cifar10-global-proj_sl-2.0-resnet56]: Epoch 23/100, Acc=0.9202, Val Loss=0.3388, lr=0.0100
[02/21 10:32:25 cifar10-global-proj_sl-2.0-resnet56]: Epoch 24/100, Acc=0.9216, Val Loss=0.3226, lr=0.0100
[02/21 10:39:50 cifar10-global-proj_sl-2.0-resnet56]: Epoch 25/100, Acc=0.9217, Val Loss=0.3370, lr=0.0100
[02/21 10:47:19 cifar10-global-proj_sl-2.0-resnet56]: Epoch 26/100, Acc=0.9214, Val Loss=0.3371, lr=0.0100
[02/21 10:54:42 cifar10-global-proj_sl-2.0-resnet56]: Epoch 27/100, Acc=0.9239, Val Loss=0.3230, lr=0.0100
[02/21 11:02:07 cifar10-global-proj_sl-2.0-resnet56]: Epoch 28/100, Acc=0.9220, Val Loss=0.3232, lr=0.0100
[02/21 11:09:30 cifar10-global-proj_sl-2.0-resnet56]: Epoch 29/100, Acc=0.9258, Val Loss=0.3033, lr=0.0100
[02/21 11:16:57 cifar10-global-proj_sl-2.0-resnet56]: Epoch 30/100, Acc=0.9222, Val Loss=0.3311, lr=0.0100
[02/21 11:24:24 cifar10-global-proj_sl-2.0-resnet56]: Epoch 31/100, Acc=0.9183, Val Loss=0.3320, lr=0.0100
[02/21 11:31:53 cifar10-global-proj_sl-2.0-resnet56]: Epoch 32/100, Acc=0.9186, Val Loss=0.3495, lr=0.0100
[02/21 11:39:17 cifar10-global-proj_sl-2.0-resnet56]: Epoch 33/100, Acc=0.9187, Val Loss=0.3576, lr=0.0100
[02/21 11:46:46 cifar10-global-proj_sl-2.0-resnet56]: Epoch 34/100, Acc=0.9233, Val Loss=0.3274, lr=0.0100
[02/21 11:54:15 cifar10-global-proj_sl-2.0-resnet56]: Epoch 35/100, Acc=0.9192, Val Loss=0.3478, lr=0.0100
[02/21 12:01:39 cifar10-global-proj_sl-2.0-resnet56]: Epoch 36/100, Acc=0.9225, Val Loss=0.3532, lr=0.0100
[02/21 12:09:14 cifar10-global-proj_sl-2.0-resnet56]: Epoch 37/100, Acc=0.9252, Val Loss=0.3230, lr=0.0100
[02/21 12:16:52 cifar10-global-proj_sl-2.0-resnet56]: Epoch 38/100, Acc=0.9233, Val Loss=0.3552, lr=0.0100
[02/21 12:24:21 cifar10-global-proj_sl-2.0-resnet56]: Epoch 39/100, Acc=0.9258, Val Loss=0.3306, lr=0.0100
[02/21 12:31:53 cifar10-global-proj_sl-2.0-resnet56]: Epoch 40/100, Acc=0.9293, Val Loss=0.3263, lr=0.0100
[02/21 12:39:38 cifar10-global-proj_sl-2.0-resnet56]: Epoch 41/100, Acc=0.9181, Val Loss=0.3872, lr=0.0100
[02/21 12:47:13 cifar10-global-proj_sl-2.0-resnet56]: Epoch 42/100, Acc=0.9271, Val Loss=0.3334, lr=0.0100
[02/21 12:54:38 cifar10-global-proj_sl-2.0-resnet56]: Epoch 43/100, Acc=0.9252, Val Loss=0.3451, lr=0.0100
[02/21 13:02:08 cifar10-global-proj_sl-2.0-resnet56]: Epoch 44/100, Acc=0.9220, Val Loss=0.3554, lr=0.0100
[02/21 13:09:56 cifar10-global-proj_sl-2.0-resnet56]: Epoch 45/100, Acc=0.9300, Val Loss=0.3233, lr=0.0100
[02/21 13:17:40 cifar10-global-proj_sl-2.0-resnet56]: Epoch 46/100, Acc=0.9267, Val Loss=0.3462, lr=0.0100
[02/21 13:25:34 cifar10-global-proj_sl-2.0-resnet56]: Epoch 47/100, Acc=0.9264, Val Loss=0.3282, lr=0.0100
[02/21 13:33:07 cifar10-global-proj_sl-2.0-resnet56]: Epoch 48/100, Acc=0.9276, Val Loss=0.3393, lr=0.0100
[02/21 13:40:43 cifar10-global-proj_sl-2.0-resnet56]: Epoch 49/100, Acc=0.9302, Val Loss=0.3308, lr=0.0100
[02/21 13:48:27 cifar10-global-proj_sl-2.0-resnet56]: Epoch 50/100, Acc=0.9209, Val Loss=0.3467, lr=0.0100
[02/21 13:56:50 cifar10-global-proj_sl-2.0-resnet56]: Epoch 51/100, Acc=0.9240, Val Loss=0.3395, lr=0.0100
[02/21 14:04:26 cifar10-global-proj_sl-2.0-resnet56]: Epoch 52/100, Acc=0.9289, Val Loss=0.3229, lr=0.0100
[02/21 14:12:04 cifar10-global-proj_sl-2.0-resnet56]: Epoch 53/100, Acc=0.9267, Val Loss=0.3371, lr=0.0100
[02/21 14:19:37 cifar10-global-proj_sl-2.0-resnet56]: Epoch 54/100, Acc=0.9187, Val Loss=0.3824, lr=0.0100
[02/21 14:27:55 cifar10-global-proj_sl-2.0-resnet56]: Epoch 55/100, Acc=0.9193, Val Loss=0.3710, lr=0.0100
[02/21 14:35:23 cifar10-global-proj_sl-2.0-resnet56]: Epoch 56/100, Acc=0.9236, Val Loss=0.3569, lr=0.0100
[02/21 14:43:22 cifar10-global-proj_sl-2.0-resnet56]: Epoch 57/100, Acc=0.9202, Val Loss=0.3789, lr=0.0100
[02/21 14:51:01 cifar10-global-proj_sl-2.0-resnet56]: Epoch 58/100, Acc=0.9272, Val Loss=0.3480, lr=0.0100
[02/21 14:58:41 cifar10-global-proj_sl-2.0-resnet56]: Epoch 59/100, Acc=0.9272, Val Loss=0.3538, lr=0.0100
[02/21 15:06:58 cifar10-global-proj_sl-2.0-resnet56]: Epoch 60/100, Acc=0.9338, Val Loss=0.3128, lr=0.0010
[02/21 15:14:42 cifar10-global-proj_sl-2.0-resnet56]: Epoch 61/100, Acc=0.9351, Val Loss=0.3089, lr=0.0010
[02/21 15:22:34 cifar10-global-proj_sl-2.0-resnet56]: Epoch 62/100, Acc=0.9359, Val Loss=0.3098, lr=0.0010
[02/21 15:30:02 cifar10-global-proj_sl-2.0-resnet56]: Epoch 63/100, Acc=0.9373, Val Loss=0.3112, lr=0.0010
[02/21 15:37:42 cifar10-global-proj_sl-2.0-resnet56]: Epoch 64/100, Acc=0.9357, Val Loss=0.3144, lr=0.0010
[02/21 15:45:24 cifar10-global-proj_sl-2.0-resnet56]: Epoch 65/100, Acc=0.9364, Val Loss=0.3102, lr=0.0010
[02/21 15:52:54 cifar10-global-proj_sl-2.0-resnet56]: Epoch 66/100, Acc=0.9384, Val Loss=0.3107, lr=0.0010
[02/21 16:00:36 cifar10-global-proj_sl-2.0-resnet56]: Epoch 67/100, Acc=0.9368, Val Loss=0.3076, lr=0.0010
[02/21 16:08:04 cifar10-global-proj_sl-2.0-resnet56]: Epoch 68/100, Acc=0.9380, Val Loss=0.3098, lr=0.0010
[02/21 16:15:34 cifar10-global-proj_sl-2.0-resnet56]: Epoch 69/100, Acc=0.9378, Val Loss=0.3080, lr=0.0010
[02/21 16:22:59 cifar10-global-proj_sl-2.0-resnet56]: Epoch 70/100, Acc=0.9388, Val Loss=0.3098, lr=0.0010
[02/21 16:30:25 cifar10-global-proj_sl-2.0-resnet56]: Epoch 71/100, Acc=0.9378, Val Loss=0.3109, lr=0.0010
[02/21 16:37:53 cifar10-global-proj_sl-2.0-resnet56]: Epoch 72/100, Acc=0.9388, Val Loss=0.3113, lr=0.0010
[02/21 16:45:19 cifar10-global-proj_sl-2.0-resnet56]: Epoch 73/100, Acc=0.9386, Val Loss=0.3149, lr=0.0010
[02/21 16:52:42 cifar10-global-proj_sl-2.0-resnet56]: Epoch 74/100, Acc=0.9366, Val Loss=0.3142, lr=0.0010
[02/21 17:00:06 cifar10-global-proj_sl-2.0-resnet56]: Epoch 75/100, Acc=0.9385, Val Loss=0.3151, lr=0.0010
[02/21 17:07:32 cifar10-global-proj_sl-2.0-resnet56]: Epoch 76/100, Acc=0.9383, Val Loss=0.3137, lr=0.0010
[02/21 17:14:57 cifar10-global-proj_sl-2.0-resnet56]: Epoch 77/100, Acc=0.9392, Val Loss=0.3139, lr=0.0010
[02/21 17:22:25 cifar10-global-proj_sl-2.0-resnet56]: Epoch 78/100, Acc=0.9395, Val Loss=0.3127, lr=0.0010
[02/21 17:29:51 cifar10-global-proj_sl-2.0-resnet56]: Epoch 79/100, Acc=0.9401, Val Loss=0.3120, lr=0.0010
[02/21 17:37:33 cifar10-global-proj_sl-2.0-resnet56]: Epoch 80/100, Acc=0.9388, Val Loss=0.3149, lr=0.0001
[02/21 17:45:12 cifar10-global-proj_sl-2.0-resnet56]: Epoch 81/100, Acc=0.9389, Val Loss=0.3154, lr=0.0001
[02/21 17:52:53 cifar10-global-proj_sl-2.0-resnet56]: Epoch 82/100, Acc=0.9392, Val Loss=0.3159, lr=0.0001
[02/21 18:00:55 cifar10-global-proj_sl-2.0-resnet56]: Epoch 83/100, Acc=0.9388, Val Loss=0.3148, lr=0.0001
[02/21 18:08:19 cifar10-global-proj_sl-2.0-resnet56]: Epoch 84/100, Acc=0.9386, Val Loss=0.3168, lr=0.0001
[02/21 18:15:51 cifar10-global-proj_sl-2.0-resnet56]: Epoch 85/100, Acc=0.9398, Val Loss=0.3177, lr=0.0001
[02/21 18:23:29 cifar10-global-proj_sl-2.0-resnet56]: Epoch 86/100, Acc=0.9393, Val Loss=0.3155, lr=0.0001
[02/21 18:31:09 cifar10-global-proj_sl-2.0-resnet56]: Epoch 87/100, Acc=0.9391, Val Loss=0.3180, lr=0.0001
[02/21 18:38:38 cifar10-global-proj_sl-2.0-resnet56]: Epoch 88/100, Acc=0.9392, Val Loss=0.3144, lr=0.0001
[02/21 18:46:08 cifar10-global-proj_sl-2.0-resnet56]: Epoch 89/100, Acc=0.9391, Val Loss=0.3173, lr=0.0001
[02/21 18:54:04 cifar10-global-proj_sl-2.0-resnet56]: Epoch 90/100, Acc=0.9394, Val Loss=0.3193, lr=0.0001
[02/21 19:01:54 cifar10-global-proj_sl-2.0-resnet56]: Epoch 91/100, Acc=0.9397, Val Loss=0.3185, lr=0.0001
[02/21 19:09:37 cifar10-global-proj_sl-2.0-resnet56]: Epoch 92/100, Acc=0.9389, Val Loss=0.3138, lr=0.0001
[02/21 19:17:32 cifar10-global-proj_sl-2.0-resnet56]: Epoch 93/100, Acc=0.9391, Val Loss=0.3119, lr=0.0001
[02/21 19:25:28 cifar10-global-proj_sl-2.0-resnet56]: Epoch 94/100, Acc=0.9395, Val Loss=0.3158, lr=0.0001
[02/21 19:33:12 cifar10-global-proj_sl-2.0-resnet56]: Epoch 95/100, Acc=0.9401, Val Loss=0.3155, lr=0.0001
[02/21 19:40:39 cifar10-global-proj_sl-2.0-resnet56]: Epoch 96/100, Acc=0.9401, Val Loss=0.3137, lr=0.0001
[02/21 19:48:32 cifar10-global-proj_sl-2.0-resnet56]: Epoch 97/100, Acc=0.9393, Val Loss=0.3140, lr=0.0001
[02/21 19:56:14 cifar10-global-proj_sl-2.0-resnet56]: Epoch 98/100, Acc=0.9388, Val Loss=0.3170, lr=0.0001
[02/21 20:04:00 cifar10-global-proj_sl-2.0-resnet56]: Epoch 99/100, Acc=0.9389, Val Loss=0.3152, lr=0.0001
[02/21 20:04:00 cifar10-global-proj_sl-2.0-resnet56]: Best Acc=0.9401
[02/21 20:04:00 cifar10-global-proj_sl-2.0-resnet56]: Loading the sparse model from run/cifar10/prune/cifar10-global-proj_sl-2.0-resnet56/reg_cifar10_resnet56_proj_sl_1e-05.pth...
[02/21 20:04:03 cifar10-global-proj_sl-2.0-resnet56]: Pruning...
[02/21 20:06:04 cifar10-global-proj_sl-2.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(10, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(10, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(10, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(10, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(10, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(10, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(10, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(10, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(10, 22, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(10, 28, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(28, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(28, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(21, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(28, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(26, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(28, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(28, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(28, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(28, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(28, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(28, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(28, 59, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(59, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(59, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(59, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(58, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(59, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(58, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(59, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(57, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(59, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(59, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(29, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(59, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(35, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=59, out_features=10, bias=True)
)
[02/21 20:06:08 cifar10-global-proj_sl-2.0-resnet56]: Params: 0.86 M => 0.57 M (66.49%)
[02/21 20:06:08 cifar10-global-proj_sl-2.0-resnet56]: FLOPs: 127.12 M => 63.32 M (49.81%, 2.01X )
[02/21 20:06:08 cifar10-global-proj_sl-2.0-resnet56]: Acc: 0.9401 => 0.1410
[02/21 20:06:08 cifar10-global-proj_sl-2.0-resnet56]: Val Loss: 0.3120 => 5.5544
[02/21 20:06:08 cifar10-global-proj_sl-2.0-resnet56]: Finetuning...
[02/21 20:06:42 cifar10-global-proj_sl-2.0-resnet56]: Epoch 0/100, Acc=0.8919, Val Loss=0.3972, lr=0.0100
[02/21 20:07:16 cifar10-global-proj_sl-2.0-resnet56]: Epoch 1/100, Acc=0.9091, Val Loss=0.3235, lr=0.0100
[02/21 20:07:52 cifar10-global-proj_sl-2.0-resnet56]: Epoch 2/100, Acc=0.8948, Val Loss=0.4010, lr=0.0100
[02/21 20:08:26 cifar10-global-proj_sl-2.0-resnet56]: Epoch 3/100, Acc=0.8982, Val Loss=0.3782, lr=0.0100
[02/21 20:09:01 cifar10-global-proj_sl-2.0-resnet56]: Epoch 4/100, Acc=0.9020, Val Loss=0.3668, lr=0.0100
[02/21 20:09:36 cifar10-global-proj_sl-2.0-resnet56]: Epoch 5/100, Acc=0.9131, Val Loss=0.3189, lr=0.0100
[02/21 20:10:10 cifar10-global-proj_sl-2.0-resnet56]: Epoch 6/100, Acc=0.9102, Val Loss=0.3267, lr=0.0100
[02/21 20:10:44 cifar10-global-proj_sl-2.0-resnet56]: Epoch 7/100, Acc=0.9063, Val Loss=0.3529, lr=0.0100
[02/21 20:11:19 cifar10-global-proj_sl-2.0-resnet56]: Epoch 8/100, Acc=0.9094, Val Loss=0.3417, lr=0.0100
[02/21 20:11:53 cifar10-global-proj_sl-2.0-resnet56]: Epoch 9/100, Acc=0.9082, Val Loss=0.3239, lr=0.0100
[02/21 20:12:27 cifar10-global-proj_sl-2.0-resnet56]: Epoch 10/100, Acc=0.9070, Val Loss=0.3347, lr=0.0100
[02/21 20:13:01 cifar10-global-proj_sl-2.0-resnet56]: Epoch 11/100, Acc=0.9036, Val Loss=0.3629, lr=0.0100
[02/21 20:13:35 cifar10-global-proj_sl-2.0-resnet56]: Epoch 12/100, Acc=0.9093, Val Loss=0.3400, lr=0.0100
[02/21 20:14:09 cifar10-global-proj_sl-2.0-resnet56]: Epoch 13/100, Acc=0.9095, Val Loss=0.3230, lr=0.0100
[02/21 20:14:42 cifar10-global-proj_sl-2.0-resnet56]: Epoch 14/100, Acc=0.8984, Val Loss=0.3745, lr=0.0100
[02/21 20:15:16 cifar10-global-proj_sl-2.0-resnet56]: Epoch 15/100, Acc=0.8899, Val Loss=0.3991, lr=0.0100
[02/21 20:15:49 cifar10-global-proj_sl-2.0-resnet56]: Epoch 16/100, Acc=0.8977, Val Loss=0.3769, lr=0.0100
[02/21 20:16:22 cifar10-global-proj_sl-2.0-resnet56]: Epoch 17/100, Acc=0.9016, Val Loss=0.3654, lr=0.0100
[02/21 20:16:56 cifar10-global-proj_sl-2.0-resnet56]: Epoch 18/100, Acc=0.9073, Val Loss=0.3390, lr=0.0100
[02/21 20:17:30 cifar10-global-proj_sl-2.0-resnet56]: Epoch 19/100, Acc=0.9021, Val Loss=0.3564, lr=0.0100
[02/21 20:18:04 cifar10-global-proj_sl-2.0-resnet56]: Epoch 20/100, Acc=0.9117, Val Loss=0.3154, lr=0.0100
[02/21 20:18:38 cifar10-global-proj_sl-2.0-resnet56]: Epoch 21/100, Acc=0.9081, Val Loss=0.3231, lr=0.0100
[02/21 20:19:12 cifar10-global-proj_sl-2.0-resnet56]: Epoch 22/100, Acc=0.9007, Val Loss=0.3458, lr=0.0100
[02/21 20:19:46 cifar10-global-proj_sl-2.0-resnet56]: Epoch 23/100, Acc=0.9036, Val Loss=0.3420, lr=0.0100
[02/21 20:20:23 cifar10-global-proj_sl-2.0-resnet56]: Epoch 24/100, Acc=0.9036, Val Loss=0.3493, lr=0.0100
[02/21 20:21:00 cifar10-global-proj_sl-2.0-resnet56]: Epoch 25/100, Acc=0.9001, Val Loss=0.3504, lr=0.0100
[02/21 20:21:33 cifar10-global-proj_sl-2.0-resnet56]: Epoch 26/100, Acc=0.8924, Val Loss=0.4037, lr=0.0100
[02/21 20:22:07 cifar10-global-proj_sl-2.0-resnet56]: Epoch 27/100, Acc=0.9135, Val Loss=0.3144, lr=0.0100
[02/21 20:22:41 cifar10-global-proj_sl-2.0-resnet56]: Epoch 28/100, Acc=0.9048, Val Loss=0.3500, lr=0.0100
[02/21 20:23:15 cifar10-global-proj_sl-2.0-resnet56]: Epoch 29/100, Acc=0.9111, Val Loss=0.3241, lr=0.0100
[02/21 20:23:49 cifar10-global-proj_sl-2.0-resnet56]: Epoch 30/100, Acc=0.9003, Val Loss=0.3702, lr=0.0100
[02/21 20:24:23 cifar10-global-proj_sl-2.0-resnet56]: Epoch 31/100, Acc=0.9118, Val Loss=0.3099, lr=0.0100
[02/21 20:24:59 cifar10-global-proj_sl-2.0-resnet56]: Epoch 32/100, Acc=0.9111, Val Loss=0.3242, lr=0.0100
[02/21 20:25:33 cifar10-global-proj_sl-2.0-resnet56]: Epoch 33/100, Acc=0.9007, Val Loss=0.3547, lr=0.0100
[02/21 20:26:07 cifar10-global-proj_sl-2.0-resnet56]: Epoch 34/100, Acc=0.9044, Val Loss=0.3519, lr=0.0100
[02/21 20:26:40 cifar10-global-proj_sl-2.0-resnet56]: Epoch 35/100, Acc=0.9114, Val Loss=0.3172, lr=0.0100
[02/21 20:27:14 cifar10-global-proj_sl-2.0-resnet56]: Epoch 36/100, Acc=0.9001, Val Loss=0.3500, lr=0.0100
[02/21 20:27:51 cifar10-global-proj_sl-2.0-resnet56]: Epoch 37/100, Acc=0.9015, Val Loss=0.3516, lr=0.0100
[02/21 20:28:29 cifar10-global-proj_sl-2.0-resnet56]: Epoch 38/100, Acc=0.9084, Val Loss=0.3128, lr=0.0100
[02/21 20:29:03 cifar10-global-proj_sl-2.0-resnet56]: Epoch 39/100, Acc=0.8872, Val Loss=0.3934, lr=0.0100
[02/21 20:29:37 cifar10-global-proj_sl-2.0-resnet56]: Epoch 40/100, Acc=0.8944, Val Loss=0.3950, lr=0.0100
[02/21 20:30:12 cifar10-global-proj_sl-2.0-resnet56]: Epoch 41/100, Acc=0.9054, Val Loss=0.3168, lr=0.0100
[02/21 20:30:48 cifar10-global-proj_sl-2.0-resnet56]: Epoch 42/100, Acc=0.9063, Val Loss=0.3465, lr=0.0100
[02/21 20:31:22 cifar10-global-proj_sl-2.0-resnet56]: Epoch 43/100, Acc=0.9086, Val Loss=0.3323, lr=0.0100
[02/21 20:31:58 cifar10-global-proj_sl-2.0-resnet56]: Epoch 44/100, Acc=0.9118, Val Loss=0.3247, lr=0.0100
[02/21 20:32:36 cifar10-global-proj_sl-2.0-resnet56]: Epoch 45/100, Acc=0.9061, Val Loss=0.3189, lr=0.0100
[02/21 20:33:11 cifar10-global-proj_sl-2.0-resnet56]: Epoch 46/100, Acc=0.8959, Val Loss=0.3582, lr=0.0100
[02/21 20:33:47 cifar10-global-proj_sl-2.0-resnet56]: Epoch 47/100, Acc=0.9066, Val Loss=0.3327, lr=0.0100
[02/21 20:34:24 cifar10-global-proj_sl-2.0-resnet56]: Epoch 48/100, Acc=0.9089, Val Loss=0.3327, lr=0.0100
[02/21 20:35:05 cifar10-global-proj_sl-2.0-resnet56]: Epoch 49/100, Acc=0.8862, Val Loss=0.4128, lr=0.0100
[02/21 20:35:40 cifar10-global-proj_sl-2.0-resnet56]: Epoch 50/100, Acc=0.8982, Val Loss=0.3774, lr=0.0100
[02/21 20:36:15 cifar10-global-proj_sl-2.0-resnet56]: Epoch 51/100, Acc=0.9032, Val Loss=0.3330, lr=0.0100
[02/21 20:36:51 cifar10-global-proj_sl-2.0-resnet56]: Epoch 52/100, Acc=0.9037, Val Loss=0.3490, lr=0.0100
[02/21 20:37:26 cifar10-global-proj_sl-2.0-resnet56]: Epoch 53/100, Acc=0.9101, Val Loss=0.3089, lr=0.0100
[02/21 20:38:03 cifar10-global-proj_sl-2.0-resnet56]: Epoch 54/100, Acc=0.8927, Val Loss=0.3890, lr=0.0100
[02/21 20:38:38 cifar10-global-proj_sl-2.0-resnet56]: Epoch 55/100, Acc=0.9059, Val Loss=0.3392, lr=0.0100
[02/21 20:39:13 cifar10-global-proj_sl-2.0-resnet56]: Epoch 56/100, Acc=0.8972, Val Loss=0.3804, lr=0.0100
[02/21 20:39:48 cifar10-global-proj_sl-2.0-resnet56]: Epoch 57/100, Acc=0.9010, Val Loss=0.3423, lr=0.0100
[02/21 20:40:23 cifar10-global-proj_sl-2.0-resnet56]: Epoch 58/100, Acc=0.9096, Val Loss=0.3202, lr=0.0100
[02/21 20:40:57 cifar10-global-proj_sl-2.0-resnet56]: Epoch 59/100, Acc=0.8986, Val Loss=0.3551, lr=0.0100
[02/21 20:41:31 cifar10-global-proj_sl-2.0-resnet56]: Epoch 60/100, Acc=0.9310, Val Loss=0.2437, lr=0.0010
[02/21 20:42:06 cifar10-global-proj_sl-2.0-resnet56]: Epoch 61/100, Acc=0.9330, Val Loss=0.2413, lr=0.0010
[02/21 20:42:40 cifar10-global-proj_sl-2.0-resnet56]: Epoch 62/100, Acc=0.9324, Val Loss=0.2482, lr=0.0010
[02/21 20:43:14 cifar10-global-proj_sl-2.0-resnet56]: Epoch 63/100, Acc=0.9340, Val Loss=0.2470, lr=0.0010
[02/21 20:43:49 cifar10-global-proj_sl-2.0-resnet56]: Epoch 64/100, Acc=0.9332, Val Loss=0.2469, lr=0.0010
[02/21 20:44:23 cifar10-global-proj_sl-2.0-resnet56]: Epoch 65/100, Acc=0.9334, Val Loss=0.2502, lr=0.0010
[02/21 20:44:57 cifar10-global-proj_sl-2.0-resnet56]: Epoch 66/100, Acc=0.9361, Val Loss=0.2501, lr=0.0010
[02/21 20:45:32 cifar10-global-proj_sl-2.0-resnet56]: Epoch 67/100, Acc=0.9367, Val Loss=0.2507, lr=0.0010
[02/21 20:46:07 cifar10-global-proj_sl-2.0-resnet56]: Epoch 68/100, Acc=0.9349, Val Loss=0.2525, lr=0.0010
[02/21 20:46:40 cifar10-global-proj_sl-2.0-resnet56]: Epoch 69/100, Acc=0.9348, Val Loss=0.2520, lr=0.0010
[02/21 20:47:14 cifar10-global-proj_sl-2.0-resnet56]: Epoch 70/100, Acc=0.9360, Val Loss=0.2529, lr=0.0010
[02/21 20:47:48 cifar10-global-proj_sl-2.0-resnet56]: Epoch 71/100, Acc=0.9352, Val Loss=0.2547, lr=0.0010
[02/21 20:48:22 cifar10-global-proj_sl-2.0-resnet56]: Epoch 72/100, Acc=0.9351, Val Loss=0.2563, lr=0.0010
[02/21 20:48:56 cifar10-global-proj_sl-2.0-resnet56]: Epoch 73/100, Acc=0.9354, Val Loss=0.2552, lr=0.0010
[02/21 20:49:31 cifar10-global-proj_sl-2.0-resnet56]: Epoch 74/100, Acc=0.9358, Val Loss=0.2572, lr=0.0010
[02/21 20:50:04 cifar10-global-proj_sl-2.0-resnet56]: Epoch 75/100, Acc=0.9350, Val Loss=0.2600, lr=0.0010
[02/21 20:50:42 cifar10-global-proj_sl-2.0-resnet56]: Epoch 76/100, Acc=0.9357, Val Loss=0.2595, lr=0.0010
[02/21 20:51:16 cifar10-global-proj_sl-2.0-resnet56]: Epoch 77/100, Acc=0.9356, Val Loss=0.2603, lr=0.0010
[02/21 20:51:49 cifar10-global-proj_sl-2.0-resnet56]: Epoch 78/100, Acc=0.9357, Val Loss=0.2620, lr=0.0010
[02/21 20:52:22 cifar10-global-proj_sl-2.0-resnet56]: Epoch 79/100, Acc=0.9361, Val Loss=0.2625, lr=0.0010
[02/21 20:52:56 cifar10-global-proj_sl-2.0-resnet56]: Epoch 80/100, Acc=0.9357, Val Loss=0.2607, lr=0.0001
[02/21 20:53:31 cifar10-global-proj_sl-2.0-resnet56]: Epoch 81/100, Acc=0.9359, Val Loss=0.2594, lr=0.0001
[02/21 20:54:06 cifar10-global-proj_sl-2.0-resnet56]: Epoch 82/100, Acc=0.9364, Val Loss=0.2604, lr=0.0001
[02/21 20:54:39 cifar10-global-proj_sl-2.0-resnet56]: Epoch 83/100, Acc=0.9354, Val Loss=0.2598, lr=0.0001
[02/21 20:55:16 cifar10-global-proj_sl-2.0-resnet56]: Epoch 84/100, Acc=0.9361, Val Loss=0.2608, lr=0.0001
[02/21 20:55:50 cifar10-global-proj_sl-2.0-resnet56]: Epoch 85/100, Acc=0.9371, Val Loss=0.2593, lr=0.0001
[02/21 20:56:23 cifar10-global-proj_sl-2.0-resnet56]: Epoch 86/100, Acc=0.9363, Val Loss=0.2605, lr=0.0001
[02/21 20:56:56 cifar10-global-proj_sl-2.0-resnet56]: Epoch 87/100, Acc=0.9366, Val Loss=0.2581, lr=0.0001
[02/21 20:57:30 cifar10-global-proj_sl-2.0-resnet56]: Epoch 88/100, Acc=0.9356, Val Loss=0.2586, lr=0.0001
[02/21 20:58:03 cifar10-global-proj_sl-2.0-resnet56]: Epoch 89/100, Acc=0.9355, Val Loss=0.2585, lr=0.0001
[02/21 20:58:37 cifar10-global-proj_sl-2.0-resnet56]: Epoch 90/100, Acc=0.9364, Val Loss=0.2599, lr=0.0001
[02/21 20:59:11 cifar10-global-proj_sl-2.0-resnet56]: Epoch 91/100, Acc=0.9366, Val Loss=0.2612, lr=0.0001
[02/21 20:59:46 cifar10-global-proj_sl-2.0-resnet56]: Epoch 92/100, Acc=0.9362, Val Loss=0.2582, lr=0.0001
[02/21 21:00:20 cifar10-global-proj_sl-2.0-resnet56]: Epoch 93/100, Acc=0.9363, Val Loss=0.2589, lr=0.0001
[02/21 21:00:54 cifar10-global-proj_sl-2.0-resnet56]: Epoch 94/100, Acc=0.9361, Val Loss=0.2624, lr=0.0001
[02/21 21:01:29 cifar10-global-proj_sl-2.0-resnet56]: Epoch 95/100, Acc=0.9360, Val Loss=0.2607, lr=0.0001
[02/21 21:02:03 cifar10-global-proj_sl-2.0-resnet56]: Epoch 96/100, Acc=0.9356, Val Loss=0.2594, lr=0.0001
[02/21 21:02:38 cifar10-global-proj_sl-2.0-resnet56]: Epoch 97/100, Acc=0.9359, Val Loss=0.2606, lr=0.0001
[02/21 21:03:12 cifar10-global-proj_sl-2.0-resnet56]: Epoch 98/100, Acc=0.9364, Val Loss=0.2580, lr=0.0001
[02/21 21:03:46 cifar10-global-proj_sl-2.0-resnet56]: Epoch 99/100, Acc=0.9350, Val Loss=0.2616, lr=0.0001
[02/21 21:03:46 cifar10-global-proj_sl-2.0-resnet56]: Best Acc=0.9371
[02/21 21:03:46 cifar10-global-proj_sl-2.0-resnet56]: Params: 0.57 M
[02/21 21:03:46 cifar10-global-proj_sl-2.0-resnet56]: ops: 63.32 M
[02/21 21:03:50 cifar10-global-proj_sl-2.0-resnet56]: Acc: 0.9350 Val Loss: 0.2616

TIME TAKEN: 00:08:54
SLURM WORKLOAD FINISH: Fri Feb 21 21:03:51 CET 2025
