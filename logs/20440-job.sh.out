SLURM WORKLOAD START: Thu Feb 20 23:27:52 CET 2025
Thu Feb 20 23:27:52 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           Off |   00000000:0B:00.0 Off |                    0 |
| N/A   64C    P0             50W /  250W |       0MiB /  32768MiB |     10%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: mode: prune
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: model: resnet56
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: verbose: False
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: dataset: cifar10
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: dataroot: data
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: batch_size: 128
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: total_epochs: 100
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: lr_decay_milestones: 60,80
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: lr_decay_gamma: 0.1
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: lr: 0.01
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: output_dir: run/cifar10/prune/cifar10-global-random-3.0-resnet56
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: finetune: True
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: last_epochs: 100
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: reps: 1
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: method: random
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: speed_up: 3.0
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: max_pruning_ratio: 1.0
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: soft_keeping_ratio: 0.0
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: reg: 1e-05
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: delta_reg: 0.0001
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: weight_decay: 0.0005
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: seed: 1
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: global_pruning: True
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: sl_total_epochs: 100
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: sl_lr: 0.01
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: sl_reg_warmup: 0
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: sl_restore: None
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: iterative_steps: 400
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: logger: <Logger cifar10-global-random-3.0-resnet56 (DEBUG)>
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: device: cuda
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: num_classes: 10
[02/20 23:27:59 cifar10-global-random-3.0-resnet56]: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/20 23:28:03 cifar10-global-random-3.0-resnet56]: Pruning...
[02/20 23:28:12 cifar10-global-random-3.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(2, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(2, 18, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(18, 50, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(18, 50, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=50, out_features=10, bias=True)
)
[02/20 23:28:16 cifar10-global-random-3.0-resnet56]: Params: 0.86 M => 0.45 M (52.04%)
[02/20 23:28:16 cifar10-global-random-3.0-resnet56]: FLOPs: 127.12 M => 39.03 M (30.70%, 3.26X )
[02/20 23:28:16 cifar10-global-random-3.0-resnet56]: Acc: 0.9392 => 0.1000
[02/20 23:28:16 cifar10-global-random-3.0-resnet56]: Val Loss: 0.2587 => 6.3800
[02/20 23:28:16 cifar10-global-random-3.0-resnet56]: Finetuning...
[02/20 23:28:47 cifar10-global-random-3.0-resnet56]: Epoch 0/100, Acc=0.3403, Val Loss=1.9855, lr=0.0100
[02/20 23:29:18 cifar10-global-random-3.0-resnet56]: Epoch 1/100, Acc=0.5158, Val Loss=1.5711, lr=0.0100
[02/20 23:29:50 cifar10-global-random-3.0-resnet56]: Epoch 2/100, Acc=0.7038, Val Loss=0.8519, lr=0.0100
[02/20 23:30:22 cifar10-global-random-3.0-resnet56]: Epoch 3/100, Acc=0.7213, Val Loss=0.8283, lr=0.0100
[02/20 23:30:54 cifar10-global-random-3.0-resnet56]: Epoch 4/100, Acc=0.7682, Val Loss=0.7098, lr=0.0100
[02/20 23:31:26 cifar10-global-random-3.0-resnet56]: Epoch 5/100, Acc=0.7941, Val Loss=0.5996, lr=0.0100
[02/20 23:31:58 cifar10-global-random-3.0-resnet56]: Epoch 6/100, Acc=0.7555, Val Loss=0.7445, lr=0.0100
[02/20 23:32:29 cifar10-global-random-3.0-resnet56]: Epoch 7/100, Acc=0.8246, Val Loss=0.5160, lr=0.0100
[02/20 23:33:01 cifar10-global-random-3.0-resnet56]: Epoch 8/100, Acc=0.7954, Val Loss=0.6016, lr=0.0100
[02/20 23:33:33 cifar10-global-random-3.0-resnet56]: Epoch 9/100, Acc=0.8137, Val Loss=0.5632, lr=0.0100
[02/20 23:34:04 cifar10-global-random-3.0-resnet56]: Epoch 10/100, Acc=0.8210, Val Loss=0.5302, lr=0.0100
[02/20 23:34:35 cifar10-global-random-3.0-resnet56]: Epoch 11/100, Acc=0.8045, Val Loss=0.6005, lr=0.0100
[02/20 23:35:07 cifar10-global-random-3.0-resnet56]: Epoch 12/100, Acc=0.8361, Val Loss=0.4849, lr=0.0100
[02/20 23:35:39 cifar10-global-random-3.0-resnet56]: Epoch 13/100, Acc=0.8310, Val Loss=0.5070, lr=0.0100
[02/20 23:36:10 cifar10-global-random-3.0-resnet56]: Epoch 14/100, Acc=0.8322, Val Loss=0.5038, lr=0.0100
[02/20 23:36:42 cifar10-global-random-3.0-resnet56]: Epoch 15/100, Acc=0.8381, Val Loss=0.4949, lr=0.0100
[02/20 23:37:13 cifar10-global-random-3.0-resnet56]: Epoch 16/100, Acc=0.8434, Val Loss=0.4616, lr=0.0100
[02/20 23:37:45 cifar10-global-random-3.0-resnet56]: Epoch 17/100, Acc=0.8256, Val Loss=0.5383, lr=0.0100
[02/20 23:38:17 cifar10-global-random-3.0-resnet56]: Epoch 18/100, Acc=0.8288, Val Loss=0.5158, lr=0.0100
[02/20 23:38:48 cifar10-global-random-3.0-resnet56]: Epoch 19/100, Acc=0.8073, Val Loss=0.5970, lr=0.0100
[02/20 23:39:20 cifar10-global-random-3.0-resnet56]: Epoch 20/100, Acc=0.8413, Val Loss=0.4885, lr=0.0100
[02/20 23:39:52 cifar10-global-random-3.0-resnet56]: Epoch 21/100, Acc=0.8001, Val Loss=0.6290, lr=0.0100
[02/20 23:40:23 cifar10-global-random-3.0-resnet56]: Epoch 22/100, Acc=0.8310, Val Loss=0.5464, lr=0.0100
[02/20 23:40:55 cifar10-global-random-3.0-resnet56]: Epoch 23/100, Acc=0.8417, Val Loss=0.4707, lr=0.0100
[02/20 23:41:26 cifar10-global-random-3.0-resnet56]: Epoch 24/100, Acc=0.8354, Val Loss=0.5228, lr=0.0100
[02/20 23:41:58 cifar10-global-random-3.0-resnet56]: Epoch 25/100, Acc=0.8470, Val Loss=0.4632, lr=0.0100
[02/20 23:42:30 cifar10-global-random-3.0-resnet56]: Epoch 26/100, Acc=0.8374, Val Loss=0.5136, lr=0.0100
[02/20 23:43:01 cifar10-global-random-3.0-resnet56]: Epoch 27/100, Acc=0.8587, Val Loss=0.4444, lr=0.0100
[02/20 23:43:33 cifar10-global-random-3.0-resnet56]: Epoch 28/100, Acc=0.8529, Val Loss=0.4559, lr=0.0100
[02/20 23:44:04 cifar10-global-random-3.0-resnet56]: Epoch 29/100, Acc=0.8494, Val Loss=0.4695, lr=0.0100
[02/20 23:44:35 cifar10-global-random-3.0-resnet56]: Epoch 30/100, Acc=0.8447, Val Loss=0.4916, lr=0.0100
[02/20 23:45:07 cifar10-global-random-3.0-resnet56]: Epoch 31/100, Acc=0.8426, Val Loss=0.4967, lr=0.0100
[02/20 23:45:39 cifar10-global-random-3.0-resnet56]: Epoch 32/100, Acc=0.8503, Val Loss=0.4693, lr=0.0100
[02/20 23:46:11 cifar10-global-random-3.0-resnet56]: Epoch 33/100, Acc=0.8497, Val Loss=0.4746, lr=0.0100
[02/20 23:46:43 cifar10-global-random-3.0-resnet56]: Epoch 34/100, Acc=0.8483, Val Loss=0.4727, lr=0.0100
[02/20 23:47:15 cifar10-global-random-3.0-resnet56]: Epoch 35/100, Acc=0.8530, Val Loss=0.4758, lr=0.0100
[02/20 23:47:47 cifar10-global-random-3.0-resnet56]: Epoch 36/100, Acc=0.8544, Val Loss=0.4428, lr=0.0100
[02/20 23:48:18 cifar10-global-random-3.0-resnet56]: Epoch 37/100, Acc=0.8528, Val Loss=0.4634, lr=0.0100
[02/20 23:48:50 cifar10-global-random-3.0-resnet56]: Epoch 38/100, Acc=0.8666, Val Loss=0.4182, lr=0.0100
[02/20 23:49:22 cifar10-global-random-3.0-resnet56]: Epoch 39/100, Acc=0.8521, Val Loss=0.4761, lr=0.0100
[02/20 23:49:53 cifar10-global-random-3.0-resnet56]: Epoch 40/100, Acc=0.8536, Val Loss=0.4532, lr=0.0100
[02/20 23:50:25 cifar10-global-random-3.0-resnet56]: Epoch 41/100, Acc=0.8519, Val Loss=0.4784, lr=0.0100
[02/20 23:50:57 cifar10-global-random-3.0-resnet56]: Epoch 42/100, Acc=0.8635, Val Loss=0.4264, lr=0.0100
[02/20 23:51:29 cifar10-global-random-3.0-resnet56]: Epoch 43/100, Acc=0.8497, Val Loss=0.4740, lr=0.0100
[02/20 23:52:01 cifar10-global-random-3.0-resnet56]: Epoch 44/100, Acc=0.8604, Val Loss=0.4429, lr=0.0100
[02/20 23:52:33 cifar10-global-random-3.0-resnet56]: Epoch 45/100, Acc=0.8517, Val Loss=0.4704, lr=0.0100
[02/20 23:53:05 cifar10-global-random-3.0-resnet56]: Epoch 46/100, Acc=0.8445, Val Loss=0.5022, lr=0.0100
[02/20 23:53:37 cifar10-global-random-3.0-resnet56]: Epoch 47/100, Acc=0.8551, Val Loss=0.4865, lr=0.0100
[02/20 23:54:08 cifar10-global-random-3.0-resnet56]: Epoch 48/100, Acc=0.8429, Val Loss=0.5139, lr=0.0100
[02/20 23:54:39 cifar10-global-random-3.0-resnet56]: Epoch 49/100, Acc=0.8587, Val Loss=0.4505, lr=0.0100
[02/20 23:55:11 cifar10-global-random-3.0-resnet56]: Epoch 50/100, Acc=0.8572, Val Loss=0.4749, lr=0.0100
[02/20 23:55:43 cifar10-global-random-3.0-resnet56]: Epoch 51/100, Acc=0.8515, Val Loss=0.4879, lr=0.0100
[02/20 23:56:14 cifar10-global-random-3.0-resnet56]: Epoch 52/100, Acc=0.8334, Val Loss=0.5703, lr=0.0100
[02/20 23:56:45 cifar10-global-random-3.0-resnet56]: Epoch 53/100, Acc=0.8518, Val Loss=0.4844, lr=0.0100
[02/20 23:57:17 cifar10-global-random-3.0-resnet56]: Epoch 54/100, Acc=0.8448, Val Loss=0.4979, lr=0.0100
[02/20 23:57:49 cifar10-global-random-3.0-resnet56]: Epoch 55/100, Acc=0.8325, Val Loss=0.5774, lr=0.0100
[02/20 23:58:21 cifar10-global-random-3.0-resnet56]: Epoch 56/100, Acc=0.8615, Val Loss=0.4359, lr=0.0100
[02/20 23:58:53 cifar10-global-random-3.0-resnet56]: Epoch 57/100, Acc=0.8613, Val Loss=0.4589, lr=0.0100
[02/20 23:59:25 cifar10-global-random-3.0-resnet56]: Epoch 58/100, Acc=0.8638, Val Loss=0.4336, lr=0.0100
[02/20 23:59:57 cifar10-global-random-3.0-resnet56]: Epoch 59/100, Acc=0.8544, Val Loss=0.4899, lr=0.0100
[02/21 00:00:28 cifar10-global-random-3.0-resnet56]: Epoch 60/100, Acc=0.8889, Val Loss=0.3606, lr=0.0010
[02/21 00:01:00 cifar10-global-random-3.0-resnet56]: Epoch 61/100, Acc=0.8929, Val Loss=0.3598, lr=0.0010
[02/21 00:01:32 cifar10-global-random-3.0-resnet56]: Epoch 62/100, Acc=0.8925, Val Loss=0.3665, lr=0.0010
[02/21 00:02:04 cifar10-global-random-3.0-resnet56]: Epoch 63/100, Acc=0.8926, Val Loss=0.3652, lr=0.0010
[02/21 00:02:35 cifar10-global-random-3.0-resnet56]: Epoch 64/100, Acc=0.8927, Val Loss=0.3699, lr=0.0010
[02/21 00:03:08 cifar10-global-random-3.0-resnet56]: Epoch 65/100, Acc=0.8942, Val Loss=0.3694, lr=0.0010
[02/21 00:03:39 cifar10-global-random-3.0-resnet56]: Epoch 66/100, Acc=0.8955, Val Loss=0.3712, lr=0.0010
[02/21 00:04:11 cifar10-global-random-3.0-resnet56]: Epoch 67/100, Acc=0.8935, Val Loss=0.3730, lr=0.0010
[02/21 00:04:43 cifar10-global-random-3.0-resnet56]: Epoch 68/100, Acc=0.8963, Val Loss=0.3702, lr=0.0010
[02/21 00:05:15 cifar10-global-random-3.0-resnet56]: Epoch 69/100, Acc=0.8950, Val Loss=0.3792, lr=0.0010
[02/21 00:05:47 cifar10-global-random-3.0-resnet56]: Epoch 70/100, Acc=0.8929, Val Loss=0.3847, lr=0.0010
[02/21 00:06:19 cifar10-global-random-3.0-resnet56]: Epoch 71/100, Acc=0.8949, Val Loss=0.3802, lr=0.0010
[02/21 00:06:51 cifar10-global-random-3.0-resnet56]: Epoch 72/100, Acc=0.8938, Val Loss=0.3836, lr=0.0010
[02/21 00:07:23 cifar10-global-random-3.0-resnet56]: Epoch 73/100, Acc=0.8962, Val Loss=0.3885, lr=0.0010
[02/21 00:07:54 cifar10-global-random-3.0-resnet56]: Epoch 74/100, Acc=0.8931, Val Loss=0.3926, lr=0.0010
[02/21 00:08:26 cifar10-global-random-3.0-resnet56]: Epoch 75/100, Acc=0.8939, Val Loss=0.3933, lr=0.0010
[02/21 00:08:58 cifar10-global-random-3.0-resnet56]: Epoch 76/100, Acc=0.8944, Val Loss=0.3920, lr=0.0010
[02/21 00:09:31 cifar10-global-random-3.0-resnet56]: Epoch 77/100, Acc=0.8926, Val Loss=0.3964, lr=0.0010
[02/21 00:10:03 cifar10-global-random-3.0-resnet56]: Epoch 78/100, Acc=0.8916, Val Loss=0.4023, lr=0.0010
[02/21 00:10:35 cifar10-global-random-3.0-resnet56]: Epoch 79/100, Acc=0.8961, Val Loss=0.3970, lr=0.0010
[02/21 00:11:07 cifar10-global-random-3.0-resnet56]: Epoch 80/100, Acc=0.8956, Val Loss=0.3959, lr=0.0001
[02/21 00:11:39 cifar10-global-random-3.0-resnet56]: Epoch 81/100, Acc=0.8955, Val Loss=0.3934, lr=0.0001
[02/21 00:12:11 cifar10-global-random-3.0-resnet56]: Epoch 82/100, Acc=0.8951, Val Loss=0.3955, lr=0.0001
[02/21 00:12:43 cifar10-global-random-3.0-resnet56]: Epoch 83/100, Acc=0.8946, Val Loss=0.3950, lr=0.0001
[02/21 00:13:15 cifar10-global-random-3.0-resnet56]: Epoch 84/100, Acc=0.8956, Val Loss=0.3943, lr=0.0001
[02/21 00:13:47 cifar10-global-random-3.0-resnet56]: Epoch 85/100, Acc=0.8943, Val Loss=0.3962, lr=0.0001
[02/21 00:14:19 cifar10-global-random-3.0-resnet56]: Epoch 86/100, Acc=0.8949, Val Loss=0.3939, lr=0.0001
[02/21 00:14:51 cifar10-global-random-3.0-resnet56]: Epoch 87/100, Acc=0.8945, Val Loss=0.3956, lr=0.0001
[02/21 00:15:23 cifar10-global-random-3.0-resnet56]: Epoch 88/100, Acc=0.8959, Val Loss=0.3937, lr=0.0001
[02/21 00:15:54 cifar10-global-random-3.0-resnet56]: Epoch 89/100, Acc=0.8946, Val Loss=0.3994, lr=0.0001
[02/21 00:16:25 cifar10-global-random-3.0-resnet56]: Epoch 90/100, Acc=0.8951, Val Loss=0.3980, lr=0.0001
[02/21 00:16:57 cifar10-global-random-3.0-resnet56]: Epoch 91/100, Acc=0.8955, Val Loss=0.3946, lr=0.0001
[02/21 00:17:28 cifar10-global-random-3.0-resnet56]: Epoch 92/100, Acc=0.8954, Val Loss=0.3994, lr=0.0001
[02/21 00:17:59 cifar10-global-random-3.0-resnet56]: Epoch 93/100, Acc=0.8946, Val Loss=0.3971, lr=0.0001
[02/21 00:18:30 cifar10-global-random-3.0-resnet56]: Epoch 94/100, Acc=0.8949, Val Loss=0.3980, lr=0.0001
[02/21 00:19:01 cifar10-global-random-3.0-resnet56]: Epoch 95/100, Acc=0.8953, Val Loss=0.4004, lr=0.0001
[02/21 00:19:32 cifar10-global-random-3.0-resnet56]: Epoch 96/100, Acc=0.8955, Val Loss=0.3993, lr=0.0001
[02/21 00:20:03 cifar10-global-random-3.0-resnet56]: Epoch 97/100, Acc=0.8944, Val Loss=0.4009, lr=0.0001
[02/21 00:20:35 cifar10-global-random-3.0-resnet56]: Epoch 98/100, Acc=0.8944, Val Loss=0.4005, lr=0.0001
[02/21 00:21:06 cifar10-global-random-3.0-resnet56]: Epoch 99/100, Acc=0.8940, Val Loss=0.3997, lr=0.0001
[02/21 00:21:06 cifar10-global-random-3.0-resnet56]: Best Acc=0.8963
[02/21 00:21:06 cifar10-global-random-3.0-resnet56]: Params: 0.45 M
[02/21 00:21:06 cifar10-global-random-3.0-resnet56]: ops: 39.03 M
[02/21 00:21:10 cifar10-global-random-3.0-resnet56]: Acc: 0.8940 Val Loss: 0.3997

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: mode: prune
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: model: resnet56
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: verbose: False
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: dataset: cifar10
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: dataroot: data
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: batch_size: 128
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: total_epochs: 100
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: lr_decay_milestones: 60,80
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: lr_decay_gamma: 0.1
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: lr: 0.01
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: output_dir: run/cifar10/prune/cifar10-global-l2-3.0-resnet56
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: finetune: True
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: last_epochs: 100
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: reps: 1
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: method: l2
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: speed_up: 3.0
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: max_pruning_ratio: 1.0
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: reg: 1e-05
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: delta_reg: 0.0001
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: weight_decay: 0.0005
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: seed: 1
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: global_pruning: True
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: sl_total_epochs: 100
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: sl_lr: 0.01
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: sl_reg_warmup: 0
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: sl_restore: None
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: iterative_steps: 400
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: logger: <Logger cifar10-global-l2-3.0-resnet56 (DEBUG)>
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: device: cuda
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: num_classes: 10
[02/21 00:21:17 cifar10-global-l2-3.0-resnet56]: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 00:21:21 cifar10-global-l2-3.0-resnet56]: Pruning...
[02/21 00:21:41 cifar10-global-l2-3.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(11, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(11, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(11, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(11, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(11, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(11, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(11, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(11, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(11, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(11, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(11, 15, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(15, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(15, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(15, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(15, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(15, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(15, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(15, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(15, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(15, 55, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(55, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(55, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(51, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(55, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(46, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(55, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(48, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(55, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(36, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(55, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(33, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(55, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(55, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(41, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=55, out_features=10, bias=True)
)
[02/21 00:21:44 cifar10-global-l2-3.0-resnet56]: Params: 0.86 M => 0.42 M (48.50%)
[02/21 00:21:44 cifar10-global-l2-3.0-resnet56]: FLOPs: 127.12 M => 41.95 M (33.00%, 3.03X )
[02/21 00:21:44 cifar10-global-l2-3.0-resnet56]: Acc: 0.9392 => 0.1325
[02/21 00:21:44 cifar10-global-l2-3.0-resnet56]: Val Loss: 0.2587 => 8.6899
[02/21 00:21:44 cifar10-global-l2-3.0-resnet56]: Finetuning...
[02/21 00:22:16 cifar10-global-l2-3.0-resnet56]: Epoch 0/100, Acc=0.8514, Val Loss=0.4564, lr=0.0100
[02/21 00:22:47 cifar10-global-l2-3.0-resnet56]: Epoch 1/100, Acc=0.8751, Val Loss=0.3798, lr=0.0100
[02/21 00:23:19 cifar10-global-l2-3.0-resnet56]: Epoch 2/100, Acc=0.8830, Val Loss=0.3702, lr=0.0100
[02/21 00:23:50 cifar10-global-l2-3.0-resnet56]: Epoch 3/100, Acc=0.8672, Val Loss=0.4086, lr=0.0100
[02/21 00:24:22 cifar10-global-l2-3.0-resnet56]: Epoch 4/100, Acc=0.8892, Val Loss=0.3672, lr=0.0100
[02/21 00:24:53 cifar10-global-l2-3.0-resnet56]: Epoch 5/100, Acc=0.8815, Val Loss=0.3736, lr=0.0100
[02/21 00:25:24 cifar10-global-l2-3.0-resnet56]: Epoch 6/100, Acc=0.8828, Val Loss=0.3690, lr=0.0100
[02/21 00:25:55 cifar10-global-l2-3.0-resnet56]: Epoch 7/100, Acc=0.8883, Val Loss=0.3541, lr=0.0100
[02/21 00:26:27 cifar10-global-l2-3.0-resnet56]: Epoch 8/100, Acc=0.8919, Val Loss=0.3490, lr=0.0100
[02/21 00:26:59 cifar10-global-l2-3.0-resnet56]: Epoch 9/100, Acc=0.8767, Val Loss=0.4102, lr=0.0100
[02/21 00:27:31 cifar10-global-l2-3.0-resnet56]: Epoch 10/100, Acc=0.8759, Val Loss=0.4133, lr=0.0100
[02/21 00:28:03 cifar10-global-l2-3.0-resnet56]: Epoch 11/100, Acc=0.8935, Val Loss=0.3345, lr=0.0100
[02/21 00:28:34 cifar10-global-l2-3.0-resnet56]: Epoch 12/100, Acc=0.8903, Val Loss=0.3518, lr=0.0100
[02/21 00:29:06 cifar10-global-l2-3.0-resnet56]: Epoch 13/100, Acc=0.8893, Val Loss=0.3724, lr=0.0100
[02/21 00:29:37 cifar10-global-l2-3.0-resnet56]: Epoch 14/100, Acc=0.8893, Val Loss=0.3600, lr=0.0100
[02/21 00:30:10 cifar10-global-l2-3.0-resnet56]: Epoch 15/100, Acc=0.8959, Val Loss=0.3453, lr=0.0100
[02/21 00:30:42 cifar10-global-l2-3.0-resnet56]: Epoch 16/100, Acc=0.8907, Val Loss=0.3534, lr=0.0100
[02/21 00:31:14 cifar10-global-l2-3.0-resnet56]: Epoch 17/100, Acc=0.8972, Val Loss=0.3310, lr=0.0100
[02/21 00:31:46 cifar10-global-l2-3.0-resnet56]: Epoch 18/100, Acc=0.8858, Val Loss=0.3627, lr=0.0100
[02/21 00:32:18 cifar10-global-l2-3.0-resnet56]: Epoch 19/100, Acc=0.8898, Val Loss=0.3627, lr=0.0100
[02/21 00:32:50 cifar10-global-l2-3.0-resnet56]: Epoch 20/100, Acc=0.8766, Val Loss=0.4364, lr=0.0100
[02/21 00:33:22 cifar10-global-l2-3.0-resnet56]: Epoch 21/100, Acc=0.8963, Val Loss=0.3457, lr=0.0100
[02/21 00:33:54 cifar10-global-l2-3.0-resnet56]: Epoch 22/100, Acc=0.8916, Val Loss=0.3674, lr=0.0100
[02/21 00:34:27 cifar10-global-l2-3.0-resnet56]: Epoch 23/100, Acc=0.8939, Val Loss=0.3425, lr=0.0100
[02/21 00:34:59 cifar10-global-l2-3.0-resnet56]: Epoch 24/100, Acc=0.8982, Val Loss=0.3377, lr=0.0100
[02/21 00:35:31 cifar10-global-l2-3.0-resnet56]: Epoch 25/100, Acc=0.8917, Val Loss=0.3574, lr=0.0100
[02/21 00:36:04 cifar10-global-l2-3.0-resnet56]: Epoch 26/100, Acc=0.8936, Val Loss=0.3576, lr=0.0100
[02/21 00:36:36 cifar10-global-l2-3.0-resnet56]: Epoch 27/100, Acc=0.8947, Val Loss=0.3522, lr=0.0100
[02/21 00:37:09 cifar10-global-l2-3.0-resnet56]: Epoch 28/100, Acc=0.8870, Val Loss=0.3804, lr=0.0100
[02/21 00:37:40 cifar10-global-l2-3.0-resnet56]: Epoch 29/100, Acc=0.8954, Val Loss=0.3382, lr=0.0100
[02/21 00:38:12 cifar10-global-l2-3.0-resnet56]: Epoch 30/100, Acc=0.8957, Val Loss=0.3472, lr=0.0100
[02/21 00:38:44 cifar10-global-l2-3.0-resnet56]: Epoch 31/100, Acc=0.8892, Val Loss=0.3805, lr=0.0100
[02/21 00:39:15 cifar10-global-l2-3.0-resnet56]: Epoch 32/100, Acc=0.9049, Val Loss=0.3139, lr=0.0100
[02/21 00:39:47 cifar10-global-l2-3.0-resnet56]: Epoch 33/100, Acc=0.8947, Val Loss=0.3444, lr=0.0100
[02/21 00:40:18 cifar10-global-l2-3.0-resnet56]: Epoch 34/100, Acc=0.9021, Val Loss=0.3209, lr=0.0100
[02/21 00:40:49 cifar10-global-l2-3.0-resnet56]: Epoch 35/100, Acc=0.8933, Val Loss=0.3560, lr=0.0100
[02/21 00:41:21 cifar10-global-l2-3.0-resnet56]: Epoch 36/100, Acc=0.8920, Val Loss=0.3594, lr=0.0100
[02/21 00:41:53 cifar10-global-l2-3.0-resnet56]: Epoch 37/100, Acc=0.8978, Val Loss=0.3427, lr=0.0100
[02/21 00:42:25 cifar10-global-l2-3.0-resnet56]: Epoch 38/100, Acc=0.8970, Val Loss=0.3386, lr=0.0100
[02/21 00:42:56 cifar10-global-l2-3.0-resnet56]: Epoch 39/100, Acc=0.8921, Val Loss=0.3675, lr=0.0100
[02/21 00:43:29 cifar10-global-l2-3.0-resnet56]: Epoch 40/100, Acc=0.9034, Val Loss=0.3138, lr=0.0100
[02/21 00:44:01 cifar10-global-l2-3.0-resnet56]: Epoch 41/100, Acc=0.8975, Val Loss=0.3446, lr=0.0100
[02/21 00:44:33 cifar10-global-l2-3.0-resnet56]: Epoch 42/100, Acc=0.8998, Val Loss=0.3282, lr=0.0100
[02/21 00:45:05 cifar10-global-l2-3.0-resnet56]: Epoch 43/100, Acc=0.8957, Val Loss=0.3432, lr=0.0100
[02/21 00:45:37 cifar10-global-l2-3.0-resnet56]: Epoch 44/100, Acc=0.9029, Val Loss=0.3156, lr=0.0100
[02/21 00:46:09 cifar10-global-l2-3.0-resnet56]: Epoch 45/100, Acc=0.9002, Val Loss=0.3317, lr=0.0100
[02/21 00:46:41 cifar10-global-l2-3.0-resnet56]: Epoch 46/100, Acc=0.9062, Val Loss=0.3189, lr=0.0100
[02/21 00:47:12 cifar10-global-l2-3.0-resnet56]: Epoch 47/100, Acc=0.8992, Val Loss=0.3459, lr=0.0100
[02/21 00:47:43 cifar10-global-l2-3.0-resnet56]: Epoch 48/100, Acc=0.8933, Val Loss=0.3489, lr=0.0100
[02/21 00:48:14 cifar10-global-l2-3.0-resnet56]: Epoch 49/100, Acc=0.9017, Val Loss=0.3276, lr=0.0100
[02/21 00:48:46 cifar10-global-l2-3.0-resnet56]: Epoch 50/100, Acc=0.8988, Val Loss=0.3523, lr=0.0100
[02/21 00:49:18 cifar10-global-l2-3.0-resnet56]: Epoch 51/100, Acc=0.8982, Val Loss=0.3459, lr=0.0100
[02/21 00:49:49 cifar10-global-l2-3.0-resnet56]: Epoch 52/100, Acc=0.8880, Val Loss=0.4012, lr=0.0100
[02/21 00:50:21 cifar10-global-l2-3.0-resnet56]: Epoch 53/100, Acc=0.9030, Val Loss=0.3294, lr=0.0100
[02/21 00:50:52 cifar10-global-l2-3.0-resnet56]: Epoch 54/100, Acc=0.8971, Val Loss=0.3539, lr=0.0100
[02/21 00:51:23 cifar10-global-l2-3.0-resnet56]: Epoch 55/100, Acc=0.9003, Val Loss=0.3271, lr=0.0100
[02/21 00:51:54 cifar10-global-l2-3.0-resnet56]: Epoch 56/100, Acc=0.9031, Val Loss=0.3286, lr=0.0100
[02/21 00:52:26 cifar10-global-l2-3.0-resnet56]: Epoch 57/100, Acc=0.9030, Val Loss=0.3377, lr=0.0100
[02/21 00:52:57 cifar10-global-l2-3.0-resnet56]: Epoch 58/100, Acc=0.8998, Val Loss=0.3439, lr=0.0100
[02/21 00:53:27 cifar10-global-l2-3.0-resnet56]: Epoch 59/100, Acc=0.8972, Val Loss=0.3584, lr=0.0100
[02/21 00:53:59 cifar10-global-l2-3.0-resnet56]: Epoch 60/100, Acc=0.9239, Val Loss=0.2652, lr=0.0010
[02/21 00:54:30 cifar10-global-l2-3.0-resnet56]: Epoch 61/100, Acc=0.9269, Val Loss=0.2667, lr=0.0010
[02/21 00:55:02 cifar10-global-l2-3.0-resnet56]: Epoch 62/100, Acc=0.9248, Val Loss=0.2715, lr=0.0010
[02/21 00:55:33 cifar10-global-l2-3.0-resnet56]: Epoch 63/100, Acc=0.9265, Val Loss=0.2732, lr=0.0010
[02/21 00:56:05 cifar10-global-l2-3.0-resnet56]: Epoch 64/100, Acc=0.9244, Val Loss=0.2725, lr=0.0010
[02/21 00:56:36 cifar10-global-l2-3.0-resnet56]: Epoch 65/100, Acc=0.9248, Val Loss=0.2784, lr=0.0010
[02/21 00:57:08 cifar10-global-l2-3.0-resnet56]: Epoch 66/100, Acc=0.9296, Val Loss=0.2769, lr=0.0010
[02/21 00:57:40 cifar10-global-l2-3.0-resnet56]: Epoch 67/100, Acc=0.9277, Val Loss=0.2821, lr=0.0010
[02/21 00:58:11 cifar10-global-l2-3.0-resnet56]: Epoch 68/100, Acc=0.9258, Val Loss=0.2815, lr=0.0010
[02/21 00:58:43 cifar10-global-l2-3.0-resnet56]: Epoch 69/100, Acc=0.9280, Val Loss=0.2815, lr=0.0010
[02/21 00:59:14 cifar10-global-l2-3.0-resnet56]: Epoch 70/100, Acc=0.9264, Val Loss=0.2886, lr=0.0010
[02/21 00:59:45 cifar10-global-l2-3.0-resnet56]: Epoch 71/100, Acc=0.9277, Val Loss=0.2858, lr=0.0010
[02/21 01:00:17 cifar10-global-l2-3.0-resnet56]: Epoch 72/100, Acc=0.9280, Val Loss=0.2865, lr=0.0010
[02/21 01:00:48 cifar10-global-l2-3.0-resnet56]: Epoch 73/100, Acc=0.9260, Val Loss=0.2934, lr=0.0010
[02/21 01:01:19 cifar10-global-l2-3.0-resnet56]: Epoch 74/100, Acc=0.9281, Val Loss=0.2884, lr=0.0010
[02/21 01:01:50 cifar10-global-l2-3.0-resnet56]: Epoch 75/100, Acc=0.9280, Val Loss=0.2901, lr=0.0010
[02/21 01:02:22 cifar10-global-l2-3.0-resnet56]: Epoch 76/100, Acc=0.9277, Val Loss=0.2908, lr=0.0010
[02/21 01:02:53 cifar10-global-l2-3.0-resnet56]: Epoch 77/100, Acc=0.9276, Val Loss=0.2942, lr=0.0010
[02/21 01:03:25 cifar10-global-l2-3.0-resnet56]: Epoch 78/100, Acc=0.9288, Val Loss=0.2926, lr=0.0010
[02/21 01:03:57 cifar10-global-l2-3.0-resnet56]: Epoch 79/100, Acc=0.9285, Val Loss=0.2890, lr=0.0010
[02/21 01:04:28 cifar10-global-l2-3.0-resnet56]: Epoch 80/100, Acc=0.9290, Val Loss=0.2909, lr=0.0001
[02/21 01:05:00 cifar10-global-l2-3.0-resnet56]: Epoch 81/100, Acc=0.9296, Val Loss=0.2877, lr=0.0001
[02/21 01:05:31 cifar10-global-l2-3.0-resnet56]: Epoch 82/100, Acc=0.9304, Val Loss=0.2880, lr=0.0001
[02/21 01:06:02 cifar10-global-l2-3.0-resnet56]: Epoch 83/100, Acc=0.9297, Val Loss=0.2904, lr=0.0001
[02/21 01:06:34 cifar10-global-l2-3.0-resnet56]: Epoch 84/100, Acc=0.9298, Val Loss=0.2899, lr=0.0001
[02/21 01:07:05 cifar10-global-l2-3.0-resnet56]: Epoch 85/100, Acc=0.9303, Val Loss=0.2894, lr=0.0001
[02/21 01:07:37 cifar10-global-l2-3.0-resnet56]: Epoch 86/100, Acc=0.9300, Val Loss=0.2887, lr=0.0001
[02/21 01:08:08 cifar10-global-l2-3.0-resnet56]: Epoch 87/100, Acc=0.9293, Val Loss=0.2915, lr=0.0001
[02/21 01:08:40 cifar10-global-l2-3.0-resnet56]: Epoch 88/100, Acc=0.9297, Val Loss=0.2870, lr=0.0001
[02/21 01:09:13 cifar10-global-l2-3.0-resnet56]: Epoch 89/100, Acc=0.9291, Val Loss=0.2912, lr=0.0001
[02/21 01:09:45 cifar10-global-l2-3.0-resnet56]: Epoch 90/100, Acc=0.9299, Val Loss=0.2909, lr=0.0001
[02/21 01:10:17 cifar10-global-l2-3.0-resnet56]: Epoch 91/100, Acc=0.9310, Val Loss=0.2903, lr=0.0001
[02/21 01:10:50 cifar10-global-l2-3.0-resnet56]: Epoch 92/100, Acc=0.9296, Val Loss=0.2896, lr=0.0001
[02/21 01:11:22 cifar10-global-l2-3.0-resnet56]: Epoch 93/100, Acc=0.9303, Val Loss=0.2903, lr=0.0001
[02/21 01:11:54 cifar10-global-l2-3.0-resnet56]: Epoch 94/100, Acc=0.9299, Val Loss=0.2917, lr=0.0001
[02/21 01:12:27 cifar10-global-l2-3.0-resnet56]: Epoch 95/100, Acc=0.9298, Val Loss=0.2929, lr=0.0001
[02/21 01:12:59 cifar10-global-l2-3.0-resnet56]: Epoch 96/100, Acc=0.9315, Val Loss=0.2905, lr=0.0001
[02/21 01:13:32 cifar10-global-l2-3.0-resnet56]: Epoch 97/100, Acc=0.9306, Val Loss=0.2905, lr=0.0001
[02/21 01:14:06 cifar10-global-l2-3.0-resnet56]: Epoch 98/100, Acc=0.9298, Val Loss=0.2912, lr=0.0001
[02/21 01:14:39 cifar10-global-l2-3.0-resnet56]: Epoch 99/100, Acc=0.9304, Val Loss=0.2915, lr=0.0001
[02/21 01:14:39 cifar10-global-l2-3.0-resnet56]: Best Acc=0.9315
[02/21 01:14:39 cifar10-global-l2-3.0-resnet56]: Params: 0.42 M
[02/21 01:14:39 cifar10-global-l2-3.0-resnet56]: ops: 41.95 M
[02/21 01:14:42 cifar10-global-l2-3.0-resnet56]: Acc: 0.9304 Val Loss: 0.2915

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: mode: prune
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: model: resnet56
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: verbose: False
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: dataset: cifar10
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: dataroot: data
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: batch_size: 128
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: total_epochs: 100
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: lr_decay_milestones: 60,80
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: lr_decay_gamma: 0.1
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: lr: 0.01
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: output_dir: run/cifar10/prune/cifar10-global-fpgm-3.0-resnet56
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: finetune: True
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: last_epochs: 100
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: reps: 1
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: method: fpgm
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: speed_up: 3.0
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: max_pruning_ratio: 1.0
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: reg: 1e-05
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: delta_reg: 0.0001
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: weight_decay: 0.0005
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: seed: 1
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: global_pruning: True
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: sl_total_epochs: 100
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: sl_lr: 0.01
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: sl_reg_warmup: 0
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: sl_restore: None
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: iterative_steps: 400
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: logger: <Logger cifar10-global-fpgm-3.0-resnet56 (DEBUG)>
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: device: cuda
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: num_classes: 10
[02/21 01:14:50 cifar10-global-fpgm-3.0-resnet56]: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 01:14:53 cifar10-global-fpgm-3.0-resnet56]: Pruning...
[02/21 01:15:13 cifar10-global-fpgm-3.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(13, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(13, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(13, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(13, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(13, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(13, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(13, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(13, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(13, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(13, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(13, 15, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(15, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(15, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(15, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(15, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(15, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(15, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(15, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(15, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(15, 31, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(31, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(58, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(31, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(54, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(31, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(48, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(31, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(51, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(31, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(46, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(31, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(44, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(31, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(33, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(31, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(34, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=31, out_features=10, bias=True)
)
[02/21 01:15:17 cifar10-global-fpgm-3.0-resnet56]: Params: 0.86 M => 0.28 M (33.29%)
[02/21 01:15:17 cifar10-global-fpgm-3.0-resnet56]: FLOPs: 127.12 M => 42.23 M (33.22%, 3.01X )
[02/21 01:15:17 cifar10-global-fpgm-3.0-resnet56]: Acc: 0.9392 => 0.1792
[02/21 01:15:17 cifar10-global-fpgm-3.0-resnet56]: Val Loss: 0.2587 => 2.2318
[02/21 01:15:17 cifar10-global-fpgm-3.0-resnet56]: Finetuning...
[02/21 01:15:48 cifar10-global-fpgm-3.0-resnet56]: Epoch 0/100, Acc=0.8186, Val Loss=0.5478, lr=0.0100
[02/21 01:16:20 cifar10-global-fpgm-3.0-resnet56]: Epoch 1/100, Acc=0.8492, Val Loss=0.4577, lr=0.0100
[02/21 01:16:52 cifar10-global-fpgm-3.0-resnet56]: Epoch 2/100, Acc=0.8683, Val Loss=0.3918, lr=0.0100
[02/21 01:17:24 cifar10-global-fpgm-3.0-resnet56]: Epoch 3/100, Acc=0.8763, Val Loss=0.3672, lr=0.0100
[02/21 01:17:56 cifar10-global-fpgm-3.0-resnet56]: Epoch 4/100, Acc=0.8740, Val Loss=0.3722, lr=0.0100
[02/21 01:18:29 cifar10-global-fpgm-3.0-resnet56]: Epoch 5/100, Acc=0.8628, Val Loss=0.4060, lr=0.0100
[02/21 01:19:01 cifar10-global-fpgm-3.0-resnet56]: Epoch 6/100, Acc=0.8778, Val Loss=0.3674, lr=0.0100
[02/21 01:19:33 cifar10-global-fpgm-3.0-resnet56]: Epoch 7/100, Acc=0.8755, Val Loss=0.3707, lr=0.0100
[02/21 01:20:05 cifar10-global-fpgm-3.0-resnet56]: Epoch 8/100, Acc=0.8627, Val Loss=0.4282, lr=0.0100
[02/21 01:20:37 cifar10-global-fpgm-3.0-resnet56]: Epoch 9/100, Acc=0.8788, Val Loss=0.3843, lr=0.0100
[02/21 01:21:10 cifar10-global-fpgm-3.0-resnet56]: Epoch 10/100, Acc=0.8799, Val Loss=0.3892, lr=0.0100
[02/21 01:21:43 cifar10-global-fpgm-3.0-resnet56]: Epoch 11/100, Acc=0.8801, Val Loss=0.3740, lr=0.0100
[02/21 01:22:16 cifar10-global-fpgm-3.0-resnet56]: Epoch 12/100, Acc=0.8685, Val Loss=0.4054, lr=0.0100
[02/21 01:22:48 cifar10-global-fpgm-3.0-resnet56]: Epoch 13/100, Acc=0.8829, Val Loss=0.3674, lr=0.0100
[02/21 01:23:20 cifar10-global-fpgm-3.0-resnet56]: Epoch 14/100, Acc=0.8753, Val Loss=0.4008, lr=0.0100
[02/21 01:23:52 cifar10-global-fpgm-3.0-resnet56]: Epoch 15/100, Acc=0.8783, Val Loss=0.3897, lr=0.0100
[02/21 01:24:24 cifar10-global-fpgm-3.0-resnet56]: Epoch 16/100, Acc=0.8697, Val Loss=0.4192, lr=0.0100
[02/21 01:24:57 cifar10-global-fpgm-3.0-resnet56]: Epoch 17/100, Acc=0.8771, Val Loss=0.3870, lr=0.0100
[02/21 01:25:30 cifar10-global-fpgm-3.0-resnet56]: Epoch 18/100, Acc=0.8864, Val Loss=0.3612, lr=0.0100
[02/21 01:26:02 cifar10-global-fpgm-3.0-resnet56]: Epoch 19/100, Acc=0.8649, Val Loss=0.4129, lr=0.0100
[02/21 01:26:35 cifar10-global-fpgm-3.0-resnet56]: Epoch 20/100, Acc=0.8877, Val Loss=0.3602, lr=0.0100
[02/21 01:27:07 cifar10-global-fpgm-3.0-resnet56]: Epoch 21/100, Acc=0.8735, Val Loss=0.4026, lr=0.0100
[02/21 01:27:40 cifar10-global-fpgm-3.0-resnet56]: Epoch 22/100, Acc=0.8801, Val Loss=0.3843, lr=0.0100
[02/21 01:28:12 cifar10-global-fpgm-3.0-resnet56]: Epoch 23/100, Acc=0.8887, Val Loss=0.3526, lr=0.0100
[02/21 01:28:45 cifar10-global-fpgm-3.0-resnet56]: Epoch 24/100, Acc=0.8697, Val Loss=0.4222, lr=0.0100
[02/21 01:29:18 cifar10-global-fpgm-3.0-resnet56]: Epoch 25/100, Acc=0.8745, Val Loss=0.4170, lr=0.0100
[02/21 01:29:51 cifar10-global-fpgm-3.0-resnet56]: Epoch 26/100, Acc=0.8874, Val Loss=0.3572, lr=0.0100
[02/21 01:30:24 cifar10-global-fpgm-3.0-resnet56]: Epoch 27/100, Acc=0.8832, Val Loss=0.3657, lr=0.0100
[02/21 01:30:58 cifar10-global-fpgm-3.0-resnet56]: Epoch 28/100, Acc=0.8861, Val Loss=0.3666, lr=0.0100
[02/21 01:31:31 cifar10-global-fpgm-3.0-resnet56]: Epoch 29/100, Acc=0.8873, Val Loss=0.3553, lr=0.0100
[02/21 01:32:05 cifar10-global-fpgm-3.0-resnet56]: Epoch 30/100, Acc=0.8949, Val Loss=0.3375, lr=0.0100
[02/21 01:32:39 cifar10-global-fpgm-3.0-resnet56]: Epoch 31/100, Acc=0.8914, Val Loss=0.3555, lr=0.0100
[02/21 01:33:12 cifar10-global-fpgm-3.0-resnet56]: Epoch 32/100, Acc=0.8841, Val Loss=0.3738, lr=0.0100
[02/21 01:33:45 cifar10-global-fpgm-3.0-resnet56]: Epoch 33/100, Acc=0.8829, Val Loss=0.3714, lr=0.0100
[02/21 01:34:18 cifar10-global-fpgm-3.0-resnet56]: Epoch 34/100, Acc=0.8917, Val Loss=0.3523, lr=0.0100
[02/21 01:34:51 cifar10-global-fpgm-3.0-resnet56]: Epoch 35/100, Acc=0.8988, Val Loss=0.3226, lr=0.0100
[02/21 01:35:24 cifar10-global-fpgm-3.0-resnet56]: Epoch 36/100, Acc=0.8934, Val Loss=0.3449, lr=0.0100
[02/21 01:35:56 cifar10-global-fpgm-3.0-resnet56]: Epoch 37/100, Acc=0.8815, Val Loss=0.3953, lr=0.0100
[02/21 01:36:29 cifar10-global-fpgm-3.0-resnet56]: Epoch 38/100, Acc=0.8943, Val Loss=0.3449, lr=0.0100
[02/21 01:37:02 cifar10-global-fpgm-3.0-resnet56]: Epoch 39/100, Acc=0.8914, Val Loss=0.3568, lr=0.0100
[02/21 01:37:34 cifar10-global-fpgm-3.0-resnet56]: Epoch 40/100, Acc=0.8862, Val Loss=0.3590, lr=0.0100
[02/21 01:38:07 cifar10-global-fpgm-3.0-resnet56]: Epoch 41/100, Acc=0.8843, Val Loss=0.3812, lr=0.0100
[02/21 01:38:40 cifar10-global-fpgm-3.0-resnet56]: Epoch 42/100, Acc=0.8897, Val Loss=0.3643, lr=0.0100
[02/21 01:39:12 cifar10-global-fpgm-3.0-resnet56]: Epoch 43/100, Acc=0.8923, Val Loss=0.3660, lr=0.0100
[02/21 01:39:44 cifar10-global-fpgm-3.0-resnet56]: Epoch 44/100, Acc=0.8783, Val Loss=0.3998, lr=0.0100
[02/21 01:40:17 cifar10-global-fpgm-3.0-resnet56]: Epoch 45/100, Acc=0.8930, Val Loss=0.3491, lr=0.0100
[02/21 01:40:49 cifar10-global-fpgm-3.0-resnet56]: Epoch 46/100, Acc=0.8952, Val Loss=0.3406, lr=0.0100
[02/21 01:41:22 cifar10-global-fpgm-3.0-resnet56]: Epoch 47/100, Acc=0.8908, Val Loss=0.3741, lr=0.0100
[02/21 01:41:55 cifar10-global-fpgm-3.0-resnet56]: Epoch 48/100, Acc=0.9046, Val Loss=0.3088, lr=0.0100
[02/21 01:42:27 cifar10-global-fpgm-3.0-resnet56]: Epoch 49/100, Acc=0.8985, Val Loss=0.3382, lr=0.0100
[02/21 01:43:00 cifar10-global-fpgm-3.0-resnet56]: Epoch 50/100, Acc=0.8892, Val Loss=0.3644, lr=0.0100
[02/21 01:43:32 cifar10-global-fpgm-3.0-resnet56]: Epoch 51/100, Acc=0.8863, Val Loss=0.3654, lr=0.0100
[02/21 01:44:06 cifar10-global-fpgm-3.0-resnet56]: Epoch 52/100, Acc=0.8881, Val Loss=0.3860, lr=0.0100
[02/21 01:44:39 cifar10-global-fpgm-3.0-resnet56]: Epoch 53/100, Acc=0.8943, Val Loss=0.3558, lr=0.0100
[02/21 01:45:11 cifar10-global-fpgm-3.0-resnet56]: Epoch 54/100, Acc=0.8866, Val Loss=0.3662, lr=0.0100
[02/21 01:45:44 cifar10-global-fpgm-3.0-resnet56]: Epoch 55/100, Acc=0.8823, Val Loss=0.3834, lr=0.0100
[02/21 01:46:16 cifar10-global-fpgm-3.0-resnet56]: Epoch 56/100, Acc=0.8948, Val Loss=0.3459, lr=0.0100
[02/21 01:46:49 cifar10-global-fpgm-3.0-resnet56]: Epoch 57/100, Acc=0.8922, Val Loss=0.3654, lr=0.0100
[02/21 01:47:22 cifar10-global-fpgm-3.0-resnet56]: Epoch 58/100, Acc=0.8888, Val Loss=0.3599, lr=0.0100
[02/21 01:47:55 cifar10-global-fpgm-3.0-resnet56]: Epoch 59/100, Acc=0.8864, Val Loss=0.3821, lr=0.0100
[02/21 01:48:28 cifar10-global-fpgm-3.0-resnet56]: Epoch 60/100, Acc=0.9176, Val Loss=0.2697, lr=0.0010
[02/21 01:49:00 cifar10-global-fpgm-3.0-resnet56]: Epoch 61/100, Acc=0.9208, Val Loss=0.2679, lr=0.0010
[02/21 01:49:33 cifar10-global-fpgm-3.0-resnet56]: Epoch 62/100, Acc=0.9211, Val Loss=0.2686, lr=0.0010
[02/21 01:50:05 cifar10-global-fpgm-3.0-resnet56]: Epoch 63/100, Acc=0.9196, Val Loss=0.2695, lr=0.0010
[02/21 01:50:38 cifar10-global-fpgm-3.0-resnet56]: Epoch 64/100, Acc=0.9227, Val Loss=0.2687, lr=0.0010
[02/21 01:51:11 cifar10-global-fpgm-3.0-resnet56]: Epoch 65/100, Acc=0.9228, Val Loss=0.2700, lr=0.0010
[02/21 01:51:43 cifar10-global-fpgm-3.0-resnet56]: Epoch 66/100, Acc=0.9208, Val Loss=0.2710, lr=0.0010
[02/21 01:52:16 cifar10-global-fpgm-3.0-resnet56]: Epoch 67/100, Acc=0.9205, Val Loss=0.2763, lr=0.0010
[02/21 01:52:48 cifar10-global-fpgm-3.0-resnet56]: Epoch 68/100, Acc=0.9204, Val Loss=0.2809, lr=0.0010
[02/21 01:53:20 cifar10-global-fpgm-3.0-resnet56]: Epoch 69/100, Acc=0.9223, Val Loss=0.2738, lr=0.0010
[02/21 01:53:52 cifar10-global-fpgm-3.0-resnet56]: Epoch 70/100, Acc=0.9208, Val Loss=0.2846, lr=0.0010
[02/21 01:54:25 cifar10-global-fpgm-3.0-resnet56]: Epoch 71/100, Acc=0.9231, Val Loss=0.2805, lr=0.0010
[02/21 01:54:57 cifar10-global-fpgm-3.0-resnet56]: Epoch 72/100, Acc=0.9231, Val Loss=0.2824, lr=0.0010
[02/21 01:55:30 cifar10-global-fpgm-3.0-resnet56]: Epoch 73/100, Acc=0.9207, Val Loss=0.2874, lr=0.0010
[02/21 01:56:03 cifar10-global-fpgm-3.0-resnet56]: Epoch 74/100, Acc=0.9235, Val Loss=0.2826, lr=0.0010
[02/21 01:56:36 cifar10-global-fpgm-3.0-resnet56]: Epoch 75/100, Acc=0.9227, Val Loss=0.2838, lr=0.0010
[02/21 01:57:08 cifar10-global-fpgm-3.0-resnet56]: Epoch 76/100, Acc=0.9232, Val Loss=0.2856, lr=0.0010
[02/21 01:57:41 cifar10-global-fpgm-3.0-resnet56]: Epoch 77/100, Acc=0.9220, Val Loss=0.2870, lr=0.0010
[02/21 01:58:14 cifar10-global-fpgm-3.0-resnet56]: Epoch 78/100, Acc=0.9207, Val Loss=0.2913, lr=0.0010
[02/21 01:58:47 cifar10-global-fpgm-3.0-resnet56]: Epoch 79/100, Acc=0.9223, Val Loss=0.2843, lr=0.0010
[02/21 01:59:20 cifar10-global-fpgm-3.0-resnet56]: Epoch 80/100, Acc=0.9215, Val Loss=0.2881, lr=0.0001
[02/21 01:59:53 cifar10-global-fpgm-3.0-resnet56]: Epoch 81/100, Acc=0.9227, Val Loss=0.2821, lr=0.0001
[02/21 02:00:27 cifar10-global-fpgm-3.0-resnet56]: Epoch 82/100, Acc=0.9220, Val Loss=0.2831, lr=0.0001
[02/21 02:00:59 cifar10-global-fpgm-3.0-resnet56]: Epoch 83/100, Acc=0.9228, Val Loss=0.2845, lr=0.0001
[02/21 02:01:32 cifar10-global-fpgm-3.0-resnet56]: Epoch 84/100, Acc=0.9231, Val Loss=0.2875, lr=0.0001
[02/21 02:02:05 cifar10-global-fpgm-3.0-resnet56]: Epoch 85/100, Acc=0.9227, Val Loss=0.2864, lr=0.0001
[02/21 02:02:38 cifar10-global-fpgm-3.0-resnet56]: Epoch 86/100, Acc=0.9232, Val Loss=0.2847, lr=0.0001
[02/21 02:03:11 cifar10-global-fpgm-3.0-resnet56]: Epoch 87/100, Acc=0.9232, Val Loss=0.2883, lr=0.0001
[02/21 02:03:44 cifar10-global-fpgm-3.0-resnet56]: Epoch 88/100, Acc=0.9235, Val Loss=0.2845, lr=0.0001
[02/21 02:04:16 cifar10-global-fpgm-3.0-resnet56]: Epoch 89/100, Acc=0.9241, Val Loss=0.2858, lr=0.0001
[02/21 02:04:50 cifar10-global-fpgm-3.0-resnet56]: Epoch 90/100, Acc=0.9227, Val Loss=0.2880, lr=0.0001
[02/21 02:05:23 cifar10-global-fpgm-3.0-resnet56]: Epoch 91/100, Acc=0.9237, Val Loss=0.2863, lr=0.0001
[02/21 02:05:56 cifar10-global-fpgm-3.0-resnet56]: Epoch 92/100, Acc=0.9223, Val Loss=0.2886, lr=0.0001
[02/21 02:06:28 cifar10-global-fpgm-3.0-resnet56]: Epoch 93/100, Acc=0.9228, Val Loss=0.2875, lr=0.0001
[02/21 02:07:01 cifar10-global-fpgm-3.0-resnet56]: Epoch 94/100, Acc=0.9233, Val Loss=0.2886, lr=0.0001
[02/21 02:07:34 cifar10-global-fpgm-3.0-resnet56]: Epoch 95/100, Acc=0.9218, Val Loss=0.2909, lr=0.0001
[02/21 02:08:07 cifar10-global-fpgm-3.0-resnet56]: Epoch 96/100, Acc=0.9230, Val Loss=0.2895, lr=0.0001
[02/21 02:08:40 cifar10-global-fpgm-3.0-resnet56]: Epoch 97/100, Acc=0.9228, Val Loss=0.2892, lr=0.0001
[02/21 02:09:13 cifar10-global-fpgm-3.0-resnet56]: Epoch 98/100, Acc=0.9239, Val Loss=0.2903, lr=0.0001
[02/21 02:09:46 cifar10-global-fpgm-3.0-resnet56]: Epoch 99/100, Acc=0.9227, Val Loss=0.2912, lr=0.0001
[02/21 02:09:46 cifar10-global-fpgm-3.0-resnet56]: Best Acc=0.9241
[02/21 02:09:46 cifar10-global-fpgm-3.0-resnet56]: Params: 0.28 M
[02/21 02:09:46 cifar10-global-fpgm-3.0-resnet56]: ops: 42.23 M
[02/21 02:09:49 cifar10-global-fpgm-3.0-resnet56]: Acc: 0.9227 Val Loss: 0.2912

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: mode: prune
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: model: resnet56
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: verbose: False
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: dataset: cifar10
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: dataroot: data
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: batch_size: 128
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: total_epochs: 100
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: lr_decay_milestones: 60,80
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: lr_decay_gamma: 0.1
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: lr: 0.01
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: output_dir: run/cifar10/prune/cifar10-global-obdc-3.0-resnet56
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: finetune: True
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: last_epochs: 100
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: reps: 1
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: method: obdc
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: speed_up: 3.0
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: max_pruning_ratio: 1.0
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: reg: 1e-05
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: delta_reg: 0.0001
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: weight_decay: 0.0005
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: seed: 1
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: global_pruning: True
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: sl_total_epochs: 100
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: sl_lr: 0.01
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: sl_reg_warmup: 0
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: sl_restore: None
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: iterative_steps: 400
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: logger: <Logger cifar10-global-obdc-3.0-resnet56 (DEBUG)>
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: device: cuda
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: num_classes: 10
[02/21 02:09:57 cifar10-global-obdc-3.0-resnet56]: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 02:10:00 cifar10-global-obdc-3.0-resnet56]: Pruning...
Traceback (most recent call last):
  File "/home/REDACTED/paper1/main.py", line 429, in <module>
    main()
  File "/home/REDACTED/paper1/main.py", line 369, in main
    progressive_pruning(
  File "/home/REDACTED/paper1/main.py", line 65, in progressive_pruning
    imp._prepare_model(model, pruner)
  File "/home/REDACTED/paper1/torch_pruning/pruner/importance.py", line 872, in _prepare_model
    for i, (dep, idxs) in enumerate(group):
TypeError: 'NoneType' object is not iterable
/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: mode: prune
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: model: resnet56
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: verbose: False
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: dataset: cifar10
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: dataroot: data
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: batch_size: 128
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: total_epochs: 100
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: lr_decay_milestones: 60,80
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: lr_decay_gamma: 0.1
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: lr: 0.01
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: output_dir: run/cifar10/prune/cifar10-global-lamp-3.0-resnet56
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: finetune: True
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: last_epochs: 100
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: reps: 1
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: method: lamp
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: speed_up: 3.0
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: max_pruning_ratio: 1.0
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: reg: 1e-05
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: delta_reg: 0.0001
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: weight_decay: 0.0005
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: seed: 1
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: global_pruning: True
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: sl_total_epochs: 100
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: sl_lr: 0.01
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: sl_reg_warmup: 0
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: sl_restore: None
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: iterative_steps: 400
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: logger: <Logger cifar10-global-lamp-3.0-resnet56 (DEBUG)>
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: device: cuda
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: num_classes: 10
[02/21 02:10:06 cifar10-global-lamp-3.0-resnet56]: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 02:10:10 cifar10-global-lamp-3.0-resnet56]: Pruning...
[02/21 02:10:40 cifar10-global-lamp-3.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(15, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(15, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(15, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(15, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(15, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(15, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(15, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(15, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(15, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(15, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(15, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(16, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(16, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(16, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 18, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(18, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(18, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(18, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(18, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(18, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(18, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=18, out_features=10, bias=True)
)
[02/21 02:10:44 cifar10-global-lamp-3.0-resnet56]: Params: 0.86 M => 0.12 M (14.09%)
[02/21 02:10:44 cifar10-global-lamp-3.0-resnet56]: FLOPs: 127.12 M => 42.23 M (33.22%, 3.01X )
[02/21 02:10:44 cifar10-global-lamp-3.0-resnet56]: Acc: 0.9392 => 0.0895
[02/21 02:10:44 cifar10-global-lamp-3.0-resnet56]: Val Loss: 0.2587 => 2.3212
[02/21 02:10:44 cifar10-global-lamp-3.0-resnet56]: Finetuning...
[02/21 02:11:17 cifar10-global-lamp-3.0-resnet56]: Epoch 0/100, Acc=0.7406, Val Loss=0.7670, lr=0.0100
[02/21 02:11:52 cifar10-global-lamp-3.0-resnet56]: Epoch 1/100, Acc=0.7864, Val Loss=0.6425, lr=0.0100
[02/21 02:12:26 cifar10-global-lamp-3.0-resnet56]: Epoch 2/100, Acc=0.7995, Val Loss=0.6076, lr=0.0100
[02/21 02:13:00 cifar10-global-lamp-3.0-resnet56]: Epoch 3/100, Acc=0.8318, Val Loss=0.4941, lr=0.0100
[02/21 02:13:34 cifar10-global-lamp-3.0-resnet56]: Epoch 4/100, Acc=0.8159, Val Loss=0.5480, lr=0.0100
[02/21 02:14:09 cifar10-global-lamp-3.0-resnet56]: Epoch 5/100, Acc=0.8369, Val Loss=0.4845, lr=0.0100
[02/21 02:14:44 cifar10-global-lamp-3.0-resnet56]: Epoch 6/100, Acc=0.8234, Val Loss=0.5352, lr=0.0100
[02/21 02:15:19 cifar10-global-lamp-3.0-resnet56]: Epoch 7/100, Acc=0.8127, Val Loss=0.5855, lr=0.0100
[02/21 02:15:54 cifar10-global-lamp-3.0-resnet56]: Epoch 8/100, Acc=0.8421, Val Loss=0.4867, lr=0.0100
[02/21 02:16:28 cifar10-global-lamp-3.0-resnet56]: Epoch 9/100, Acc=0.8409, Val Loss=0.4923, lr=0.0100
[02/21 02:17:03 cifar10-global-lamp-3.0-resnet56]: Epoch 10/100, Acc=0.8513, Val Loss=0.4526, lr=0.0100
[02/21 02:17:37 cifar10-global-lamp-3.0-resnet56]: Epoch 11/100, Acc=0.8303, Val Loss=0.5201, lr=0.0100
[02/21 02:18:12 cifar10-global-lamp-3.0-resnet56]: Epoch 12/100, Acc=0.7781, Val Loss=0.7436, lr=0.0100
[02/21 02:18:47 cifar10-global-lamp-3.0-resnet56]: Epoch 13/100, Acc=0.8457, Val Loss=0.4896, lr=0.0100
[02/21 02:19:22 cifar10-global-lamp-3.0-resnet56]: Epoch 14/100, Acc=0.8552, Val Loss=0.4492, lr=0.0100
[02/21 02:19:56 cifar10-global-lamp-3.0-resnet56]: Epoch 15/100, Acc=0.8186, Val Loss=0.5513, lr=0.0100
[02/21 02:20:31 cifar10-global-lamp-3.0-resnet56]: Epoch 16/100, Acc=0.8591, Val Loss=0.4245, lr=0.0100
[02/21 02:21:06 cifar10-global-lamp-3.0-resnet56]: Epoch 17/100, Acc=0.8557, Val Loss=0.4323, lr=0.0100
[02/21 02:21:42 cifar10-global-lamp-3.0-resnet56]: Epoch 18/100, Acc=0.8634, Val Loss=0.4057, lr=0.0100
[02/21 02:22:17 cifar10-global-lamp-3.0-resnet56]: Epoch 19/100, Acc=0.8376, Val Loss=0.4817, lr=0.0100
[02/21 02:22:52 cifar10-global-lamp-3.0-resnet56]: Epoch 20/100, Acc=0.8499, Val Loss=0.4744, lr=0.0100
[02/21 02:23:26 cifar10-global-lamp-3.0-resnet56]: Epoch 21/100, Acc=0.8440, Val Loss=0.4837, lr=0.0100
[02/21 02:24:01 cifar10-global-lamp-3.0-resnet56]: Epoch 22/100, Acc=0.8123, Val Loss=0.6209, lr=0.0100
[02/21 02:24:35 cifar10-global-lamp-3.0-resnet56]: Epoch 23/100, Acc=0.8166, Val Loss=0.5867, lr=0.0100
[02/21 02:25:09 cifar10-global-lamp-3.0-resnet56]: Epoch 24/100, Acc=0.8053, Val Loss=0.6610, lr=0.0100
[02/21 02:25:44 cifar10-global-lamp-3.0-resnet56]: Epoch 25/100, Acc=0.8541, Val Loss=0.4721, lr=0.0100
[02/21 02:26:18 cifar10-global-lamp-3.0-resnet56]: Epoch 26/100, Acc=0.8567, Val Loss=0.4436, lr=0.0100
[02/21 02:26:53 cifar10-global-lamp-3.0-resnet56]: Epoch 27/100, Acc=0.8566, Val Loss=0.4388, lr=0.0100
[02/21 02:27:27 cifar10-global-lamp-3.0-resnet56]: Epoch 28/100, Acc=0.8482, Val Loss=0.4907, lr=0.0100
[02/21 02:28:01 cifar10-global-lamp-3.0-resnet56]: Epoch 29/100, Acc=0.8605, Val Loss=0.4466, lr=0.0100
[02/21 02:28:36 cifar10-global-lamp-3.0-resnet56]: Epoch 30/100, Acc=0.8643, Val Loss=0.4182, lr=0.0100
[02/21 02:29:10 cifar10-global-lamp-3.0-resnet56]: Epoch 31/100, Acc=0.8568, Val Loss=0.4534, lr=0.0100
[02/21 02:29:45 cifar10-global-lamp-3.0-resnet56]: Epoch 32/100, Acc=0.8554, Val Loss=0.4555, lr=0.0100
[02/21 02:30:20 cifar10-global-lamp-3.0-resnet56]: Epoch 33/100, Acc=0.8616, Val Loss=0.4356, lr=0.0100
[02/21 02:30:54 cifar10-global-lamp-3.0-resnet56]: Epoch 34/100, Acc=0.8764, Val Loss=0.3742, lr=0.0100
[02/21 02:31:29 cifar10-global-lamp-3.0-resnet56]: Epoch 35/100, Acc=0.8512, Val Loss=0.4777, lr=0.0100
[02/21 02:32:03 cifar10-global-lamp-3.0-resnet56]: Epoch 36/100, Acc=0.8751, Val Loss=0.3930, lr=0.0100
[02/21 02:32:37 cifar10-global-lamp-3.0-resnet56]: Epoch 37/100, Acc=0.8470, Val Loss=0.4997, lr=0.0100
[02/21 02:33:12 cifar10-global-lamp-3.0-resnet56]: Epoch 38/100, Acc=0.8802, Val Loss=0.3712, lr=0.0100
[02/21 02:33:46 cifar10-global-lamp-3.0-resnet56]: Epoch 39/100, Acc=0.8599, Val Loss=0.4437, lr=0.0100
[02/21 02:34:21 cifar10-global-lamp-3.0-resnet56]: Epoch 40/100, Acc=0.8633, Val Loss=0.4253, lr=0.0100
[02/21 02:34:55 cifar10-global-lamp-3.0-resnet56]: Epoch 41/100, Acc=0.8625, Val Loss=0.4328, lr=0.0100
[02/21 02:35:30 cifar10-global-lamp-3.0-resnet56]: Epoch 42/100, Acc=0.8698, Val Loss=0.4065, lr=0.0100
[02/21 02:36:04 cifar10-global-lamp-3.0-resnet56]: Epoch 43/100, Acc=0.8484, Val Loss=0.4778, lr=0.0100
[02/21 02:36:39 cifar10-global-lamp-3.0-resnet56]: Epoch 44/100, Acc=0.8770, Val Loss=0.3650, lr=0.0100
[02/21 02:37:14 cifar10-global-lamp-3.0-resnet56]: Epoch 45/100, Acc=0.8630, Val Loss=0.4256, lr=0.0100
[02/21 02:37:48 cifar10-global-lamp-3.0-resnet56]: Epoch 46/100, Acc=0.8585, Val Loss=0.4393, lr=0.0100
[02/21 02:38:23 cifar10-global-lamp-3.0-resnet56]: Epoch 47/100, Acc=0.8757, Val Loss=0.4013, lr=0.0100
[02/21 02:38:58 cifar10-global-lamp-3.0-resnet56]: Epoch 48/100, Acc=0.8767, Val Loss=0.3794, lr=0.0100
[02/21 02:39:32 cifar10-global-lamp-3.0-resnet56]: Epoch 49/100, Acc=0.8605, Val Loss=0.4302, lr=0.0100
[02/21 02:40:07 cifar10-global-lamp-3.0-resnet56]: Epoch 50/100, Acc=0.8676, Val Loss=0.4147, lr=0.0100
[02/21 02:40:41 cifar10-global-lamp-3.0-resnet56]: Epoch 51/100, Acc=0.8669, Val Loss=0.4044, lr=0.0100
[02/21 02:41:16 cifar10-global-lamp-3.0-resnet56]: Epoch 52/100, Acc=0.8727, Val Loss=0.3859, lr=0.0100
[02/21 02:41:50 cifar10-global-lamp-3.0-resnet56]: Epoch 53/100, Acc=0.8765, Val Loss=0.3823, lr=0.0100
[02/21 02:42:24 cifar10-global-lamp-3.0-resnet56]: Epoch 54/100, Acc=0.8468, Val Loss=0.4945, lr=0.0100
[02/21 02:42:59 cifar10-global-lamp-3.0-resnet56]: Epoch 55/100, Acc=0.8767, Val Loss=0.3800, lr=0.0100
[02/21 02:43:34 cifar10-global-lamp-3.0-resnet56]: Epoch 56/100, Acc=0.8492, Val Loss=0.4746, lr=0.0100
[02/21 02:44:08 cifar10-global-lamp-3.0-resnet56]: Epoch 57/100, Acc=0.8618, Val Loss=0.4436, lr=0.0100
[02/21 02:44:42 cifar10-global-lamp-3.0-resnet56]: Epoch 58/100, Acc=0.8638, Val Loss=0.4448, lr=0.0100
[02/21 02:45:17 cifar10-global-lamp-3.0-resnet56]: Epoch 59/100, Acc=0.8607, Val Loss=0.4697, lr=0.0100
[02/21 02:45:52 cifar10-global-lamp-3.0-resnet56]: Epoch 60/100, Acc=0.9049, Val Loss=0.2874, lr=0.0010
[02/21 02:46:27 cifar10-global-lamp-3.0-resnet56]: Epoch 61/100, Acc=0.9101, Val Loss=0.2839, lr=0.0010
[02/21 02:47:02 cifar10-global-lamp-3.0-resnet56]: Epoch 62/100, Acc=0.9085, Val Loss=0.2853, lr=0.0010
[02/21 02:47:37 cifar10-global-lamp-3.0-resnet56]: Epoch 63/100, Acc=0.9098, Val Loss=0.2834, lr=0.0010
[02/21 02:48:12 cifar10-global-lamp-3.0-resnet56]: Epoch 64/100, Acc=0.9104, Val Loss=0.2832, lr=0.0010
[02/21 02:48:47 cifar10-global-lamp-3.0-resnet56]: Epoch 65/100, Acc=0.9093, Val Loss=0.2854, lr=0.0010
[02/21 02:49:22 cifar10-global-lamp-3.0-resnet56]: Epoch 66/100, Acc=0.9080, Val Loss=0.2889, lr=0.0010
[02/21 02:49:57 cifar10-global-lamp-3.0-resnet56]: Epoch 67/100, Acc=0.9088, Val Loss=0.2877, lr=0.0010
[02/21 02:50:32 cifar10-global-lamp-3.0-resnet56]: Epoch 68/100, Acc=0.9086, Val Loss=0.2909, lr=0.0010
[02/21 02:51:07 cifar10-global-lamp-3.0-resnet56]: Epoch 69/100, Acc=0.9133, Val Loss=0.2853, lr=0.0010
[02/21 02:51:41 cifar10-global-lamp-3.0-resnet56]: Epoch 70/100, Acc=0.9103, Val Loss=0.2932, lr=0.0010
[02/21 02:52:16 cifar10-global-lamp-3.0-resnet56]: Epoch 71/100, Acc=0.9097, Val Loss=0.2893, lr=0.0010
[02/21 02:52:51 cifar10-global-lamp-3.0-resnet56]: Epoch 72/100, Acc=0.9109, Val Loss=0.2910, lr=0.0010
[02/21 02:53:26 cifar10-global-lamp-3.0-resnet56]: Epoch 73/100, Acc=0.9095, Val Loss=0.2934, lr=0.0010
[02/21 02:54:00 cifar10-global-lamp-3.0-resnet56]: Epoch 74/100, Acc=0.9104, Val Loss=0.2967, lr=0.0010
[02/21 02:54:35 cifar10-global-lamp-3.0-resnet56]: Epoch 75/100, Acc=0.9097, Val Loss=0.2962, lr=0.0010
[02/21 02:55:10 cifar10-global-lamp-3.0-resnet56]: Epoch 76/100, Acc=0.9098, Val Loss=0.2978, lr=0.0010
[02/21 02:55:45 cifar10-global-lamp-3.0-resnet56]: Epoch 77/100, Acc=0.9080, Val Loss=0.3011, lr=0.0010
[02/21 02:56:19 cifar10-global-lamp-3.0-resnet56]: Epoch 78/100, Acc=0.9078, Val Loss=0.2985, lr=0.0010
[02/21 02:56:54 cifar10-global-lamp-3.0-resnet56]: Epoch 79/100, Acc=0.9105, Val Loss=0.3012, lr=0.0010
[02/21 02:57:28 cifar10-global-lamp-3.0-resnet56]: Epoch 80/100, Acc=0.9103, Val Loss=0.2987, lr=0.0001
[02/21 02:58:01 cifar10-global-lamp-3.0-resnet56]: Epoch 81/100, Acc=0.9114, Val Loss=0.2949, lr=0.0001
[02/21 02:58:35 cifar10-global-lamp-3.0-resnet56]: Epoch 82/100, Acc=0.9111, Val Loss=0.2961, lr=0.0001
[02/21 02:59:09 cifar10-global-lamp-3.0-resnet56]: Epoch 83/100, Acc=0.9130, Val Loss=0.2951, lr=0.0001
[02/21 02:59:43 cifar10-global-lamp-3.0-resnet56]: Epoch 84/100, Acc=0.9114, Val Loss=0.2957, lr=0.0001
[02/21 03:00:17 cifar10-global-lamp-3.0-resnet56]: Epoch 85/100, Acc=0.9108, Val Loss=0.2957, lr=0.0001
[02/21 03:00:52 cifar10-global-lamp-3.0-resnet56]: Epoch 86/100, Acc=0.9114, Val Loss=0.2950, lr=0.0001
[02/21 03:01:26 cifar10-global-lamp-3.0-resnet56]: Epoch 87/100, Acc=0.9111, Val Loss=0.2962, lr=0.0001
[02/21 03:02:00 cifar10-global-lamp-3.0-resnet56]: Epoch 88/100, Acc=0.9124, Val Loss=0.2957, lr=0.0001
[02/21 03:02:34 cifar10-global-lamp-3.0-resnet56]: Epoch 89/100, Acc=0.9122, Val Loss=0.2959, lr=0.0001
[02/21 03:03:08 cifar10-global-lamp-3.0-resnet56]: Epoch 90/100, Acc=0.9117, Val Loss=0.2955, lr=0.0001
[02/21 03:03:42 cifar10-global-lamp-3.0-resnet56]: Epoch 91/100, Acc=0.9108, Val Loss=0.2950, lr=0.0001
[02/21 03:04:17 cifar10-global-lamp-3.0-resnet56]: Epoch 92/100, Acc=0.9121, Val Loss=0.2965, lr=0.0001
[02/21 03:04:51 cifar10-global-lamp-3.0-resnet56]: Epoch 93/100, Acc=0.9118, Val Loss=0.2970, lr=0.0001
[02/21 03:05:25 cifar10-global-lamp-3.0-resnet56]: Epoch 94/100, Acc=0.9111, Val Loss=0.2993, lr=0.0001
[02/21 03:05:59 cifar10-global-lamp-3.0-resnet56]: Epoch 95/100, Acc=0.9111, Val Loss=0.2971, lr=0.0001
[02/21 03:06:33 cifar10-global-lamp-3.0-resnet56]: Epoch 96/100, Acc=0.9123, Val Loss=0.2967, lr=0.0001
[02/21 03:07:08 cifar10-global-lamp-3.0-resnet56]: Epoch 97/100, Acc=0.9117, Val Loss=0.2991, lr=0.0001
[02/21 03:07:42 cifar10-global-lamp-3.0-resnet56]: Epoch 98/100, Acc=0.9113, Val Loss=0.2990, lr=0.0001
[02/21 03:08:17 cifar10-global-lamp-3.0-resnet56]: Epoch 99/100, Acc=0.9114, Val Loss=0.2983, lr=0.0001
[02/21 03:08:17 cifar10-global-lamp-3.0-resnet56]: Best Acc=0.9133
[02/21 03:08:17 cifar10-global-lamp-3.0-resnet56]: Params: 0.12 M
[02/21 03:08:17 cifar10-global-lamp-3.0-resnet56]: ops: 42.23 M
[02/21 03:08:21 cifar10-global-lamp-3.0-resnet56]: Acc: 0.9114 Val Loss: 0.2983

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: mode: prune
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: model: resnet56
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: verbose: False
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: dataset: cifar10
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: dataroot: data
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: batch_size: 128
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: total_epochs: 100
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: lr_decay_milestones: 60,80
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: lr_decay_gamma: 0.1
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: lr: 0.01
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: output_dir: run/cifar10/prune/cifar10-global-slim-3.0-resnet56
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: finetune: True
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: last_epochs: 100
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: reps: 1
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: method: slim
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: speed_up: 3.0
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: max_pruning_ratio: 1.0
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: reg: 1e-05
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: delta_reg: 0.0001
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: weight_decay: 0.0005
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: seed: 1
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: global_pruning: True
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: sl_total_epochs: 100
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: sl_lr: 0.01
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: sl_reg_warmup: 0
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: sl_restore: None
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: iterative_steps: 400
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: logger: <Logger cifar10-global-slim-3.0-resnet56 (DEBUG)>
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: device: cuda
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: num_classes: 10
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 03:08:28 cifar10-global-slim-3.0-resnet56]: Regularizing...
[02/21 03:09:04 cifar10-global-slim-3.0-resnet56]: Epoch 0/100, Acc=0.9126, Val Loss=0.3228, lr=0.0100
[02/21 03:09:39 cifar10-global-slim-3.0-resnet56]: Epoch 1/100, Acc=0.9060, Val Loss=0.3291, lr=0.0100
[02/21 03:10:15 cifar10-global-slim-3.0-resnet56]: Epoch 2/100, Acc=0.9076, Val Loss=0.3395, lr=0.0100
[02/21 03:10:51 cifar10-global-slim-3.0-resnet56]: Epoch 3/100, Acc=0.9116, Val Loss=0.3198, lr=0.0100
[02/21 03:11:28 cifar10-global-slim-3.0-resnet56]: Epoch 4/100, Acc=0.9218, Val Loss=0.2937, lr=0.0100
[02/21 03:12:04 cifar10-global-slim-3.0-resnet56]: Epoch 5/100, Acc=0.9172, Val Loss=0.3193, lr=0.0100
[02/21 03:12:39 cifar10-global-slim-3.0-resnet56]: Epoch 6/100, Acc=0.9063, Val Loss=0.3521, lr=0.0100
[02/21 03:13:15 cifar10-global-slim-3.0-resnet56]: Epoch 7/100, Acc=0.9129, Val Loss=0.3420, lr=0.0100
[02/21 03:13:51 cifar10-global-slim-3.0-resnet56]: Epoch 8/100, Acc=0.9207, Val Loss=0.2947, lr=0.0100
[02/21 03:14:27 cifar10-global-slim-3.0-resnet56]: Epoch 9/100, Acc=0.9110, Val Loss=0.3530, lr=0.0100
[02/21 03:15:02 cifar10-global-slim-3.0-resnet56]: Epoch 10/100, Acc=0.9170, Val Loss=0.3205, lr=0.0100
[02/21 03:15:38 cifar10-global-slim-3.0-resnet56]: Epoch 11/100, Acc=0.9187, Val Loss=0.3191, lr=0.0100
[02/21 03:16:14 cifar10-global-slim-3.0-resnet56]: Epoch 12/100, Acc=0.9212, Val Loss=0.3187, lr=0.0100
[02/21 03:16:49 cifar10-global-slim-3.0-resnet56]: Epoch 13/100, Acc=0.9198, Val Loss=0.3091, lr=0.0100
[02/21 03:17:24 cifar10-global-slim-3.0-resnet56]: Epoch 14/100, Acc=0.9176, Val Loss=0.3328, lr=0.0100
[02/21 03:18:00 cifar10-global-slim-3.0-resnet56]: Epoch 15/100, Acc=0.9170, Val Loss=0.3271, lr=0.0100
[02/21 03:18:35 cifar10-global-slim-3.0-resnet56]: Epoch 16/100, Acc=0.9245, Val Loss=0.2920, lr=0.0100
[02/21 03:19:11 cifar10-global-slim-3.0-resnet56]: Epoch 17/100, Acc=0.9164, Val Loss=0.3519, lr=0.0100
[02/21 03:19:46 cifar10-global-slim-3.0-resnet56]: Epoch 18/100, Acc=0.9220, Val Loss=0.3474, lr=0.0100
[02/21 03:20:21 cifar10-global-slim-3.0-resnet56]: Epoch 19/100, Acc=0.9086, Val Loss=0.3869, lr=0.0100
[02/21 03:20:57 cifar10-global-slim-3.0-resnet56]: Epoch 20/100, Acc=0.9243, Val Loss=0.3252, lr=0.0100
[02/21 03:21:33 cifar10-global-slim-3.0-resnet56]: Epoch 21/100, Acc=0.9208, Val Loss=0.3396, lr=0.0100
[02/21 03:22:09 cifar10-global-slim-3.0-resnet56]: Epoch 22/100, Acc=0.9247, Val Loss=0.3167, lr=0.0100
[02/21 03:22:45 cifar10-global-slim-3.0-resnet56]: Epoch 23/100, Acc=0.9227, Val Loss=0.3350, lr=0.0100
[02/21 03:23:21 cifar10-global-slim-3.0-resnet56]: Epoch 24/100, Acc=0.9217, Val Loss=0.3372, lr=0.0100
[02/21 03:23:56 cifar10-global-slim-3.0-resnet56]: Epoch 25/100, Acc=0.9233, Val Loss=0.3524, lr=0.0100
[02/21 03:24:32 cifar10-global-slim-3.0-resnet56]: Epoch 26/100, Acc=0.9225, Val Loss=0.3323, lr=0.0100
[02/21 03:25:08 cifar10-global-slim-3.0-resnet56]: Epoch 27/100, Acc=0.9204, Val Loss=0.3282, lr=0.0100
[02/21 03:25:43 cifar10-global-slim-3.0-resnet56]: Epoch 28/100, Acc=0.9207, Val Loss=0.3556, lr=0.0100
[02/21 03:26:19 cifar10-global-slim-3.0-resnet56]: Epoch 29/100, Acc=0.9254, Val Loss=0.3233, lr=0.0100
[02/21 03:26:55 cifar10-global-slim-3.0-resnet56]: Epoch 30/100, Acc=0.9260, Val Loss=0.3390, lr=0.0100
[02/21 03:27:31 cifar10-global-slim-3.0-resnet56]: Epoch 31/100, Acc=0.9250, Val Loss=0.3361, lr=0.0100
[02/21 03:28:07 cifar10-global-slim-3.0-resnet56]: Epoch 32/100, Acc=0.9201, Val Loss=0.3710, lr=0.0100
[02/21 03:28:42 cifar10-global-slim-3.0-resnet56]: Epoch 33/100, Acc=0.9237, Val Loss=0.3624, lr=0.0100
[02/21 03:29:18 cifar10-global-slim-3.0-resnet56]: Epoch 34/100, Acc=0.9233, Val Loss=0.3431, lr=0.0100
[02/21 03:29:53 cifar10-global-slim-3.0-resnet56]: Epoch 35/100, Acc=0.9259, Val Loss=0.3364, lr=0.0100
[02/21 03:30:29 cifar10-global-slim-3.0-resnet56]: Epoch 36/100, Acc=0.9308, Val Loss=0.3270, lr=0.0100
[02/21 03:31:05 cifar10-global-slim-3.0-resnet56]: Epoch 37/100, Acc=0.9266, Val Loss=0.3335, lr=0.0100
[02/21 03:31:40 cifar10-global-slim-3.0-resnet56]: Epoch 38/100, Acc=0.9245, Val Loss=0.3597, lr=0.0100
[02/21 03:32:15 cifar10-global-slim-3.0-resnet56]: Epoch 39/100, Acc=0.9271, Val Loss=0.3495, lr=0.0100
[02/21 03:32:51 cifar10-global-slim-3.0-resnet56]: Epoch 40/100, Acc=0.9278, Val Loss=0.3268, lr=0.0100
[02/21 03:33:27 cifar10-global-slim-3.0-resnet56]: Epoch 41/100, Acc=0.9253, Val Loss=0.3414, lr=0.0100
[02/21 03:34:03 cifar10-global-slim-3.0-resnet56]: Epoch 42/100, Acc=0.9291, Val Loss=0.3349, lr=0.0100
[02/21 03:34:38 cifar10-global-slim-3.0-resnet56]: Epoch 43/100, Acc=0.9286, Val Loss=0.3284, lr=0.0100
[02/21 03:35:15 cifar10-global-slim-3.0-resnet56]: Epoch 44/100, Acc=0.9308, Val Loss=0.3308, lr=0.0100
[02/21 03:35:51 cifar10-global-slim-3.0-resnet56]: Epoch 45/100, Acc=0.9278, Val Loss=0.3449, lr=0.0100
[02/21 03:36:27 cifar10-global-slim-3.0-resnet56]: Epoch 46/100, Acc=0.9304, Val Loss=0.3347, lr=0.0100
[02/21 03:37:02 cifar10-global-slim-3.0-resnet56]: Epoch 47/100, Acc=0.9255, Val Loss=0.3707, lr=0.0100
[02/21 03:37:39 cifar10-global-slim-3.0-resnet56]: Epoch 48/100, Acc=0.9279, Val Loss=0.3519, lr=0.0100
[02/21 03:38:14 cifar10-global-slim-3.0-resnet56]: Epoch 49/100, Acc=0.9269, Val Loss=0.3515, lr=0.0100
[02/21 03:38:50 cifar10-global-slim-3.0-resnet56]: Epoch 50/100, Acc=0.9315, Val Loss=0.3393, lr=0.0100
[02/21 03:39:26 cifar10-global-slim-3.0-resnet56]: Epoch 51/100, Acc=0.9288, Val Loss=0.3561, lr=0.0100
[02/21 03:40:01 cifar10-global-slim-3.0-resnet56]: Epoch 52/100, Acc=0.9285, Val Loss=0.3501, lr=0.0100
[02/21 03:40:37 cifar10-global-slim-3.0-resnet56]: Epoch 53/100, Acc=0.9255, Val Loss=0.3692, lr=0.0100
[02/21 03:41:12 cifar10-global-slim-3.0-resnet56]: Epoch 54/100, Acc=0.9273, Val Loss=0.3630, lr=0.0100
[02/21 03:41:47 cifar10-global-slim-3.0-resnet56]: Epoch 55/100, Acc=0.9240, Val Loss=0.3642, lr=0.0100
[02/21 03:42:22 cifar10-global-slim-3.0-resnet56]: Epoch 56/100, Acc=0.9267, Val Loss=0.3645, lr=0.0100
[02/21 03:42:57 cifar10-global-slim-3.0-resnet56]: Epoch 57/100, Acc=0.9246, Val Loss=0.3884, lr=0.0100
[02/21 03:43:32 cifar10-global-slim-3.0-resnet56]: Epoch 58/100, Acc=0.9241, Val Loss=0.3683, lr=0.0100
[02/21 03:44:08 cifar10-global-slim-3.0-resnet56]: Epoch 59/100, Acc=0.9272, Val Loss=0.3529, lr=0.0100
[02/21 03:44:43 cifar10-global-slim-3.0-resnet56]: Epoch 60/100, Acc=0.9340, Val Loss=0.3220, lr=0.0010
[02/21 03:45:19 cifar10-global-slim-3.0-resnet56]: Epoch 61/100, Acc=0.9360, Val Loss=0.3110, lr=0.0010
[02/21 03:45:55 cifar10-global-slim-3.0-resnet56]: Epoch 62/100, Acc=0.9378, Val Loss=0.3110, lr=0.0010
[02/21 03:46:32 cifar10-global-slim-3.0-resnet56]: Epoch 63/100, Acc=0.9380, Val Loss=0.3117, lr=0.0010
[02/21 03:47:08 cifar10-global-slim-3.0-resnet56]: Epoch 64/100, Acc=0.9386, Val Loss=0.3158, lr=0.0010
[02/21 03:47:43 cifar10-global-slim-3.0-resnet56]: Epoch 65/100, Acc=0.9401, Val Loss=0.3146, lr=0.0010
[02/21 03:48:19 cifar10-global-slim-3.0-resnet56]: Epoch 66/100, Acc=0.9396, Val Loss=0.3178, lr=0.0010
[02/21 03:48:55 cifar10-global-slim-3.0-resnet56]: Epoch 67/100, Acc=0.9391, Val Loss=0.3130, lr=0.0010
[02/21 03:49:31 cifar10-global-slim-3.0-resnet56]: Epoch 68/100, Acc=0.9408, Val Loss=0.3151, lr=0.0010
[02/21 03:50:07 cifar10-global-slim-3.0-resnet56]: Epoch 69/100, Acc=0.9396, Val Loss=0.3123, lr=0.0010
[02/21 03:50:42 cifar10-global-slim-3.0-resnet56]: Epoch 70/100, Acc=0.9401, Val Loss=0.3135, lr=0.0010
[02/21 03:51:18 cifar10-global-slim-3.0-resnet56]: Epoch 71/100, Acc=0.9394, Val Loss=0.3173, lr=0.0010
[02/21 03:51:55 cifar10-global-slim-3.0-resnet56]: Epoch 72/100, Acc=0.9396, Val Loss=0.3162, lr=0.0010
[02/21 03:52:30 cifar10-global-slim-3.0-resnet56]: Epoch 73/100, Acc=0.9393, Val Loss=0.3161, lr=0.0010
[02/21 03:53:06 cifar10-global-slim-3.0-resnet56]: Epoch 74/100, Acc=0.9394, Val Loss=0.3131, lr=0.0010
[02/21 03:53:42 cifar10-global-slim-3.0-resnet56]: Epoch 75/100, Acc=0.9400, Val Loss=0.3147, lr=0.0010
[02/21 03:54:18 cifar10-global-slim-3.0-resnet56]: Epoch 76/100, Acc=0.9407, Val Loss=0.3152, lr=0.0010
[02/21 03:54:53 cifar10-global-slim-3.0-resnet56]: Epoch 77/100, Acc=0.9402, Val Loss=0.3179, lr=0.0010
[02/21 03:55:29 cifar10-global-slim-3.0-resnet56]: Epoch 78/100, Acc=0.9397, Val Loss=0.3169, lr=0.0010
[02/21 03:56:04 cifar10-global-slim-3.0-resnet56]: Epoch 79/100, Acc=0.9398, Val Loss=0.3203, lr=0.0010
[02/21 03:56:40 cifar10-global-slim-3.0-resnet56]: Epoch 80/100, Acc=0.9390, Val Loss=0.3219, lr=0.0001
[02/21 03:57:15 cifar10-global-slim-3.0-resnet56]: Epoch 81/100, Acc=0.9396, Val Loss=0.3246, lr=0.0001
[02/21 03:57:51 cifar10-global-slim-3.0-resnet56]: Epoch 82/100, Acc=0.9407, Val Loss=0.3204, lr=0.0001
[02/21 03:58:27 cifar10-global-slim-3.0-resnet56]: Epoch 83/100, Acc=0.9392, Val Loss=0.3197, lr=0.0001
[02/21 03:59:03 cifar10-global-slim-3.0-resnet56]: Epoch 84/100, Acc=0.9399, Val Loss=0.3207, lr=0.0001
[02/21 03:59:39 cifar10-global-slim-3.0-resnet56]: Epoch 85/100, Acc=0.9400, Val Loss=0.3215, lr=0.0001
[02/21 04:00:15 cifar10-global-slim-3.0-resnet56]: Epoch 86/100, Acc=0.9401, Val Loss=0.3189, lr=0.0001
[02/21 04:00:50 cifar10-global-slim-3.0-resnet56]: Epoch 87/100, Acc=0.9397, Val Loss=0.3208, lr=0.0001
[02/21 04:01:26 cifar10-global-slim-3.0-resnet56]: Epoch 88/100, Acc=0.9400, Val Loss=0.3195, lr=0.0001
[02/21 04:02:02 cifar10-global-slim-3.0-resnet56]: Epoch 89/100, Acc=0.9399, Val Loss=0.3230, lr=0.0001
[02/21 04:02:37 cifar10-global-slim-3.0-resnet56]: Epoch 90/100, Acc=0.9402, Val Loss=0.3223, lr=0.0001
[02/21 04:03:12 cifar10-global-slim-3.0-resnet56]: Epoch 91/100, Acc=0.9399, Val Loss=0.3209, lr=0.0001
[02/21 04:03:48 cifar10-global-slim-3.0-resnet56]: Epoch 92/100, Acc=0.9402, Val Loss=0.3189, lr=0.0001
[02/21 04:04:24 cifar10-global-slim-3.0-resnet56]: Epoch 93/100, Acc=0.9398, Val Loss=0.3166, lr=0.0001
[02/21 04:04:59 cifar10-global-slim-3.0-resnet56]: Epoch 94/100, Acc=0.9400, Val Loss=0.3189, lr=0.0001
[02/21 04:05:34 cifar10-global-slim-3.0-resnet56]: Epoch 95/100, Acc=0.9399, Val Loss=0.3201, lr=0.0001
[02/21 04:06:09 cifar10-global-slim-3.0-resnet56]: Epoch 96/100, Acc=0.9396, Val Loss=0.3183, lr=0.0001
[02/21 04:06:44 cifar10-global-slim-3.0-resnet56]: Epoch 97/100, Acc=0.9402, Val Loss=0.3205, lr=0.0001
[02/21 04:07:20 cifar10-global-slim-3.0-resnet56]: Epoch 98/100, Acc=0.9400, Val Loss=0.3187, lr=0.0001
[02/21 04:07:56 cifar10-global-slim-3.0-resnet56]: Epoch 99/100, Acc=0.9400, Val Loss=0.3212, lr=0.0001
[02/21 04:07:56 cifar10-global-slim-3.0-resnet56]: Best Acc=0.9408
[02/21 04:07:56 cifar10-global-slim-3.0-resnet56]: Loading the sparse model from run/cifar10/prune/cifar10-global-slim-3.0-resnet56/reg_cifar10_resnet56_slim_1e-05.pth...
[02/21 04:07:59 cifar10-global-slim-3.0-resnet56]: Pruning...
[02/21 04:08:14 cifar10-global-slim-3.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(7, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(7, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(7, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(7, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(7, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(7, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(7, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(7, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(7, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(7, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(7, 25, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(25, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(21, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(25, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(25, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(26, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(25, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(25, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(25, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(25, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(25, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(25, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(25, 35, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(35, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(49, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(35, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(54, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(35, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(35, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(35, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(53, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(35, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(40, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(35, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(42, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(35, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(48, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=35, out_features=10, bias=True)
)
[02/21 04:08:18 cifar10-global-slim-3.0-resnet56]: Params: 0.86 M => 0.36 M (41.59%)
[02/21 04:08:18 cifar10-global-slim-3.0-resnet56]: FLOPs: 127.12 M => 42.31 M (33.29%, 3.00X )
[02/21 04:08:18 cifar10-global-slim-3.0-resnet56]: Acc: 0.9408 => 0.0990
[02/21 04:08:18 cifar10-global-slim-3.0-resnet56]: Val Loss: 0.3151 => 2.9671
[02/21 04:08:18 cifar10-global-slim-3.0-resnet56]: Finetuning...
[02/21 04:08:49 cifar10-global-slim-3.0-resnet56]: Epoch 0/100, Acc=0.7665, Val Loss=0.6856, lr=0.0100
[02/21 04:09:22 cifar10-global-slim-3.0-resnet56]: Epoch 1/100, Acc=0.8237, Val Loss=0.5314, lr=0.0100
[02/21 04:09:54 cifar10-global-slim-3.0-resnet56]: Epoch 2/100, Acc=0.8234, Val Loss=0.5438, lr=0.0100
[02/21 04:10:27 cifar10-global-slim-3.0-resnet56]: Epoch 3/100, Acc=0.8463, Val Loss=0.4753, lr=0.0100
[02/21 04:11:00 cifar10-global-slim-3.0-resnet56]: Epoch 4/100, Acc=0.8533, Val Loss=0.4526, lr=0.0100
[02/21 04:11:32 cifar10-global-slim-3.0-resnet56]: Epoch 5/100, Acc=0.8670, Val Loss=0.4006, lr=0.0100
[02/21 04:12:04 cifar10-global-slim-3.0-resnet56]: Epoch 6/100, Acc=0.8541, Val Loss=0.4511, lr=0.0100
[02/21 04:12:37 cifar10-global-slim-3.0-resnet56]: Epoch 7/100, Acc=0.8733, Val Loss=0.3891, lr=0.0100
[02/21 04:13:11 cifar10-global-slim-3.0-resnet56]: Epoch 8/100, Acc=0.8460, Val Loss=0.4892, lr=0.0100
[02/21 04:13:44 cifar10-global-slim-3.0-resnet56]: Epoch 9/100, Acc=0.8413, Val Loss=0.4952, lr=0.0100
[02/21 04:14:16 cifar10-global-slim-3.0-resnet56]: Epoch 10/100, Acc=0.8616, Val Loss=0.4320, lr=0.0100
[02/21 04:14:49 cifar10-global-slim-3.0-resnet56]: Epoch 11/100, Acc=0.8602, Val Loss=0.4326, lr=0.0100
[02/21 04:15:21 cifar10-global-slim-3.0-resnet56]: Epoch 12/100, Acc=0.8610, Val Loss=0.4558, lr=0.0100
[02/21 04:15:53 cifar10-global-slim-3.0-resnet56]: Epoch 13/100, Acc=0.8728, Val Loss=0.3917, lr=0.0100
[02/21 04:16:25 cifar10-global-slim-3.0-resnet56]: Epoch 14/100, Acc=0.8469, Val Loss=0.4819, lr=0.0100
[02/21 04:16:58 cifar10-global-slim-3.0-resnet56]: Epoch 15/100, Acc=0.8654, Val Loss=0.4473, lr=0.0100
[02/21 04:17:31 cifar10-global-slim-3.0-resnet56]: Epoch 16/100, Acc=0.8569, Val Loss=0.4732, lr=0.0100
[02/21 04:18:03 cifar10-global-slim-3.0-resnet56]: Epoch 17/100, Acc=0.8703, Val Loss=0.4061, lr=0.0100
[02/21 04:18:36 cifar10-global-slim-3.0-resnet56]: Epoch 18/100, Acc=0.8657, Val Loss=0.4355, lr=0.0100
[02/21 04:19:08 cifar10-global-slim-3.0-resnet56]: Epoch 19/100, Acc=0.8611, Val Loss=0.4599, lr=0.0100
[02/21 04:19:40 cifar10-global-slim-3.0-resnet56]: Epoch 20/100, Acc=0.8417, Val Loss=0.5358, lr=0.0100
[02/21 04:20:13 cifar10-global-slim-3.0-resnet56]: Epoch 21/100, Acc=0.8856, Val Loss=0.3643, lr=0.0100
[02/21 04:20:45 cifar10-global-slim-3.0-resnet56]: Epoch 22/100, Acc=0.8792, Val Loss=0.3838, lr=0.0100
[02/21 04:21:18 cifar10-global-slim-3.0-resnet56]: Epoch 23/100, Acc=0.8611, Val Loss=0.4676, lr=0.0100
[02/21 04:21:50 cifar10-global-slim-3.0-resnet56]: Epoch 24/100, Acc=0.8633, Val Loss=0.4505, lr=0.0100
[02/21 04:22:22 cifar10-global-slim-3.0-resnet56]: Epoch 25/100, Acc=0.8787, Val Loss=0.3800, lr=0.0100
[02/21 04:22:55 cifar10-global-slim-3.0-resnet56]: Epoch 26/100, Acc=0.8799, Val Loss=0.4096, lr=0.0100
[02/21 04:23:27 cifar10-global-slim-3.0-resnet56]: Epoch 27/100, Acc=0.8920, Val Loss=0.3503, lr=0.0100
[02/21 04:24:00 cifar10-global-slim-3.0-resnet56]: Epoch 28/100, Acc=0.8795, Val Loss=0.3866, lr=0.0100
[02/21 04:24:33 cifar10-global-slim-3.0-resnet56]: Epoch 29/100, Acc=0.8674, Val Loss=0.4400, lr=0.0100
[02/21 04:25:05 cifar10-global-slim-3.0-resnet56]: Epoch 30/100, Acc=0.8843, Val Loss=0.3739, lr=0.0100
[02/21 04:25:38 cifar10-global-slim-3.0-resnet56]: Epoch 31/100, Acc=0.8501, Val Loss=0.4915, lr=0.0100
[02/21 04:26:11 cifar10-global-slim-3.0-resnet56]: Epoch 32/100, Acc=0.8790, Val Loss=0.3788, lr=0.0100
[02/21 04:26:43 cifar10-global-slim-3.0-resnet56]: Epoch 33/100, Acc=0.8808, Val Loss=0.4004, lr=0.0100
[02/21 04:27:15 cifar10-global-slim-3.0-resnet56]: Epoch 34/100, Acc=0.8780, Val Loss=0.3906, lr=0.0100
[02/21 04:27:47 cifar10-global-slim-3.0-resnet56]: Epoch 35/100, Acc=0.8424, Val Loss=0.5171, lr=0.0100
[02/21 04:28:20 cifar10-global-slim-3.0-resnet56]: Epoch 36/100, Acc=0.8579, Val Loss=0.4785, lr=0.0100
[02/21 04:28:52 cifar10-global-slim-3.0-resnet56]: Epoch 37/100, Acc=0.8814, Val Loss=0.3981, lr=0.0100
[02/21 04:29:25 cifar10-global-slim-3.0-resnet56]: Epoch 38/100, Acc=0.8778, Val Loss=0.4014, lr=0.0100
[02/21 04:29:59 cifar10-global-slim-3.0-resnet56]: Epoch 39/100, Acc=0.8923, Val Loss=0.3352, lr=0.0100
[02/21 04:30:33 cifar10-global-slim-3.0-resnet56]: Epoch 40/100, Acc=0.8912, Val Loss=0.3719, lr=0.0100
[02/21 04:31:06 cifar10-global-slim-3.0-resnet56]: Epoch 41/100, Acc=0.8827, Val Loss=0.3788, lr=0.0100
[02/21 04:31:39 cifar10-global-slim-3.0-resnet56]: Epoch 42/100, Acc=0.8869, Val Loss=0.3820, lr=0.0100
[02/21 04:32:12 cifar10-global-slim-3.0-resnet56]: Epoch 43/100, Acc=0.8869, Val Loss=0.3794, lr=0.0100
[02/21 04:32:45 cifar10-global-slim-3.0-resnet56]: Epoch 44/100, Acc=0.8676, Val Loss=0.4713, lr=0.0100
[02/21 04:33:18 cifar10-global-slim-3.0-resnet56]: Epoch 45/100, Acc=0.8812, Val Loss=0.4043, lr=0.0100
[02/21 04:33:50 cifar10-global-slim-3.0-resnet56]: Epoch 46/100, Acc=0.8739, Val Loss=0.4211, lr=0.0100
[02/21 04:34:23 cifar10-global-slim-3.0-resnet56]: Epoch 47/100, Acc=0.8755, Val Loss=0.4210, lr=0.0100
[02/21 04:34:56 cifar10-global-slim-3.0-resnet56]: Epoch 48/100, Acc=0.8791, Val Loss=0.4025, lr=0.0100
[02/21 04:35:28 cifar10-global-slim-3.0-resnet56]: Epoch 49/100, Acc=0.8880, Val Loss=0.3662, lr=0.0100
[02/21 04:36:01 cifar10-global-slim-3.0-resnet56]: Epoch 50/100, Acc=0.8732, Val Loss=0.4459, lr=0.0100
[02/21 04:36:34 cifar10-global-slim-3.0-resnet56]: Epoch 51/100, Acc=0.8771, Val Loss=0.4228, lr=0.0100
[02/21 04:37:06 cifar10-global-slim-3.0-resnet56]: Epoch 52/100, Acc=0.8816, Val Loss=0.3790, lr=0.0100
[02/21 04:37:39 cifar10-global-slim-3.0-resnet56]: Epoch 53/100, Acc=0.8831, Val Loss=0.3967, lr=0.0100
[02/21 04:38:11 cifar10-global-slim-3.0-resnet56]: Epoch 54/100, Acc=0.8823, Val Loss=0.3995, lr=0.0100
[02/21 04:38:44 cifar10-global-slim-3.0-resnet56]: Epoch 55/100, Acc=0.8856, Val Loss=0.3742, lr=0.0100
[02/21 04:39:16 cifar10-global-slim-3.0-resnet56]: Epoch 56/100, Acc=0.8807, Val Loss=0.4204, lr=0.0100
[02/21 04:39:49 cifar10-global-slim-3.0-resnet56]: Epoch 57/100, Acc=0.8843, Val Loss=0.3818, lr=0.0100
[02/21 04:40:21 cifar10-global-slim-3.0-resnet56]: Epoch 58/100, Acc=0.8924, Val Loss=0.3740, lr=0.0100
[02/21 04:40:54 cifar10-global-slim-3.0-resnet56]: Epoch 59/100, Acc=0.8767, Val Loss=0.4279, lr=0.0100
[02/21 04:41:26 cifar10-global-slim-3.0-resnet56]: Epoch 60/100, Acc=0.9166, Val Loss=0.2800, lr=0.0010
[02/21 04:41:59 cifar10-global-slim-3.0-resnet56]: Epoch 61/100, Acc=0.9185, Val Loss=0.2770, lr=0.0010
[02/21 04:42:32 cifar10-global-slim-3.0-resnet56]: Epoch 62/100, Acc=0.9189, Val Loss=0.2826, lr=0.0010
[02/21 04:43:05 cifar10-global-slim-3.0-resnet56]: Epoch 63/100, Acc=0.9209, Val Loss=0.2777, lr=0.0010
[02/21 04:43:37 cifar10-global-slim-3.0-resnet56]: Epoch 64/100, Acc=0.9211, Val Loss=0.2792, lr=0.0010
[02/21 04:44:10 cifar10-global-slim-3.0-resnet56]: Epoch 65/100, Acc=0.9206, Val Loss=0.2841, lr=0.0010
[02/21 04:44:42 cifar10-global-slim-3.0-resnet56]: Epoch 66/100, Acc=0.9216, Val Loss=0.2896, lr=0.0010
[02/21 04:45:14 cifar10-global-slim-3.0-resnet56]: Epoch 67/100, Acc=0.9215, Val Loss=0.2836, lr=0.0010
[02/21 04:45:46 cifar10-global-slim-3.0-resnet56]: Epoch 68/100, Acc=0.9226, Val Loss=0.2909, lr=0.0010
[02/21 04:46:18 cifar10-global-slim-3.0-resnet56]: Epoch 69/100, Acc=0.9210, Val Loss=0.2890, lr=0.0010
[02/21 04:46:50 cifar10-global-slim-3.0-resnet56]: Epoch 70/100, Acc=0.9211, Val Loss=0.2921, lr=0.0010
[02/21 04:47:22 cifar10-global-slim-3.0-resnet56]: Epoch 71/100, Acc=0.9207, Val Loss=0.2871, lr=0.0010
[02/21 04:47:54 cifar10-global-slim-3.0-resnet56]: Epoch 72/100, Acc=0.9214, Val Loss=0.2953, lr=0.0010
[02/21 04:48:27 cifar10-global-slim-3.0-resnet56]: Epoch 73/100, Acc=0.9210, Val Loss=0.2892, lr=0.0010
[02/21 04:49:00 cifar10-global-slim-3.0-resnet56]: Epoch 74/100, Acc=0.9211, Val Loss=0.2938, lr=0.0010
[02/21 04:49:31 cifar10-global-slim-3.0-resnet56]: Epoch 75/100, Acc=0.9219, Val Loss=0.2986, lr=0.0010
[02/21 04:50:02 cifar10-global-slim-3.0-resnet56]: Epoch 76/100, Acc=0.9217, Val Loss=0.3039, lr=0.0010
[02/21 04:50:35 cifar10-global-slim-3.0-resnet56]: Epoch 77/100, Acc=0.9233, Val Loss=0.3021, lr=0.0010
[02/21 04:51:08 cifar10-global-slim-3.0-resnet56]: Epoch 78/100, Acc=0.9223, Val Loss=0.2999, lr=0.0010
[02/21 04:51:41 cifar10-global-slim-3.0-resnet56]: Epoch 79/100, Acc=0.9223, Val Loss=0.3044, lr=0.0010
[02/21 04:52:13 cifar10-global-slim-3.0-resnet56]: Epoch 80/100, Acc=0.9228, Val Loss=0.3027, lr=0.0001
[02/21 04:52:46 cifar10-global-slim-3.0-resnet56]: Epoch 81/100, Acc=0.9225, Val Loss=0.3043, lr=0.0001
[02/21 04:53:19 cifar10-global-slim-3.0-resnet56]: Epoch 82/100, Acc=0.9230, Val Loss=0.3034, lr=0.0001
[02/21 04:53:52 cifar10-global-slim-3.0-resnet56]: Epoch 83/100, Acc=0.9233, Val Loss=0.3019, lr=0.0001
[02/21 04:54:24 cifar10-global-slim-3.0-resnet56]: Epoch 84/100, Acc=0.9237, Val Loss=0.3032, lr=0.0001
[02/21 04:54:57 cifar10-global-slim-3.0-resnet56]: Epoch 85/100, Acc=0.9235, Val Loss=0.3031, lr=0.0001
[02/21 04:55:30 cifar10-global-slim-3.0-resnet56]: Epoch 86/100, Acc=0.9244, Val Loss=0.3040, lr=0.0001
[02/21 04:56:02 cifar10-global-slim-3.0-resnet56]: Epoch 87/100, Acc=0.9242, Val Loss=0.3025, lr=0.0001
[02/21 04:56:35 cifar10-global-slim-3.0-resnet56]: Epoch 88/100, Acc=0.9239, Val Loss=0.3006, lr=0.0001
[02/21 04:57:08 cifar10-global-slim-3.0-resnet56]: Epoch 89/100, Acc=0.9242, Val Loss=0.3052, lr=0.0001
[02/21 04:57:41 cifar10-global-slim-3.0-resnet56]: Epoch 90/100, Acc=0.9227, Val Loss=0.3024, lr=0.0001
[02/21 04:58:13 cifar10-global-slim-3.0-resnet56]: Epoch 91/100, Acc=0.9247, Val Loss=0.3039, lr=0.0001
[02/21 04:58:45 cifar10-global-slim-3.0-resnet56]: Epoch 92/100, Acc=0.9226, Val Loss=0.3024, lr=0.0001
[02/21 04:59:18 cifar10-global-slim-3.0-resnet56]: Epoch 93/100, Acc=0.9231, Val Loss=0.3052, lr=0.0001
[02/21 04:59:50 cifar10-global-slim-3.0-resnet56]: Epoch 94/100, Acc=0.9233, Val Loss=0.3074, lr=0.0001
[02/21 05:00:22 cifar10-global-slim-3.0-resnet56]: Epoch 95/100, Acc=0.9227, Val Loss=0.3056, lr=0.0001
[02/21 05:00:55 cifar10-global-slim-3.0-resnet56]: Epoch 96/100, Acc=0.9226, Val Loss=0.3024, lr=0.0001
[02/21 05:01:28 cifar10-global-slim-3.0-resnet56]: Epoch 97/100, Acc=0.9238, Val Loss=0.3038, lr=0.0001
[02/21 05:02:00 cifar10-global-slim-3.0-resnet56]: Epoch 98/100, Acc=0.9235, Val Loss=0.3025, lr=0.0001
[02/21 05:02:33 cifar10-global-slim-3.0-resnet56]: Epoch 99/100, Acc=0.9234, Val Loss=0.3047, lr=0.0001
[02/21 05:02:33 cifar10-global-slim-3.0-resnet56]: Best Acc=0.9247
[02/21 05:02:33 cifar10-global-slim-3.0-resnet56]: Params: 0.36 M
[02/21 05:02:33 cifar10-global-slim-3.0-resnet56]: ops: 42.31 M
[02/21 05:02:36 cifar10-global-slim-3.0-resnet56]: Acc: 0.9234 Val Loss: 0.3047

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: mode: prune
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: model: resnet56
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: verbose: False
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: dataset: cifar10
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: dataroot: data
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: batch_size: 128
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: total_epochs: 100
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: lr_decay_milestones: 60,80
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: lr_decay_gamma: 0.1
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: lr: 0.01
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: output_dir: run/cifar10/prune/cifar10-global-group_norm-3.0-resnet56
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: finetune: True
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: last_epochs: 100
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: reps: 1
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: method: group_norm
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: speed_up: 3.0
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: max_pruning_ratio: 1.0
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: reg: 1e-05
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: delta_reg: 0.0001
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: weight_decay: 0.0005
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: seed: 1
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: global_pruning: True
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: sl_total_epochs: 100
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: sl_lr: 0.01
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: sl_reg_warmup: 0
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: sl_restore: None
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: iterative_steps: 400
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: logger: <Logger cifar10-global-group_norm-3.0-resnet56 (DEBUG)>
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: device: cuda
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: num_classes: 10
[02/21 05:02:44 cifar10-global-group_norm-3.0-resnet56]: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 05:02:47 cifar10-global-group_norm-3.0-resnet56]: Pruning...
[02/21 05:03:09 cifar10-global-group_norm-3.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(11, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(11, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(11, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(11, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(11, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(11, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(11, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(11, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(11, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(11, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(11, 15, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(15, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(15, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(15, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(15, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(15, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(15, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(15, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(15, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(15, 55, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(55, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(55, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(51, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(55, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(46, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(55, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(48, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(55, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(36, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(55, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(33, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(55, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(55, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(41, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=55, out_features=10, bias=True)
)
[02/21 05:03:12 cifar10-global-group_norm-3.0-resnet56]: Params: 0.86 M => 0.42 M (48.50%)
[02/21 05:03:12 cifar10-global-group_norm-3.0-resnet56]: FLOPs: 127.12 M => 41.95 M (33.00%, 3.03X )
[02/21 05:03:12 cifar10-global-group_norm-3.0-resnet56]: Acc: 0.9392 => 0.1325
[02/21 05:03:12 cifar10-global-group_norm-3.0-resnet56]: Val Loss: 0.2587 => 8.6899
[02/21 05:03:12 cifar10-global-group_norm-3.0-resnet56]: Finetuning...
[02/21 05:03:45 cifar10-global-group_norm-3.0-resnet56]: Epoch 0/100, Acc=0.8597, Val Loss=0.4275, lr=0.0100
[02/21 05:04:17 cifar10-global-group_norm-3.0-resnet56]: Epoch 1/100, Acc=0.8745, Val Loss=0.3840, lr=0.0100
[02/21 05:04:50 cifar10-global-group_norm-3.0-resnet56]: Epoch 2/100, Acc=0.8619, Val Loss=0.4392, lr=0.0100
[02/21 05:05:22 cifar10-global-group_norm-3.0-resnet56]: Epoch 3/100, Acc=0.8883, Val Loss=0.3423, lr=0.0100
[02/21 05:05:55 cifar10-global-group_norm-3.0-resnet56]: Epoch 4/100, Acc=0.8805, Val Loss=0.3744, lr=0.0100
[02/21 05:06:28 cifar10-global-group_norm-3.0-resnet56]: Epoch 5/100, Acc=0.8802, Val Loss=0.3678, lr=0.0100
[02/21 05:07:00 cifar10-global-group_norm-3.0-resnet56]: Epoch 6/100, Acc=0.8861, Val Loss=0.3590, lr=0.0100
[02/21 05:07:32 cifar10-global-group_norm-3.0-resnet56]: Epoch 7/100, Acc=0.8750, Val Loss=0.3836, lr=0.0100
[02/21 05:08:05 cifar10-global-group_norm-3.0-resnet56]: Epoch 8/100, Acc=0.8774, Val Loss=0.3883, lr=0.0100
[02/21 05:08:37 cifar10-global-group_norm-3.0-resnet56]: Epoch 9/100, Acc=0.8860, Val Loss=0.3726, lr=0.0100
[02/21 05:09:10 cifar10-global-group_norm-3.0-resnet56]: Epoch 10/100, Acc=0.8877, Val Loss=0.3539, lr=0.0100
[02/21 05:09:43 cifar10-global-group_norm-3.0-resnet56]: Epoch 11/100, Acc=0.8905, Val Loss=0.3482, lr=0.0100
[02/21 05:10:16 cifar10-global-group_norm-3.0-resnet56]: Epoch 12/100, Acc=0.8812, Val Loss=0.3918, lr=0.0100
[02/21 05:10:48 cifar10-global-group_norm-3.0-resnet56]: Epoch 13/100, Acc=0.8983, Val Loss=0.3360, lr=0.0100
[02/21 05:11:21 cifar10-global-group_norm-3.0-resnet56]: Epoch 14/100, Acc=0.8964, Val Loss=0.3423, lr=0.0100
[02/21 05:11:53 cifar10-global-group_norm-3.0-resnet56]: Epoch 15/100, Acc=0.8998, Val Loss=0.3189, lr=0.0100
[02/21 05:12:26 cifar10-global-group_norm-3.0-resnet56]: Epoch 16/100, Acc=0.8970, Val Loss=0.3302, lr=0.0100
[02/21 05:12:59 cifar10-global-group_norm-3.0-resnet56]: Epoch 17/100, Acc=0.8970, Val Loss=0.3356, lr=0.0100
[02/21 05:13:32 cifar10-global-group_norm-3.0-resnet56]: Epoch 18/100, Acc=0.8901, Val Loss=0.3617, lr=0.0100
[02/21 05:14:04 cifar10-global-group_norm-3.0-resnet56]: Epoch 19/100, Acc=0.8946, Val Loss=0.3278, lr=0.0100
[02/21 05:14:38 cifar10-global-group_norm-3.0-resnet56]: Epoch 20/100, Acc=0.8914, Val Loss=0.3479, lr=0.0100
[02/21 05:15:10 cifar10-global-group_norm-3.0-resnet56]: Epoch 21/100, Acc=0.8952, Val Loss=0.3429, lr=0.0100
[02/21 05:15:43 cifar10-global-group_norm-3.0-resnet56]: Epoch 22/100, Acc=0.8940, Val Loss=0.3547, lr=0.0100
[02/21 05:16:16 cifar10-global-group_norm-3.0-resnet56]: Epoch 23/100, Acc=0.8985, Val Loss=0.3320, lr=0.0100
[02/21 05:16:49 cifar10-global-group_norm-3.0-resnet56]: Epoch 24/100, Acc=0.8973, Val Loss=0.3489, lr=0.0100
[02/21 05:17:22 cifar10-global-group_norm-3.0-resnet56]: Epoch 25/100, Acc=0.8871, Val Loss=0.3844, lr=0.0100
[02/21 05:17:55 cifar10-global-group_norm-3.0-resnet56]: Epoch 26/100, Acc=0.8887, Val Loss=0.3751, lr=0.0100
[02/21 05:18:27 cifar10-global-group_norm-3.0-resnet56]: Epoch 27/100, Acc=0.8881, Val Loss=0.3667, lr=0.0100
[02/21 05:19:00 cifar10-global-group_norm-3.0-resnet56]: Epoch 28/100, Acc=0.8900, Val Loss=0.3705, lr=0.0100
[02/21 05:19:33 cifar10-global-group_norm-3.0-resnet56]: Epoch 29/100, Acc=0.8912, Val Loss=0.3627, lr=0.0100
[02/21 05:20:06 cifar10-global-group_norm-3.0-resnet56]: Epoch 30/100, Acc=0.8977, Val Loss=0.3360, lr=0.0100
[02/21 05:20:38 cifar10-global-group_norm-3.0-resnet56]: Epoch 31/100, Acc=0.9022, Val Loss=0.3204, lr=0.0100
[02/21 05:21:11 cifar10-global-group_norm-3.0-resnet56]: Epoch 32/100, Acc=0.8948, Val Loss=0.3400, lr=0.0100
[02/21 05:21:44 cifar10-global-group_norm-3.0-resnet56]: Epoch 33/100, Acc=0.8848, Val Loss=0.3884, lr=0.0100
[02/21 05:22:17 cifar10-global-group_norm-3.0-resnet56]: Epoch 34/100, Acc=0.8903, Val Loss=0.3714, lr=0.0100
[02/21 05:22:50 cifar10-global-group_norm-3.0-resnet56]: Epoch 35/100, Acc=0.8862, Val Loss=0.3954, lr=0.0100
[02/21 05:23:23 cifar10-global-group_norm-3.0-resnet56]: Epoch 36/100, Acc=0.8927, Val Loss=0.3469, lr=0.0100
[02/21 05:23:55 cifar10-global-group_norm-3.0-resnet56]: Epoch 37/100, Acc=0.8819, Val Loss=0.3929, lr=0.0100
[02/21 05:24:28 cifar10-global-group_norm-3.0-resnet56]: Epoch 38/100, Acc=0.8921, Val Loss=0.3635, lr=0.0100
[02/21 05:25:01 cifar10-global-group_norm-3.0-resnet56]: Epoch 39/100, Acc=0.8936, Val Loss=0.3579, lr=0.0100
[02/21 05:25:33 cifar10-global-group_norm-3.0-resnet56]: Epoch 40/100, Acc=0.9020, Val Loss=0.3314, lr=0.0100
[02/21 05:26:06 cifar10-global-group_norm-3.0-resnet56]: Epoch 41/100, Acc=0.8983, Val Loss=0.3466, lr=0.0100
[02/21 05:26:39 cifar10-global-group_norm-3.0-resnet56]: Epoch 42/100, Acc=0.9050, Val Loss=0.3113, lr=0.0100
[02/21 05:27:13 cifar10-global-group_norm-3.0-resnet56]: Epoch 43/100, Acc=0.9007, Val Loss=0.3390, lr=0.0100
[02/21 05:27:45 cifar10-global-group_norm-3.0-resnet56]: Epoch 44/100, Acc=0.9073, Val Loss=0.3019, lr=0.0100
[02/21 05:28:18 cifar10-global-group_norm-3.0-resnet56]: Epoch 45/100, Acc=0.8964, Val Loss=0.3515, lr=0.0100
[02/21 05:28:52 cifar10-global-group_norm-3.0-resnet56]: Epoch 46/100, Acc=0.8909, Val Loss=0.3620, lr=0.0100
[02/21 05:29:25 cifar10-global-group_norm-3.0-resnet56]: Epoch 47/100, Acc=0.8954, Val Loss=0.3512, lr=0.0100
[02/21 05:29:58 cifar10-global-group_norm-3.0-resnet56]: Epoch 48/100, Acc=0.8996, Val Loss=0.3306, lr=0.0100
[02/21 05:30:30 cifar10-global-group_norm-3.0-resnet56]: Epoch 49/100, Acc=0.8957, Val Loss=0.3489, lr=0.0100
[02/21 05:31:03 cifar10-global-group_norm-3.0-resnet56]: Epoch 50/100, Acc=0.8987, Val Loss=0.3534, lr=0.0100
[02/21 05:31:35 cifar10-global-group_norm-3.0-resnet56]: Epoch 51/100, Acc=0.9022, Val Loss=0.3262, lr=0.0100
[02/21 05:32:08 cifar10-global-group_norm-3.0-resnet56]: Epoch 52/100, Acc=0.8786, Val Loss=0.4206, lr=0.0100
[02/21 05:32:41 cifar10-global-group_norm-3.0-resnet56]: Epoch 53/100, Acc=0.8970, Val Loss=0.3536, lr=0.0100
[02/21 05:33:14 cifar10-global-group_norm-3.0-resnet56]: Epoch 54/100, Acc=0.8968, Val Loss=0.3382, lr=0.0100
[02/21 05:33:47 cifar10-global-group_norm-3.0-resnet56]: Epoch 55/100, Acc=0.8943, Val Loss=0.3612, lr=0.0100
[02/21 05:34:20 cifar10-global-group_norm-3.0-resnet56]: Epoch 56/100, Acc=0.8989, Val Loss=0.3318, lr=0.0100
[02/21 05:34:52 cifar10-global-group_norm-3.0-resnet56]: Epoch 57/100, Acc=0.9050, Val Loss=0.3231, lr=0.0100
[02/21 05:35:25 cifar10-global-group_norm-3.0-resnet56]: Epoch 58/100, Acc=0.8964, Val Loss=0.3484, lr=0.0100
[02/21 05:35:58 cifar10-global-group_norm-3.0-resnet56]: Epoch 59/100, Acc=0.8753, Val Loss=0.4356, lr=0.0100
[02/21 05:36:30 cifar10-global-group_norm-3.0-resnet56]: Epoch 60/100, Acc=0.9229, Val Loss=0.2615, lr=0.0010
[02/21 05:37:02 cifar10-global-group_norm-3.0-resnet56]: Epoch 61/100, Acc=0.9259, Val Loss=0.2614, lr=0.0010
[02/21 05:37:34 cifar10-global-group_norm-3.0-resnet56]: Epoch 62/100, Acc=0.9268, Val Loss=0.2628, lr=0.0010
[02/21 05:38:07 cifar10-global-group_norm-3.0-resnet56]: Epoch 63/100, Acc=0.9264, Val Loss=0.2633, lr=0.0010
[02/21 05:38:39 cifar10-global-group_norm-3.0-resnet56]: Epoch 64/100, Acc=0.9257, Val Loss=0.2604, lr=0.0010
[02/21 05:39:12 cifar10-global-group_norm-3.0-resnet56]: Epoch 65/100, Acc=0.9278, Val Loss=0.2598, lr=0.0010
[02/21 05:39:44 cifar10-global-group_norm-3.0-resnet56]: Epoch 66/100, Acc=0.9272, Val Loss=0.2623, lr=0.0010
[02/21 05:40:16 cifar10-global-group_norm-3.0-resnet56]: Epoch 67/100, Acc=0.9276, Val Loss=0.2654, lr=0.0010
[02/21 05:40:49 cifar10-global-group_norm-3.0-resnet56]: Epoch 68/100, Acc=0.9270, Val Loss=0.2670, lr=0.0010
[02/21 05:41:21 cifar10-global-group_norm-3.0-resnet56]: Epoch 69/100, Acc=0.9288, Val Loss=0.2666, lr=0.0010
[02/21 05:41:54 cifar10-global-group_norm-3.0-resnet56]: Epoch 70/100, Acc=0.9291, Val Loss=0.2689, lr=0.0010
[02/21 05:42:26 cifar10-global-group_norm-3.0-resnet56]: Epoch 71/100, Acc=0.9281, Val Loss=0.2683, lr=0.0010
[02/21 05:42:58 cifar10-global-group_norm-3.0-resnet56]: Epoch 72/100, Acc=0.9284, Val Loss=0.2714, lr=0.0010
[02/21 05:43:30 cifar10-global-group_norm-3.0-resnet56]: Epoch 73/100, Acc=0.9280, Val Loss=0.2751, lr=0.0010
[02/21 05:44:01 cifar10-global-group_norm-3.0-resnet56]: Epoch 74/100, Acc=0.9296, Val Loss=0.2785, lr=0.0010
[02/21 05:44:34 cifar10-global-group_norm-3.0-resnet56]: Epoch 75/100, Acc=0.9288, Val Loss=0.2783, lr=0.0010
[02/21 05:45:06 cifar10-global-group_norm-3.0-resnet56]: Epoch 76/100, Acc=0.9285, Val Loss=0.2760, lr=0.0010
[02/21 05:45:39 cifar10-global-group_norm-3.0-resnet56]: Epoch 77/100, Acc=0.9285, Val Loss=0.2780, lr=0.0010
[02/21 05:46:11 cifar10-global-group_norm-3.0-resnet56]: Epoch 78/100, Acc=0.9306, Val Loss=0.2759, lr=0.0010
[02/21 05:46:44 cifar10-global-group_norm-3.0-resnet56]: Epoch 79/100, Acc=0.9303, Val Loss=0.2767, lr=0.0010
[02/21 05:47:16 cifar10-global-group_norm-3.0-resnet56]: Epoch 80/100, Acc=0.9305, Val Loss=0.2786, lr=0.0001
[02/21 05:47:48 cifar10-global-group_norm-3.0-resnet56]: Epoch 81/100, Acc=0.9306, Val Loss=0.2757, lr=0.0001
[02/21 05:48:21 cifar10-global-group_norm-3.0-resnet56]: Epoch 82/100, Acc=0.9313, Val Loss=0.2754, lr=0.0001
[02/21 05:48:53 cifar10-global-group_norm-3.0-resnet56]: Epoch 83/100, Acc=0.9301, Val Loss=0.2771, lr=0.0001
[02/21 05:49:26 cifar10-global-group_norm-3.0-resnet56]: Epoch 84/100, Acc=0.9299, Val Loss=0.2790, lr=0.0001
[02/21 05:49:58 cifar10-global-group_norm-3.0-resnet56]: Epoch 85/100, Acc=0.9308, Val Loss=0.2786, lr=0.0001
[02/21 05:50:31 cifar10-global-group_norm-3.0-resnet56]: Epoch 86/100, Acc=0.9301, Val Loss=0.2784, lr=0.0001
[02/21 05:51:04 cifar10-global-group_norm-3.0-resnet56]: Epoch 87/100, Acc=0.9303, Val Loss=0.2793, lr=0.0001
[02/21 05:51:37 cifar10-global-group_norm-3.0-resnet56]: Epoch 88/100, Acc=0.9304, Val Loss=0.2761, lr=0.0001
[02/21 05:52:10 cifar10-global-group_norm-3.0-resnet56]: Epoch 89/100, Acc=0.9312, Val Loss=0.2786, lr=0.0001
[02/21 05:52:44 cifar10-global-group_norm-3.0-resnet56]: Epoch 90/100, Acc=0.9311, Val Loss=0.2783, lr=0.0001
[02/21 05:53:17 cifar10-global-group_norm-3.0-resnet56]: Epoch 91/100, Acc=0.9307, Val Loss=0.2791, lr=0.0001
[02/21 05:53:50 cifar10-global-group_norm-3.0-resnet56]: Epoch 92/100, Acc=0.9303, Val Loss=0.2787, lr=0.0001
[02/21 05:54:23 cifar10-global-group_norm-3.0-resnet56]: Epoch 93/100, Acc=0.9288, Val Loss=0.2778, lr=0.0001
[02/21 05:54:57 cifar10-global-group_norm-3.0-resnet56]: Epoch 94/100, Acc=0.9305, Val Loss=0.2788, lr=0.0001
[02/21 05:55:30 cifar10-global-group_norm-3.0-resnet56]: Epoch 95/100, Acc=0.9305, Val Loss=0.2807, lr=0.0001
[02/21 05:56:03 cifar10-global-group_norm-3.0-resnet56]: Epoch 96/100, Acc=0.9302, Val Loss=0.2790, lr=0.0001
[02/21 05:56:36 cifar10-global-group_norm-3.0-resnet56]: Epoch 97/100, Acc=0.9299, Val Loss=0.2779, lr=0.0001
[02/21 05:57:09 cifar10-global-group_norm-3.0-resnet56]: Epoch 98/100, Acc=0.9295, Val Loss=0.2786, lr=0.0001
[02/21 05:57:42 cifar10-global-group_norm-3.0-resnet56]: Epoch 99/100, Acc=0.9306, Val Loss=0.2793, lr=0.0001
[02/21 05:57:42 cifar10-global-group_norm-3.0-resnet56]: Best Acc=0.9313
[02/21 05:57:42 cifar10-global-group_norm-3.0-resnet56]: Params: 0.42 M
[02/21 05:57:42 cifar10-global-group_norm-3.0-resnet56]: ops: 41.95 M
[02/21 05:57:46 cifar10-global-group_norm-3.0-resnet56]: Acc: 0.9306 Val Loss: 0.2793

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: mode: prune
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: model: resnet56
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: verbose: False
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: dataset: cifar10
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: dataroot: data
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: batch_size: 128
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: total_epochs: 100
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: lr_decay_milestones: 60,80
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: lr_decay_gamma: 0.1
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: lr: 0.01
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: output_dir: run/cifar10/prune/cifar10-global-group_sl-3.0-resnet56
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: finetune: True
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: last_epochs: 100
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: reps: 1
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: method: group_sl
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: speed_up: 3.0
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: max_pruning_ratio: 1.0
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: reg: 1e-05
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: delta_reg: 0.0001
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: weight_decay: 0.0005
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: seed: 1
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: global_pruning: True
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: sl_total_epochs: 100
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: sl_lr: 0.01
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: sl_reg_warmup: 0
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: sl_restore: None
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: iterative_steps: 400
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: logger: <Logger cifar10-global-group_sl-3.0-resnet56 (DEBUG)>
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: device: cuda
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: num_classes: 10
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 05:57:53 cifar10-global-group_sl-3.0-resnet56]: Regularizing...
[02/21 05:59:04 cifar10-global-group_sl-3.0-resnet56]: Epoch 0/100, Acc=0.9080, Val Loss=0.3196, lr=0.0100
[02/21 06:00:14 cifar10-global-group_sl-3.0-resnet56]: Epoch 1/100, Acc=0.8891, Val Loss=0.4071, lr=0.0100
[02/21 06:01:25 cifar10-global-group_sl-3.0-resnet56]: Epoch 2/100, Acc=0.8972, Val Loss=0.3853, lr=0.0100
[02/21 06:02:37 cifar10-global-group_sl-3.0-resnet56]: Epoch 3/100, Acc=0.9148, Val Loss=0.3024, lr=0.0100
[02/21 06:03:48 cifar10-global-group_sl-3.0-resnet56]: Epoch 4/100, Acc=0.9153, Val Loss=0.3189, lr=0.0100
[02/21 06:04:59 cifar10-global-group_sl-3.0-resnet56]: Epoch 5/100, Acc=0.9088, Val Loss=0.3368, lr=0.0100
[02/21 06:06:10 cifar10-global-group_sl-3.0-resnet56]: Epoch 6/100, Acc=0.8999, Val Loss=0.3757, lr=0.0100
[02/21 06:07:21 cifar10-global-group_sl-3.0-resnet56]: Epoch 7/100, Acc=0.9138, Val Loss=0.3117, lr=0.0100
[02/21 06:08:31 cifar10-global-group_sl-3.0-resnet56]: Epoch 8/100, Acc=0.9192, Val Loss=0.3154, lr=0.0100
[02/21 06:09:42 cifar10-global-group_sl-3.0-resnet56]: Epoch 9/100, Acc=0.9166, Val Loss=0.3162, lr=0.0100
[02/21 06:10:53 cifar10-global-group_sl-3.0-resnet56]: Epoch 10/100, Acc=0.9157, Val Loss=0.3292, lr=0.0100
[02/21 06:12:05 cifar10-global-group_sl-3.0-resnet56]: Epoch 11/100, Acc=0.9133, Val Loss=0.3334, lr=0.0100
[02/21 06:13:16 cifar10-global-group_sl-3.0-resnet56]: Epoch 12/100, Acc=0.9149, Val Loss=0.3284, lr=0.0100
[02/21 06:14:27 cifar10-global-group_sl-3.0-resnet56]: Epoch 13/100, Acc=0.9168, Val Loss=0.3274, lr=0.0100
[02/21 06:15:38 cifar10-global-group_sl-3.0-resnet56]: Epoch 14/100, Acc=0.9130, Val Loss=0.3384, lr=0.0100
[02/21 06:16:48 cifar10-global-group_sl-3.0-resnet56]: Epoch 15/100, Acc=0.9114, Val Loss=0.3424, lr=0.0100
[02/21 06:17:59 cifar10-global-group_sl-3.0-resnet56]: Epoch 16/100, Acc=0.9208, Val Loss=0.3248, lr=0.0100
[02/21 06:19:11 cifar10-global-group_sl-3.0-resnet56]: Epoch 17/100, Acc=0.9222, Val Loss=0.3122, lr=0.0100
[02/21 06:20:23 cifar10-global-group_sl-3.0-resnet56]: Epoch 18/100, Acc=0.9126, Val Loss=0.3474, lr=0.0100
[02/21 06:21:34 cifar10-global-group_sl-3.0-resnet56]: Epoch 19/100, Acc=0.9102, Val Loss=0.3464, lr=0.0100
[02/21 06:22:46 cifar10-global-group_sl-3.0-resnet56]: Epoch 20/100, Acc=0.9164, Val Loss=0.3317, lr=0.0100
[02/21 06:23:57 cifar10-global-group_sl-3.0-resnet56]: Epoch 21/100, Acc=0.9207, Val Loss=0.3104, lr=0.0100
[02/21 06:25:09 cifar10-global-group_sl-3.0-resnet56]: Epoch 22/100, Acc=0.9239, Val Loss=0.3210, lr=0.0100
[02/21 06:26:21 cifar10-global-group_sl-3.0-resnet56]: Epoch 23/100, Acc=0.9211, Val Loss=0.3476, lr=0.0100
[02/21 06:27:32 cifar10-global-group_sl-3.0-resnet56]: Epoch 24/100, Acc=0.9222, Val Loss=0.3332, lr=0.0100
[02/21 06:28:44 cifar10-global-group_sl-3.0-resnet56]: Epoch 25/100, Acc=0.9239, Val Loss=0.3328, lr=0.0100
[02/21 06:29:55 cifar10-global-group_sl-3.0-resnet56]: Epoch 26/100, Acc=0.9200, Val Loss=0.3384, lr=0.0100
[02/21 06:31:06 cifar10-global-group_sl-3.0-resnet56]: Epoch 27/100, Acc=0.9206, Val Loss=0.3284, lr=0.0100
[02/21 06:32:18 cifar10-global-group_sl-3.0-resnet56]: Epoch 28/100, Acc=0.9184, Val Loss=0.3455, lr=0.0100
[02/21 06:33:30 cifar10-global-group_sl-3.0-resnet56]: Epoch 29/100, Acc=0.9225, Val Loss=0.3269, lr=0.0100
[02/21 06:34:42 cifar10-global-group_sl-3.0-resnet56]: Epoch 30/100, Acc=0.9279, Val Loss=0.3193, lr=0.0100
[02/21 06:35:54 cifar10-global-group_sl-3.0-resnet56]: Epoch 31/100, Acc=0.9286, Val Loss=0.2991, lr=0.0100
[02/21 06:37:06 cifar10-global-group_sl-3.0-resnet56]: Epoch 32/100, Acc=0.9217, Val Loss=0.3342, lr=0.0100
[02/21 06:38:18 cifar10-global-group_sl-3.0-resnet56]: Epoch 33/100, Acc=0.9285, Val Loss=0.3033, lr=0.0100
[02/21 06:39:30 cifar10-global-group_sl-3.0-resnet56]: Epoch 34/100, Acc=0.9293, Val Loss=0.3132, lr=0.0100
[02/21 06:40:43 cifar10-global-group_sl-3.0-resnet56]: Epoch 35/100, Acc=0.9240, Val Loss=0.3330, lr=0.0100
[02/21 06:41:55 cifar10-global-group_sl-3.0-resnet56]: Epoch 36/100, Acc=0.9160, Val Loss=0.3936, lr=0.0100
[02/21 06:43:07 cifar10-global-group_sl-3.0-resnet56]: Epoch 37/100, Acc=0.9120, Val Loss=0.3943, lr=0.0100
[02/21 06:44:20 cifar10-global-group_sl-3.0-resnet56]: Epoch 38/100, Acc=0.9242, Val Loss=0.3525, lr=0.0100
[02/21 06:45:32 cifar10-global-group_sl-3.0-resnet56]: Epoch 39/100, Acc=0.9220, Val Loss=0.3484, lr=0.0100
[02/21 06:46:45 cifar10-global-group_sl-3.0-resnet56]: Epoch 40/100, Acc=0.9254, Val Loss=0.3312, lr=0.0100
[02/21 06:47:56 cifar10-global-group_sl-3.0-resnet56]: Epoch 41/100, Acc=0.9193, Val Loss=0.3665, lr=0.0100
[02/21 06:49:08 cifar10-global-group_sl-3.0-resnet56]: Epoch 42/100, Acc=0.9265, Val Loss=0.3280, lr=0.0100
[02/21 06:50:20 cifar10-global-group_sl-3.0-resnet56]: Epoch 43/100, Acc=0.9252, Val Loss=0.3368, lr=0.0100
[02/21 06:51:32 cifar10-global-group_sl-3.0-resnet56]: Epoch 44/100, Acc=0.9224, Val Loss=0.3577, lr=0.0100
[02/21 06:52:42 cifar10-global-group_sl-3.0-resnet56]: Epoch 45/100, Acc=0.9280, Val Loss=0.3222, lr=0.0100
[02/21 06:53:52 cifar10-global-group_sl-3.0-resnet56]: Epoch 46/100, Acc=0.9251, Val Loss=0.3425, lr=0.0100
[02/21 06:55:02 cifar10-global-group_sl-3.0-resnet56]: Epoch 47/100, Acc=0.9271, Val Loss=0.3291, lr=0.0100
[02/21 06:56:12 cifar10-global-group_sl-3.0-resnet56]: Epoch 48/100, Acc=0.9259, Val Loss=0.3362, lr=0.0100
[02/21 06:57:22 cifar10-global-group_sl-3.0-resnet56]: Epoch 49/100, Acc=0.9297, Val Loss=0.3273, lr=0.0100
[02/21 06:58:33 cifar10-global-group_sl-3.0-resnet56]: Epoch 50/100, Acc=0.9288, Val Loss=0.3429, lr=0.0100
[02/21 06:59:45 cifar10-global-group_sl-3.0-resnet56]: Epoch 51/100, Acc=0.9265, Val Loss=0.3290, lr=0.0100
[02/21 07:00:58 cifar10-global-group_sl-3.0-resnet56]: Epoch 52/100, Acc=0.9235, Val Loss=0.3369, lr=0.0100
[02/21 07:02:11 cifar10-global-group_sl-3.0-resnet56]: Epoch 53/100, Acc=0.9276, Val Loss=0.3304, lr=0.0100
[02/21 07:03:23 cifar10-global-group_sl-3.0-resnet56]: Epoch 54/100, Acc=0.9230, Val Loss=0.3534, lr=0.0100
[02/21 07:04:36 cifar10-global-group_sl-3.0-resnet56]: Epoch 55/100, Acc=0.9211, Val Loss=0.3684, lr=0.0100
[02/21 07:05:49 cifar10-global-group_sl-3.0-resnet56]: Epoch 56/100, Acc=0.9235, Val Loss=0.3611, lr=0.0100
[02/21 07:07:03 cifar10-global-group_sl-3.0-resnet56]: Epoch 57/100, Acc=0.9286, Val Loss=0.3426, lr=0.0100
[02/21 07:08:16 cifar10-global-group_sl-3.0-resnet56]: Epoch 58/100, Acc=0.9239, Val Loss=0.3411, lr=0.0100
[02/21 07:09:30 cifar10-global-group_sl-3.0-resnet56]: Epoch 59/100, Acc=0.9238, Val Loss=0.3592, lr=0.0100
[02/21 07:10:43 cifar10-global-group_sl-3.0-resnet56]: Epoch 60/100, Acc=0.9353, Val Loss=0.3089, lr=0.0010
[02/21 07:11:55 cifar10-global-group_sl-3.0-resnet56]: Epoch 61/100, Acc=0.9364, Val Loss=0.3006, lr=0.0010
[02/21 07:13:07 cifar10-global-group_sl-3.0-resnet56]: Epoch 62/100, Acc=0.9389, Val Loss=0.3029, lr=0.0010
[02/21 07:14:18 cifar10-global-group_sl-3.0-resnet56]: Epoch 63/100, Acc=0.9382, Val Loss=0.3002, lr=0.0010
[02/21 07:15:30 cifar10-global-group_sl-3.0-resnet56]: Epoch 64/100, Acc=0.9395, Val Loss=0.2979, lr=0.0010
[02/21 07:16:42 cifar10-global-group_sl-3.0-resnet56]: Epoch 65/100, Acc=0.9402, Val Loss=0.2991, lr=0.0010
[02/21 07:17:54 cifar10-global-group_sl-3.0-resnet56]: Epoch 66/100, Acc=0.9402, Val Loss=0.2976, lr=0.0010
[02/21 07:19:06 cifar10-global-group_sl-3.0-resnet56]: Epoch 67/100, Acc=0.9402, Val Loss=0.2935, lr=0.0010
[02/21 07:20:18 cifar10-global-group_sl-3.0-resnet56]: Epoch 68/100, Acc=0.9401, Val Loss=0.2989, lr=0.0010
[02/21 07:21:31 cifar10-global-group_sl-3.0-resnet56]: Epoch 69/100, Acc=0.9394, Val Loss=0.2972, lr=0.0010
[02/21 07:22:44 cifar10-global-group_sl-3.0-resnet56]: Epoch 70/100, Acc=0.9397, Val Loss=0.2999, lr=0.0010
[02/21 07:23:56 cifar10-global-group_sl-3.0-resnet56]: Epoch 71/100, Acc=0.9401, Val Loss=0.3010, lr=0.0010
[02/21 07:25:07 cifar10-global-group_sl-3.0-resnet56]: Epoch 72/100, Acc=0.9409, Val Loss=0.3017, lr=0.0010
[02/21 07:26:19 cifar10-global-group_sl-3.0-resnet56]: Epoch 73/100, Acc=0.9416, Val Loss=0.3018, lr=0.0010
[02/21 07:27:31 cifar10-global-group_sl-3.0-resnet56]: Epoch 74/100, Acc=0.9398, Val Loss=0.3014, lr=0.0010
[02/21 07:28:42 cifar10-global-group_sl-3.0-resnet56]: Epoch 75/100, Acc=0.9409, Val Loss=0.3059, lr=0.0010
[02/21 07:29:53 cifar10-global-group_sl-3.0-resnet56]: Epoch 76/100, Acc=0.9407, Val Loss=0.3041, lr=0.0010
[02/21 07:31:05 cifar10-global-group_sl-3.0-resnet56]: Epoch 77/100, Acc=0.9408, Val Loss=0.3064, lr=0.0010
[02/21 07:32:16 cifar10-global-group_sl-3.0-resnet56]: Epoch 78/100, Acc=0.9408, Val Loss=0.3071, lr=0.0010
[02/21 07:33:28 cifar10-global-group_sl-3.0-resnet56]: Epoch 79/100, Acc=0.9407, Val Loss=0.3083, lr=0.0010
[02/21 07:34:39 cifar10-global-group_sl-3.0-resnet56]: Epoch 80/100, Acc=0.9407, Val Loss=0.3093, lr=0.0001
[02/21 07:35:51 cifar10-global-group_sl-3.0-resnet56]: Epoch 81/100, Acc=0.9406, Val Loss=0.3102, lr=0.0001
[02/21 07:37:02 cifar10-global-group_sl-3.0-resnet56]: Epoch 82/100, Acc=0.9409, Val Loss=0.3094, lr=0.0001
[02/21 07:38:13 cifar10-global-group_sl-3.0-resnet56]: Epoch 83/100, Acc=0.9417, Val Loss=0.3074, lr=0.0001
[02/21 07:39:24 cifar10-global-group_sl-3.0-resnet56]: Epoch 84/100, Acc=0.9418, Val Loss=0.3103, lr=0.0001
[02/21 07:40:36 cifar10-global-group_sl-3.0-resnet56]: Epoch 85/100, Acc=0.9409, Val Loss=0.3105, lr=0.0001
[02/21 07:41:47 cifar10-global-group_sl-3.0-resnet56]: Epoch 86/100, Acc=0.9411, Val Loss=0.3071, lr=0.0001
[02/21 07:42:58 cifar10-global-group_sl-3.0-resnet56]: Epoch 87/100, Acc=0.9419, Val Loss=0.3080, lr=0.0001
[02/21 07:44:10 cifar10-global-group_sl-3.0-resnet56]: Epoch 88/100, Acc=0.9408, Val Loss=0.3079, lr=0.0001
[02/21 07:45:21 cifar10-global-group_sl-3.0-resnet56]: Epoch 89/100, Acc=0.9412, Val Loss=0.3080, lr=0.0001
[02/21 07:46:31 cifar10-global-group_sl-3.0-resnet56]: Epoch 90/100, Acc=0.9419, Val Loss=0.3099, lr=0.0001
[02/21 07:47:42 cifar10-global-group_sl-3.0-resnet56]: Epoch 91/100, Acc=0.9415, Val Loss=0.3086, lr=0.0001
[02/21 07:48:52 cifar10-global-group_sl-3.0-resnet56]: Epoch 92/100, Acc=0.9414, Val Loss=0.3070, lr=0.0001
[02/21 07:50:03 cifar10-global-group_sl-3.0-resnet56]: Epoch 93/100, Acc=0.9411, Val Loss=0.3047, lr=0.0001
[02/21 07:51:14 cifar10-global-group_sl-3.0-resnet56]: Epoch 94/100, Acc=0.9407, Val Loss=0.3076, lr=0.0001
[02/21 07:52:26 cifar10-global-group_sl-3.0-resnet56]: Epoch 95/100, Acc=0.9417, Val Loss=0.3068, lr=0.0001
[02/21 07:53:37 cifar10-global-group_sl-3.0-resnet56]: Epoch 96/100, Acc=0.9410, Val Loss=0.3049, lr=0.0001
[02/21 07:54:50 cifar10-global-group_sl-3.0-resnet56]: Epoch 97/100, Acc=0.9410, Val Loss=0.3064, lr=0.0001
[02/21 07:56:02 cifar10-global-group_sl-3.0-resnet56]: Epoch 98/100, Acc=0.9413, Val Loss=0.3089, lr=0.0001
[02/21 07:57:14 cifar10-global-group_sl-3.0-resnet56]: Epoch 99/100, Acc=0.9407, Val Loss=0.3082, lr=0.0001
[02/21 07:57:14 cifar10-global-group_sl-3.0-resnet56]: Best Acc=0.9419
[02/21 07:57:14 cifar10-global-group_sl-3.0-resnet56]: Loading the sparse model from run/cifar10/prune/cifar10-global-group_sl-3.0-resnet56/reg_cifar10_resnet56_group_sl_1e-05.pth...
[02/21 07:57:18 cifar10-global-group_sl-3.0-resnet56]: Pruning...
[02/21 07:57:39 cifar10-global-group_sl-3.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(12, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(12, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(12, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(12, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(12, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(12, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(12, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(12, 13, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(12, 15, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(15, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(15, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(15, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(15, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(15, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(15, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(15, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(15, 25, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(15, 44, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(44, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(44, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(44, 55, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(55, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(55, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(44, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(47, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(44, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(52, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(44, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(46, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(44, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(30, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(44, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=44, out_features=10, bias=True)
)
[02/21 07:57:43 cifar10-global-group_sl-3.0-resnet56]: Params: 0.86 M => 0.36 M (41.65%)
[02/21 07:57:43 cifar10-global-group_sl-3.0-resnet56]: FLOPs: 127.12 M => 42.36 M (33.32%, 3.00X )
[02/21 07:57:43 cifar10-global-group_sl-3.0-resnet56]: Acc: 0.9419 => 0.1036
[02/21 07:57:43 cifar10-global-group_sl-3.0-resnet56]: Val Loss: 0.3080 => 3.5750
[02/21 07:57:43 cifar10-global-group_sl-3.0-resnet56]: Finetuning...
[02/21 07:58:16 cifar10-global-group_sl-3.0-resnet56]: Epoch 0/100, Acc=0.8399, Val Loss=0.4846, lr=0.0100
[02/21 07:58:49 cifar10-global-group_sl-3.0-resnet56]: Epoch 1/100, Acc=0.8727, Val Loss=0.3970, lr=0.0100
[02/21 07:59:22 cifar10-global-group_sl-3.0-resnet56]: Epoch 2/100, Acc=0.8612, Val Loss=0.4689, lr=0.0100
[02/21 07:59:55 cifar10-global-group_sl-3.0-resnet56]: Epoch 3/100, Acc=0.8843, Val Loss=0.3705, lr=0.0100
[02/21 08:00:29 cifar10-global-group_sl-3.0-resnet56]: Epoch 4/100, Acc=0.8887, Val Loss=0.3596, lr=0.0100
[02/21 08:01:04 cifar10-global-group_sl-3.0-resnet56]: Epoch 5/100, Acc=0.8806, Val Loss=0.3828, lr=0.0100
[02/21 08:01:37 cifar10-global-group_sl-3.0-resnet56]: Epoch 6/100, Acc=0.8885, Val Loss=0.3650, lr=0.0100
[02/21 08:02:10 cifar10-global-group_sl-3.0-resnet56]: Epoch 7/100, Acc=0.8874, Val Loss=0.3624, lr=0.0100
[02/21 08:02:43 cifar10-global-group_sl-3.0-resnet56]: Epoch 8/100, Acc=0.8923, Val Loss=0.3546, lr=0.0100
[02/21 08:03:17 cifar10-global-group_sl-3.0-resnet56]: Epoch 9/100, Acc=0.8893, Val Loss=0.3540, lr=0.0100
[02/21 08:03:51 cifar10-global-group_sl-3.0-resnet56]: Epoch 10/100, Acc=0.8904, Val Loss=0.3512, lr=0.0100
[02/21 08:04:25 cifar10-global-group_sl-3.0-resnet56]: Epoch 11/100, Acc=0.8800, Val Loss=0.3878, lr=0.0100
[02/21 08:04:58 cifar10-global-group_sl-3.0-resnet56]: Epoch 12/100, Acc=0.8829, Val Loss=0.3757, lr=0.0100
[02/21 08:05:31 cifar10-global-group_sl-3.0-resnet56]: Epoch 13/100, Acc=0.8953, Val Loss=0.3313, lr=0.0100
[02/21 08:06:04 cifar10-global-group_sl-3.0-resnet56]: Epoch 14/100, Acc=0.8890, Val Loss=0.3549, lr=0.0100
[02/21 08:06:37 cifar10-global-group_sl-3.0-resnet56]: Epoch 15/100, Acc=0.8772, Val Loss=0.4024, lr=0.0100
[02/21 08:07:11 cifar10-global-group_sl-3.0-resnet56]: Epoch 16/100, Acc=0.8943, Val Loss=0.3637, lr=0.0100
[02/21 08:07:44 cifar10-global-group_sl-3.0-resnet56]: Epoch 17/100, Acc=0.8941, Val Loss=0.3483, lr=0.0100
[02/21 08:08:17 cifar10-global-group_sl-3.0-resnet56]: Epoch 18/100, Acc=0.8884, Val Loss=0.3711, lr=0.0100
[02/21 08:08:50 cifar10-global-group_sl-3.0-resnet56]: Epoch 19/100, Acc=0.8851, Val Loss=0.3956, lr=0.0100
[02/21 08:09:23 cifar10-global-group_sl-3.0-resnet56]: Epoch 20/100, Acc=0.8830, Val Loss=0.3993, lr=0.0100
[02/21 08:09:55 cifar10-global-group_sl-3.0-resnet56]: Epoch 21/100, Acc=0.8957, Val Loss=0.3453, lr=0.0100
[02/21 08:10:28 cifar10-global-group_sl-3.0-resnet56]: Epoch 22/100, Acc=0.8961, Val Loss=0.3464, lr=0.0100
[02/21 08:11:02 cifar10-global-group_sl-3.0-resnet56]: Epoch 23/100, Acc=0.8934, Val Loss=0.3445, lr=0.0100
[02/21 08:11:37 cifar10-global-group_sl-3.0-resnet56]: Epoch 24/100, Acc=0.8900, Val Loss=0.3592, lr=0.0100
[02/21 08:12:11 cifar10-global-group_sl-3.0-resnet56]: Epoch 25/100, Acc=0.8946, Val Loss=0.3340, lr=0.0100
[02/21 08:12:45 cifar10-global-group_sl-3.0-resnet56]: Epoch 26/100, Acc=0.8824, Val Loss=0.4091, lr=0.0100
[02/21 08:13:19 cifar10-global-group_sl-3.0-resnet56]: Epoch 27/100, Acc=0.8937, Val Loss=0.3471, lr=0.0100
[02/21 08:13:54 cifar10-global-group_sl-3.0-resnet56]: Epoch 28/100, Acc=0.8805, Val Loss=0.3992, lr=0.0100
[02/21 08:14:28 cifar10-global-group_sl-3.0-resnet56]: Epoch 29/100, Acc=0.8867, Val Loss=0.3877, lr=0.0100
[02/21 08:15:01 cifar10-global-group_sl-3.0-resnet56]: Epoch 30/100, Acc=0.8904, Val Loss=0.3661, lr=0.0100
[02/21 08:15:34 cifar10-global-group_sl-3.0-resnet56]: Epoch 31/100, Acc=0.8977, Val Loss=0.3523, lr=0.0100
[02/21 08:16:07 cifar10-global-group_sl-3.0-resnet56]: Epoch 32/100, Acc=0.8907, Val Loss=0.3669, lr=0.0100
[02/21 08:16:42 cifar10-global-group_sl-3.0-resnet56]: Epoch 33/100, Acc=0.8827, Val Loss=0.4030, lr=0.0100
[02/21 08:17:17 cifar10-global-group_sl-3.0-resnet56]: Epoch 34/100, Acc=0.8832, Val Loss=0.3813, lr=0.0100
[02/21 08:17:50 cifar10-global-group_sl-3.0-resnet56]: Epoch 35/100, Acc=0.8904, Val Loss=0.3641, lr=0.0100
[02/21 08:18:23 cifar10-global-group_sl-3.0-resnet56]: Epoch 36/100, Acc=0.8884, Val Loss=0.3629, lr=0.0100
[02/21 08:18:56 cifar10-global-group_sl-3.0-resnet56]: Epoch 37/100, Acc=0.8977, Val Loss=0.3525, lr=0.0100
[02/21 08:19:29 cifar10-global-group_sl-3.0-resnet56]: Epoch 38/100, Acc=0.8845, Val Loss=0.4165, lr=0.0100
[02/21 08:20:02 cifar10-global-group_sl-3.0-resnet56]: Epoch 39/100, Acc=0.8983, Val Loss=0.3398, lr=0.0100
[02/21 08:20:36 cifar10-global-group_sl-3.0-resnet56]: Epoch 40/100, Acc=0.8972, Val Loss=0.3403, lr=0.0100
[02/21 08:21:09 cifar10-global-group_sl-3.0-resnet56]: Epoch 41/100, Acc=0.8828, Val Loss=0.4088, lr=0.0100
[02/21 08:21:43 cifar10-global-group_sl-3.0-resnet56]: Epoch 42/100, Acc=0.8968, Val Loss=0.3536, lr=0.0100
[02/21 08:22:16 cifar10-global-group_sl-3.0-resnet56]: Epoch 43/100, Acc=0.9024, Val Loss=0.3319, lr=0.0100
[02/21 08:22:50 cifar10-global-group_sl-3.0-resnet56]: Epoch 44/100, Acc=0.8970, Val Loss=0.3472, lr=0.0100
[02/21 08:23:24 cifar10-global-group_sl-3.0-resnet56]: Epoch 45/100, Acc=0.8917, Val Loss=0.3784, lr=0.0100
[02/21 08:23:59 cifar10-global-group_sl-3.0-resnet56]: Epoch 46/100, Acc=0.8942, Val Loss=0.3687, lr=0.0100
[02/21 08:24:33 cifar10-global-group_sl-3.0-resnet56]: Epoch 47/100, Acc=0.8974, Val Loss=0.3474, lr=0.0100
[02/21 08:25:07 cifar10-global-group_sl-3.0-resnet56]: Epoch 48/100, Acc=0.8962, Val Loss=0.3613, lr=0.0100
[02/21 08:25:41 cifar10-global-group_sl-3.0-resnet56]: Epoch 49/100, Acc=0.8902, Val Loss=0.3720, lr=0.0100
[02/21 08:26:15 cifar10-global-group_sl-3.0-resnet56]: Epoch 50/100, Acc=0.8940, Val Loss=0.3659, lr=0.0100
[02/21 08:26:48 cifar10-global-group_sl-3.0-resnet56]: Epoch 51/100, Acc=0.8849, Val Loss=0.4062, lr=0.0100
[02/21 08:27:22 cifar10-global-group_sl-3.0-resnet56]: Epoch 52/100, Acc=0.9014, Val Loss=0.3223, lr=0.0100
[02/21 08:27:56 cifar10-global-group_sl-3.0-resnet56]: Epoch 53/100, Acc=0.8865, Val Loss=0.3718, lr=0.0100
[02/21 08:28:30 cifar10-global-group_sl-3.0-resnet56]: Epoch 54/100, Acc=0.8838, Val Loss=0.4001, lr=0.0100
[02/21 08:29:04 cifar10-global-group_sl-3.0-resnet56]: Epoch 55/100, Acc=0.9009, Val Loss=0.3340, lr=0.0100
[02/21 08:29:37 cifar10-global-group_sl-3.0-resnet56]: Epoch 56/100, Acc=0.8827, Val Loss=0.4077, lr=0.0100
[02/21 08:30:11 cifar10-global-group_sl-3.0-resnet56]: Epoch 57/100, Acc=0.8886, Val Loss=0.3732, lr=0.0100
[02/21 08:30:44 cifar10-global-group_sl-3.0-resnet56]: Epoch 58/100, Acc=0.9011, Val Loss=0.3459, lr=0.0100
[02/21 08:31:17 cifar10-global-group_sl-3.0-resnet56]: Epoch 59/100, Acc=0.8838, Val Loss=0.4013, lr=0.0100
[02/21 08:31:50 cifar10-global-group_sl-3.0-resnet56]: Epoch 60/100, Acc=0.9185, Val Loss=0.2737, lr=0.0010
[02/21 08:32:23 cifar10-global-group_sl-3.0-resnet56]: Epoch 61/100, Acc=0.9217, Val Loss=0.2694, lr=0.0010
[02/21 08:32:57 cifar10-global-group_sl-3.0-resnet56]: Epoch 62/100, Acc=0.9217, Val Loss=0.2748, lr=0.0010
[02/21 08:33:31 cifar10-global-group_sl-3.0-resnet56]: Epoch 63/100, Acc=0.9239, Val Loss=0.2708, lr=0.0010
[02/21 08:34:04 cifar10-global-group_sl-3.0-resnet56]: Epoch 64/100, Acc=0.9238, Val Loss=0.2729, lr=0.0010
[02/21 08:34:38 cifar10-global-group_sl-3.0-resnet56]: Epoch 65/100, Acc=0.9243, Val Loss=0.2733, lr=0.0010
[02/21 08:35:12 cifar10-global-group_sl-3.0-resnet56]: Epoch 66/100, Acc=0.9228, Val Loss=0.2768, lr=0.0010
[02/21 08:35:46 cifar10-global-group_sl-3.0-resnet56]: Epoch 67/100, Acc=0.9250, Val Loss=0.2733, lr=0.0010
[02/21 08:36:20 cifar10-global-group_sl-3.0-resnet56]: Epoch 68/100, Acc=0.9239, Val Loss=0.2773, lr=0.0010
[02/21 08:36:54 cifar10-global-group_sl-3.0-resnet56]: Epoch 69/100, Acc=0.9239, Val Loss=0.2792, lr=0.0010
[02/21 08:37:27 cifar10-global-group_sl-3.0-resnet56]: Epoch 70/100, Acc=0.9244, Val Loss=0.2790, lr=0.0010
[02/21 08:38:01 cifar10-global-group_sl-3.0-resnet56]: Epoch 71/100, Acc=0.9254, Val Loss=0.2821, lr=0.0010
[02/21 08:38:36 cifar10-global-group_sl-3.0-resnet56]: Epoch 72/100, Acc=0.9252, Val Loss=0.2821, lr=0.0010
[02/21 08:39:10 cifar10-global-group_sl-3.0-resnet56]: Epoch 73/100, Acc=0.9230, Val Loss=0.2848, lr=0.0010
[02/21 08:39:43 cifar10-global-group_sl-3.0-resnet56]: Epoch 74/100, Acc=0.9232, Val Loss=0.2875, lr=0.0010
[02/21 08:40:18 cifar10-global-group_sl-3.0-resnet56]: Epoch 75/100, Acc=0.9244, Val Loss=0.2857, lr=0.0010
[02/21 08:40:52 cifar10-global-group_sl-3.0-resnet56]: Epoch 76/100, Acc=0.9235, Val Loss=0.2906, lr=0.0010
[02/21 08:41:26 cifar10-global-group_sl-3.0-resnet56]: Epoch 77/100, Acc=0.9223, Val Loss=0.2938, lr=0.0010
[02/21 08:42:00 cifar10-global-group_sl-3.0-resnet56]: Epoch 78/100, Acc=0.9242, Val Loss=0.2932, lr=0.0010
[02/21 08:42:34 cifar10-global-group_sl-3.0-resnet56]: Epoch 79/100, Acc=0.9248, Val Loss=0.2914, lr=0.0010
[02/21 08:43:07 cifar10-global-group_sl-3.0-resnet56]: Epoch 80/100, Acc=0.9241, Val Loss=0.2912, lr=0.0001
[02/21 08:43:40 cifar10-global-group_sl-3.0-resnet56]: Epoch 81/100, Acc=0.9243, Val Loss=0.2900, lr=0.0001
[02/21 08:44:14 cifar10-global-group_sl-3.0-resnet56]: Epoch 82/100, Acc=0.9253, Val Loss=0.2896, lr=0.0001
[02/21 08:44:47 cifar10-global-group_sl-3.0-resnet56]: Epoch 83/100, Acc=0.9251, Val Loss=0.2900, lr=0.0001
[02/21 08:45:20 cifar10-global-group_sl-3.0-resnet56]: Epoch 84/100, Acc=0.9243, Val Loss=0.2890, lr=0.0001
[02/21 08:45:54 cifar10-global-group_sl-3.0-resnet56]: Epoch 85/100, Acc=0.9243, Val Loss=0.2896, lr=0.0001
[02/21 08:46:27 cifar10-global-group_sl-3.0-resnet56]: Epoch 86/100, Acc=0.9235, Val Loss=0.2894, lr=0.0001
[02/21 08:47:00 cifar10-global-group_sl-3.0-resnet56]: Epoch 87/100, Acc=0.9248, Val Loss=0.2877, lr=0.0001
[02/21 08:47:33 cifar10-global-group_sl-3.0-resnet56]: Epoch 88/100, Acc=0.9253, Val Loss=0.2879, lr=0.0001
[02/21 08:48:06 cifar10-global-group_sl-3.0-resnet56]: Epoch 89/100, Acc=0.9242, Val Loss=0.2889, lr=0.0001
[02/21 08:48:40 cifar10-global-group_sl-3.0-resnet56]: Epoch 90/100, Acc=0.9246, Val Loss=0.2899, lr=0.0001
[02/21 08:49:13 cifar10-global-group_sl-3.0-resnet56]: Epoch 91/100, Acc=0.9246, Val Loss=0.2904, lr=0.0001
[02/21 08:49:47 cifar10-global-group_sl-3.0-resnet56]: Epoch 92/100, Acc=0.9251, Val Loss=0.2898, lr=0.0001
[02/21 08:50:20 cifar10-global-group_sl-3.0-resnet56]: Epoch 93/100, Acc=0.9243, Val Loss=0.2904, lr=0.0001
[02/21 08:50:54 cifar10-global-group_sl-3.0-resnet56]: Epoch 94/100, Acc=0.9244, Val Loss=0.2906, lr=0.0001
[02/21 08:51:27 cifar10-global-group_sl-3.0-resnet56]: Epoch 95/100, Acc=0.9244, Val Loss=0.2917, lr=0.0001
[02/21 08:52:00 cifar10-global-group_sl-3.0-resnet56]: Epoch 96/100, Acc=0.9242, Val Loss=0.2906, lr=0.0001
[02/21 08:52:34 cifar10-global-group_sl-3.0-resnet56]: Epoch 97/100, Acc=0.9244, Val Loss=0.2914, lr=0.0001
[02/21 08:53:07 cifar10-global-group_sl-3.0-resnet56]: Epoch 98/100, Acc=0.9242, Val Loss=0.2915, lr=0.0001
[02/21 08:53:40 cifar10-global-group_sl-3.0-resnet56]: Epoch 99/100, Acc=0.9247, Val Loss=0.2940, lr=0.0001
[02/21 08:53:40 cifar10-global-group_sl-3.0-resnet56]: Best Acc=0.9254
[02/21 08:53:40 cifar10-global-group_sl-3.0-resnet56]: Params: 0.36 M
[02/21 08:53:40 cifar10-global-group_sl-3.0-resnet56]: ops: 42.36 M
[02/21 08:53:44 cifar10-global-group_sl-3.0-resnet56]: Acc: 0.9247 Val Loss: 0.2940

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: mode: prune
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: model: resnet56
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: verbose: False
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: dataset: cifar10
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: dataroot: data
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: batch_size: 128
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: total_epochs: 100
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: lr_decay_milestones: 60,80
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: lr_decay_gamma: 0.1
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: lr: 0.01
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: output_dir: run/cifar10/prune/cifar10-global-proj-3.0-resnet56
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: finetune: True
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: last_epochs: 100
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: reps: 1
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: method: proj
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: speed_up: 3.0
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: max_pruning_ratio: 1.0
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: reg: 1e-05
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: delta_reg: 0.0001
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: weight_decay: 0.0005
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: seed: 1
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: global_pruning: True
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: sl_total_epochs: 100
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: sl_lr: 0.01
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: sl_reg_warmup: 0
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: sl_restore: None
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: iterative_steps: 400
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: logger: <Logger cifar10-global-proj-3.0-resnet56 (DEBUG)>
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: device: cuda
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: num_classes: 10
[02/21 08:53:51 cifar10-global-proj-3.0-resnet56]: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 08:53:55 cifar10-global-proj-3.0-resnet56]: Pruning...
[02/21 08:56:56 cifar10-global-proj-3.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(8, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(8, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(8, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(8, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(8, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(8, 19, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(8, 28, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(28, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(28, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(28, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(28, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(28, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(28, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(28, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(28, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(28, 60, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(28, 56, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(56, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(51, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(56, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(40, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(56, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(41, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(56, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(44, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(56, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(35, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(56, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(56, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(56, 30, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(30, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=56, out_features=10, bias=True)
)
[02/21 08:56:59 cifar10-global-proj-3.0-resnet56]: Params: 0.86 M => 0.39 M (45.44%)
[02/21 08:56:59 cifar10-global-proj-3.0-resnet56]: FLOPs: 127.12 M => 41.96 M (33.01%, 3.03X )
[02/21 08:56:59 cifar10-global-proj-3.0-resnet56]: Acc: 0.9392 => 0.1053
[02/21 08:56:59 cifar10-global-proj-3.0-resnet56]: Val Loss: 0.2587 => 8.3454
[02/21 08:56:59 cifar10-global-proj-3.0-resnet56]: Finetuning...
[02/21 08:57:32 cifar10-global-proj-3.0-resnet56]: Epoch 0/100, Acc=0.8381, Val Loss=0.5072, lr=0.0100
[02/21 08:58:03 cifar10-global-proj-3.0-resnet56]: Epoch 1/100, Acc=0.8699, Val Loss=0.4013, lr=0.0100
[02/21 08:58:36 cifar10-global-proj-3.0-resnet56]: Epoch 2/100, Acc=0.8818, Val Loss=0.3703, lr=0.0100
[02/21 08:59:08 cifar10-global-proj-3.0-resnet56]: Epoch 3/100, Acc=0.8774, Val Loss=0.3862, lr=0.0100
[02/21 08:59:39 cifar10-global-proj-3.0-resnet56]: Epoch 4/100, Acc=0.8731, Val Loss=0.3941, lr=0.0100
[02/21 09:00:11 cifar10-global-proj-3.0-resnet56]: Epoch 5/100, Acc=0.8748, Val Loss=0.4117, lr=0.0100
[02/21 09:00:43 cifar10-global-proj-3.0-resnet56]: Epoch 6/100, Acc=0.8888, Val Loss=0.3471, lr=0.0100
[02/21 09:01:15 cifar10-global-proj-3.0-resnet56]: Epoch 7/100, Acc=0.8636, Val Loss=0.4493, lr=0.0100
[02/21 09:01:47 cifar10-global-proj-3.0-resnet56]: Epoch 8/100, Acc=0.8913, Val Loss=0.3403, lr=0.0100
[02/21 09:02:20 cifar10-global-proj-3.0-resnet56]: Epoch 9/100, Acc=0.8804, Val Loss=0.3753, lr=0.0100
[02/21 09:02:52 cifar10-global-proj-3.0-resnet56]: Epoch 10/100, Acc=0.8676, Val Loss=0.4370, lr=0.0100
[02/21 09:03:24 cifar10-global-proj-3.0-resnet56]: Epoch 11/100, Acc=0.8797, Val Loss=0.3846, lr=0.0100
[02/21 09:03:57 cifar10-global-proj-3.0-resnet56]: Epoch 12/100, Acc=0.8849, Val Loss=0.3535, lr=0.0100
[02/21 09:04:30 cifar10-global-proj-3.0-resnet56]: Epoch 13/100, Acc=0.8827, Val Loss=0.3759, lr=0.0100
[02/21 09:05:03 cifar10-global-proj-3.0-resnet56]: Epoch 14/100, Acc=0.8834, Val Loss=0.3674, lr=0.0100
[02/21 09:05:36 cifar10-global-proj-3.0-resnet56]: Epoch 15/100, Acc=0.8980, Val Loss=0.3404, lr=0.0100
[02/21 09:06:09 cifar10-global-proj-3.0-resnet56]: Epoch 16/100, Acc=0.8803, Val Loss=0.3680, lr=0.0100
[02/21 09:06:41 cifar10-global-proj-3.0-resnet56]: Epoch 17/100, Acc=0.9006, Val Loss=0.3108, lr=0.0100
[02/21 09:07:13 cifar10-global-proj-3.0-resnet56]: Epoch 18/100, Acc=0.8881, Val Loss=0.3628, lr=0.0100
[02/21 09:07:45 cifar10-global-proj-3.0-resnet56]: Epoch 19/100, Acc=0.8907, Val Loss=0.3606, lr=0.0100
[02/21 09:08:17 cifar10-global-proj-3.0-resnet56]: Epoch 20/100, Acc=0.8971, Val Loss=0.3395, lr=0.0100
[02/21 09:08:49 cifar10-global-proj-3.0-resnet56]: Epoch 21/100, Acc=0.8941, Val Loss=0.3494, lr=0.0100
[02/21 09:09:22 cifar10-global-proj-3.0-resnet56]: Epoch 22/100, Acc=0.8872, Val Loss=0.3769, lr=0.0100
[02/21 09:09:54 cifar10-global-proj-3.0-resnet56]: Epoch 23/100, Acc=0.8948, Val Loss=0.3451, lr=0.0100
[02/21 09:10:26 cifar10-global-proj-3.0-resnet56]: Epoch 24/100, Acc=0.8901, Val Loss=0.3674, lr=0.0100
[02/21 09:10:58 cifar10-global-proj-3.0-resnet56]: Epoch 25/100, Acc=0.8943, Val Loss=0.3579, lr=0.0100
[02/21 09:11:29 cifar10-global-proj-3.0-resnet56]: Epoch 26/100, Acc=0.8888, Val Loss=0.3480, lr=0.0100
[02/21 09:12:01 cifar10-global-proj-3.0-resnet56]: Epoch 27/100, Acc=0.8897, Val Loss=0.3687, lr=0.0100
[02/21 09:12:33 cifar10-global-proj-3.0-resnet56]: Epoch 28/100, Acc=0.8935, Val Loss=0.3415, lr=0.0100
[02/21 09:13:05 cifar10-global-proj-3.0-resnet56]: Epoch 29/100, Acc=0.8972, Val Loss=0.3348, lr=0.0100
[02/21 09:13:37 cifar10-global-proj-3.0-resnet56]: Epoch 30/100, Acc=0.8925, Val Loss=0.3683, lr=0.0100
[02/21 09:14:10 cifar10-global-proj-3.0-resnet56]: Epoch 31/100, Acc=0.8972, Val Loss=0.3481, lr=0.0100
[02/21 09:14:41 cifar10-global-proj-3.0-resnet56]: Epoch 32/100, Acc=0.9017, Val Loss=0.3192, lr=0.0100
[02/21 09:15:14 cifar10-global-proj-3.0-resnet56]: Epoch 33/100, Acc=0.8914, Val Loss=0.3633, lr=0.0100
[02/21 09:15:46 cifar10-global-proj-3.0-resnet56]: Epoch 34/100, Acc=0.8804, Val Loss=0.4057, lr=0.0100
[02/21 09:16:18 cifar10-global-proj-3.0-resnet56]: Epoch 35/100, Acc=0.8879, Val Loss=0.3804, lr=0.0100
[02/21 09:16:50 cifar10-global-proj-3.0-resnet56]: Epoch 36/100, Acc=0.8956, Val Loss=0.3573, lr=0.0100
[02/21 09:17:22 cifar10-global-proj-3.0-resnet56]: Epoch 37/100, Acc=0.8984, Val Loss=0.3484, lr=0.0100
[02/21 09:17:53 cifar10-global-proj-3.0-resnet56]: Epoch 38/100, Acc=0.9005, Val Loss=0.3192, lr=0.0100
[02/21 09:18:25 cifar10-global-proj-3.0-resnet56]: Epoch 39/100, Acc=0.8900, Val Loss=0.3757, lr=0.0100
[02/21 09:18:57 cifar10-global-proj-3.0-resnet56]: Epoch 40/100, Acc=0.8924, Val Loss=0.3633, lr=0.0100
[02/21 09:19:29 cifar10-global-proj-3.0-resnet56]: Epoch 41/100, Acc=0.8962, Val Loss=0.3545, lr=0.0100
[02/21 09:20:01 cifar10-global-proj-3.0-resnet56]: Epoch 42/100, Acc=0.8979, Val Loss=0.3601, lr=0.0100
[02/21 09:20:32 cifar10-global-proj-3.0-resnet56]: Epoch 43/100, Acc=0.8953, Val Loss=0.3639, lr=0.0100
[02/21 09:21:04 cifar10-global-proj-3.0-resnet56]: Epoch 44/100, Acc=0.8916, Val Loss=0.3592, lr=0.0100
[02/21 09:21:37 cifar10-global-proj-3.0-resnet56]: Epoch 45/100, Acc=0.8894, Val Loss=0.3866, lr=0.0100
[02/21 09:22:08 cifar10-global-proj-3.0-resnet56]: Epoch 46/100, Acc=0.8879, Val Loss=0.3688, lr=0.0100
[02/21 09:22:40 cifar10-global-proj-3.0-resnet56]: Epoch 47/100, Acc=0.9022, Val Loss=0.3388, lr=0.0100
[02/21 09:23:12 cifar10-global-proj-3.0-resnet56]: Epoch 48/100, Acc=0.8990, Val Loss=0.3477, lr=0.0100
[02/21 09:23:43 cifar10-global-proj-3.0-resnet56]: Epoch 49/100, Acc=0.8997, Val Loss=0.3401, lr=0.0100
[02/21 09:24:15 cifar10-global-proj-3.0-resnet56]: Epoch 50/100, Acc=0.8901, Val Loss=0.3779, lr=0.0100
[02/21 09:24:47 cifar10-global-proj-3.0-resnet56]: Epoch 51/100, Acc=0.8995, Val Loss=0.3305, lr=0.0100
[02/21 09:25:19 cifar10-global-proj-3.0-resnet56]: Epoch 52/100, Acc=0.8794, Val Loss=0.4328, lr=0.0100
[02/21 09:25:51 cifar10-global-proj-3.0-resnet56]: Epoch 53/100, Acc=0.8957, Val Loss=0.3461, lr=0.0100
[02/21 09:26:23 cifar10-global-proj-3.0-resnet56]: Epoch 54/100, Acc=0.8894, Val Loss=0.3833, lr=0.0100
[02/21 09:26:56 cifar10-global-proj-3.0-resnet56]: Epoch 55/100, Acc=0.8976, Val Loss=0.3558, lr=0.0100
[02/21 09:27:28 cifar10-global-proj-3.0-resnet56]: Epoch 56/100, Acc=0.8984, Val Loss=0.3378, lr=0.0100
[02/21 09:28:00 cifar10-global-proj-3.0-resnet56]: Epoch 57/100, Acc=0.8953, Val Loss=0.3773, lr=0.0100
[02/21 09:28:32 cifar10-global-proj-3.0-resnet56]: Epoch 58/100, Acc=0.8970, Val Loss=0.3569, lr=0.0100
[02/21 09:29:04 cifar10-global-proj-3.0-resnet56]: Epoch 59/100, Acc=0.8950, Val Loss=0.3507, lr=0.0100
[02/21 09:29:36 cifar10-global-proj-3.0-resnet56]: Epoch 60/100, Acc=0.9217, Val Loss=0.2608, lr=0.0010
[02/21 09:30:09 cifar10-global-proj-3.0-resnet56]: Epoch 61/100, Acc=0.9256, Val Loss=0.2544, lr=0.0010
[02/21 09:30:41 cifar10-global-proj-3.0-resnet56]: Epoch 62/100, Acc=0.9269, Val Loss=0.2555, lr=0.0010
[02/21 09:31:13 cifar10-global-proj-3.0-resnet56]: Epoch 63/100, Acc=0.9274, Val Loss=0.2565, lr=0.0010
[02/21 09:31:45 cifar10-global-proj-3.0-resnet56]: Epoch 64/100, Acc=0.9277, Val Loss=0.2543, lr=0.0010
[02/21 09:32:17 cifar10-global-proj-3.0-resnet56]: Epoch 65/100, Acc=0.9282, Val Loss=0.2574, lr=0.0010
[02/21 09:32:50 cifar10-global-proj-3.0-resnet56]: Epoch 66/100, Acc=0.9302, Val Loss=0.2597, lr=0.0010
[02/21 09:33:22 cifar10-global-proj-3.0-resnet56]: Epoch 67/100, Acc=0.9301, Val Loss=0.2595, lr=0.0010
[02/21 09:33:55 cifar10-global-proj-3.0-resnet56]: Epoch 68/100, Acc=0.9296, Val Loss=0.2621, lr=0.0010
[02/21 09:34:27 cifar10-global-proj-3.0-resnet56]: Epoch 69/100, Acc=0.9294, Val Loss=0.2644, lr=0.0010
[02/21 09:34:59 cifar10-global-proj-3.0-resnet56]: Epoch 70/100, Acc=0.9311, Val Loss=0.2614, lr=0.0010
[02/21 09:35:32 cifar10-global-proj-3.0-resnet56]: Epoch 71/100, Acc=0.9299, Val Loss=0.2643, lr=0.0010
[02/21 09:36:03 cifar10-global-proj-3.0-resnet56]: Epoch 72/100, Acc=0.9293, Val Loss=0.2678, lr=0.0010
[02/21 09:36:35 cifar10-global-proj-3.0-resnet56]: Epoch 73/100, Acc=0.9284, Val Loss=0.2671, lr=0.0010
[02/21 09:37:07 cifar10-global-proj-3.0-resnet56]: Epoch 74/100, Acc=0.9302, Val Loss=0.2730, lr=0.0010
[02/21 09:37:39 cifar10-global-proj-3.0-resnet56]: Epoch 75/100, Acc=0.9295, Val Loss=0.2686, lr=0.0010
[02/21 09:38:12 cifar10-global-proj-3.0-resnet56]: Epoch 76/100, Acc=0.9285, Val Loss=0.2732, lr=0.0010
[02/21 09:38:44 cifar10-global-proj-3.0-resnet56]: Epoch 77/100, Acc=0.9275, Val Loss=0.2773, lr=0.0010
[02/21 09:39:16 cifar10-global-proj-3.0-resnet56]: Epoch 78/100, Acc=0.9304, Val Loss=0.2706, lr=0.0010
[02/21 09:39:49 cifar10-global-proj-3.0-resnet56]: Epoch 79/100, Acc=0.9301, Val Loss=0.2721, lr=0.0010
[02/21 09:40:21 cifar10-global-proj-3.0-resnet56]: Epoch 80/100, Acc=0.9309, Val Loss=0.2731, lr=0.0001
[02/21 09:40:54 cifar10-global-proj-3.0-resnet56]: Epoch 81/100, Acc=0.9315, Val Loss=0.2700, lr=0.0001
[02/21 09:41:27 cifar10-global-proj-3.0-resnet56]: Epoch 82/100, Acc=0.9306, Val Loss=0.2708, lr=0.0001
[02/21 09:41:59 cifar10-global-proj-3.0-resnet56]: Epoch 83/100, Acc=0.9313, Val Loss=0.2720, lr=0.0001
[02/21 09:42:32 cifar10-global-proj-3.0-resnet56]: Epoch 84/100, Acc=0.9310, Val Loss=0.2722, lr=0.0001
[02/21 09:43:04 cifar10-global-proj-3.0-resnet56]: Epoch 85/100, Acc=0.9317, Val Loss=0.2722, lr=0.0001
[02/21 09:43:37 cifar10-global-proj-3.0-resnet56]: Epoch 86/100, Acc=0.9311, Val Loss=0.2706, lr=0.0001
[02/21 09:44:10 cifar10-global-proj-3.0-resnet56]: Epoch 87/100, Acc=0.9306, Val Loss=0.2734, lr=0.0001
[02/21 09:44:42 cifar10-global-proj-3.0-resnet56]: Epoch 88/100, Acc=0.9315, Val Loss=0.2698, lr=0.0001
[02/21 09:45:15 cifar10-global-proj-3.0-resnet56]: Epoch 89/100, Acc=0.9321, Val Loss=0.2693, lr=0.0001
[02/21 09:45:47 cifar10-global-proj-3.0-resnet56]: Epoch 90/100, Acc=0.9310, Val Loss=0.2721, lr=0.0001
[02/21 09:46:20 cifar10-global-proj-3.0-resnet56]: Epoch 91/100, Acc=0.9317, Val Loss=0.2704, lr=0.0001
[02/21 09:46:52 cifar10-global-proj-3.0-resnet56]: Epoch 92/100, Acc=0.9319, Val Loss=0.2729, lr=0.0001
[02/21 09:47:25 cifar10-global-proj-3.0-resnet56]: Epoch 93/100, Acc=0.9320, Val Loss=0.2707, lr=0.0001
[02/21 09:47:57 cifar10-global-proj-3.0-resnet56]: Epoch 94/100, Acc=0.9318, Val Loss=0.2715, lr=0.0001
[02/21 09:48:30 cifar10-global-proj-3.0-resnet56]: Epoch 95/100, Acc=0.9308, Val Loss=0.2731, lr=0.0001
[02/21 09:49:02 cifar10-global-proj-3.0-resnet56]: Epoch 96/100, Acc=0.9326, Val Loss=0.2718, lr=0.0001
[02/21 09:49:34 cifar10-global-proj-3.0-resnet56]: Epoch 97/100, Acc=0.9313, Val Loss=0.2729, lr=0.0001
[02/21 09:50:06 cifar10-global-proj-3.0-resnet56]: Epoch 98/100, Acc=0.9310, Val Loss=0.2727, lr=0.0001
[02/21 09:50:38 cifar10-global-proj-3.0-resnet56]: Epoch 99/100, Acc=0.9310, Val Loss=0.2726, lr=0.0001
[02/21 09:50:38 cifar10-global-proj-3.0-resnet56]: Best Acc=0.9326
[02/21 09:50:38 cifar10-global-proj-3.0-resnet56]: Params: 0.39 M
[02/21 09:50:38 cifar10-global-proj-3.0-resnet56]: ops: 41.96 M
[02/21 09:50:42 cifar10-global-proj-3.0-resnet56]: Acc: 0.9310 Val Loss: 0.2726

/home/REDACTED/paper1/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: mode: prune
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: model: resnet56
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: verbose: False
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: dataset: cifar10
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: dataroot: data
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: batch_size: 128
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: total_epochs: 100
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: lr_decay_milestones: 60,80
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: lr_decay_gamma: 0.1
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: lr: 0.01
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: output_dir: run/cifar10/prune/cifar10-global-proj_sl-3.0-resnet56
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: finetune: True
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: last_epochs: 100
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: reps: 1
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: method: proj_sl
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: speed_up: 3.0
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: max_pruning_ratio: 1.0
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: soft_keeping_ratio: 0.0
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: reg: 1e-05
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: delta_reg: 0.0001
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: weight_decay: 0.0005
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: seed: 1
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: global_pruning: True
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: sl_total_epochs: 100
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: sl_lr: 0.01
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: sl_lr_decay_milestones: 60,80
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: sl_reg_warmup: 0
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: sl_restore: None
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: iterative_steps: 400
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: logger: <Logger cifar10-global-proj_sl-3.0-resnet56 (DEBUG)>
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: device: cuda
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: num_classes: 10
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 09:50:50 cifar10-global-proj_sl-3.0-resnet56]: Regularizing...
[02/21 09:58:20 cifar10-global-proj_sl-3.0-resnet56]: Epoch 0/100, Acc=0.9056, Val Loss=0.3218, lr=0.0100
[02/21 10:05:57 cifar10-global-proj_sl-3.0-resnet56]: Epoch 1/100, Acc=0.9100, Val Loss=0.3296, lr=0.0100
[02/21 10:13:30 cifar10-global-proj_sl-3.0-resnet56]: Epoch 2/100, Acc=0.9058, Val Loss=0.3340, lr=0.0100
[02/21 10:21:04 cifar10-global-proj_sl-3.0-resnet56]: Epoch 3/100, Acc=0.9130, Val Loss=0.3144, lr=0.0100
[02/21 10:28:37 cifar10-global-proj_sl-3.0-resnet56]: Epoch 4/100, Acc=0.9023, Val Loss=0.3850, lr=0.0100
[02/21 10:36:13 cifar10-global-proj_sl-3.0-resnet56]: Epoch 5/100, Acc=0.9123, Val Loss=0.3292, lr=0.0100
[02/21 10:43:49 cifar10-global-proj_sl-3.0-resnet56]: Epoch 6/100, Acc=0.8975, Val Loss=0.3929, lr=0.0100
[02/21 10:51:20 cifar10-global-proj_sl-3.0-resnet56]: Epoch 7/100, Acc=0.9000, Val Loss=0.3940, lr=0.0100
[02/21 10:58:50 cifar10-global-proj_sl-3.0-resnet56]: Epoch 8/100, Acc=0.9180, Val Loss=0.3110, lr=0.0100
[02/21 11:06:21 cifar10-global-proj_sl-3.0-resnet56]: Epoch 9/100, Acc=0.9180, Val Loss=0.3167, lr=0.0100
[02/21 11:13:50 cifar10-global-proj_sl-3.0-resnet56]: Epoch 10/100, Acc=0.9194, Val Loss=0.3120, lr=0.0100
[02/21 11:21:23 cifar10-global-proj_sl-3.0-resnet56]: Epoch 11/100, Acc=0.9148, Val Loss=0.3353, lr=0.0100
[02/21 11:28:57 cifar10-global-proj_sl-3.0-resnet56]: Epoch 12/100, Acc=0.9210, Val Loss=0.3050, lr=0.0100
[02/21 11:36:34 cifar10-global-proj_sl-3.0-resnet56]: Epoch 13/100, Acc=0.9180, Val Loss=0.3207, lr=0.0100
[02/21 11:44:08 cifar10-global-proj_sl-3.0-resnet56]: Epoch 14/100, Acc=0.9198, Val Loss=0.3229, lr=0.0100
[02/21 11:51:45 cifar10-global-proj_sl-3.0-resnet56]: Epoch 15/100, Acc=0.9161, Val Loss=0.3364, lr=0.0100
[02/21 11:59:16 cifar10-global-proj_sl-3.0-resnet56]: Epoch 16/100, Acc=0.9215, Val Loss=0.3250, lr=0.0100
[02/21 12:06:48 cifar10-global-proj_sl-3.0-resnet56]: Epoch 17/100, Acc=0.9174, Val Loss=0.3314, lr=0.0100
[02/21 12:14:22 cifar10-global-proj_sl-3.0-resnet56]: Epoch 18/100, Acc=0.9220, Val Loss=0.3067, lr=0.0100
[02/21 12:21:51 cifar10-global-proj_sl-3.0-resnet56]: Epoch 19/100, Acc=0.9182, Val Loss=0.3409, lr=0.0100
[02/21 12:29:20 cifar10-global-proj_sl-3.0-resnet56]: Epoch 20/100, Acc=0.9215, Val Loss=0.3118, lr=0.0100
[02/21 12:36:52 cifar10-global-proj_sl-3.0-resnet56]: Epoch 21/100, Acc=0.9198, Val Loss=0.3344, lr=0.0100
[02/21 12:44:26 cifar10-global-proj_sl-3.0-resnet56]: Epoch 22/100, Acc=0.9240, Val Loss=0.3254, lr=0.0100
[02/21 12:51:59 cifar10-global-proj_sl-3.0-resnet56]: Epoch 23/100, Acc=0.9232, Val Loss=0.3203, lr=0.0100
[02/21 12:59:30 cifar10-global-proj_sl-3.0-resnet56]: Epoch 24/100, Acc=0.9259, Val Loss=0.3220, lr=0.0100
[02/21 13:07:07 cifar10-global-proj_sl-3.0-resnet56]: Epoch 25/100, Acc=0.9232, Val Loss=0.3240, lr=0.0100
[02/21 13:14:45 cifar10-global-proj_sl-3.0-resnet56]: Epoch 26/100, Acc=0.9216, Val Loss=0.3296, lr=0.0100
[02/21 13:22:16 cifar10-global-proj_sl-3.0-resnet56]: Epoch 27/100, Acc=0.9200, Val Loss=0.3308, lr=0.0100
[02/21 13:29:45 cifar10-global-proj_sl-3.0-resnet56]: Epoch 28/100, Acc=0.9184, Val Loss=0.3307, lr=0.0100
[02/21 13:37:17 cifar10-global-proj_sl-3.0-resnet56]: Epoch 29/100, Acc=0.9268, Val Loss=0.3127, lr=0.0100
[02/21 13:44:48 cifar10-global-proj_sl-3.0-resnet56]: Epoch 30/100, Acc=0.9167, Val Loss=0.3670, lr=0.0100
[02/21 13:52:23 cifar10-global-proj_sl-3.0-resnet56]: Epoch 31/100, Acc=0.9227, Val Loss=0.3158, lr=0.0100
[02/21 13:59:56 cifar10-global-proj_sl-3.0-resnet56]: Epoch 32/100, Acc=0.9244, Val Loss=0.3285, lr=0.0100
[02/21 14:07:30 cifar10-global-proj_sl-3.0-resnet56]: Epoch 33/100, Acc=0.9265, Val Loss=0.3245, lr=0.0100
[02/21 14:15:05 cifar10-global-proj_sl-3.0-resnet56]: Epoch 34/100, Acc=0.9264, Val Loss=0.3243, lr=0.0100
[02/21 14:22:40 cifar10-global-proj_sl-3.0-resnet56]: Epoch 35/100, Acc=0.9266, Val Loss=0.3244, lr=0.0100
[02/21 14:30:18 cifar10-global-proj_sl-3.0-resnet56]: Epoch 36/100, Acc=0.9247, Val Loss=0.3299, lr=0.0100
[02/21 14:37:53 cifar10-global-proj_sl-3.0-resnet56]: Epoch 37/100, Acc=0.9199, Val Loss=0.3493, lr=0.0100
[02/21 14:45:30 cifar10-global-proj_sl-3.0-resnet56]: Epoch 38/100, Acc=0.9237, Val Loss=0.3336, lr=0.0100
[02/21 14:53:09 cifar10-global-proj_sl-3.0-resnet56]: Epoch 39/100, Acc=0.9225, Val Loss=0.3438, lr=0.0100
[02/21 15:00:42 cifar10-global-proj_sl-3.0-resnet56]: Epoch 40/100, Acc=0.9250, Val Loss=0.3393, lr=0.0100
[02/21 15:08:18 cifar10-global-proj_sl-3.0-resnet56]: Epoch 41/100, Acc=0.9235, Val Loss=0.3481, lr=0.0100
[02/21 15:15:54 cifar10-global-proj_sl-3.0-resnet56]: Epoch 42/100, Acc=0.9240, Val Loss=0.3337, lr=0.0100
[02/21 15:23:27 cifar10-global-proj_sl-3.0-resnet56]: Epoch 43/100, Acc=0.9243, Val Loss=0.3419, lr=0.0100
[02/21 15:31:01 cifar10-global-proj_sl-3.0-resnet56]: Epoch 44/100, Acc=0.9273, Val Loss=0.3283, lr=0.0100
[02/21 15:38:36 cifar10-global-proj_sl-3.0-resnet56]: Epoch 45/100, Acc=0.9259, Val Loss=0.3346, lr=0.0100
[02/21 15:46:13 cifar10-global-proj_sl-3.0-resnet56]: Epoch 46/100, Acc=0.9241, Val Loss=0.3563, lr=0.0100
[02/21 15:53:39 cifar10-global-proj_sl-3.0-resnet56]: Epoch 47/100, Acc=0.9249, Val Loss=0.3292, lr=0.0100
[02/21 16:01:07 cifar10-global-proj_sl-3.0-resnet56]: Epoch 48/100, Acc=0.9245, Val Loss=0.3447, lr=0.0100
[02/21 16:08:33 cifar10-global-proj_sl-3.0-resnet56]: Epoch 49/100, Acc=0.9266, Val Loss=0.3381, lr=0.0100
[02/21 16:15:57 cifar10-global-proj_sl-3.0-resnet56]: Epoch 50/100, Acc=0.9235, Val Loss=0.3495, lr=0.0100
[02/21 16:23:28 cifar10-global-proj_sl-3.0-resnet56]: Epoch 51/100, Acc=0.9236, Val Loss=0.3659, lr=0.0100
[02/21 16:31:00 cifar10-global-proj_sl-3.0-resnet56]: Epoch 52/100, Acc=0.9261, Val Loss=0.3499, lr=0.0100
[02/21 16:38:26 cifar10-global-proj_sl-3.0-resnet56]: Epoch 53/100, Acc=0.9235, Val Loss=0.3608, lr=0.0100
[02/21 16:45:58 cifar10-global-proj_sl-3.0-resnet56]: Epoch 54/100, Acc=0.9158, Val Loss=0.3954, lr=0.0100
[02/21 16:53:31 cifar10-global-proj_sl-3.0-resnet56]: Epoch 55/100, Acc=0.9236, Val Loss=0.3643, lr=0.0100
[02/21 17:01:05 cifar10-global-proj_sl-3.0-resnet56]: Epoch 56/100, Acc=0.9223, Val Loss=0.3618, lr=0.0100
[02/21 17:08:37 cifar10-global-proj_sl-3.0-resnet56]: Epoch 57/100, Acc=0.9256, Val Loss=0.3568, lr=0.0100
[02/21 17:16:05 cifar10-global-proj_sl-3.0-resnet56]: Epoch 58/100, Acc=0.9251, Val Loss=0.3408, lr=0.0100
[02/21 17:23:32 cifar10-global-proj_sl-3.0-resnet56]: Epoch 59/100, Acc=0.9234, Val Loss=0.3554, lr=0.0100
[02/21 17:31:01 cifar10-global-proj_sl-3.0-resnet56]: Epoch 60/100, Acc=0.9326, Val Loss=0.3141, lr=0.0010
[02/21 17:38:30 cifar10-global-proj_sl-3.0-resnet56]: Epoch 61/100, Acc=0.9331, Val Loss=0.3078, lr=0.0010
[02/21 17:45:57 cifar10-global-proj_sl-3.0-resnet56]: Epoch 62/100, Acc=0.9346, Val Loss=0.3103, lr=0.0010
[02/21 17:53:29 cifar10-global-proj_sl-3.0-resnet56]: Epoch 63/100, Acc=0.9345, Val Loss=0.3071, lr=0.0010
[02/21 18:01:01 cifar10-global-proj_sl-3.0-resnet56]: Epoch 64/100, Acc=0.9340, Val Loss=0.3133, lr=0.0010
[02/21 18:08:34 cifar10-global-proj_sl-3.0-resnet56]: Epoch 65/100, Acc=0.9357, Val Loss=0.3077, lr=0.0010
[02/21 18:16:07 cifar10-global-proj_sl-3.0-resnet56]: Epoch 66/100, Acc=0.9358, Val Loss=0.3099, lr=0.0010
[02/21 18:23:42 cifar10-global-proj_sl-3.0-resnet56]: Epoch 67/100, Acc=0.9366, Val Loss=0.3039, lr=0.0010
[02/21 18:31:12 cifar10-global-proj_sl-3.0-resnet56]: Epoch 68/100, Acc=0.9372, Val Loss=0.3056, lr=0.0010
[02/21 18:38:46 cifar10-global-proj_sl-3.0-resnet56]: Epoch 69/100, Acc=0.9363, Val Loss=0.3056, lr=0.0010
[02/21 18:46:17 cifar10-global-proj_sl-3.0-resnet56]: Epoch 70/100, Acc=0.9373, Val Loss=0.3061, lr=0.0010
[02/21 18:53:47 cifar10-global-proj_sl-3.0-resnet56]: Epoch 71/100, Acc=0.9356, Val Loss=0.3085, lr=0.0010
[02/21 19:01:25 cifar10-global-proj_sl-3.0-resnet56]: Epoch 72/100, Acc=0.9358, Val Loss=0.3092, lr=0.0010
[02/21 19:08:59 cifar10-global-proj_sl-3.0-resnet56]: Epoch 73/100, Acc=0.9363, Val Loss=0.3101, lr=0.0010
[02/21 19:16:32 cifar10-global-proj_sl-3.0-resnet56]: Epoch 74/100, Acc=0.9370, Val Loss=0.3055, lr=0.0010
[02/21 19:24:06 cifar10-global-proj_sl-3.0-resnet56]: Epoch 75/100, Acc=0.9369, Val Loss=0.3100, lr=0.0010
[02/21 19:31:37 cifar10-global-proj_sl-3.0-resnet56]: Epoch 76/100, Acc=0.9355, Val Loss=0.3090, lr=0.0010
[02/21 19:39:09 cifar10-global-proj_sl-3.0-resnet56]: Epoch 77/100, Acc=0.9368, Val Loss=0.3061, lr=0.0010
[02/21 19:46:40 cifar10-global-proj_sl-3.0-resnet56]: Epoch 78/100, Acc=0.9376, Val Loss=0.3053, lr=0.0010
[02/21 19:54:10 cifar10-global-proj_sl-3.0-resnet56]: Epoch 79/100, Acc=0.9376, Val Loss=0.3080, lr=0.0010
[02/21 20:01:45 cifar10-global-proj_sl-3.0-resnet56]: Epoch 80/100, Acc=0.9375, Val Loss=0.3081, lr=0.0001
[02/21 20:09:21 cifar10-global-proj_sl-3.0-resnet56]: Epoch 81/100, Acc=0.9373, Val Loss=0.3111, lr=0.0001
[02/21 20:16:53 cifar10-global-proj_sl-3.0-resnet56]: Epoch 82/100, Acc=0.9369, Val Loss=0.3089, lr=0.0001
[02/21 20:24:26 cifar10-global-proj_sl-3.0-resnet56]: Epoch 83/100, Acc=0.9370, Val Loss=0.3097, lr=0.0001
[02/21 20:32:02 cifar10-global-proj_sl-3.0-resnet56]: Epoch 84/100, Acc=0.9370, Val Loss=0.3107, lr=0.0001
[02/21 20:39:38 cifar10-global-proj_sl-3.0-resnet56]: Epoch 85/100, Acc=0.9376, Val Loss=0.3113, lr=0.0001
[02/21 20:47:13 cifar10-global-proj_sl-3.0-resnet56]: Epoch 86/100, Acc=0.9374, Val Loss=0.3103, lr=0.0001
[02/21 20:54:47 cifar10-global-proj_sl-3.0-resnet56]: Epoch 87/100, Acc=0.9369, Val Loss=0.3080, lr=0.0001
[02/21 21:02:19 cifar10-global-proj_sl-3.0-resnet56]: Epoch 88/100, Acc=0.9374, Val Loss=0.3085, lr=0.0001
[02/21 21:09:56 cifar10-global-proj_sl-3.0-resnet56]: Epoch 89/100, Acc=0.9386, Val Loss=0.3089, lr=0.0001
[02/21 21:17:24 cifar10-global-proj_sl-3.0-resnet56]: Epoch 90/100, Acc=0.9376, Val Loss=0.3113, lr=0.0001
[02/21 21:24:54 cifar10-global-proj_sl-3.0-resnet56]: Epoch 91/100, Acc=0.9370, Val Loss=0.3101, lr=0.0001
[02/21 21:32:23 cifar10-global-proj_sl-3.0-resnet56]: Epoch 92/100, Acc=0.9376, Val Loss=0.3079, lr=0.0001
[02/21 21:39:54 cifar10-global-proj_sl-3.0-resnet56]: Epoch 93/100, Acc=0.9373, Val Loss=0.3050, lr=0.0001
[02/21 21:47:26 cifar10-global-proj_sl-3.0-resnet56]: Epoch 94/100, Acc=0.9377, Val Loss=0.3087, lr=0.0001
[02/21 21:54:58 cifar10-global-proj_sl-3.0-resnet56]: Epoch 95/100, Acc=0.9376, Val Loss=0.3105, lr=0.0001
[02/21 22:02:31 cifar10-global-proj_sl-3.0-resnet56]: Epoch 96/100, Acc=0.9367, Val Loss=0.3073, lr=0.0001
[02/21 22:10:03 cifar10-global-proj_sl-3.0-resnet56]: Epoch 97/100, Acc=0.9367, Val Loss=0.3092, lr=0.0001
[02/21 22:17:36 cifar10-global-proj_sl-3.0-resnet56]: Epoch 98/100, Acc=0.9381, Val Loss=0.3097, lr=0.0001
[02/21 22:25:07 cifar10-global-proj_sl-3.0-resnet56]: Epoch 99/100, Acc=0.9378, Val Loss=0.3107, lr=0.0001
[02/21 22:25:07 cifar10-global-proj_sl-3.0-resnet56]: Best Acc=0.9386
[02/21 22:25:07 cifar10-global-proj_sl-3.0-resnet56]: Loading the sparse model from run/cifar10/prune/cifar10-global-proj_sl-3.0-resnet56/reg_cifar10_resnet56_proj_sl_1e-05.pth...
[02/21 22:25:10 cifar10-global-proj_sl-3.0-resnet56]: Pruning...
[02/21 22:27:52 cifar10-global-proj_sl-3.0-resnet56]: ResNet(
  (conv1): Conv2d(3, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(7, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(7, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(7, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(7, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(7, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(7, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(7, 17, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(7, 24, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(24, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(21, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(24, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(24, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(24, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(24, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(24, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(24, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(24, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(24, 54, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(54, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(24, 48, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(48, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(48, 52, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(52, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(52, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(48, 53, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(53, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(48, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(42, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(48, 49, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(49, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(49, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(48, 41, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(41, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(48, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(48, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(26, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=48, out_features=10, bias=True)
)
[02/21 22:27:56 cifar10-global-proj_sl-3.0-resnet56]: Params: 0.86 M => 0.39 M (46.01%)
[02/21 22:27:56 cifar10-global-proj_sl-3.0-resnet56]: FLOPs: 127.12 M => 41.46 M (32.61%, 3.07X )
[02/21 22:27:56 cifar10-global-proj_sl-3.0-resnet56]: Acc: 0.9386 => 0.1001
[02/21 22:27:56 cifar10-global-proj_sl-3.0-resnet56]: Val Loss: 0.3089 => 10.8062
[02/21 22:27:56 cifar10-global-proj_sl-3.0-resnet56]: Finetuning...
[02/21 22:28:29 cifar10-global-proj_sl-3.0-resnet56]: Epoch 0/100, Acc=0.7968, Val Loss=0.6047, lr=0.0100
[02/21 22:29:02 cifar10-global-proj_sl-3.0-resnet56]: Epoch 1/100, Acc=0.8495, Val Loss=0.4646, lr=0.0100
[02/21 22:29:35 cifar10-global-proj_sl-3.0-resnet56]: Epoch 2/100, Acc=0.8670, Val Loss=0.4169, lr=0.0100
[02/21 22:30:08 cifar10-global-proj_sl-3.0-resnet56]: Epoch 3/100, Acc=0.8541, Val Loss=0.4552, lr=0.0100
[02/21 22:30:41 cifar10-global-proj_sl-3.0-resnet56]: Epoch 4/100, Acc=0.8664, Val Loss=0.4336, lr=0.0100
[02/21 22:31:14 cifar10-global-proj_sl-3.0-resnet56]: Epoch 5/100, Acc=0.8805, Val Loss=0.3836, lr=0.0100
[02/21 22:31:48 cifar10-global-proj_sl-3.0-resnet56]: Epoch 6/100, Acc=0.8724, Val Loss=0.3993, lr=0.0100
[02/21 22:32:21 cifar10-global-proj_sl-3.0-resnet56]: Epoch 7/100, Acc=0.8783, Val Loss=0.3854, lr=0.0100
[02/21 22:32:54 cifar10-global-proj_sl-3.0-resnet56]: Epoch 8/100, Acc=0.8762, Val Loss=0.3975, lr=0.0100
[02/21 22:33:27 cifar10-global-proj_sl-3.0-resnet56]: Epoch 9/100, Acc=0.8819, Val Loss=0.3752, lr=0.0100
[02/21 22:34:00 cifar10-global-proj_sl-3.0-resnet56]: Epoch 10/100, Acc=0.8811, Val Loss=0.3765, lr=0.0100
[02/21 22:34:33 cifar10-global-proj_sl-3.0-resnet56]: Epoch 11/100, Acc=0.8885, Val Loss=0.3554, lr=0.0100
[02/21 22:35:06 cifar10-global-proj_sl-3.0-resnet56]: Epoch 12/100, Acc=0.8776, Val Loss=0.4123, lr=0.0100
[02/21 22:35:39 cifar10-global-proj_sl-3.0-resnet56]: Epoch 13/100, Acc=0.8873, Val Loss=0.3645, lr=0.0100
[02/21 22:36:12 cifar10-global-proj_sl-3.0-resnet56]: Epoch 14/100, Acc=0.8847, Val Loss=0.3706, lr=0.0100
[02/21 22:36:45 cifar10-global-proj_sl-3.0-resnet56]: Epoch 15/100, Acc=0.8772, Val Loss=0.3994, lr=0.0100
[02/21 22:37:18 cifar10-global-proj_sl-3.0-resnet56]: Epoch 16/100, Acc=0.8962, Val Loss=0.3450, lr=0.0100
[02/21 22:37:52 cifar10-global-proj_sl-3.0-resnet56]: Epoch 17/100, Acc=0.8874, Val Loss=0.3766, lr=0.0100
[02/21 22:38:25 cifar10-global-proj_sl-3.0-resnet56]: Epoch 18/100, Acc=0.8734, Val Loss=0.4205, lr=0.0100
[02/21 22:38:58 cifar10-global-proj_sl-3.0-resnet56]: Epoch 19/100, Acc=0.8784, Val Loss=0.4112, lr=0.0100
[02/21 22:39:31 cifar10-global-proj_sl-3.0-resnet56]: Epoch 20/100, Acc=0.8891, Val Loss=0.3729, lr=0.0100
[02/21 22:40:04 cifar10-global-proj_sl-3.0-resnet56]: Epoch 21/100, Acc=0.8883, Val Loss=0.3584, lr=0.0100
[02/21 22:40:37 cifar10-global-proj_sl-3.0-resnet56]: Epoch 22/100, Acc=0.8810, Val Loss=0.3997, lr=0.0100
[02/21 22:41:10 cifar10-global-proj_sl-3.0-resnet56]: Epoch 23/100, Acc=0.8878, Val Loss=0.3798, lr=0.0100
[02/21 22:41:43 cifar10-global-proj_sl-3.0-resnet56]: Epoch 24/100, Acc=0.8784, Val Loss=0.4458, lr=0.0100
[02/21 22:42:17 cifar10-global-proj_sl-3.0-resnet56]: Epoch 25/100, Acc=0.9000, Val Loss=0.3322, lr=0.0100
[02/21 22:42:50 cifar10-global-proj_sl-3.0-resnet56]: Epoch 26/100, Acc=0.8764, Val Loss=0.4250, lr=0.0100
[02/21 22:43:24 cifar10-global-proj_sl-3.0-resnet56]: Epoch 27/100, Acc=0.8948, Val Loss=0.3533, lr=0.0100
[02/21 22:43:58 cifar10-global-proj_sl-3.0-resnet56]: Epoch 28/100, Acc=0.8705, Val Loss=0.4246, lr=0.0100
[02/21 22:44:31 cifar10-global-proj_sl-3.0-resnet56]: Epoch 29/100, Acc=0.8818, Val Loss=0.3990, lr=0.0100
[02/21 22:45:05 cifar10-global-proj_sl-3.0-resnet56]: Epoch 30/100, Acc=0.8937, Val Loss=0.3567, lr=0.0100
[02/21 22:45:38 cifar10-global-proj_sl-3.0-resnet56]: Epoch 31/100, Acc=0.9000, Val Loss=0.3396, lr=0.0100
[02/21 22:46:11 cifar10-global-proj_sl-3.0-resnet56]: Epoch 32/100, Acc=0.8831, Val Loss=0.3948, lr=0.0100
[02/21 22:46:44 cifar10-global-proj_sl-3.0-resnet56]: Epoch 33/100, Acc=0.8781, Val Loss=0.4092, lr=0.0100
[02/21 22:47:17 cifar10-global-proj_sl-3.0-resnet56]: Epoch 34/100, Acc=0.8696, Val Loss=0.4742, lr=0.0100
[02/21 22:47:50 cifar10-global-proj_sl-3.0-resnet56]: Epoch 35/100, Acc=0.8834, Val Loss=0.3992, lr=0.0100
[02/21 22:48:23 cifar10-global-proj_sl-3.0-resnet56]: Epoch 36/100, Acc=0.8855, Val Loss=0.3917, lr=0.0100
[02/21 22:48:56 cifar10-global-proj_sl-3.0-resnet56]: Epoch 37/100, Acc=0.8878, Val Loss=0.3917, lr=0.0100
[02/21 22:49:29 cifar10-global-proj_sl-3.0-resnet56]: Epoch 38/100, Acc=0.8945, Val Loss=0.3475, lr=0.0100
[02/21 22:50:01 cifar10-global-proj_sl-3.0-resnet56]: Epoch 39/100, Acc=0.8851, Val Loss=0.4068, lr=0.0100
[02/21 22:50:34 cifar10-global-proj_sl-3.0-resnet56]: Epoch 40/100, Acc=0.8860, Val Loss=0.4078, lr=0.0100
[02/21 22:51:07 cifar10-global-proj_sl-3.0-resnet56]: Epoch 41/100, Acc=0.8851, Val Loss=0.4069, lr=0.0100
[02/21 22:51:40 cifar10-global-proj_sl-3.0-resnet56]: Epoch 42/100, Acc=0.8703, Val Loss=0.4618, lr=0.0100
[02/21 22:52:13 cifar10-global-proj_sl-3.0-resnet56]: Epoch 43/100, Acc=0.8923, Val Loss=0.3735, lr=0.0100
[02/21 22:52:46 cifar10-global-proj_sl-3.0-resnet56]: Epoch 44/100, Acc=0.8921, Val Loss=0.3587, lr=0.0100
[02/21 22:53:19 cifar10-global-proj_sl-3.0-resnet56]: Epoch 45/100, Acc=0.8874, Val Loss=0.3822, lr=0.0100
[02/21 22:53:52 cifar10-global-proj_sl-3.0-resnet56]: Epoch 46/100, Acc=0.8898, Val Loss=0.3763, lr=0.0100
[02/21 22:54:26 cifar10-global-proj_sl-3.0-resnet56]: Epoch 47/100, Acc=0.8772, Val Loss=0.4198, lr=0.0100
[02/21 22:54:59 cifar10-global-proj_sl-3.0-resnet56]: Epoch 48/100, Acc=0.8991, Val Loss=0.3457, lr=0.0100
[02/21 22:55:32 cifar10-global-proj_sl-3.0-resnet56]: Epoch 49/100, Acc=0.8964, Val Loss=0.3617, lr=0.0100
[02/21 22:56:06 cifar10-global-proj_sl-3.0-resnet56]: Epoch 50/100, Acc=0.8876, Val Loss=0.3946, lr=0.0100
[02/21 22:56:39 cifar10-global-proj_sl-3.0-resnet56]: Epoch 51/100, Acc=0.8949, Val Loss=0.3562, lr=0.0100
[02/21 22:57:12 cifar10-global-proj_sl-3.0-resnet56]: Epoch 52/100, Acc=0.8930, Val Loss=0.3671, lr=0.0100
[02/21 22:57:44 cifar10-global-proj_sl-3.0-resnet56]: Epoch 53/100, Acc=0.8867, Val Loss=0.3907, lr=0.0100
[02/21 22:58:17 cifar10-global-proj_sl-3.0-resnet56]: Epoch 54/100, Acc=0.8869, Val Loss=0.3911, lr=0.0100
[02/21 22:58:50 cifar10-global-proj_sl-3.0-resnet56]: Epoch 55/100, Acc=0.8919, Val Loss=0.3760, lr=0.0100
[02/21 22:59:22 cifar10-global-proj_sl-3.0-resnet56]: Epoch 56/100, Acc=0.8963, Val Loss=0.3568, lr=0.0100
[02/21 22:59:55 cifar10-global-proj_sl-3.0-resnet56]: Epoch 57/100, Acc=0.8929, Val Loss=0.3717, lr=0.0100
[02/21 23:00:28 cifar10-global-proj_sl-3.0-resnet56]: Epoch 58/100, Acc=0.8876, Val Loss=0.3976, lr=0.0100
[02/21 23:01:01 cifar10-global-proj_sl-3.0-resnet56]: Epoch 59/100, Acc=0.8792, Val Loss=0.4055, lr=0.0100
[02/21 23:01:35 cifar10-global-proj_sl-3.0-resnet56]: Epoch 60/100, Acc=0.9205, Val Loss=0.2702, lr=0.0010
[02/21 23:02:09 cifar10-global-proj_sl-3.0-resnet56]: Epoch 61/100, Acc=0.9221, Val Loss=0.2684, lr=0.0010
[02/21 23:02:42 cifar10-global-proj_sl-3.0-resnet56]: Epoch 62/100, Acc=0.9231, Val Loss=0.2708, lr=0.0010
[02/21 23:03:16 cifar10-global-proj_sl-3.0-resnet56]: Epoch 63/100, Acc=0.9244, Val Loss=0.2650, lr=0.0010
[02/21 23:03:49 cifar10-global-proj_sl-3.0-resnet56]: Epoch 64/100, Acc=0.9238, Val Loss=0.2706, lr=0.0010
[02/21 23:04:23 cifar10-global-proj_sl-3.0-resnet56]: Epoch 65/100, Acc=0.9249, Val Loss=0.2701, lr=0.0010
[02/21 23:04:56 cifar10-global-proj_sl-3.0-resnet56]: Epoch 66/100, Acc=0.9236, Val Loss=0.2756, lr=0.0010
[02/21 23:05:30 cifar10-global-proj_sl-3.0-resnet56]: Epoch 67/100, Acc=0.9245, Val Loss=0.2729, lr=0.0010
[02/21 23:06:03 cifar10-global-proj_sl-3.0-resnet56]: Epoch 68/100, Acc=0.9221, Val Loss=0.2776, lr=0.0010
[02/21 23:06:36 cifar10-global-proj_sl-3.0-resnet56]: Epoch 69/100, Acc=0.9230, Val Loss=0.2788, lr=0.0010
[02/21 23:07:09 cifar10-global-proj_sl-3.0-resnet56]: Epoch 70/100, Acc=0.9224, Val Loss=0.2764, lr=0.0010
[02/21 23:07:41 cifar10-global-proj_sl-3.0-resnet56]: Epoch 71/100, Acc=0.9253, Val Loss=0.2774, lr=0.0010
[02/21 23:08:14 cifar10-global-proj_sl-3.0-resnet56]: Epoch 72/100, Acc=0.9247, Val Loss=0.2782, lr=0.0010
[02/21 23:08:46 cifar10-global-proj_sl-3.0-resnet56]: Epoch 73/100, Acc=0.9231, Val Loss=0.2824, lr=0.0010
[02/21 23:09:20 cifar10-global-proj_sl-3.0-resnet56]: Epoch 74/100, Acc=0.9251, Val Loss=0.2878, lr=0.0010
[02/21 23:09:53 cifar10-global-proj_sl-3.0-resnet56]: Epoch 75/100, Acc=0.9259, Val Loss=0.2877, lr=0.0010
[02/21 23:10:26 cifar10-global-proj_sl-3.0-resnet56]: Epoch 76/100, Acc=0.9260, Val Loss=0.2872, lr=0.0010
[02/21 23:10:59 cifar10-global-proj_sl-3.0-resnet56]: Epoch 77/100, Acc=0.9254, Val Loss=0.2868, lr=0.0010
[02/21 23:11:32 cifar10-global-proj_sl-3.0-resnet56]: Epoch 78/100, Acc=0.9243, Val Loss=0.2854, lr=0.0010
[02/21 23:12:05 cifar10-global-proj_sl-3.0-resnet56]: Epoch 79/100, Acc=0.9265, Val Loss=0.2914, lr=0.0010
[02/21 23:12:39 cifar10-global-proj_sl-3.0-resnet56]: Epoch 80/100, Acc=0.9255, Val Loss=0.2887, lr=0.0001
[02/21 23:13:12 cifar10-global-proj_sl-3.0-resnet56]: Epoch 81/100, Acc=0.9265, Val Loss=0.2874, lr=0.0001
[02/21 23:13:46 cifar10-global-proj_sl-3.0-resnet56]: Epoch 82/100, Acc=0.9264, Val Loss=0.2900, lr=0.0001
[02/21 23:14:19 cifar10-global-proj_sl-3.0-resnet56]: Epoch 83/100, Acc=0.9274, Val Loss=0.2877, lr=0.0001
[02/21 23:14:53 cifar10-global-proj_sl-3.0-resnet56]: Epoch 84/100, Acc=0.9262, Val Loss=0.2889, lr=0.0001
[02/21 23:15:26 cifar10-global-proj_sl-3.0-resnet56]: Epoch 85/100, Acc=0.9264, Val Loss=0.2876, lr=0.0001
[02/21 23:16:00 cifar10-global-proj_sl-3.0-resnet56]: Epoch 86/100, Acc=0.9269, Val Loss=0.2879, lr=0.0001
[02/21 23:16:33 cifar10-global-proj_sl-3.0-resnet56]: Epoch 87/100, Acc=0.9274, Val Loss=0.2888, lr=0.0001
[02/21 23:17:06 cifar10-global-proj_sl-3.0-resnet56]: Epoch 88/100, Acc=0.9255, Val Loss=0.2877, lr=0.0001
[02/21 23:17:39 cifar10-global-proj_sl-3.0-resnet56]: Epoch 89/100, Acc=0.9263, Val Loss=0.2900, lr=0.0001
[02/21 23:18:12 cifar10-global-proj_sl-3.0-resnet56]: Epoch 90/100, Acc=0.9252, Val Loss=0.2892, lr=0.0001
[02/21 23:18:44 cifar10-global-proj_sl-3.0-resnet56]: Epoch 91/100, Acc=0.9264, Val Loss=0.2887, lr=0.0001
[02/21 23:19:17 cifar10-global-proj_sl-3.0-resnet56]: Epoch 92/100, Acc=0.9261, Val Loss=0.2891, lr=0.0001
[02/21 23:19:50 cifar10-global-proj_sl-3.0-resnet56]: Epoch 93/100, Acc=0.9267, Val Loss=0.2899, lr=0.0001
[02/21 23:20:23 cifar10-global-proj_sl-3.0-resnet56]: Epoch 94/100, Acc=0.9271, Val Loss=0.2915, lr=0.0001
[02/21 23:20:56 cifar10-global-proj_sl-3.0-resnet56]: Epoch 95/100, Acc=0.9263, Val Loss=0.2901, lr=0.0001
[02/21 23:21:29 cifar10-global-proj_sl-3.0-resnet56]: Epoch 96/100, Acc=0.9264, Val Loss=0.2875, lr=0.0001
[02/21 23:22:02 cifar10-global-proj_sl-3.0-resnet56]: Epoch 97/100, Acc=0.9268, Val Loss=0.2890, lr=0.0001
[02/21 23:22:34 cifar10-global-proj_sl-3.0-resnet56]: Epoch 98/100, Acc=0.9267, Val Loss=0.2890, lr=0.0001
[02/21 23:23:07 cifar10-global-proj_sl-3.0-resnet56]: Epoch 99/100, Acc=0.9274, Val Loss=0.2914, lr=0.0001
[02/21 23:23:07 cifar10-global-proj_sl-3.0-resnet56]: Best Acc=0.9274
[02/21 23:23:07 cifar10-global-proj_sl-3.0-resnet56]: Params: 0.39 M
[02/21 23:23:07 cifar10-global-proj_sl-3.0-resnet56]: ops: 41.46 M
[02/21 23:23:10 cifar10-global-proj_sl-3.0-resnet56]: Acc: 0.9274 Val Loss: 0.2914

TIME TAKEN: 23:55:19
SLURM WORKLOAD FINISH: Fri Feb 21 23:23:11 CET 2025
