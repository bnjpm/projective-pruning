[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: mode: prune
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: model: resnet56
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: verbose: False
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: dataset: cifar10
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: dataroot: data
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: batch_size: 128
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: total_epochs: 100
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: lr_decay_milestones: 60,80
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: lr_decay_gamma: 0.1
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: lr: 0.01
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: output_dir: run/cifar10/prune/cifar10-global-lamp-3.0-resnet56
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: finetune: True
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: last_epochs: 100
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: reps: 1
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: method: lamp
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: speed_up: 3.0
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: max_pruning_ratio: 1.0
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: soft_keeping_ratio: 0.0
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: reg: 1e-05
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: delta_reg: 0.0001
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: weight_decay: 0.0005
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: seed: 1
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: global_pruning: True
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: sl_total_epochs: 100
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: sl_lr: 0.01
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: sl_lr_decay_milestones: 60,80
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: sl_reg_warmup: 0
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: sl_restore: None
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: iterative_steps: 400
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: logger: <Logger cifar10-global-lamp-3.0-resnet56 (DEBUG)>
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: device: cuda
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: num_classes: 10
[02/21 02:10:06] cifar10-global-lamp-3.0-resnet56 INFO: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 02:10:10] cifar10-global-lamp-3.0-resnet56 INFO: Pruning...
[02/21 02:10:40] cifar10-global-lamp-3.0-resnet56 INFO: ResNet(
  (conv1): Conv2d(3, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(15, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(15, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(15, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(15, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(15, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(15, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(15, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(15, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(15, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(15, 18, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(15, 16, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(16, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(16, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(16, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 18, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(18, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(18, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(18, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(18, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(18, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(18, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=18, out_features=10, bias=True)
)
[02/21 02:10:44] cifar10-global-lamp-3.0-resnet56 INFO: Params: 0.86 M => 0.12 M (14.09%)
[02/21 02:10:44] cifar10-global-lamp-3.0-resnet56 INFO: FLOPs: 127.12 M => 42.23 M (33.22%, 3.01X )
[02/21 02:10:44] cifar10-global-lamp-3.0-resnet56 INFO: Acc: 0.9392 => 0.0895
[02/21 02:10:44] cifar10-global-lamp-3.0-resnet56 INFO: Val Loss: 0.2587 => 2.3212
[02/21 02:10:44] cifar10-global-lamp-3.0-resnet56 INFO: Finetuning...
[02/21 02:11:17] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 0/100, Acc=0.7406, Val Loss=0.7670, lr=0.0100
[02/21 02:11:52] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 1/100, Acc=0.7864, Val Loss=0.6425, lr=0.0100
[02/21 02:12:26] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 2/100, Acc=0.7995, Val Loss=0.6076, lr=0.0100
[02/21 02:13:00] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 3/100, Acc=0.8318, Val Loss=0.4941, lr=0.0100
[02/21 02:13:34] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 4/100, Acc=0.8159, Val Loss=0.5480, lr=0.0100
[02/21 02:14:09] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 5/100, Acc=0.8369, Val Loss=0.4845, lr=0.0100
[02/21 02:14:44] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 6/100, Acc=0.8234, Val Loss=0.5352, lr=0.0100
[02/21 02:15:19] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 7/100, Acc=0.8127, Val Loss=0.5855, lr=0.0100
[02/21 02:15:54] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 8/100, Acc=0.8421, Val Loss=0.4867, lr=0.0100
[02/21 02:16:28] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 9/100, Acc=0.8409, Val Loss=0.4923, lr=0.0100
[02/21 02:17:03] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 10/100, Acc=0.8513, Val Loss=0.4526, lr=0.0100
[02/21 02:17:37] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 11/100, Acc=0.8303, Val Loss=0.5201, lr=0.0100
[02/21 02:18:12] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 12/100, Acc=0.7781, Val Loss=0.7436, lr=0.0100
[02/21 02:18:47] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 13/100, Acc=0.8457, Val Loss=0.4896, lr=0.0100
[02/21 02:19:22] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 14/100, Acc=0.8552, Val Loss=0.4492, lr=0.0100
[02/21 02:19:56] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 15/100, Acc=0.8186, Val Loss=0.5513, lr=0.0100
[02/21 02:20:31] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 16/100, Acc=0.8591, Val Loss=0.4245, lr=0.0100
[02/21 02:21:06] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 17/100, Acc=0.8557, Val Loss=0.4323, lr=0.0100
[02/21 02:21:42] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 18/100, Acc=0.8634, Val Loss=0.4057, lr=0.0100
[02/21 02:22:17] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 19/100, Acc=0.8376, Val Loss=0.4817, lr=0.0100
[02/21 02:22:52] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 20/100, Acc=0.8499, Val Loss=0.4744, lr=0.0100
[02/21 02:23:26] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 21/100, Acc=0.8440, Val Loss=0.4837, lr=0.0100
[02/21 02:24:01] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 22/100, Acc=0.8123, Val Loss=0.6209, lr=0.0100
[02/21 02:24:35] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 23/100, Acc=0.8166, Val Loss=0.5867, lr=0.0100
[02/21 02:25:09] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 24/100, Acc=0.8053, Val Loss=0.6610, lr=0.0100
[02/21 02:25:44] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 25/100, Acc=0.8541, Val Loss=0.4721, lr=0.0100
[02/21 02:26:18] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 26/100, Acc=0.8567, Val Loss=0.4436, lr=0.0100
[02/21 02:26:53] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 27/100, Acc=0.8566, Val Loss=0.4388, lr=0.0100
[02/21 02:27:27] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 28/100, Acc=0.8482, Val Loss=0.4907, lr=0.0100
[02/21 02:28:01] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 29/100, Acc=0.8605, Val Loss=0.4466, lr=0.0100
[02/21 02:28:36] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 30/100, Acc=0.8643, Val Loss=0.4182, lr=0.0100
[02/21 02:29:10] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 31/100, Acc=0.8568, Val Loss=0.4534, lr=0.0100
[02/21 02:29:45] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 32/100, Acc=0.8554, Val Loss=0.4555, lr=0.0100
[02/21 02:30:20] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 33/100, Acc=0.8616, Val Loss=0.4356, lr=0.0100
[02/21 02:30:54] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 34/100, Acc=0.8764, Val Loss=0.3742, lr=0.0100
[02/21 02:31:29] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 35/100, Acc=0.8512, Val Loss=0.4777, lr=0.0100
[02/21 02:32:03] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 36/100, Acc=0.8751, Val Loss=0.3930, lr=0.0100
[02/21 02:32:37] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 37/100, Acc=0.8470, Val Loss=0.4997, lr=0.0100
[02/21 02:33:12] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 38/100, Acc=0.8802, Val Loss=0.3712, lr=0.0100
[02/21 02:33:46] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 39/100, Acc=0.8599, Val Loss=0.4437, lr=0.0100
[02/21 02:34:21] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 40/100, Acc=0.8633, Val Loss=0.4253, lr=0.0100
[02/21 02:34:55] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 41/100, Acc=0.8625, Val Loss=0.4328, lr=0.0100
[02/21 02:35:30] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 42/100, Acc=0.8698, Val Loss=0.4065, lr=0.0100
[02/21 02:36:04] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 43/100, Acc=0.8484, Val Loss=0.4778, lr=0.0100
[02/21 02:36:39] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 44/100, Acc=0.8770, Val Loss=0.3650, lr=0.0100
[02/21 02:37:14] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 45/100, Acc=0.8630, Val Loss=0.4256, lr=0.0100
[02/21 02:37:48] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 46/100, Acc=0.8585, Val Loss=0.4393, lr=0.0100
[02/21 02:38:23] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 47/100, Acc=0.8757, Val Loss=0.4013, lr=0.0100
[02/21 02:38:58] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 48/100, Acc=0.8767, Val Loss=0.3794, lr=0.0100
[02/21 02:39:32] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 49/100, Acc=0.8605, Val Loss=0.4302, lr=0.0100
[02/21 02:40:07] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 50/100, Acc=0.8676, Val Loss=0.4147, lr=0.0100
[02/21 02:40:41] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 51/100, Acc=0.8669, Val Loss=0.4044, lr=0.0100
[02/21 02:41:16] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 52/100, Acc=0.8727, Val Loss=0.3859, lr=0.0100
[02/21 02:41:50] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 53/100, Acc=0.8765, Val Loss=0.3823, lr=0.0100
[02/21 02:42:24] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 54/100, Acc=0.8468, Val Loss=0.4945, lr=0.0100
[02/21 02:42:59] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 55/100, Acc=0.8767, Val Loss=0.3800, lr=0.0100
[02/21 02:43:34] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 56/100, Acc=0.8492, Val Loss=0.4746, lr=0.0100
[02/21 02:44:08] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 57/100, Acc=0.8618, Val Loss=0.4436, lr=0.0100
[02/21 02:44:42] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 58/100, Acc=0.8638, Val Loss=0.4448, lr=0.0100
[02/21 02:45:17] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 59/100, Acc=0.8607, Val Loss=0.4697, lr=0.0100
[02/21 02:45:52] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 60/100, Acc=0.9049, Val Loss=0.2874, lr=0.0010
[02/21 02:46:27] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 61/100, Acc=0.9101, Val Loss=0.2839, lr=0.0010
[02/21 02:47:02] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 62/100, Acc=0.9085, Val Loss=0.2853, lr=0.0010
[02/21 02:47:37] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 63/100, Acc=0.9098, Val Loss=0.2834, lr=0.0010
[02/21 02:48:12] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 64/100, Acc=0.9104, Val Loss=0.2832, lr=0.0010
[02/21 02:48:47] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 65/100, Acc=0.9093, Val Loss=0.2854, lr=0.0010
[02/21 02:49:22] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 66/100, Acc=0.9080, Val Loss=0.2889, lr=0.0010
[02/21 02:49:57] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 67/100, Acc=0.9088, Val Loss=0.2877, lr=0.0010
[02/21 02:50:32] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 68/100, Acc=0.9086, Val Loss=0.2909, lr=0.0010
[02/21 02:51:07] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 69/100, Acc=0.9133, Val Loss=0.2853, lr=0.0010
[02/21 02:51:41] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 70/100, Acc=0.9103, Val Loss=0.2932, lr=0.0010
[02/21 02:52:16] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 71/100, Acc=0.9097, Val Loss=0.2893, lr=0.0010
[02/21 02:52:51] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 72/100, Acc=0.9109, Val Loss=0.2910, lr=0.0010
[02/21 02:53:26] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 73/100, Acc=0.9095, Val Loss=0.2934, lr=0.0010
[02/21 02:54:00] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 74/100, Acc=0.9104, Val Loss=0.2967, lr=0.0010
[02/21 02:54:35] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 75/100, Acc=0.9097, Val Loss=0.2962, lr=0.0010
[02/21 02:55:10] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 76/100, Acc=0.9098, Val Loss=0.2978, lr=0.0010
[02/21 02:55:45] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 77/100, Acc=0.9080, Val Loss=0.3011, lr=0.0010
[02/21 02:56:19] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 78/100, Acc=0.9078, Val Loss=0.2985, lr=0.0010
[02/21 02:56:54] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 79/100, Acc=0.9105, Val Loss=0.3012, lr=0.0010
[02/21 02:57:28] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 80/100, Acc=0.9103, Val Loss=0.2987, lr=0.0001
[02/21 02:58:01] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 81/100, Acc=0.9114, Val Loss=0.2949, lr=0.0001
[02/21 02:58:35] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 82/100, Acc=0.9111, Val Loss=0.2961, lr=0.0001
[02/21 02:59:09] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 83/100, Acc=0.9130, Val Loss=0.2951, lr=0.0001
[02/21 02:59:43] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 84/100, Acc=0.9114, Val Loss=0.2957, lr=0.0001
[02/21 03:00:17] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 85/100, Acc=0.9108, Val Loss=0.2957, lr=0.0001
[02/21 03:00:52] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 86/100, Acc=0.9114, Val Loss=0.2950, lr=0.0001
[02/21 03:01:26] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 87/100, Acc=0.9111, Val Loss=0.2962, lr=0.0001
[02/21 03:02:00] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 88/100, Acc=0.9124, Val Loss=0.2957, lr=0.0001
[02/21 03:02:34] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 89/100, Acc=0.9122, Val Loss=0.2959, lr=0.0001
[02/21 03:03:08] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 90/100, Acc=0.9117, Val Loss=0.2955, lr=0.0001
[02/21 03:03:42] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 91/100, Acc=0.9108, Val Loss=0.2950, lr=0.0001
[02/21 03:04:17] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 92/100, Acc=0.9121, Val Loss=0.2965, lr=0.0001
[02/21 03:04:51] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 93/100, Acc=0.9118, Val Loss=0.2970, lr=0.0001
[02/21 03:05:25] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 94/100, Acc=0.9111, Val Loss=0.2993, lr=0.0001
[02/21 03:05:59] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 95/100, Acc=0.9111, Val Loss=0.2971, lr=0.0001
[02/21 03:06:33] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 96/100, Acc=0.9123, Val Loss=0.2967, lr=0.0001
[02/21 03:07:08] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 97/100, Acc=0.9117, Val Loss=0.2991, lr=0.0001
[02/21 03:07:42] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 98/100, Acc=0.9113, Val Loss=0.2990, lr=0.0001
[02/21 03:08:17] cifar10-global-lamp-3.0-resnet56 INFO: Epoch 99/100, Acc=0.9114, Val Loss=0.2983, lr=0.0001
[02/21 03:08:17] cifar10-global-lamp-3.0-resnet56 INFO: Best Acc=0.9133
[02/21 03:08:17] cifar10-global-lamp-3.0-resnet56 INFO: Params: 0.12 M
[02/21 03:08:17] cifar10-global-lamp-3.0-resnet56 INFO: ops: 42.23 M
[02/21 03:08:21] cifar10-global-lamp-3.0-resnet56 INFO: Acc: 0.9114 Val Loss: 0.2983

