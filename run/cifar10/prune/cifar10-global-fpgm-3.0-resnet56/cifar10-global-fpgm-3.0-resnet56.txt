[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: mode: prune
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: model: resnet56
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: verbose: False
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: dataset: cifar10
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: dataroot: data
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: batch_size: 128
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: total_epochs: 100
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: lr_decay_milestones: 60,80
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: lr_decay_gamma: 0.1
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: lr: 0.01
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: restore: run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: output_dir: run/cifar10/prune/cifar10-global-fpgm-3.0-resnet56
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: finetune: True
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: last_epochs: 100
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: reps: 1
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: method: fpgm
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: speed_up: 3.0
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: max_pruning_ratio: 1.0
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: soft_keeping_ratio: 0.0
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: reg: 1e-05
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: delta_reg: 0.0001
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: weight_decay: 0.0005
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: seed: 1
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: global_pruning: True
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: sl_total_epochs: 100
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: sl_lr: 0.01
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: sl_lr_decay_milestones: 60,80
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: sl_reg_warmup: 0
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: sl_restore: None
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: iterative_steps: 400
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: logger: <Logger cifar10-global-fpgm-3.0-resnet56 (DEBUG)>
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: device: cuda
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: num_classes: 10
[02/21 01:14:50] cifar10-global-fpgm-3.0-resnet56 INFO: Loading model from run/cifar10/pretrain/cifar10_resnet56.pth
[02/21 01:14:53] cifar10-global-fpgm-3.0-resnet56 INFO: Pruning...
[02/21 01:15:13] cifar10-global-fpgm-3.0-resnet56 INFO: ResNet(
  (conv1): Conv2d(3, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(13, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(13, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(13, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(13, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(13, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(13, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(13, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(13, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(13, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(13, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(13, 15, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(15, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(15, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(15, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(15, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(15, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(15, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(15, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(15, 61, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(15, 31, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(31, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(58, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(31, 54, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(54, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(31, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(48, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(31, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(51, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(31, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(46, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(31, 44, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(44, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(31, 33, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(33, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(31, 34, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(34, 31, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(31, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=31, out_features=10, bias=True)
)
[02/21 01:15:17] cifar10-global-fpgm-3.0-resnet56 INFO: Params: 0.86 M => 0.28 M (33.29%)
[02/21 01:15:17] cifar10-global-fpgm-3.0-resnet56 INFO: FLOPs: 127.12 M => 42.23 M (33.22%, 3.01X )
[02/21 01:15:17] cifar10-global-fpgm-3.0-resnet56 INFO: Acc: 0.9392 => 0.1792
[02/21 01:15:17] cifar10-global-fpgm-3.0-resnet56 INFO: Val Loss: 0.2587 => 2.2318
[02/21 01:15:17] cifar10-global-fpgm-3.0-resnet56 INFO: Finetuning...
[02/21 01:15:48] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 0/100, Acc=0.8186, Val Loss=0.5478, lr=0.0100
[02/21 01:16:20] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 1/100, Acc=0.8492, Val Loss=0.4577, lr=0.0100
[02/21 01:16:52] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 2/100, Acc=0.8683, Val Loss=0.3918, lr=0.0100
[02/21 01:17:24] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 3/100, Acc=0.8763, Val Loss=0.3672, lr=0.0100
[02/21 01:17:56] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 4/100, Acc=0.8740, Val Loss=0.3722, lr=0.0100
[02/21 01:18:29] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 5/100, Acc=0.8628, Val Loss=0.4060, lr=0.0100
[02/21 01:19:01] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 6/100, Acc=0.8778, Val Loss=0.3674, lr=0.0100
[02/21 01:19:33] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 7/100, Acc=0.8755, Val Loss=0.3707, lr=0.0100
[02/21 01:20:05] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 8/100, Acc=0.8627, Val Loss=0.4282, lr=0.0100
[02/21 01:20:37] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 9/100, Acc=0.8788, Val Loss=0.3843, lr=0.0100
[02/21 01:21:10] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 10/100, Acc=0.8799, Val Loss=0.3892, lr=0.0100
[02/21 01:21:43] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 11/100, Acc=0.8801, Val Loss=0.3740, lr=0.0100
[02/21 01:22:16] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 12/100, Acc=0.8685, Val Loss=0.4054, lr=0.0100
[02/21 01:22:48] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 13/100, Acc=0.8829, Val Loss=0.3674, lr=0.0100
[02/21 01:23:20] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 14/100, Acc=0.8753, Val Loss=0.4008, lr=0.0100
[02/21 01:23:52] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 15/100, Acc=0.8783, Val Loss=0.3897, lr=0.0100
[02/21 01:24:24] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 16/100, Acc=0.8697, Val Loss=0.4192, lr=0.0100
[02/21 01:24:57] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 17/100, Acc=0.8771, Val Loss=0.3870, lr=0.0100
[02/21 01:25:30] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 18/100, Acc=0.8864, Val Loss=0.3612, lr=0.0100
[02/21 01:26:02] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 19/100, Acc=0.8649, Val Loss=0.4129, lr=0.0100
[02/21 01:26:35] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 20/100, Acc=0.8877, Val Loss=0.3602, lr=0.0100
[02/21 01:27:07] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 21/100, Acc=0.8735, Val Loss=0.4026, lr=0.0100
[02/21 01:27:40] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 22/100, Acc=0.8801, Val Loss=0.3843, lr=0.0100
[02/21 01:28:12] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 23/100, Acc=0.8887, Val Loss=0.3526, lr=0.0100
[02/21 01:28:45] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 24/100, Acc=0.8697, Val Loss=0.4222, lr=0.0100
[02/21 01:29:18] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 25/100, Acc=0.8745, Val Loss=0.4170, lr=0.0100
[02/21 01:29:51] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 26/100, Acc=0.8874, Val Loss=0.3572, lr=0.0100
[02/21 01:30:24] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 27/100, Acc=0.8832, Val Loss=0.3657, lr=0.0100
[02/21 01:30:58] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 28/100, Acc=0.8861, Val Loss=0.3666, lr=0.0100
[02/21 01:31:31] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 29/100, Acc=0.8873, Val Loss=0.3553, lr=0.0100
[02/21 01:32:05] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 30/100, Acc=0.8949, Val Loss=0.3375, lr=0.0100
[02/21 01:32:39] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 31/100, Acc=0.8914, Val Loss=0.3555, lr=0.0100
[02/21 01:33:12] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 32/100, Acc=0.8841, Val Loss=0.3738, lr=0.0100
[02/21 01:33:45] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 33/100, Acc=0.8829, Val Loss=0.3714, lr=0.0100
[02/21 01:34:18] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 34/100, Acc=0.8917, Val Loss=0.3523, lr=0.0100
[02/21 01:34:51] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 35/100, Acc=0.8988, Val Loss=0.3226, lr=0.0100
[02/21 01:35:24] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 36/100, Acc=0.8934, Val Loss=0.3449, lr=0.0100
[02/21 01:35:56] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 37/100, Acc=0.8815, Val Loss=0.3953, lr=0.0100
[02/21 01:36:29] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 38/100, Acc=0.8943, Val Loss=0.3449, lr=0.0100
[02/21 01:37:02] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 39/100, Acc=0.8914, Val Loss=0.3568, lr=0.0100
[02/21 01:37:34] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 40/100, Acc=0.8862, Val Loss=0.3590, lr=0.0100
[02/21 01:38:07] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 41/100, Acc=0.8843, Val Loss=0.3812, lr=0.0100
[02/21 01:38:40] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 42/100, Acc=0.8897, Val Loss=0.3643, lr=0.0100
[02/21 01:39:12] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 43/100, Acc=0.8923, Val Loss=0.3660, lr=0.0100
[02/21 01:39:44] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 44/100, Acc=0.8783, Val Loss=0.3998, lr=0.0100
[02/21 01:40:17] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 45/100, Acc=0.8930, Val Loss=0.3491, lr=0.0100
[02/21 01:40:49] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 46/100, Acc=0.8952, Val Loss=0.3406, lr=0.0100
[02/21 01:41:22] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 47/100, Acc=0.8908, Val Loss=0.3741, lr=0.0100
[02/21 01:41:55] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 48/100, Acc=0.9046, Val Loss=0.3088, lr=0.0100
[02/21 01:42:27] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 49/100, Acc=0.8985, Val Loss=0.3382, lr=0.0100
[02/21 01:43:00] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 50/100, Acc=0.8892, Val Loss=0.3644, lr=0.0100
[02/21 01:43:32] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 51/100, Acc=0.8863, Val Loss=0.3654, lr=0.0100
[02/21 01:44:06] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 52/100, Acc=0.8881, Val Loss=0.3860, lr=0.0100
[02/21 01:44:39] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 53/100, Acc=0.8943, Val Loss=0.3558, lr=0.0100
[02/21 01:45:11] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 54/100, Acc=0.8866, Val Loss=0.3662, lr=0.0100
[02/21 01:45:44] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 55/100, Acc=0.8823, Val Loss=0.3834, lr=0.0100
[02/21 01:46:16] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 56/100, Acc=0.8948, Val Loss=0.3459, lr=0.0100
[02/21 01:46:49] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 57/100, Acc=0.8922, Val Loss=0.3654, lr=0.0100
[02/21 01:47:22] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 58/100, Acc=0.8888, Val Loss=0.3599, lr=0.0100
[02/21 01:47:55] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 59/100, Acc=0.8864, Val Loss=0.3821, lr=0.0100
[02/21 01:48:28] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 60/100, Acc=0.9176, Val Loss=0.2697, lr=0.0010
[02/21 01:49:00] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 61/100, Acc=0.9208, Val Loss=0.2679, lr=0.0010
[02/21 01:49:33] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 62/100, Acc=0.9211, Val Loss=0.2686, lr=0.0010
[02/21 01:50:05] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 63/100, Acc=0.9196, Val Loss=0.2695, lr=0.0010
[02/21 01:50:38] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 64/100, Acc=0.9227, Val Loss=0.2687, lr=0.0010
[02/21 01:51:11] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 65/100, Acc=0.9228, Val Loss=0.2700, lr=0.0010
[02/21 01:51:43] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 66/100, Acc=0.9208, Val Loss=0.2710, lr=0.0010
[02/21 01:52:16] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 67/100, Acc=0.9205, Val Loss=0.2763, lr=0.0010
[02/21 01:52:48] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 68/100, Acc=0.9204, Val Loss=0.2809, lr=0.0010
[02/21 01:53:20] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 69/100, Acc=0.9223, Val Loss=0.2738, lr=0.0010
[02/21 01:53:52] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 70/100, Acc=0.9208, Val Loss=0.2846, lr=0.0010
[02/21 01:54:25] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 71/100, Acc=0.9231, Val Loss=0.2805, lr=0.0010
[02/21 01:54:57] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 72/100, Acc=0.9231, Val Loss=0.2824, lr=0.0010
[02/21 01:55:30] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 73/100, Acc=0.9207, Val Loss=0.2874, lr=0.0010
[02/21 01:56:03] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 74/100, Acc=0.9235, Val Loss=0.2826, lr=0.0010
[02/21 01:56:36] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 75/100, Acc=0.9227, Val Loss=0.2838, lr=0.0010
[02/21 01:57:08] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 76/100, Acc=0.9232, Val Loss=0.2856, lr=0.0010
[02/21 01:57:41] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 77/100, Acc=0.9220, Val Loss=0.2870, lr=0.0010
[02/21 01:58:14] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 78/100, Acc=0.9207, Val Loss=0.2913, lr=0.0010
[02/21 01:58:47] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 79/100, Acc=0.9223, Val Loss=0.2843, lr=0.0010
[02/21 01:59:20] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 80/100, Acc=0.9215, Val Loss=0.2881, lr=0.0001
[02/21 01:59:53] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 81/100, Acc=0.9227, Val Loss=0.2821, lr=0.0001
[02/21 02:00:27] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 82/100, Acc=0.9220, Val Loss=0.2831, lr=0.0001
[02/21 02:00:59] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 83/100, Acc=0.9228, Val Loss=0.2845, lr=0.0001
[02/21 02:01:32] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 84/100, Acc=0.9231, Val Loss=0.2875, lr=0.0001
[02/21 02:02:05] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 85/100, Acc=0.9227, Val Loss=0.2864, lr=0.0001
[02/21 02:02:38] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 86/100, Acc=0.9232, Val Loss=0.2847, lr=0.0001
[02/21 02:03:11] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 87/100, Acc=0.9232, Val Loss=0.2883, lr=0.0001
[02/21 02:03:44] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 88/100, Acc=0.9235, Val Loss=0.2845, lr=0.0001
[02/21 02:04:16] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 89/100, Acc=0.9241, Val Loss=0.2858, lr=0.0001
[02/21 02:04:50] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 90/100, Acc=0.9227, Val Loss=0.2880, lr=0.0001
[02/21 02:05:23] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 91/100, Acc=0.9237, Val Loss=0.2863, lr=0.0001
[02/21 02:05:56] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 92/100, Acc=0.9223, Val Loss=0.2886, lr=0.0001
[02/21 02:06:28] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 93/100, Acc=0.9228, Val Loss=0.2875, lr=0.0001
[02/21 02:07:01] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 94/100, Acc=0.9233, Val Loss=0.2886, lr=0.0001
[02/21 02:07:34] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 95/100, Acc=0.9218, Val Loss=0.2909, lr=0.0001
[02/21 02:08:07] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 96/100, Acc=0.9230, Val Loss=0.2895, lr=0.0001
[02/21 02:08:40] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 97/100, Acc=0.9228, Val Loss=0.2892, lr=0.0001
[02/21 02:09:13] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 98/100, Acc=0.9239, Val Loss=0.2903, lr=0.0001
[02/21 02:09:46] cifar10-global-fpgm-3.0-resnet56 INFO: Epoch 99/100, Acc=0.9227, Val Loss=0.2912, lr=0.0001
[02/21 02:09:46] cifar10-global-fpgm-3.0-resnet56 INFO: Best Acc=0.9241
[02/21 02:09:46] cifar10-global-fpgm-3.0-resnet56 INFO: Params: 0.28 M
[02/21 02:09:46] cifar10-global-fpgm-3.0-resnet56 INFO: ops: 42.23 M
[02/21 02:09:49] cifar10-global-fpgm-3.0-resnet56 INFO: Acc: 0.9227 Val Loss: 0.2912

