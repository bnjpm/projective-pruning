[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: mode: prune
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: model: resnet56
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: verbose: False
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: dataset: cifar100
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: dataroot: data
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: batch_size: 128
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: total_epochs: 100
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: lr_decay_milestones: 60,80
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: lr_decay_gamma: 0.1
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: lr: 0.01
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: restore: run/cifar100/pretrain/cifar10_resnet56.pth
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: output_dir: run/cifar100/prune/cifar100-global-fpgm-2.0-resnet56
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: finetune: True
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: last_epochs: 100
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: reps: 1
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: method: fpgm
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: speed_up: 2.0
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: max_pruning_ratio: 1.0
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: soft_keeping_ratio: 0.0
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: reg: 1e-05
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: delta_reg: 0.0001
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: weight_decay: 0.0005
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: seed: 1
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: global_pruning: True
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: sl_total_epochs: 100
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: sl_lr: 0.01
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: sl_lr_decay_milestones: 60,80
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: sl_reg_warmup: 0
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: sl_restore: None
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: iterative_steps: 400
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: logger: <Logger cifar100-global-fpgm-2.0-resnet56 (DEBUG)>
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: device: cuda
[02/21 12:30:05] cifar100-global-fpgm-2.0-resnet56 INFO: num_classes: 100
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: mode: prune
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: model: resnet56
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: verbose: False
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: dataset: cifar100
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: dataroot: data
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: batch_size: 128
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: total_epochs: 100
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: lr_decay_milestones: 60,80
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: lr_decay_gamma: 0.1
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: lr: 0.01
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: output_dir: run/cifar100/prune/cifar100-global-fpgm-2.0-resnet56
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: finetune: True
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: last_epochs: 100
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: reps: 1
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: method: fpgm
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: speed_up: 2.0
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: max_pruning_ratio: 1.0
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: soft_keeping_ratio: 0.0
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: reg: 1e-05
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: delta_reg: 0.0001
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: weight_decay: 0.0005
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: seed: 1
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: global_pruning: True
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: sl_total_epochs: 100
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: sl_lr: 0.01
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: sl_lr_decay_milestones: 60,80
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: sl_reg_warmup: 0
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: sl_restore: None
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: iterative_steps: 400
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: logger: <Logger cifar100-global-fpgm-2.0-resnet56 (DEBUG)>
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: device: cuda
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: num_classes: 100
[02/21 15:51:38] cifar100-global-fpgm-2.0-resnet56 INFO: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 15:51:41] cifar100-global-fpgm-2.0-resnet56 INFO: Pruning...
[02/21 15:51:54] cifar100-global-fpgm-2.0-resnet56 INFO: ResNet(
  (conv1): Conv2d(3, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(13, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(13, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(13, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(13, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(13, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(13, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(13, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(13, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(13, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(13, 27, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(13, 17, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(17, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(17, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(17, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(17, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(17, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(17, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(17, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(8, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(17, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(17, 37, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(37, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(37, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(17, 62, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(62, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(62, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(62, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(62, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(62, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(42, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=62, out_features=100, bias=True)
)
[02/21 15:51:57] cifar100-global-fpgm-2.0-resnet56 INFO: Params: 0.86 M => 0.62 M (71.98%)
[02/21 15:51:57] cifar100-global-fpgm-2.0-resnet56 INFO: FLOPs: 127.12 M => 63.21 M (49.72%, 2.01X )
[02/21 15:51:57] cifar100-global-fpgm-2.0-resnet56 INFO: Acc: 0.7269 => 0.0346
[02/21 15:51:57] cifar100-global-fpgm-2.0-resnet56 INFO: Val Loss: 1.1578 => 8.9722
[02/21 15:51:57] cifar100-global-fpgm-2.0-resnet56 INFO: Finetuning...
[02/21 15:52:24] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 0/100, Acc=0.5929, Val Loss=1.5502, lr=0.0100
[02/21 15:52:52] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 1/100, Acc=0.6030, Val Loss=1.5524, lr=0.0100
[02/21 15:53:19] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 2/100, Acc=0.6281, Val Loss=1.4052, lr=0.0100
[02/21 15:53:46] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 3/100, Acc=0.6479, Val Loss=1.3154, lr=0.0100
[02/21 15:54:14] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 4/100, Acc=0.6256, Val Loss=1.4585, lr=0.0100
[02/21 15:54:42] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 5/100, Acc=0.6406, Val Loss=1.3703, lr=0.0100
[02/21 15:55:10] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 6/100, Acc=0.6361, Val Loss=1.4408, lr=0.0100
[02/21 15:55:37] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 7/100, Acc=0.6286, Val Loss=1.4230, lr=0.0100
[02/21 15:56:04] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 8/100, Acc=0.6349, Val Loss=1.4313, lr=0.0100
[02/21 15:56:32] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 9/100, Acc=0.6245, Val Loss=1.4811, lr=0.0100
[02/21 15:56:59] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 10/100, Acc=0.5966, Val Loss=1.6281, lr=0.0100
[02/21 15:57:26] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 11/100, Acc=0.6184, Val Loss=1.4765, lr=0.0100
[02/21 15:57:54] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 12/100, Acc=0.6305, Val Loss=1.4359, lr=0.0100
[02/21 15:58:21] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 13/100, Acc=0.6557, Val Loss=1.3329, lr=0.0100
[02/21 15:58:49] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 14/100, Acc=0.6388, Val Loss=1.3926, lr=0.0100
[02/21 15:59:16] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 15/100, Acc=0.6461, Val Loss=1.3800, lr=0.0100
[02/21 15:59:44] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 16/100, Acc=0.6307, Val Loss=1.4372, lr=0.0100
[02/21 16:00:11] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 17/100, Acc=0.6488, Val Loss=1.3658, lr=0.0100
[02/21 16:00:38] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 18/100, Acc=0.6587, Val Loss=1.3288, lr=0.0100
[02/21 16:01:05] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 19/100, Acc=0.6362, Val Loss=1.4724, lr=0.0100
[02/21 16:01:33] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 20/100, Acc=0.6365, Val Loss=1.4121, lr=0.0100
[02/21 16:02:00] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 21/100, Acc=0.6575, Val Loss=1.3451, lr=0.0100
[02/21 16:02:27] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 22/100, Acc=0.6455, Val Loss=1.4177, lr=0.0100
[02/21 16:02:54] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 23/100, Acc=0.6587, Val Loss=1.3210, lr=0.0100
[02/21 16:03:22] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 24/100, Acc=0.6482, Val Loss=1.3813, lr=0.0100
[02/21 16:03:49] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 25/100, Acc=0.6419, Val Loss=1.4407, lr=0.0100
[02/21 16:04:17] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 26/100, Acc=0.6686, Val Loss=1.2873, lr=0.0100
[02/21 16:04:44] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 27/100, Acc=0.6435, Val Loss=1.4552, lr=0.0100
[02/21 16:05:11] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 28/100, Acc=0.6500, Val Loss=1.3777, lr=0.0100
[02/21 16:05:39] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 29/100, Acc=0.6316, Val Loss=1.4832, lr=0.0100
[02/21 16:06:06] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 30/100, Acc=0.6435, Val Loss=1.4478, lr=0.0100
[02/21 16:06:33] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 31/100, Acc=0.6379, Val Loss=1.4332, lr=0.0100
[02/21 16:07:01] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 32/100, Acc=0.6430, Val Loss=1.4213, lr=0.0100
[02/21 16:07:28] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 33/100, Acc=0.6505, Val Loss=1.3890, lr=0.0100
[02/21 16:07:56] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 34/100, Acc=0.6498, Val Loss=1.3769, lr=0.0100
[02/21 16:08:24] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 35/100, Acc=0.6561, Val Loss=1.3576, lr=0.0100
[02/21 16:08:51] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 36/100, Acc=0.6344, Val Loss=1.5060, lr=0.0100
[02/21 16:09:19] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 37/100, Acc=0.6532, Val Loss=1.3571, lr=0.0100
[02/21 16:09:47] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 38/100, Acc=0.6323, Val Loss=1.5630, lr=0.0100
[02/21 16:10:14] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 39/100, Acc=0.6467, Val Loss=1.4236, lr=0.0100
[02/21 16:10:42] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 40/100, Acc=0.6452, Val Loss=1.4228, lr=0.0100
[02/21 16:11:09] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 41/100, Acc=0.6586, Val Loss=1.3737, lr=0.0100
[02/21 16:11:37] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 42/100, Acc=0.6295, Val Loss=1.5089, lr=0.0100
[02/21 16:12:05] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 43/100, Acc=0.6465, Val Loss=1.4233, lr=0.0100
[02/21 16:12:33] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 44/100, Acc=0.6485, Val Loss=1.3968, lr=0.0100
[02/21 16:13:01] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 45/100, Acc=0.6566, Val Loss=1.3808, lr=0.0100
[02/21 16:13:29] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 46/100, Acc=0.6551, Val Loss=1.3695, lr=0.0100
[02/21 16:13:57] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 47/100, Acc=0.6442, Val Loss=1.4586, lr=0.0100
[02/21 16:14:25] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 48/100, Acc=0.6455, Val Loss=1.4450, lr=0.0100
[02/21 16:14:53] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 49/100, Acc=0.6411, Val Loss=1.4719, lr=0.0100
[02/21 16:15:21] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 50/100, Acc=0.6447, Val Loss=1.4513, lr=0.0100
[02/21 16:15:49] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 51/100, Acc=0.6507, Val Loss=1.4115, lr=0.0100
[02/21 16:16:17] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 52/100, Acc=0.6420, Val Loss=1.4773, lr=0.0100
[02/21 16:16:44] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 53/100, Acc=0.6269, Val Loss=1.5619, lr=0.0100
[02/21 16:17:12] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 54/100, Acc=0.6359, Val Loss=1.4898, lr=0.0100
[02/21 16:17:40] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 55/100, Acc=0.6317, Val Loss=1.5227, lr=0.0100
[02/21 16:18:08] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 56/100, Acc=0.6435, Val Loss=1.4473, lr=0.0100
[02/21 16:18:36] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 57/100, Acc=0.6508, Val Loss=1.4299, lr=0.0100
[02/21 16:19:04] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 58/100, Acc=0.6428, Val Loss=1.4340, lr=0.0100
[02/21 16:19:31] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 59/100, Acc=0.6473, Val Loss=1.3950, lr=0.0100
[02/21 16:19:58] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 60/100, Acc=0.7041, Val Loss=1.1585, lr=0.0010
[02/21 16:20:26] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 61/100, Acc=0.7052, Val Loss=1.1634, lr=0.0010
[02/21 16:20:53] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 62/100, Acc=0.7058, Val Loss=1.1583, lr=0.0010
[02/21 16:21:21] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 63/100, Acc=0.7048, Val Loss=1.1660, lr=0.0010
[02/21 16:21:48] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 64/100, Acc=0.7030, Val Loss=1.1737, lr=0.0010
[02/21 16:22:16] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 65/100, Acc=0.7035, Val Loss=1.1750, lr=0.0010
[02/21 16:22:44] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 66/100, Acc=0.7037, Val Loss=1.1972, lr=0.0010
[02/21 16:23:12] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 67/100, Acc=0.7067, Val Loss=1.1940, lr=0.0010
[02/21 16:23:39] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 68/100, Acc=0.7046, Val Loss=1.2016, lr=0.0010
[02/21 16:24:06] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 69/100, Acc=0.7025, Val Loss=1.1998, lr=0.0010
[02/21 16:24:34] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 70/100, Acc=0.7038, Val Loss=1.2073, lr=0.0010
[02/21 16:25:01] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 71/100, Acc=0.7058, Val Loss=1.2109, lr=0.0010
[02/21 16:25:28] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 72/100, Acc=0.7041, Val Loss=1.2163, lr=0.0010
[02/21 16:25:56] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 73/100, Acc=0.7038, Val Loss=1.2221, lr=0.0010
[02/21 16:26:23] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 74/100, Acc=0.7040, Val Loss=1.2194, lr=0.0010
[02/21 16:26:50] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 75/100, Acc=0.7055, Val Loss=1.2228, lr=0.0010
[02/21 16:27:18] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 76/100, Acc=0.7046, Val Loss=1.2183, lr=0.0010
[02/21 16:27:45] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 77/100, Acc=0.7076, Val Loss=1.2271, lr=0.0010
[02/21 16:28:13] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 78/100, Acc=0.7028, Val Loss=1.2354, lr=0.0010
[02/21 16:28:41] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 79/100, Acc=0.7053, Val Loss=1.2470, lr=0.0010
[02/21 16:29:08] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 80/100, Acc=0.7038, Val Loss=1.2353, lr=0.0001
[02/21 16:29:35] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 81/100, Acc=0.7047, Val Loss=1.2319, lr=0.0001
[02/21 16:30:03] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 82/100, Acc=0.7046, Val Loss=1.2279, lr=0.0001
[02/21 16:30:30] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 83/100, Acc=0.7040, Val Loss=1.2312, lr=0.0001
[02/21 16:30:57] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 84/100, Acc=0.7024, Val Loss=1.2374, lr=0.0001
[02/21 16:31:25] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 85/100, Acc=0.7041, Val Loss=1.2318, lr=0.0001
[02/21 16:31:52] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 86/100, Acc=0.7033, Val Loss=1.2360, lr=0.0001
[02/21 16:32:20] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 87/100, Acc=0.7045, Val Loss=1.2368, lr=0.0001
[02/21 16:32:47] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 88/100, Acc=0.7045, Val Loss=1.2311, lr=0.0001
[02/21 16:33:14] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 89/100, Acc=0.7018, Val Loss=1.2355, lr=0.0001
[02/21 16:33:41] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 90/100, Acc=0.7049, Val Loss=1.2344, lr=0.0001
[02/21 16:34:08] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 91/100, Acc=0.7045, Val Loss=1.2422, lr=0.0001
[02/21 16:34:36] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 92/100, Acc=0.7050, Val Loss=1.2304, lr=0.0001
[02/21 16:35:03] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 93/100, Acc=0.7021, Val Loss=1.2397, lr=0.0001
[02/21 16:35:30] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 94/100, Acc=0.7055, Val Loss=1.2312, lr=0.0001
[02/21 16:35:58] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 95/100, Acc=0.7029, Val Loss=1.2392, lr=0.0001
[02/21 16:36:25] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 96/100, Acc=0.7029, Val Loss=1.2402, lr=0.0001
[02/21 16:36:53] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 97/100, Acc=0.7049, Val Loss=1.2371, lr=0.0001
[02/21 16:37:21] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 98/100, Acc=0.7031, Val Loss=1.2335, lr=0.0001
[02/21 16:37:49] cifar100-global-fpgm-2.0-resnet56 INFO: Epoch 99/100, Acc=0.7034, Val Loss=1.2392, lr=0.0001
[02/21 16:37:49] cifar100-global-fpgm-2.0-resnet56 INFO: Best Acc=0.7076
[02/21 16:37:49] cifar100-global-fpgm-2.0-resnet56 INFO: Params: 0.62 M
[02/21 16:37:49] cifar100-global-fpgm-2.0-resnet56 INFO: ops: 63.21 M
[02/21 16:37:52] cifar100-global-fpgm-2.0-resnet56 INFO: Acc: 0.7034 Val Loss: 1.2392

