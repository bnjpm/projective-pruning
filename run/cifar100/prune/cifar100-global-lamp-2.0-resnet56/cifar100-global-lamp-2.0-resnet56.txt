[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: mode: prune
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: model: resnet56
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: verbose: False
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: dataset: cifar100
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: dataroot: data
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: batch_size: 128
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: total_epochs: 100
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: lr_decay_milestones: 60,80
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: lr_decay_gamma: 0.1
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: lr: 0.01
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: restore: run/cifar100/pretrain/cifar10_resnet56.pth
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: output_dir: run/cifar100/prune/cifar100-global-lamp-2.0-resnet56
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: finetune: True
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: last_epochs: 100
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: reps: 1
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: method: lamp
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: speed_up: 2.0
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: max_pruning_ratio: 1.0
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: soft_keeping_ratio: 0.0
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: reg: 1e-05
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: delta_reg: 0.0001
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: weight_decay: 0.0005
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: seed: 1
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: global_pruning: True
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: sl_total_epochs: 100
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: sl_lr: 0.01
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: sl_lr_decay_milestones: 60,80
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: sl_reg_warmup: 0
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: sl_restore: None
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: iterative_steps: 400
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: logger: <Logger cifar100-global-lamp-2.0-resnet56 (DEBUG)>
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: device: cuda
[02/21 12:30:15] cifar100-global-lamp-2.0-resnet56 INFO: num_classes: 100
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: mode: prune
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: model: resnet56
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: verbose: False
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: dataset: cifar100
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: dataroot: data
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: batch_size: 128
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: total_epochs: 100
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: lr_decay_milestones: 60,80
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: lr_decay_gamma: 0.1
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: lr: 0.01
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: output_dir: run/cifar100/prune/cifar100-global-lamp-2.0-resnet56
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: finetune: True
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: last_epochs: 100
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: reps: 1
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: method: lamp
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: speed_up: 2.0
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: max_pruning_ratio: 1.0
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: soft_keeping_ratio: 0.0
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: reg: 1e-05
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: delta_reg: 0.0001
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: weight_decay: 0.0005
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: seed: 1
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: global_pruning: True
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: sl_total_epochs: 100
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: sl_lr: 0.01
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: sl_lr_decay_milestones: 60,80
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: sl_reg_warmup: 0
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: sl_restore: None
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: iterative_steps: 400
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: logger: <Logger cifar100-global-lamp-2.0-resnet56 (DEBUG)>
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: device: cuda
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: num_classes: 100
[02/21 16:38:08] cifar100-global-lamp-2.0-resnet56 INFO: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 16:38:11] cifar100-global-lamp-2.0-resnet56 INFO: Pruning...
[02/21 16:38:30] cifar100-global-lamp-2.0-resnet56 INFO: ResNet(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(16, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(16, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(16, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(16, 25, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(16, 25, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(25, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(25, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(25, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(25, 23, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(23, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(23, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(25, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(25, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(25, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(25, 28, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(25, 29, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(29, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(29, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(29, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(29, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(29, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(29, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(29, 28, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(28, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(29, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(29, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(29, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(27, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(29, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=29, out_features=100, bias=True)
)
[02/21 16:38:33] cifar100-global-lamp-2.0-resnet56 INFO: Params: 0.86 M => 0.26 M (30.06%)
[02/21 16:38:33] cifar100-global-lamp-2.0-resnet56 INFO: FLOPs: 127.12 M => 63.04 M (49.59%, 2.02X )
[02/21 16:38:33] cifar100-global-lamp-2.0-resnet56 INFO: Acc: 0.7269 => 0.0106
[02/21 16:38:33] cifar100-global-lamp-2.0-resnet56 INFO: Val Loss: 1.1578 => 24.4241
[02/21 16:38:33] cifar100-global-lamp-2.0-resnet56 INFO: Finetuning...
[02/21 16:39:00] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 0/100, Acc=0.4207, Val Loss=2.1230, lr=0.0100
[02/21 16:39:27] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 1/100, Acc=0.4753, Val Loss=1.9222, lr=0.0100
[02/21 16:39:55] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 2/100, Acc=0.5055, Val Loss=1.7657, lr=0.0100
[02/21 16:40:22] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 3/100, Acc=0.5223, Val Loss=1.7619, lr=0.0100
[02/21 16:40:49] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 4/100, Acc=0.5026, Val Loss=1.9050, lr=0.0100
[02/21 16:41:17] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 5/100, Acc=0.5384, Val Loss=1.6762, lr=0.0100
[02/21 16:41:44] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 6/100, Acc=0.5486, Val Loss=1.6400, lr=0.0100
[02/21 16:42:11] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 7/100, Acc=0.5468, Val Loss=1.7013, lr=0.0100
[02/21 16:42:38] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 8/100, Acc=0.5491, Val Loss=1.6716, lr=0.0100
[02/21 16:43:06] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 9/100, Acc=0.5384, Val Loss=1.6887, lr=0.0100
[02/21 16:43:33] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 10/100, Acc=0.5156, Val Loss=1.8660, lr=0.0100
[02/21 16:44:00] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 11/100, Acc=0.5600, Val Loss=1.5957, lr=0.0100
[02/21 16:44:27] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 12/100, Acc=0.5736, Val Loss=1.5363, lr=0.0100
[02/21 16:44:55] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 13/100, Acc=0.5832, Val Loss=1.4807, lr=0.0100
[02/21 16:45:22] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 14/100, Acc=0.5713, Val Loss=1.5841, lr=0.0100
[02/21 16:45:50] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 15/100, Acc=0.5831, Val Loss=1.5056, lr=0.0100
[02/21 16:46:17] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 16/100, Acc=0.5877, Val Loss=1.4773, lr=0.0100
[02/21 16:46:44] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 17/100, Acc=0.5811, Val Loss=1.5608, lr=0.0100
[02/21 16:47:12] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 18/100, Acc=0.5622, Val Loss=1.6802, lr=0.0100
[02/21 16:47:39] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 19/100, Acc=0.5744, Val Loss=1.5888, lr=0.0100
[02/21 16:48:06] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 20/100, Acc=0.5934, Val Loss=1.4703, lr=0.0100
[02/21 16:48:34] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 21/100, Acc=0.5922, Val Loss=1.4809, lr=0.0100
[02/21 16:49:01] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 22/100, Acc=0.5735, Val Loss=1.5635, lr=0.0100
[02/21 16:49:28] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 23/100, Acc=0.5936, Val Loss=1.4551, lr=0.0100
[02/21 16:49:55] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 24/100, Acc=0.6039, Val Loss=1.4105, lr=0.0100
[02/21 16:50:23] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 25/100, Acc=0.5809, Val Loss=1.5355, lr=0.0100
[02/21 16:50:50] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 26/100, Acc=0.5830, Val Loss=1.4861, lr=0.0100
[02/21 16:51:17] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 27/100, Acc=0.5769, Val Loss=1.5709, lr=0.0100
[02/21 16:51:45] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 28/100, Acc=0.5750, Val Loss=1.5644, lr=0.0100
[02/21 16:52:12] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 29/100, Acc=0.5883, Val Loss=1.4844, lr=0.0100
[02/21 16:52:39] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 30/100, Acc=0.5651, Val Loss=1.6800, lr=0.0100
[02/21 16:53:07] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 31/100, Acc=0.5919, Val Loss=1.4723, lr=0.0100
[02/21 16:53:34] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 32/100, Acc=0.6024, Val Loss=1.4654, lr=0.0100
[02/21 16:54:01] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 33/100, Acc=0.6008, Val Loss=1.4622, lr=0.0100
[02/21 16:54:29] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 34/100, Acc=0.5911, Val Loss=1.5214, lr=0.0100
[02/21 16:54:56] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 35/100, Acc=0.6019, Val Loss=1.4332, lr=0.0100
[02/21 16:55:24] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 36/100, Acc=0.5770, Val Loss=1.6070, lr=0.0100
[02/21 16:55:51] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 37/100, Acc=0.6046, Val Loss=1.4518, lr=0.0100
[02/21 16:56:18] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 38/100, Acc=0.5709, Val Loss=1.6455, lr=0.0100
[02/21 16:56:46] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 39/100, Acc=0.5958, Val Loss=1.4802, lr=0.0100
[02/21 16:57:13] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 40/100, Acc=0.6035, Val Loss=1.4408, lr=0.0100
[02/21 16:57:41] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 41/100, Acc=0.6092, Val Loss=1.4508, lr=0.0100
[02/21 16:58:08] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 42/100, Acc=0.5956, Val Loss=1.5036, lr=0.0100
[02/21 16:58:36] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 43/100, Acc=0.5876, Val Loss=1.5655, lr=0.0100
[02/21 16:59:03] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 44/100, Acc=0.5982, Val Loss=1.5001, lr=0.0100
[02/21 16:59:31] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 45/100, Acc=0.5935, Val Loss=1.5501, lr=0.0100
[02/21 16:59:58] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 46/100, Acc=0.5981, Val Loss=1.4722, lr=0.0100
[02/21 17:00:26] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 47/100, Acc=0.6168, Val Loss=1.4132, lr=0.0100
[02/21 17:00:53] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 48/100, Acc=0.5994, Val Loss=1.4767, lr=0.0100
[02/21 17:01:21] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 49/100, Acc=0.5935, Val Loss=1.5173, lr=0.0100
[02/21 17:01:48] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 50/100, Acc=0.5932, Val Loss=1.5140, lr=0.0100
[02/21 17:02:15] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 51/100, Acc=0.6053, Val Loss=1.4808, lr=0.0100
[02/21 17:02:43] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 52/100, Acc=0.5989, Val Loss=1.5030, lr=0.0100
[02/21 17:03:11] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 53/100, Acc=0.5973, Val Loss=1.4710, lr=0.0100
[02/21 17:03:38] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 54/100, Acc=0.5933, Val Loss=1.5164, lr=0.0100
[02/21 17:04:05] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 55/100, Acc=0.5916, Val Loss=1.5147, lr=0.0100
[02/21 17:04:33] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 56/100, Acc=0.5981, Val Loss=1.4757, lr=0.0100
[02/21 17:05:00] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 57/100, Acc=0.6062, Val Loss=1.4774, lr=0.0100
[02/21 17:05:28] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 58/100, Acc=0.6089, Val Loss=1.4624, lr=0.0100
[02/21 17:05:55] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 59/100, Acc=0.6140, Val Loss=1.4591, lr=0.0100
[02/21 17:06:22] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 60/100, Acc=0.6730, Val Loss=1.1637, lr=0.0010
[02/21 17:06:50] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 61/100, Acc=0.6719, Val Loss=1.1685, lr=0.0010
[02/21 17:07:17] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 62/100, Acc=0.6750, Val Loss=1.1626, lr=0.0010
[02/21 17:07:45] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 63/100, Acc=0.6706, Val Loss=1.1648, lr=0.0010
[02/21 17:08:13] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 64/100, Acc=0.6713, Val Loss=1.1732, lr=0.0010
[02/21 17:08:40] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 65/100, Acc=0.6713, Val Loss=1.1679, lr=0.0010
[02/21 17:09:08] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 66/100, Acc=0.6681, Val Loss=1.1845, lr=0.0010
[02/21 17:09:35] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 67/100, Acc=0.6679, Val Loss=1.1748, lr=0.0010
[02/21 17:10:03] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 68/100, Acc=0.6730, Val Loss=1.1794, lr=0.0010
[02/21 17:10:30] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 69/100, Acc=0.6707, Val Loss=1.1880, lr=0.0010
[02/21 17:10:58] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 70/100, Acc=0.6702, Val Loss=1.1937, lr=0.0010
[02/21 17:11:26] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 71/100, Acc=0.6709, Val Loss=1.1961, lr=0.0010
[02/21 17:11:53] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 72/100, Acc=0.6701, Val Loss=1.1900, lr=0.0010
[02/21 17:12:20] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 73/100, Acc=0.6675, Val Loss=1.1990, lr=0.0010
[02/21 17:12:47] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 74/100, Acc=0.6685, Val Loss=1.2033, lr=0.0010
[02/21 17:13:15] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 75/100, Acc=0.6689, Val Loss=1.2074, lr=0.0010
[02/21 17:13:42] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 76/100, Acc=0.6707, Val Loss=1.1992, lr=0.0010
[02/21 17:14:09] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 77/100, Acc=0.6702, Val Loss=1.2031, lr=0.0010
[02/21 17:14:37] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 78/100, Acc=0.6696, Val Loss=1.2240, lr=0.0010
[02/21 17:15:04] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 79/100, Acc=0.6680, Val Loss=1.2213, lr=0.0010
[02/21 17:15:31] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 80/100, Acc=0.6715, Val Loss=1.2019, lr=0.0001
[02/21 17:15:58] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 81/100, Acc=0.6729, Val Loss=1.1971, lr=0.0001
[02/21 17:16:26] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 82/100, Acc=0.6713, Val Loss=1.1989, lr=0.0001
[02/21 17:16:53] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 83/100, Acc=0.6732, Val Loss=1.2016, lr=0.0001
[02/21 17:17:20] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 84/100, Acc=0.6724, Val Loss=1.2052, lr=0.0001
[02/21 17:17:47] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 85/100, Acc=0.6705, Val Loss=1.2038, lr=0.0001
[02/21 17:18:13] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 86/100, Acc=0.6710, Val Loss=1.2031, lr=0.0001
[02/21 17:18:40] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 87/100, Acc=0.6690, Val Loss=1.2053, lr=0.0001
[02/21 17:19:07] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 88/100, Acc=0.6707, Val Loss=1.2074, lr=0.0001
[02/21 17:19:34] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 89/100, Acc=0.6705, Val Loss=1.2038, lr=0.0001
[02/21 17:20:01] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 90/100, Acc=0.6723, Val Loss=1.2019, lr=0.0001
[02/21 17:20:28] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 91/100, Acc=0.6710, Val Loss=1.2092, lr=0.0001
[02/21 17:20:55] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 92/100, Acc=0.6719, Val Loss=1.1980, lr=0.0001
[02/21 17:21:22] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 93/100, Acc=0.6696, Val Loss=1.2062, lr=0.0001
[02/21 17:21:49] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 94/100, Acc=0.6700, Val Loss=1.2042, lr=0.0001
[02/21 17:22:16] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 95/100, Acc=0.6694, Val Loss=1.2118, lr=0.0001
[02/21 17:22:43] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 96/100, Acc=0.6698, Val Loss=1.2041, lr=0.0001
[02/21 17:23:10] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 97/100, Acc=0.6702, Val Loss=1.1998, lr=0.0001
[02/21 17:23:37] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 98/100, Acc=0.6708, Val Loss=1.2026, lr=0.0001
[02/21 17:24:04] cifar100-global-lamp-2.0-resnet56 INFO: Epoch 99/100, Acc=0.6692, Val Loss=1.2047, lr=0.0001
[02/21 17:24:04] cifar100-global-lamp-2.0-resnet56 INFO: Best Acc=0.6750
[02/21 17:24:04] cifar100-global-lamp-2.0-resnet56 INFO: Params: 0.26 M
[02/21 17:24:04] cifar100-global-lamp-2.0-resnet56 INFO: ops: 63.04 M
[02/21 17:24:06] cifar100-global-lamp-2.0-resnet56 INFO: Acc: 0.6692 Val Loss: 1.2047

