[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: mode: prune
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: model: resnet56
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: verbose: False
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: dataset: cifar100
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: dataroot: data
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: batch_size: 128
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: total_epochs: 100
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: lr_decay_milestones: 60,80
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: lr_decay_gamma: 0.1
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: lr: 0.01
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: restore: run/cifar100/pretrain/cifar10_resnet56.pth
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: output_dir: run/cifar100/prune/cifar100-global-lamp-3.0-resnet56
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: finetune: True
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: last_epochs: 100
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: reps: 1
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: method: lamp
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: speed_up: 3.0
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: max_pruning_ratio: 1.0
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: soft_keeping_ratio: 0.0
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: reg: 1e-05
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: delta_reg: 0.0001
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: weight_decay: 0.0005
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: seed: 1
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: global_pruning: True
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: sl_total_epochs: 100
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: sl_lr: 0.01
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: sl_lr_decay_milestones: 60,80
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: sl_reg_warmup: 0
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: sl_restore: None
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: iterative_steps: 400
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: logger: <Logger cifar100-global-lamp-3.0-resnet56 (DEBUG)>
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: device: cuda
[02/21 12:30:36] cifar100-global-lamp-3.0-resnet56 INFO: num_classes: 100
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: mode: prune
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: model: resnet56
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: verbose: False
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: dataset: cifar100
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: dataroot: data
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: batch_size: 128
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: total_epochs: 100
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: lr_decay_milestones: 60,80
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: lr_decay_gamma: 0.1
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: lr: 0.01
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: output_dir: run/cifar100/prune/cifar100-global-lamp-3.0-resnet56
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: finetune: True
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: last_epochs: 100
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: reps: 1
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: method: lamp
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: speed_up: 3.0
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: max_pruning_ratio: 1.0
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: soft_keeping_ratio: 0.0
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: reg: 1e-05
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: delta_reg: 0.0001
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: weight_decay: 0.0005
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: seed: 1
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: global_pruning: True
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: sl_total_epochs: 100
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: sl_lr: 0.01
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: sl_lr_decay_milestones: 60,80
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: sl_reg_warmup: 0
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: sl_restore: None
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: iterative_steps: 400
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: logger: <Logger cifar100-global-lamp-3.0-resnet56 (DEBUG)>
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: device: cuda
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: num_classes: 100
[02/21 18:32:23] cifar100-global-lamp-3.0-resnet56 INFO: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 18:32:26] cifar100-global-lamp-3.0-resnet56 INFO: Pruning...
[02/21 18:32:51] cifar100-global-lamp-3.0-resnet56 INFO: ResNet(
  (conv1): Conv2d(3, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(14, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(11, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(14, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(14, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(14, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(14, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(14, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(14, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(14, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(14, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(13, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(14, 19, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(14, 18, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(18, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(18, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(18, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(18, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(18, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(18, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(18, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(18, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(18, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(18, 22, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(22, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(21, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(22, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(22, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(22, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(21, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(22, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(21, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(21, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(22, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(22, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=22, out_features=100, bias=True)
)
[02/21 18:32:54] cifar100-global-lamp-3.0-resnet56 INFO: Params: 0.86 M => 0.15 M (17.64%)
[02/21 18:32:54] cifar100-global-lamp-3.0-resnet56 INFO: FLOPs: 127.12 M => 42.14 M (33.15%, 3.02X )
[02/21 18:32:54] cifar100-global-lamp-3.0-resnet56 INFO: Acc: 0.7269 => 0.0109
[02/21 18:32:54] cifar100-global-lamp-3.0-resnet56 INFO: Val Loss: 1.1578 => 25.7632
[02/21 18:32:54] cifar100-global-lamp-3.0-resnet56 INFO: Finetuning...
[02/21 18:33:21] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 0/100, Acc=0.2799, Val Loss=2.7470, lr=0.0100
[02/21 18:33:48] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 1/100, Acc=0.3707, Val Loss=2.3242, lr=0.0100
[02/21 18:34:15] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 2/100, Acc=0.4036, Val Loss=2.2138, lr=0.0100
[02/21 18:34:43] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 3/100, Acc=0.4056, Val Loss=2.2973, lr=0.0100
[02/21 18:35:10] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 4/100, Acc=0.4319, Val Loss=2.1367, lr=0.0100
[02/21 18:35:37] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 5/100, Acc=0.4488, Val Loss=2.0157, lr=0.0100
[02/21 18:36:04] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 6/100, Acc=0.4796, Val Loss=1.8683, lr=0.0100
[02/21 18:36:31] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 7/100, Acc=0.4640, Val Loss=1.9882, lr=0.0100
[02/21 18:36:58] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 8/100, Acc=0.4714, Val Loss=1.9116, lr=0.0100
[02/21 18:37:25] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 9/100, Acc=0.4810, Val Loss=1.9361, lr=0.0100
[02/21 18:37:52] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 10/100, Acc=0.4839, Val Loss=1.9148, lr=0.0100
[02/21 18:38:19] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 11/100, Acc=0.5003, Val Loss=1.8110, lr=0.0100
[02/21 18:38:46] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 12/100, Acc=0.4861, Val Loss=1.8702, lr=0.0100
[02/21 18:39:13] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 13/100, Acc=0.4926, Val Loss=1.8885, lr=0.0100
[02/21 18:39:40] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 14/100, Acc=0.5232, Val Loss=1.7036, lr=0.0100
[02/21 18:40:07] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 15/100, Acc=0.5112, Val Loss=1.8031, lr=0.0100
[02/21 18:40:34] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 16/100, Acc=0.5265, Val Loss=1.7248, lr=0.0100
[02/21 18:41:01] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 17/100, Acc=0.5055, Val Loss=1.7738, lr=0.0100
[02/21 18:41:28] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 18/100, Acc=0.5059, Val Loss=1.8261, lr=0.0100
[02/21 18:41:55] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 19/100, Acc=0.5099, Val Loss=1.8280, lr=0.0100
[02/21 18:42:22] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 20/100, Acc=0.5182, Val Loss=1.7676, lr=0.0100
[02/21 18:42:50] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 21/100, Acc=0.5292, Val Loss=1.7436, lr=0.0100
[02/21 18:43:17] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 22/100, Acc=0.5076, Val Loss=1.8610, lr=0.0100
[02/21 18:43:44] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 23/100, Acc=0.5315, Val Loss=1.6742, lr=0.0100
[02/21 18:44:12] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 24/100, Acc=0.5167, Val Loss=1.7890, lr=0.0100
[02/21 18:44:39] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 25/100, Acc=0.5264, Val Loss=1.7302, lr=0.0100
[02/21 18:45:07] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 26/100, Acc=0.5441, Val Loss=1.6426, lr=0.0100
[02/21 18:45:34] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 27/100, Acc=0.5304, Val Loss=1.7116, lr=0.0100
[02/21 18:46:01] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 28/100, Acc=0.5411, Val Loss=1.6588, lr=0.0100
[02/21 18:46:29] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 29/100, Acc=0.5306, Val Loss=1.7243, lr=0.0100
[02/21 18:46:56] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 30/100, Acc=0.4860, Val Loss=1.9328, lr=0.0100
[02/21 18:47:24] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 31/100, Acc=0.5270, Val Loss=1.7219, lr=0.0100
[02/21 18:47:51] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 32/100, Acc=0.5395, Val Loss=1.7242, lr=0.0100
[02/21 18:48:18] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 33/100, Acc=0.5339, Val Loss=1.7129, lr=0.0100
[02/21 18:48:45] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 34/100, Acc=0.5187, Val Loss=1.8022, lr=0.0100
[02/21 18:49:12] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 35/100, Acc=0.5306, Val Loss=1.7099, lr=0.0100
[02/21 18:49:39] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 36/100, Acc=0.5341, Val Loss=1.7121, lr=0.0100
[02/21 18:50:06] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 37/100, Acc=0.5374, Val Loss=1.7047, lr=0.0100
[02/21 18:50:33] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 38/100, Acc=0.5390, Val Loss=1.7014, lr=0.0100
[02/21 18:51:00] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 39/100, Acc=0.5349, Val Loss=1.7154, lr=0.0100
[02/21 18:51:27] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 40/100, Acc=0.5435, Val Loss=1.7356, lr=0.0100
[02/21 18:51:54] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 41/100, Acc=0.5294, Val Loss=1.7731, lr=0.0100
[02/21 18:52:22] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 42/100, Acc=0.5172, Val Loss=1.8203, lr=0.0100
[02/21 18:52:49] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 43/100, Acc=0.5506, Val Loss=1.6313, lr=0.0100
[02/21 18:53:16] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 44/100, Acc=0.5259, Val Loss=1.7659, lr=0.0100
[02/21 18:53:43] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 45/100, Acc=0.5575, Val Loss=1.6341, lr=0.0100
[02/21 18:54:10] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 46/100, Acc=0.5278, Val Loss=1.7523, lr=0.0100
[02/21 18:54:37] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 47/100, Acc=0.5270, Val Loss=1.7894, lr=0.0100
[02/21 18:55:04] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 48/100, Acc=0.5278, Val Loss=1.7678, lr=0.0100
[02/21 18:55:31] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 49/100, Acc=0.5659, Val Loss=1.5754, lr=0.0100
[02/21 18:55:59] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 50/100, Acc=0.5522, Val Loss=1.6335, lr=0.0100
[02/21 18:56:26] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 51/100, Acc=0.5318, Val Loss=1.7868, lr=0.0100
[02/21 18:56:53] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 52/100, Acc=0.5517, Val Loss=1.6284, lr=0.0100
[02/21 18:57:20] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 53/100, Acc=0.5383, Val Loss=1.7115, lr=0.0100
[02/21 18:57:48] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 54/100, Acc=0.5246, Val Loss=1.7681, lr=0.0100
[02/21 18:58:15] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 55/100, Acc=0.5401, Val Loss=1.6944, lr=0.0100
[02/21 18:58:42] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 56/100, Acc=0.5472, Val Loss=1.6713, lr=0.0100
[02/21 18:59:09] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 57/100, Acc=0.5542, Val Loss=1.6475, lr=0.0100
[02/21 18:59:36] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 58/100, Acc=0.5617, Val Loss=1.6442, lr=0.0100
[02/21 19:00:03] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 59/100, Acc=0.5091, Val Loss=1.8898, lr=0.0100
[02/21 19:00:30] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 60/100, Acc=0.6224, Val Loss=1.3340, lr=0.0010
[02/21 19:00:58] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 61/100, Acc=0.6276, Val Loss=1.3317, lr=0.0010
[02/21 19:01:25] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 62/100, Acc=0.6287, Val Loss=1.3223, lr=0.0010
[02/21 19:01:52] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 63/100, Acc=0.6295, Val Loss=1.3182, lr=0.0010
[02/21 19:02:20] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 64/100, Acc=0.6276, Val Loss=1.3228, lr=0.0010
[02/21 19:02:47] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 65/100, Acc=0.6280, Val Loss=1.3237, lr=0.0010
[02/21 19:03:14] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 66/100, Acc=0.6279, Val Loss=1.3318, lr=0.0010
[02/21 19:03:41] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 67/100, Acc=0.6285, Val Loss=1.3260, lr=0.0010
[02/21 19:04:08] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 68/100, Acc=0.6284, Val Loss=1.3205, lr=0.0010
[02/21 19:04:35] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 69/100, Acc=0.6290, Val Loss=1.3382, lr=0.0010
[02/21 19:05:03] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 70/100, Acc=0.6330, Val Loss=1.3287, lr=0.0010
[02/21 19:05:30] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 71/100, Acc=0.6289, Val Loss=1.3443, lr=0.0010
[02/21 19:05:58] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 72/100, Acc=0.6300, Val Loss=1.3367, lr=0.0010
[02/21 19:06:25] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 73/100, Acc=0.6278, Val Loss=1.3440, lr=0.0010
[02/21 19:06:53] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 74/100, Acc=0.6309, Val Loss=1.3388, lr=0.0010
[02/21 19:07:20] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 75/100, Acc=0.6319, Val Loss=1.3376, lr=0.0010
[02/21 19:07:48] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 76/100, Acc=0.6292, Val Loss=1.3337, lr=0.0010
[02/21 19:08:15] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 77/100, Acc=0.6279, Val Loss=1.3351, lr=0.0010
[02/21 19:08:43] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 78/100, Acc=0.6303, Val Loss=1.3455, lr=0.0010
[02/21 19:09:10] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 79/100, Acc=0.6286, Val Loss=1.3609, lr=0.0010
[02/21 19:09:38] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 80/100, Acc=0.6330, Val Loss=1.3336, lr=0.0001
[02/21 19:10:05] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 81/100, Acc=0.6337, Val Loss=1.3287, lr=0.0001
[02/21 19:10:33] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 82/100, Acc=0.6363, Val Loss=1.3263, lr=0.0001
[02/21 19:11:00] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 83/100, Acc=0.6367, Val Loss=1.3285, lr=0.0001
[02/21 19:11:28] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 84/100, Acc=0.6344, Val Loss=1.3301, lr=0.0001
[02/21 19:11:55] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 85/100, Acc=0.6344, Val Loss=1.3316, lr=0.0001
[02/21 19:12:23] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 86/100, Acc=0.6356, Val Loss=1.3315, lr=0.0001
[02/21 19:12:51] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 87/100, Acc=0.6348, Val Loss=1.3362, lr=0.0001
[02/21 19:13:18] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 88/100, Acc=0.6331, Val Loss=1.3321, lr=0.0001
[02/21 19:13:46] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 89/100, Acc=0.6344, Val Loss=1.3326, lr=0.0001
[02/21 19:14:13] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 90/100, Acc=0.6327, Val Loss=1.3291, lr=0.0001
[02/21 19:14:41] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 91/100, Acc=0.6331, Val Loss=1.3343, lr=0.0001
[02/21 19:15:08] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 92/100, Acc=0.6354, Val Loss=1.3244, lr=0.0001
[02/21 19:15:36] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 93/100, Acc=0.6349, Val Loss=1.3340, lr=0.0001
[02/21 19:16:03] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 94/100, Acc=0.6351, Val Loss=1.3332, lr=0.0001
[02/21 19:16:31] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 95/100, Acc=0.6345, Val Loss=1.3318, lr=0.0001
[02/21 19:16:59] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 96/100, Acc=0.6339, Val Loss=1.3296, lr=0.0001
[02/21 19:17:26] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 97/100, Acc=0.6362, Val Loss=1.3269, lr=0.0001
[02/21 19:17:54] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 98/100, Acc=0.6378, Val Loss=1.3267, lr=0.0001
[02/21 19:18:22] cifar100-global-lamp-3.0-resnet56 INFO: Epoch 99/100, Acc=0.6366, Val Loss=1.3295, lr=0.0001
[02/21 19:18:22] cifar100-global-lamp-3.0-resnet56 INFO: Best Acc=0.6378
[02/21 19:18:22] cifar100-global-lamp-3.0-resnet56 INFO: Params: 0.15 M
[02/21 19:18:22] cifar100-global-lamp-3.0-resnet56 INFO: ops: 42.14 M
[02/21 19:18:24] cifar100-global-lamp-3.0-resnet56 INFO: Acc: 0.6366 Val Loss: 1.3295

