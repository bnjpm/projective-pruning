[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: mode: prune
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: model: resnet56
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: verbose: False
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: dataset: cifar100
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: dataroot: data
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: batch_size: 128
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: total_epochs: 100
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: lr_decay_milestones: 60,80
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: lr_decay_gamma: 0.1
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: lr: 0.01
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: restore: run/cifar100/pretrain/cifar10_resnet56.pth
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: output_dir: run/cifar100/prune/cifar100-global-fpgm-3.0-resnet56
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: finetune: True
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: last_epochs: 100
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: reps: 1
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: method: fpgm
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: speed_up: 3.0
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: max_pruning_ratio: 1.0
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: soft_keeping_ratio: 0.0
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: reg: 1e-05
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: delta_reg: 0.0001
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: weight_decay: 0.0005
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: seed: 1
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: global_pruning: True
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: sl_total_epochs: 100
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: sl_lr: 0.01
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: sl_lr_decay_milestones: 60,80
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: sl_reg_warmup: 0
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: sl_restore: None
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: iterative_steps: 400
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: logger: <Logger cifar100-global-fpgm-3.0-resnet56 (DEBUG)>
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: device: cuda
[02/21 12:30:16] cifar100-global-fpgm-3.0-resnet56 INFO: num_classes: 100
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: mode: prune
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: model: resnet56
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: verbose: False
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: dataset: cifar100
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: dataroot: data
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: batch_size: 128
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: total_epochs: 100
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: lr_decay_milestones: 60,80
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: lr_decay_gamma: 0.1
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: lr: 0.01
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: output_dir: run/cifar100/prune/cifar100-global-fpgm-3.0-resnet56
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: finetune: True
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: last_epochs: 100
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: reps: 1
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: method: fpgm
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: speed_up: 3.0
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: max_pruning_ratio: 1.0
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: soft_keeping_ratio: 0.0
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: reg: 1e-05
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: delta_reg: 0.0001
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: weight_decay: 0.0005
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: seed: 1
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: global_pruning: True
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: sl_total_epochs: 100
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: sl_lr: 0.01
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: sl_lr_decay_milestones: 60,80
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: sl_reg_warmup: 0
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: sl_restore: None
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: iterative_steps: 400
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: logger: <Logger cifar100-global-fpgm-3.0-resnet56 (DEBUG)>
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: device: cuda
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: num_classes: 100
[02/21 17:45:56] cifar100-global-fpgm-3.0-resnet56 INFO: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 17:46:00] cifar100-global-fpgm-3.0-resnet56 INFO: Pruning...
[02/21 17:46:20] cifar100-global-fpgm-3.0-resnet56 INFO: ResNet(
  (conv1): Conv2d(3, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(11, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(11, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(11, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(11, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(11, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(11, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(11, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(11, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(11, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(11, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(16, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(11, 11, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(11, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(11, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(7, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(11, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(15, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(11, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(11, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(11, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(12, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(11, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(11, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(11, 9, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(9, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(11, 57, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(57, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(47, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(47, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(57, 58, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(58, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(57, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(57, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(57, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(57, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(50, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(57, 61, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(61, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(61, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(57, 46, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(46, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(57, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 57, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(57, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=57, out_features=100, bias=True)
)
[02/21 17:46:22] cifar100-global-fpgm-3.0-resnet56 INFO: Params: 0.86 M => 0.44 M (51.21%)
[02/21 17:46:22] cifar100-global-fpgm-3.0-resnet56 INFO: FLOPs: 127.12 M => 42.26 M (33.25%, 3.01X )
[02/21 17:46:22] cifar100-global-fpgm-3.0-resnet56 INFO: Acc: 0.7269 => 0.0193
[02/21 17:46:22] cifar100-global-fpgm-3.0-resnet56 INFO: Val Loss: 1.1578 => 7.9602
[02/21 17:46:22] cifar100-global-fpgm-3.0-resnet56 INFO: Finetuning...
[02/21 17:46:49] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 0/100, Acc=0.5029, Val Loss=1.8702, lr=0.0100
[02/21 17:47:17] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 1/100, Acc=0.5682, Val Loss=1.5498, lr=0.0100
[02/21 17:47:44] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 2/100, Acc=0.5768, Val Loss=1.5428, lr=0.0100
[02/21 17:48:11] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 3/100, Acc=0.5700, Val Loss=1.6043, lr=0.0100
[02/21 17:48:38] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 4/100, Acc=0.5861, Val Loss=1.5265, lr=0.0100
[02/21 17:49:06] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 5/100, Acc=0.6032, Val Loss=1.4439, lr=0.0100
[02/21 17:49:33] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 6/100, Acc=0.6076, Val Loss=1.4333, lr=0.0100
[02/21 17:50:00] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 7/100, Acc=0.5954, Val Loss=1.5184, lr=0.0100
[02/21 17:50:27] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 8/100, Acc=0.6026, Val Loss=1.4884, lr=0.0100
[02/21 17:50:55] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 9/100, Acc=0.5950, Val Loss=1.5088, lr=0.0100
[02/21 17:51:22] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 10/100, Acc=0.5372, Val Loss=1.8422, lr=0.0100
[02/21 17:51:49] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 11/100, Acc=0.6103, Val Loss=1.4378, lr=0.0100
[02/21 17:52:17] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 12/100, Acc=0.5982, Val Loss=1.5152, lr=0.0100
[02/21 17:52:44] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 13/100, Acc=0.6097, Val Loss=1.4545, lr=0.0100
[02/21 17:53:12] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 14/100, Acc=0.6187, Val Loss=1.4072, lr=0.0100
[02/21 17:53:39] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 15/100, Acc=0.6134, Val Loss=1.4246, lr=0.0100
[02/21 17:54:07] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 16/100, Acc=0.6035, Val Loss=1.4483, lr=0.0100
[02/21 17:54:34] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 17/100, Acc=0.6076, Val Loss=1.4725, lr=0.0100
[02/21 17:55:02] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 18/100, Acc=0.6132, Val Loss=1.4507, lr=0.0100
[02/21 17:55:29] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 19/100, Acc=0.6152, Val Loss=1.4596, lr=0.0100
[02/21 17:55:57] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 20/100, Acc=0.6197, Val Loss=1.3881, lr=0.0100
[02/21 17:56:24] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 21/100, Acc=0.6165, Val Loss=1.4172, lr=0.0100
[02/21 17:56:52] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 22/100, Acc=0.6173, Val Loss=1.4433, lr=0.0100
[02/21 17:57:19] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 23/100, Acc=0.6044, Val Loss=1.5104, lr=0.0100
[02/21 17:57:47] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 24/100, Acc=0.6215, Val Loss=1.4004, lr=0.0100
[02/21 17:58:15] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 25/100, Acc=0.6141, Val Loss=1.4644, lr=0.0100
[02/21 17:58:42] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 26/100, Acc=0.5965, Val Loss=1.5339, lr=0.0100
[02/21 17:59:10] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 27/100, Acc=0.6097, Val Loss=1.5093, lr=0.0100
[02/21 17:59:38] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 28/100, Acc=0.6142, Val Loss=1.4499, lr=0.0100
[02/21 18:00:05] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 29/100, Acc=0.6134, Val Loss=1.4782, lr=0.0100
[02/21 18:00:33] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 30/100, Acc=0.6085, Val Loss=1.5155, lr=0.0100
[02/21 18:01:00] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 31/100, Acc=0.6041, Val Loss=1.5326, lr=0.0100
[02/21 18:01:28] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 32/100, Acc=0.6336, Val Loss=1.3782, lr=0.0100
[02/21 18:01:56] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 33/100, Acc=0.6389, Val Loss=1.3489, lr=0.0100
[02/21 18:02:24] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 34/100, Acc=0.6244, Val Loss=1.4232, lr=0.0100
[02/21 18:02:51] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 35/100, Acc=0.6340, Val Loss=1.3732, lr=0.0100
[02/21 18:03:19] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 36/100, Acc=0.6163, Val Loss=1.4571, lr=0.0100
[02/21 18:03:46] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 37/100, Acc=0.6271, Val Loss=1.4101, lr=0.0100
[02/21 18:04:14] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 38/100, Acc=0.6191, Val Loss=1.4989, lr=0.0100
[02/21 18:04:42] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 39/100, Acc=0.6282, Val Loss=1.4387, lr=0.0100
[02/21 18:05:09] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 40/100, Acc=0.6086, Val Loss=1.4862, lr=0.0100
[02/21 18:05:36] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 41/100, Acc=0.6014, Val Loss=1.5582, lr=0.0100
[02/21 18:06:03] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 42/100, Acc=0.6182, Val Loss=1.4508, lr=0.0100
[02/21 18:06:30] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 43/100, Acc=0.6251, Val Loss=1.4568, lr=0.0100
[02/21 18:06:57] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 44/100, Acc=0.6347, Val Loss=1.3721, lr=0.0100
[02/21 18:07:24] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 45/100, Acc=0.6292, Val Loss=1.4179, lr=0.0100
[02/21 18:07:51] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 46/100, Acc=0.6128, Val Loss=1.5214, lr=0.0100
[02/21 18:08:18] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 47/100, Acc=0.6287, Val Loss=1.4577, lr=0.0100
[02/21 18:08:45] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 48/100, Acc=0.6069, Val Loss=1.5202, lr=0.0100
[02/21 18:09:12] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 49/100, Acc=0.6270, Val Loss=1.4035, lr=0.0100
[02/21 18:09:39] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 50/100, Acc=0.6173, Val Loss=1.4964, lr=0.0100
[02/21 18:10:07] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 51/100, Acc=0.6167, Val Loss=1.4742, lr=0.0100
[02/21 18:10:35] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 52/100, Acc=0.5911, Val Loss=1.6572, lr=0.0100
[02/21 18:11:02] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 53/100, Acc=0.5973, Val Loss=1.5692, lr=0.0100
[02/21 18:11:30] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 54/100, Acc=0.6129, Val Loss=1.5102, lr=0.0100
[02/21 18:11:57] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 55/100, Acc=0.6246, Val Loss=1.4592, lr=0.0100
[02/21 18:12:25] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 56/100, Acc=0.6219, Val Loss=1.4798, lr=0.0100
[02/21 18:12:53] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 57/100, Acc=0.6219, Val Loss=1.4805, lr=0.0100
[02/21 18:13:20] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 58/100, Acc=0.6226, Val Loss=1.4572, lr=0.0100
[02/21 18:13:48] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 59/100, Acc=0.5840, Val Loss=1.6393, lr=0.0100
[02/21 18:14:15] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 60/100, Acc=0.6856, Val Loss=1.1591, lr=0.0010
[02/21 18:14:43] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 61/100, Acc=0.6883, Val Loss=1.1535, lr=0.0010
[02/21 18:15:11] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 62/100, Acc=0.6900, Val Loss=1.1491, lr=0.0010
[02/21 18:15:38] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 63/100, Acc=0.6885, Val Loss=1.1654, lr=0.0010
[02/21 18:16:06] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 64/100, Acc=0.6897, Val Loss=1.1673, lr=0.0010
[02/21 18:16:34] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 65/100, Acc=0.6900, Val Loss=1.1690, lr=0.0010
[02/21 18:17:01] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 66/100, Acc=0.6919, Val Loss=1.1778, lr=0.0010
[02/21 18:17:29] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 67/100, Acc=0.6907, Val Loss=1.1820, lr=0.0010
[02/21 18:17:56] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 68/100, Acc=0.6904, Val Loss=1.1806, lr=0.0010
[02/21 18:18:24] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 69/100, Acc=0.6883, Val Loss=1.1925, lr=0.0010
[02/21 18:18:51] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 70/100, Acc=0.6882, Val Loss=1.1919, lr=0.0010
[02/21 18:19:19] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 71/100, Acc=0.6867, Val Loss=1.1979, lr=0.0010
[02/21 18:19:46] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 72/100, Acc=0.6873, Val Loss=1.1942, lr=0.0010
[02/21 18:20:13] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 73/100, Acc=0.6886, Val Loss=1.2101, lr=0.0010
[02/21 18:20:40] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 74/100, Acc=0.6883, Val Loss=1.2001, lr=0.0010
[02/21 18:21:07] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 75/100, Acc=0.6838, Val Loss=1.2182, lr=0.0010
[02/21 18:21:34] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 76/100, Acc=0.6868, Val Loss=1.2203, lr=0.0010
[02/21 18:22:01] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 77/100, Acc=0.6856, Val Loss=1.2190, lr=0.0010
[02/21 18:22:28] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 78/100, Acc=0.6878, Val Loss=1.2210, lr=0.0010
[02/21 18:22:56] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 79/100, Acc=0.6869, Val Loss=1.2353, lr=0.0010
[02/21 18:23:23] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 80/100, Acc=0.6887, Val Loss=1.2241, lr=0.0001
[02/21 18:23:50] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 81/100, Acc=0.6876, Val Loss=1.2199, lr=0.0001
[02/21 18:24:17] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 82/100, Acc=0.6889, Val Loss=1.2183, lr=0.0001
[02/21 18:24:45] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 83/100, Acc=0.6872, Val Loss=1.2201, lr=0.0001
[02/21 18:25:12] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 84/100, Acc=0.6868, Val Loss=1.2248, lr=0.0001
[02/21 18:25:39] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 85/100, Acc=0.6876, Val Loss=1.2223, lr=0.0001
[02/21 18:26:07] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 86/100, Acc=0.6890, Val Loss=1.2211, lr=0.0001
[02/21 18:26:34] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 87/100, Acc=0.6883, Val Loss=1.2226, lr=0.0001
[02/21 18:27:02] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 88/100, Acc=0.6878, Val Loss=1.2221, lr=0.0001
[02/21 18:27:29] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 89/100, Acc=0.6884, Val Loss=1.2218, lr=0.0001
[02/21 18:27:56] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 90/100, Acc=0.6883, Val Loss=1.2202, lr=0.0001
[02/21 18:28:24] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 91/100, Acc=0.6870, Val Loss=1.2311, lr=0.0001
[02/21 18:28:51] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 92/100, Acc=0.6901, Val Loss=1.2172, lr=0.0001
[02/21 18:29:19] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 93/100, Acc=0.6871, Val Loss=1.2256, lr=0.0001
[02/21 18:29:46] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 94/100, Acc=0.6878, Val Loss=1.2215, lr=0.0001
[02/21 18:30:14] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 95/100, Acc=0.6861, Val Loss=1.2304, lr=0.0001
[02/21 18:30:41] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 96/100, Acc=0.6900, Val Loss=1.2252, lr=0.0001
[02/21 18:31:09] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 97/100, Acc=0.6865, Val Loss=1.2248, lr=0.0001
[02/21 18:31:36] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 98/100, Acc=0.6881, Val Loss=1.2231, lr=0.0001
[02/21 18:32:04] cifar100-global-fpgm-3.0-resnet56 INFO: Epoch 99/100, Acc=0.6862, Val Loss=1.2246, lr=0.0001
[02/21 18:32:04] cifar100-global-fpgm-3.0-resnet56 INFO: Best Acc=0.6919
[02/21 18:32:04] cifar100-global-fpgm-3.0-resnet56 INFO: Params: 0.44 M
[02/21 18:32:04] cifar100-global-fpgm-3.0-resnet56 INFO: ops: 42.26 M
[02/21 18:32:07] cifar100-global-fpgm-3.0-resnet56 INFO: Acc: 0.6862 Val Loss: 1.2246

