[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: mode: prune
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: model: resnet56
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: verbose: False
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: dataset: cifar100
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: dataroot: data
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: batch_size: 128
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: total_epochs: 100
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: lr_decay_milestones: 60,80
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: lr_decay_gamma: 0.1
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: lr: 0.01
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: restore: run/cifar100/pretrain/cifar10_resnet56.pth
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: output_dir: run/cifar100/prune/cifar100-global-group_norm-2.0-resnet56
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: finetune: True
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: last_epochs: 100
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: reps: 1
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: method: group_norm
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: speed_up: 2.0
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: max_pruning_ratio: 1.0
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: soft_keeping_ratio: 0.0
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: reg: 1e-05
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: delta_reg: 0.0001
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: weight_decay: 0.0005
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: seed: 1
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: global_pruning: True
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: sl_total_epochs: 100
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: sl_lr: 0.01
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: sl_lr_decay_milestones: 60,80
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: sl_reg_warmup: 0
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: sl_restore: None
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: iterative_steps: 400
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: logger: <Logger cifar100-global-group_norm-2.0-resnet56 (DEBUG)>
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: device: cuda
[02/21 12:30:26] cifar100-global-group_norm-2.0-resnet56 INFO: num_classes: 100
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: mode: prune
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: model: resnet56
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: verbose: False
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: dataset: cifar100
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: dataroot: data
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: batch_size: 128
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: total_epochs: 100
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: lr_decay_milestones: 60,80
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: lr_decay_gamma: 0.1
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: lr: 0.01
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: restore: run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: output_dir: run/cifar100/prune/cifar100-global-group_norm-2.0-resnet56
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: finetune: True
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: last_epochs: 100
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: reps: 1
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: method: group_norm
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: speed_up: 2.0
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: max_pruning_ratio: 1.0
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: soft_keeping_ratio: 0.0
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: reg: 1e-05
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: delta_reg: 0.0001
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: weight_decay: 0.0005
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: seed: 1
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: global_pruning: True
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: sl_total_epochs: 100
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: sl_lr: 0.01
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: sl_lr_decay_milestones: 60,80
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: sl_reg_warmup: 0
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: sl_restore: None
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: iterative_steps: 400
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: logger: <Logger cifar100-global-group_norm-2.0-resnet56 (DEBUG)>
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: device: cuda
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: num_classes: 100
[02/21 18:57:44] cifar100-global-group_norm-2.0-resnet56 INFO: Loading model from run/cifar100/pretrain/cifar100_resnet56.pth
[02/21 18:57:48] cifar100-global-group_norm-2.0-resnet56 INFO: Pruning...
[02/21 18:57:58] cifar100-global-group_norm-2.0-resnet56 INFO: ResNet(
  (conv1): Conv2d(3, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(11, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(11, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(1, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(11, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(4, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(11, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(11, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(11, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(5, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(11, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(11, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(2, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(11, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(6, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(11, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(11, 25, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(25, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(11, 22, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(22, 19, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(19, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(19, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(22, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(17, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(22, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(20, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(22, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(22, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(24, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(22, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(10, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(22, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(14, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(22, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(56, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(22, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 51, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(51, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(51, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): BasicBlock(
      (conv1): Conv2d(64, 59, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(59, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(59, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): BasicBlock(
      (conv1): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): BasicBlock(
      (conv1): Conv2d(64, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(60, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): BasicBlock(
      (conv1): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): BasicBlock(
      (conv1): Conv2d(64, 62, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(62, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): BasicBlock(
      (conv1): Conv2d(64, 63, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(63, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(63, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (fc): Linear(in_features=64, out_features=100, bias=True)
)
[02/21 18:58:01] cifar100-global-group_norm-2.0-resnet56 INFO: Params: 0.86 M => 0.68 M (79.43%)
[02/21 18:58:01] cifar100-global-group_norm-2.0-resnet56 INFO: FLOPs: 127.12 M => 63.42 M (49.89%, 2.00X )
[02/21 18:58:01] cifar100-global-group_norm-2.0-resnet56 INFO: Acc: 0.7269 => 0.0168
[02/21 18:58:01] cifar100-global-group_norm-2.0-resnet56 INFO: Val Loss: 1.1578 => 14.3494
[02/21 18:58:01] cifar100-global-group_norm-2.0-resnet56 INFO: Finetuning...
[02/21 18:58:29] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 0/100, Acc=0.5970, Val Loss=1.6188, lr=0.0100
[02/21 18:58:57] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 1/100, Acc=0.6340, Val Loss=1.3881, lr=0.0100
[02/21 18:59:25] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 2/100, Acc=0.6383, Val Loss=1.3882, lr=0.0100
[02/21 18:59:53] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 3/100, Acc=0.6559, Val Loss=1.2951, lr=0.0100
[02/21 19:00:20] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 4/100, Acc=0.6516, Val Loss=1.3710, lr=0.0100
[02/21 19:00:48] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 5/100, Acc=0.6439, Val Loss=1.3531, lr=0.0100
[02/21 19:01:16] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 6/100, Acc=0.6554, Val Loss=1.3360, lr=0.0100
[02/21 19:01:44] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 7/100, Acc=0.6427, Val Loss=1.3941, lr=0.0100
[02/21 19:02:12] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 8/100, Acc=0.6525, Val Loss=1.3519, lr=0.0100
[02/21 19:02:39] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 9/100, Acc=0.6438, Val Loss=1.3890, lr=0.0100
[02/21 19:03:07] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 10/100, Acc=0.6296, Val Loss=1.4441, lr=0.0100
[02/21 19:03:35] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 11/100, Acc=0.6487, Val Loss=1.3812, lr=0.0100
[02/21 19:04:03] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 12/100, Acc=0.6556, Val Loss=1.3419, lr=0.0100
[02/21 19:04:31] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 13/100, Acc=0.6409, Val Loss=1.3950, lr=0.0100
[02/21 19:04:59] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 14/100, Acc=0.6628, Val Loss=1.3218, lr=0.0100
[02/21 19:05:27] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 15/100, Acc=0.6525, Val Loss=1.3630, lr=0.0100
[02/21 19:05:54] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 16/100, Acc=0.6444, Val Loss=1.3867, lr=0.0100
[02/21 19:06:22] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 17/100, Acc=0.6480, Val Loss=1.4242, lr=0.0100
[02/21 19:06:50] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 18/100, Acc=0.6456, Val Loss=1.4312, lr=0.0100
[02/21 19:07:18] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 19/100, Acc=0.6462, Val Loss=1.4478, lr=0.0100
[02/21 19:07:45] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 20/100, Acc=0.6526, Val Loss=1.3585, lr=0.0100
[02/21 19:08:13] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 21/100, Acc=0.6601, Val Loss=1.3419, lr=0.0100
[02/21 19:08:41] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 22/100, Acc=0.6589, Val Loss=1.3677, lr=0.0100
[02/21 19:09:09] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 23/100, Acc=0.6552, Val Loss=1.3554, lr=0.0100
[02/21 19:09:36] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 24/100, Acc=0.6425, Val Loss=1.4458, lr=0.0100
[02/21 19:10:04] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 25/100, Acc=0.6518, Val Loss=1.4206, lr=0.0100
[02/21 19:10:32] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 26/100, Acc=0.6611, Val Loss=1.3289, lr=0.0100
[02/21 19:11:00] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 27/100, Acc=0.6339, Val Loss=1.4999, lr=0.0100
[02/21 19:11:28] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 28/100, Acc=0.6535, Val Loss=1.3988, lr=0.0100
[02/21 19:11:56] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 29/100, Acc=0.6641, Val Loss=1.3780, lr=0.0100
[02/21 19:12:24] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 30/100, Acc=0.6412, Val Loss=1.4859, lr=0.0100
[02/21 19:12:51] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 31/100, Acc=0.6413, Val Loss=1.4526, lr=0.0100
[02/21 19:13:19] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 32/100, Acc=0.6540, Val Loss=1.4152, lr=0.0100
[02/21 19:13:47] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 33/100, Acc=0.6517, Val Loss=1.4306, lr=0.0100
[02/21 19:14:15] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 34/100, Acc=0.6456, Val Loss=1.4415, lr=0.0100
[02/21 19:14:43] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 35/100, Acc=0.6550, Val Loss=1.3707, lr=0.0100
[02/21 19:15:11] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 36/100, Acc=0.6452, Val Loss=1.4570, lr=0.0100
[02/21 19:15:39] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 37/100, Acc=0.6586, Val Loss=1.3741, lr=0.0100
[02/21 19:16:07] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 38/100, Acc=0.6560, Val Loss=1.3849, lr=0.0100
[02/21 19:16:35] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 39/100, Acc=0.6603, Val Loss=1.3828, lr=0.0100
[02/21 19:17:02] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 40/100, Acc=0.6597, Val Loss=1.3898, lr=0.0100
[02/21 19:17:30] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 41/100, Acc=0.6626, Val Loss=1.4092, lr=0.0100
[02/21 19:17:58] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 42/100, Acc=0.6423, Val Loss=1.4961, lr=0.0100
[02/21 19:18:25] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 43/100, Acc=0.6477, Val Loss=1.4444, lr=0.0100
[02/21 19:18:53] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 44/100, Acc=0.6652, Val Loss=1.3662, lr=0.0100
[02/21 19:19:21] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 45/100, Acc=0.6561, Val Loss=1.3875, lr=0.0100
[02/21 19:19:49] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 46/100, Acc=0.6573, Val Loss=1.4076, lr=0.0100
[02/21 19:20:16] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 47/100, Acc=0.6561, Val Loss=1.4165, lr=0.0100
[02/21 19:20:44] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 48/100, Acc=0.6706, Val Loss=1.3514, lr=0.0100
[02/21 19:21:12] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 49/100, Acc=0.6488, Val Loss=1.4527, lr=0.0100
[02/21 19:21:39] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 50/100, Acc=0.6461, Val Loss=1.4854, lr=0.0100
[02/21 19:22:07] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 51/100, Acc=0.6566, Val Loss=1.4102, lr=0.0100
[02/21 19:22:34] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 52/100, Acc=0.6496, Val Loss=1.4158, lr=0.0100
[02/21 19:23:01] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 53/100, Acc=0.6581, Val Loss=1.4100, lr=0.0100
[02/21 19:23:29] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 54/100, Acc=0.6509, Val Loss=1.4950, lr=0.0100
[02/21 19:23:56] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 55/100, Acc=0.6623, Val Loss=1.3560, lr=0.0100
[02/21 19:24:24] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 56/100, Acc=0.6565, Val Loss=1.3677, lr=0.0100
[02/21 19:24:51] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 57/100, Acc=0.6598, Val Loss=1.4093, lr=0.0100
[02/21 19:25:19] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 58/100, Acc=0.6500, Val Loss=1.4189, lr=0.0100
[02/21 19:25:46] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 59/100, Acc=0.6555, Val Loss=1.4012, lr=0.0100
[02/21 19:26:13] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 60/100, Acc=0.7137, Val Loss=1.1455, lr=0.0010
[02/21 19:26:41] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 61/100, Acc=0.7135, Val Loss=1.1522, lr=0.0010
[02/21 19:27:08] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 62/100, Acc=0.7133, Val Loss=1.1562, lr=0.0010
[02/21 19:27:35] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 63/100, Acc=0.7176, Val Loss=1.1616, lr=0.0010
[02/21 19:28:03] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 64/100, Acc=0.7164, Val Loss=1.1667, lr=0.0010
[02/21 19:28:31] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 65/100, Acc=0.7159, Val Loss=1.1714, lr=0.0010
[02/21 19:28:59] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 66/100, Acc=0.7129, Val Loss=1.1787, lr=0.0010
[02/21 19:29:26] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 67/100, Acc=0.7159, Val Loss=1.1868, lr=0.0010
[02/21 19:29:54] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 68/100, Acc=0.7153, Val Loss=1.1851, lr=0.0010
[02/21 19:30:22] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 69/100, Acc=0.7139, Val Loss=1.1969, lr=0.0010
[02/21 19:30:50] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 70/100, Acc=0.7165, Val Loss=1.1969, lr=0.0010
[02/21 19:31:17] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 71/100, Acc=0.7139, Val Loss=1.1993, lr=0.0010
[02/21 19:31:44] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 72/100, Acc=0.7140, Val Loss=1.2011, lr=0.0010
[02/21 19:32:12] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 73/100, Acc=0.7147, Val Loss=1.2147, lr=0.0010
[02/21 19:32:39] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 74/100, Acc=0.7138, Val Loss=1.2155, lr=0.0010
[02/21 19:33:06] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 75/100, Acc=0.7147, Val Loss=1.2174, lr=0.0010
[02/21 19:33:34] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 76/100, Acc=0.7129, Val Loss=1.2253, lr=0.0010
[02/21 19:34:02] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 77/100, Acc=0.7161, Val Loss=1.2236, lr=0.0010
[02/21 19:34:29] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 78/100, Acc=0.7144, Val Loss=1.2315, lr=0.0010
[02/21 19:34:57] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 79/100, Acc=0.7113, Val Loss=1.2368, lr=0.0010
[02/21 19:35:25] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 80/100, Acc=0.7137, Val Loss=1.2268, lr=0.0001
[02/21 19:35:52] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 81/100, Acc=0.7177, Val Loss=1.2283, lr=0.0001
[02/21 19:36:20] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 82/100, Acc=0.7159, Val Loss=1.2267, lr=0.0001
[02/21 19:36:48] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 83/100, Acc=0.7151, Val Loss=1.2305, lr=0.0001
[02/21 19:37:16] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 84/100, Acc=0.7168, Val Loss=1.2323, lr=0.0001
[02/21 19:37:44] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 85/100, Acc=0.7155, Val Loss=1.2273, lr=0.0001
[02/21 19:38:11] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 86/100, Acc=0.7153, Val Loss=1.2319, lr=0.0001
[02/21 19:38:39] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 87/100, Acc=0.7146, Val Loss=1.2346, lr=0.0001
[02/21 19:39:06] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 88/100, Acc=0.7146, Val Loss=1.2284, lr=0.0001
[02/21 19:39:34] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 89/100, Acc=0.7152, Val Loss=1.2305, lr=0.0001
[02/21 19:40:01] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 90/100, Acc=0.7159, Val Loss=1.2264, lr=0.0001
[02/21 19:40:29] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 91/100, Acc=0.7164, Val Loss=1.2388, lr=0.0001
[02/21 19:40:56] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 92/100, Acc=0.7158, Val Loss=1.2291, lr=0.0001
[02/21 19:41:24] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 93/100, Acc=0.7158, Val Loss=1.2320, lr=0.0001
[02/21 19:41:51] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 94/100, Acc=0.7162, Val Loss=1.2295, lr=0.0001
[02/21 19:42:19] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 95/100, Acc=0.7158, Val Loss=1.2339, lr=0.0001
[02/21 19:42:46] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 96/100, Acc=0.7144, Val Loss=1.2375, lr=0.0001
[02/21 19:43:14] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 97/100, Acc=0.7156, Val Loss=1.2330, lr=0.0001
[02/21 19:43:42] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 98/100, Acc=0.7141, Val Loss=1.2311, lr=0.0001
[02/21 19:44:09] cifar100-global-group_norm-2.0-resnet56 INFO: Epoch 99/100, Acc=0.7150, Val Loss=1.2334, lr=0.0001
[02/21 19:44:09] cifar100-global-group_norm-2.0-resnet56 INFO: Best Acc=0.7177
[02/21 19:44:09] cifar100-global-group_norm-2.0-resnet56 INFO: Params: 0.68 M
[02/21 19:44:09] cifar100-global-group_norm-2.0-resnet56 INFO: ops: 63.42 M
[02/21 19:44:12] cifar100-global-group_norm-2.0-resnet56 INFO: Acc: 0.7150 Val Loss: 1.2334

